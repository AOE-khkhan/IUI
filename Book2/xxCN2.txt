7.1.2 properties of audio digital audio (including digitized speech and music) has significantly lower bandwidth requirements than video. digital audio, however, has its own unique properties that must be considered when designing multimedia network applications. to understand these properties, let’s first consider how analog audio (which humans and musical instruments generate) is converted to a digital signal:
• each of the samples is then rounded to one of a finite number of values. this operation is referred to as quantization. the number of such finite values— called quantization values—is typically a power of two, for example, 256 quantization values.
• each of the quantization values is represented by a fixed number of bits. for example, if there are 256 quantization values, then each value—and hence each audio sample—is represented by one byte. the bit representations of all the samples are then concatenated together to form the digital representation of the signal. as an example, if an analog audio signal is sampled at 8,000 samples per second and each sample is quantized and represented by 8 bits, then the resulting digital signal will have a rate of 64,000 bits per second. for playback through audio speakers, the digital signal can then be converted back—that is, decoded— to an analog signal. however, the decoded analog signal is only an approximation of the original signal, and the sound quality may be noticeably degraded (for example, high-frequency sounds may be missing in the decoded signal). by increasing the sampling rate and the number of quantization values, the decoded signal can better approximate the original analog signal. thus (as with video), there is a trade-off between the quality of the decoded signal and the bit-rate and storage requirements of the digital signal.
the basic encoding technique that we just described is called pulse code modulation (pcm). speech encoding often uses pcm, with a sampling rate of 8,000 samples per second and 8 bits per sample, resulting in a rate of 64 kbps. the audio compact disk (cd) also uses pcm, with a sampling rate of 44,100 samples per second with 16 bits per sample; this gives a rate of 705.6 kbps for mono and 1.411 mbps for stereo.
pcm-encoded  speech  and  music,  however,  are  rarely  used  in  the  internet. instead, as with video, compression techniques are used to reduce the bit rates of the stream. human speech can be compressed to less than 10 kbps and still be intelligible. a popular compression technique for near cd-quality stereo music is mpeg 1 layer 3, more commonly known as mp3. mp3 encoders can compress to many different rates; 128 kbps is the most common encoding rate and produces very little sound degradation. a related standard is advanced audio coding (aac), which has been popularized by apple. as with video, multiple versions of a prerecorded audio stream can be created, each at a different bit rate.
• available bit rate (abr) atm network service. with the internet offering socalled best-effort service, atm’s abr might best be characterized as being a slightly-better-than-best-effort  service. as  with  the  internet  service  model, cells may be lost under abr service. unlike in the internet, however, cells cannot be reordered (although they may be lost), and a minimum cell transmission rate (mcr) is guaranteed to a connection using abr service. if the network has enough free resources at a given time, a sender may also be able to send cells successfully at a higher rate than the mcr. additionally, as we saw in section 3.6, atm abr service can provide feedback to the sender (in terms of a congestion notification bit, or an explicit rate at which to send) that controls how the sender adjusts its rate between the mcr and an allowable peak cell rate.
4.2 virtual circuit and datagram networks recall from chapter 3 that a transport layer can offer applications connectionless service or connection-oriented service between two processes. for example, the internet’s transport layer provides each application a choice between two services: udp, a connectionless service; or tcp, a connection-oriented service. in a similar manner, a network layer can provide connectionless service or connection service between two hosts. network-layer connection and connectionless services in many ways parallel transport-layer connection-oriented and connectionless services. for example, a network-layer connection service begins with handshaking between the source and destination  hosts;  and  a  network-layer  connectionless  service  does  not  have  any handshaking preliminaries.
although the network-layer connection and connectionless services have some parallels with transport-layer connection-oriented and connectionless services, there are crucial differences:
in the network layer, these services are host-to-host services provided by the network layer for the transport layer. in the transport layer these services are processto-process services provided by the transport layer for the application layer. in all major computer network architectures to date (internet, atm, frame relay, and so on), the network layer provides either a host-to-host connectionless service or a host-to-host connection service, but not both. computer networks that provide only a connection service at the network layer are called virtual-circuit (vc) networks; computer networks that provide only a connectionless service at the network layer are called datagram networks.
• the implementations of connection-oriented service in the transport layer and the connection service in the network layer are fundamentally different. we saw in the previous chapter that the transport-layer connection-oriented service is
3.6 principles of congestion control in the previous sections, we examined both the general principles and specific tcp mechanisms used to provide for a reliable data transfer service in the face of packet loss. we mentioned earlier that, in practice, such loss typically results from the  overflowing  of  router  buffers  as  the  network  becomes  congested.  packet retransmission thus treats a symptom of network congestion (the loss of a specific transport-layer segment) but does not treat the cause of network congestion—too many sources attempting to send data at too high a rate. to treat the cause of network congestion, mechanisms are needed to throttle senders in the face of network congestion.
in this section, we consider the problem of congestion control in a general context, seeking to understand why congestion is a bad thing, how network congestion is manifested in the performance received by upper-layer applications, and various approaches that can be taken to avoid, or react to, network congestion. this more general study of congestion control is appropriate since, as with reliable data transfer, it is high on our “top-ten” list of fundamentally important problems in networking.  we  conclude  this  section  with  a  discussion  of  congestion  control  in  the available bit-rate (abr) service  in asynchronous transfer mode (atm) networks. the following section contains a detailed study of tcp’s congestioncontrol algorithm.
3.6.1 the causes and the costs of congestion let’s begin our general study of congestion control by examining three increasingly complex scenarios in which congestion occurs. in each case, we’ll look at why congestion occurs in the first place and at the cost of congestion (in terms of resources not fully utilized and poor performance received by the end systems). we’ll not (yet) focus on how to react to, or avoid, congestion but rather focus on the simpler issue of understanding what happens as hosts increase their transmission rate and the network becomes congested.
scenario 1: two senders, a router with infinite buffers we begin by considering perhaps the simplest congestion scenario possible: two hosts (a and b) each have a connection that shares a single hop between source and destination, as shown in figure 3.43.
let’s assume that the application in host a is sending data into the connection (for example, passing data to the transport-level protocol via a socket) at an average rate of ␭in bytes/sec. these data are original in the sense that each unit of data is  sent  into  the  socket  only  once. the  underlying  transport-level  protocol  is  a
where the snmp version is determined. the pdu is then processed in the messageprocessing system, where the pdu is wrapped in a message header containing the snmp version number, a message id, and message size information. if encryption or authentication  is  needed,  the  appropriate  header  fields  for  this  information  are included as well; see [rfc 3411] for details. finally, the snmp message (the application-generated pdu plus the message header information) is passed to the appropriate transport protocol. the preferred transport protocol for carrying snmp messages is udp (that is, snmp messages are carried as the payload in a udp datagram), and the preferred port number for the snmp is port 161. port 162 is used for trap messages.
we have seen above that snmp messages are used not just to monitor, but also to control (for example, through the setrequest command) network elements. clearly, an intruder that could intercept snmp messages and/or generate its own snmp packets into the management infrastructure could wreak havoc in the network. thus, it is crucial that snmp messages be transmitted securely. surprisingly, it is only in the most recent version of snmp that security has received the attention that it deserves. snmpv3 security is known as user-based security [rfc 3414] in that there is the traditional concept of a user, identified by a username, with which security information such as a password, key value, or access privileges are associated. snmpv3 provides for encryption, authentication, protection against playback attacks (see section 8.3), and access control.
• encryption. snmp pdus can be encrypted using the data encryption standard (des) in cipher block chaining (cbc) mode. note that since des is a sharedkey system, the secret key of the user encrypting data must be known by the receiving entity that must decrypt the data.
• authentication. snmp uses the message authentication code (mac) technique that we studied in section 8.3.1 to provide both authentication and protection against  tampering  [rfc  4301].  recall  that  a  mac requires  the  sender  and receiver both to know a common secret key.
• protection against playback. recall from our discussion in chapter 8 that nonces can  be  used  to  guard  against  playback  attacks.  snmpv3  adopts  a  related approach. in order to ensure that a received message is not a replay of some earlier message, the receiver requires that the sender include a value in each message that is based on a counter in the receiver. this counter, which functions as a nonce, reflects the amount of time since the last reboot of the receiver’s network management software and the total number of reboots since the receiver’s network management software was last configured. as long as the counter in a received message is within some margin of error of the receiver’s actual value, the message is accepted as a nonreplay message, at which point it may be authenticated and/or decrypted. see [rfc 3414] for details.
• access control. snmpv3 provides a view-based access control [rfc 3415] that controls  which  network  management  information  can  be  queried  and/or  set  by which users. an snmp entity retains information about access rights and 
policies in a local configuration datastore (lcd). portions of the lcd are themselves accessible as managed objects, defined in the view-based access control model configuration mib [rfc 3415], and thus can be managed and manipulated remotely via snmp.
9.4 asn.1 in this book, we have covered a number of interesting topics in computer networking. this section on asn.1, however, may not make the top-ten list of interesting topics. like vegetables, knowledge about asn.1 and the broader issue of presentation services is something that is “good for you.” asn.1 is an iso-originated standard that is used in a number of internet-related protocols, particularly in the area of network management. for example, we saw in section 9.3 that mib variables in snmp were inextricably tied to asn.1. so while the material on asn.1 in this section may be rather dry, we hope the reader will take it on faith that the material is important.
in order to motivate our discussion here, consider the following thought experiment. suppose one could reliably copy data from one computer’s memory directly into a remote computer’s memory. if one could do this, would the communication problem be “solved?” the answer to the question depends on one’s definition of “the communication problem.” certainly, a perfect memory-to-memory copy would exactly communicate the bits and bytes from one machine to another. but does such an exact copy of the bits and bytes mean that when software running on the receiving computer accesses this data, it will see the same values that were stored into the sending computer’s memory? the answer to this question is “not necessarily!” the crux of the problem is that different computer architectures, different operating systems, and different compilers have different conventions for storing and representing data. if data is to be communicated and stored among multiple computers (as it is  in  every  communication  network),  this  problem  of  data  representation  must clearly be solved.
the left side of figure 9.6 shows a possible layout of this data on one hypothetical architecture: there is a single byte of memory containing the character a,
filtering can also be based on whether or not the tcp ack bit is set. this trick is quite useful if an organization wants to let its internal clients connect to external servers but wants to prevent external clients from connecting to internal servers. recall from section 3.5 that the first segment in every tcp connection has the ack bit set to 0, whereas all the other segments in the connection have the ack bit set to 1. thus, if an organization wants to prevent external clients from initiating connections to internal servers, it simply filters all incoming segments with the ack bit set to 0. this policy kills all tcp connections originating from the outside, but permits connections originating internally.
firewall rules are implemented in routers with access control lists, with each router interface having its own list. an example of an access control list for an organization 222.22/16 is shown in table 8.6. this access control list is for an interface that connects the router to the organization’s external isps. rules are applied to each datagram that passes through the interface from top to bottom. the first two rules together allow internal users to surf the web: the first rule allows any tcp packet with destination port 80 to leave the organization’s network; the second rule allows any tcp packet with source port 80 and the ack bit set to enter the organization’s network. note that if an external source attempts to establish a tcp connection with an internal host, the connection will be blocked, even if the source or destination port is 80. the second two rules together allow dns packets to enter and leave the organization’s network. in summary, this rather restrictive access control list blocks all traffic except web traffic initiated from within the organization  and  dns  traffic.  [cert filtering  2012]  provides  a  list  of  recommended port/protocol packet filterings to avoid a number of well-known security holes in existing network applications.
the packet. as a second example, suppose that an internal user wants to surf an external web site. because this user first sends a tcp syn segment, the user’s tcp connection gets recorded in the connection table. when the web server sends back packets (with the ack bit necessarily set), the firewall checks the table and sees that a corresponding connection is in progress. the firewall will thus let these packets pass, thereby not interfering with the internal user’s web surfing activity.
application gateway in the examples above, we have seen that packet-level filtering allows an organization  to  perform  coarse-grain  filtering  on  the  basis  of  the  contents  of  ip and tcp/udp headers, including ip addresses, port numbers, and acknowledgment bits. but what if an organization wants to provide a telnet service to a restricted set of internal users (as opposed to ip addresses)? and what if the organization wants such privileged users to authenticate themselves first before being allowed to create telnet sessions to the outside world? such tasks are beyond the capabilities of traditional and stateful filters. indeed, information about the identity of the internal users is application-layer data and is not included in the ip/tcp/udp headers.
to have finer-level security, firewalls must combine packet filters with application gateways. application gateways look beyond the ip/tcp/udp headers and make policy decisions based on application data. an application gateway is an application-specific server through which all application data (inbound and outbound) must pass. multiple application gateways can run on the same host, but each gateway is a separate server with its own processes.
the total response time—that is, the time from the browser’s request of an object until its receipt of the object—is the sum of the lan delay, the access delay (that is, the delay between the two routers), and the internet delay. let’s now do a very crude calculation to estimate this delay. the traffic intensity on the lan (see section 1.4.2) is
a traffic intensity of 0.15 on a lan typically results in, at most, tens of milliseconds of delay; hence, we can neglect the lan delay. however, as discussed in section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link in figure 2.12), the delay on a link becomes very large and grows without bound. thus, the average response time to satisfy requests is going to be on the order of minutes, if not more, which is unacceptable for the institution’s users. clearly something must be done.
one possible solution is to increase the access rate from 15 mbps to, say, 100 mbps. this will lower the traffic intensity on the access link to 0.15, which translates to negligible delays between the two routers. in this case, the total response time will roughly be two seconds, that is, the internet delay. but this solution also means that the institution must upgrade its access link from 15 mbps to 100 mbps, a costly proposition.
now  consider  the  alternative  solution  of  not  upgrading  the  access  link  but instead installing a web cache in the institutional network. this solution is illustrated  in  figure  2.13.  hit  rates—the  fraction  of  requests  that  are  satisfied  by  a cache—typically range from 0.2 to 0.7 in practice. for illustrative purposes, let’s suppose that the cache provides a hit rate of 0.4 for this institution. because the clients and the cache are connected to the same high-speed lan, 40 percent of the requests will be satisfied almost immediately, say, within 10 milliseconds, by the cache. nevertheless, the remaining 60 percent of the requests still need to be satisfied by the origin servers. but with only 60 percent of the requested objects passing through the access link, the traffic intensity on the access link is reduced from 1.0 to 0.6. typically, a traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds, on a 15 mbps link. this delay is negligible compared with the twosecond internet delay. given these considerations, average delay therefore is
which is just slightly greater than 1.2 seconds. thus, this second solution provides an even lower response time than the first solution, and it doesn’t require the institution to upgrade its link to the internet. the institution does, of course, have to purchase
1.3.3 a network of networks we saw earlier that end systems (pcs, smartphones, web servers, mail servers, and so on) connect into the internet via an access isp. the access isp can provide either wired or wireless connectivity, using an array of access technologies including dsl, cable, ftth, wi-fi, and cellular. note that the access isp does not have to be a telco or a cable company; instead it can be, for example, a university (providing internet access to students, staff, and faculty), or a company (providing  access  for  its  employees).  but  connecting  end  users  and  content providers into an access isp is only a small piece of solving the puzzle of connecting the billions of end systems that make up the internet. to complete this puzzle, the access isps themselves must be interconnected. this is done by creating a network of networks—understanding this phrase is the key to understanding the internet.
over the years, the network of networks that forms the internet has evolved into a  very  complex  structure.  much  of  this  evolution  is  driven  by  economics  and national policy, rather than by performance considerations. in order to understand today’s internet network structure, let’s incrementally build a series of network structures, with each new structure being a better approximation of the complex internet that we have today. recall that the overarching goal is to interconnect the access  isps  so  that  all  end  systems  can  send  packets  to  each  other.  one  naive approach would be to have each access isp directly connect with every other access isp. such a mesh design is, of course, much too costly for the access isps, as it would require each access isp to have a separate communication link to each of the hundreds of thousands of other access isps all over the world.
our  first  network  structure,  network  structure  1,  interconnects  all  of  the access isps with a single global transit isp. our (imaginary) global transit isp is a network of routers and communication links that not only spans the globe, but also has at least one router near each of the hundreds of thousands of access isps. of course, it would be very costly for the global isp to build such an extensive network. to be profitable, it would naturally charge each of the access isps for connectivity, with the pricing reflecting (but not necessarily directly proportional to) the amount of traffic an access isp exchanges with the global isp. since the access isp pays the global transit isp, the access isp is said to be a customer and the global transit isp is said to be a provider.
now if some company builds and operates a global transit isp that is profitable, then it is natural for other companies to build their own global transit isps and compete with the original global transit isp. this leads to network structure 2, which consists of the hundreds of thousands of access isps and multiple global transit isps. the access isps certainly prefer network structure 2 over network structure 1 since they can now choose among the competing global transit providers as a function of their pricing and services. note, however, that the global transit isps
themselves must interconnect: otherwise access isps connected to one of the global transit providers would not be able to communicate with access isps connected to the other global transit providers.
network structure 2, just described, is a two-tier hierarchy with global transit providers residing at the top tier and access isps at the bottom tier. this assumes that global transit isps are not only capable of getting close to each and every access isp, but also find it economically desirable to do so. in reality, although some isps do have impressive global coverage and do directly connect with many access isps, no isp has presence in each and every city in the world. instead, in any given region, there may be a regional isp to which the access isps in the region connect. each regional isp then connects to tier-1 isps. tier-1 isps are similar to our (imaginary) global transit isp; but tier-1 isps, which actually do exist, do not have a presence in every city in the world. there are approximately a dozen tier-1 isps, including level 3 communications, at&t, sprint, and ntt. interestingly, no group officially sanctions tier-1 status; as the saying goes—if you have to ask if you’re a member of a group, you’re probably not.
returning to this network of networks, not only are there multiple competing tier1 isps, there may be multiple competing regional isps in a region. in such a hierarchy, each access isp pays the regional isp to which it connects, and each regional isp pays the tier-1 isp to which it connects. (an access isp can also connect directly to a tier-1 isp, in which case it pays the tier-1 isp). thus, there is customer-provider relationship at each level of the hierarchy. note that the tier-1 isps do not pay anyone as they are at the top of the hierarchy. to further complicate matters, in some regions, there may be a larger regional isp (possibly spanning an entire country) to which the smaller regional isps in that region connect; the larger regional isp then connects to a tier-1 isp. for example, in china, there are access isps in each city, which connect to provincial isps, which in turn connect to national isps, which finally connect to tier-1 isps [tian 2012]. we refer to this multi-tier hierarchy, which is still only a crude approximation of today’s internet, as network structure 3.
to build a network that more closely resembles today’s internet, we must add points of presence (pops), multi-homing, peering, and internet exchange points (ixps) to the hierarchical network structure 3. pops exist in all levels of the hierarchy, except for the bottom (access isp) level. a pop is simply a group of one or more routers (at the same location) in the provider’s network where customer isps can  connect  into  the  provider  isp.  for  a  customer  network  to  connect  to  a provider’s pop, it can lease a high-speed link from a third-party telecommunications provider to directly connect one of its routers to a router at the pop. any isp (except for tier-1 isps) may choose to multi-home, that is, to connect to two or more provider isps. so, for example, an access isp may multi-home with two regional isps, or it may multi-home with two regional isps and also with a tier-1 isp. similarly, a regional isp may multi-home with multiple tier-1 isps. when an
1.2.1 access networks having considered the applications and end systems at the “edge of the network,” let’s next consider the access network—the network that physically connects an end system to the first router (also known as the “edge router”) on a path from the end system to any other distant end system. figure 1.4 shows several types of access
wide-area wireless access: 3g and lte increasingly, devices such as iphones, blackberrys, and android devices are being used to send email, surf the web, tweet, and download music while on the run. these devices employ the same wireless infrastructure used for cellular telephony to send/receive packets through a base station that is operated by the cellular network provider. unlike wifi, a user need only be within a few tens of kilometers (as opposed to a few tens of meters) of the base station.
telecommunications companies have made enormous investments in so-called third-generation (3g) wireless, which provides packet-switched wide-area wireless internet access at speeds in excess of 1 mbps. but even higher-speed wide-area access technologies—a fourth-generation (4g) of wide-area wireless networks—are already being deployed. lte ( for “long-term evolution”—a candidate for bad acronym of the year award) has its roots in 3g technology, and can potentially achieve rates in excess of 10 mbps. lte downstream rates of many tens of mbps have been reported in commercial deployments. we’ll cover the basic principles of wireless networks and mobility, as well as wifi, 3g, and lte technologies (and more!) in chapter 6.
1.2.2 physical media in the previous subsection, we gave an overview of some of the most important network access technologies in the internet. as we described these technologies, we also indicated the physical media used. for example, we said that hfc uses a combination of fiber cable and coaxial cable. we said that dsl and ethernet use copper wire. and we said that mobile access networks use the radio spectrum. 
section 1.3.1). on the customer side, a splitter separates the data and telephone signals arriving to the home and forwards the data signal to the dsl modem. on the telco side, in the co, the dslam separates the data and phone signals and sends the data into the internet. hundreds or even thousands of households connect to a single dslam [dischinger 2007].
the  dsl standards  define  transmission  rates  of  12  mbps  downstream  and 1.8 mbps upstream [itu 1999], and 24 mbps downstream and 2.5 mbps upstream [itu 2003]. because the downstream and upstream rates are different, the access is said to be asymmetric. the actual downstream and upstream transmission rates achieved may be less than the rates noted above, as the dsl provider may purposefully limit a residential rate when tiered service (different rates, available at different prices) are offered, or because the maximum rate can be limited by the distance between the home and the co, the gauge of the twisted-pair line and the degree of electrical interference. engineers have expressly designed dsl for short distances between the home and the co; generally, if the residence is not located within 5 to 10 miles of the co, the residence must resort to an alternative form of internet access.
while dsl makes use of the telco’s existing local telephone infrastructure, cable internet access makes use of the cable television company’s existing cable television infrastructure. a residence obtains cable internet access from the same company that provides its cable television. as illustrated in figure 1.6, fiber optics connect the cable head end to neighborhood-level junctions, from which traditional coaxial cable is then used to reach individual houses and apartments. each neighborhood junction typically supports 500 to 5,000 homes. because both fiber and coaxial cable are employed in this system, it is often referred to as hybrid fiber coax (hfc).
cable internet access requires special modems, called cable modems. as with a dsl modem, the cable modem is typically an external device and connects to the home pc through an ethernet port. (we will discuss ethernet in great detail in chapter 5.) at the cable head end, the cable modem termination system (cmts) serves a similar function as the dsl network’s dslam—turning the analog signal sent from the cable modems in many downstream homes back into digital format. cable modems divide the hfc network into two channels, a downstream and an upstream channel. as with dsl, access is typically asymmetric, with the downstream channel typically allocated a higher transmission rate than the upstream channel. the docsis 2.0 standard defines downstream rates up to 42.8 mbps and upstream rates of up to 30.7 mbps. as in the case of dsl networks, the maximum achievable rate may not be realized due to lower contracted data rates or media impairments.
one  important  characteristic  of  cable  internet  access  is  that  it  is  a  shared broadcast medium. in particular, every packet sent by the head end travels downstream on every link to every home and every packet sent by a home travels on the upstream channel to the head end. for this reason, if several users are simultaneously downloading a video file on the downstream channel, the actual rate at which each user receives its video file will be significantly lower than the aggregate cable downstream rate. on the other hand, if there are only a few active users and they are all web surfing, then each of the users may actually receive web pages at the full cable downstream rate, because the users will rarely request a web page at exactly the same time. because the upstream channel is also shared, a distributed multiple access protocol is needed to coordinate transmissions and avoid collisions. (we’ll discuss this collision issue in some detail in chapter 5.)
although dsl and cable networks currently represent more than 90 percent of residential broadband access in the united states, an up-and-coming technology that promises  even  higher  speeds  is  the  deployment  of  fiber to  the  home  (ftth) [ftth  council  2011a]. as  the  name  suggests,  the  ftth  concept  is  simple— provide an optical fiber path from the co directly to the home. in the united states, verizon has been particularly aggressive with ftth with its fios service [verizon fios 2012].
there are several competing technologies for optical distribution from the co to the homes. the simplest optical distribution network is called direct fiber, with one fiber leaving the co for each home. more commonly, each fiber leaving the central office is actually shared by many homes; it is not until the fiber gets relatively close to the homes that it is split into individual customer-specific fibers. there are two competing optical-distribution network architectures that perform this splitting: active optical networks (aons) and passive optical networks  (pons). aon  is  essentially  switched  ethernet,  which  is  discussed  in chapter 5.
here,  we  briefly  discuss  pon,  which  is  used  in  verizon’s  fios  service. figure 1.7 shows ftth using the pon distribution architecture. each home has
release the token, then some recovery procedure must be invoked to get the token back in circulation. over the years many token-passing protocols have been developed, including the fiber distributed data interface (fddi) protocol [jain 1994] and the ieee 802.5 token ring protocol [ieee 802.5 2012], and each one had to address these as well as other sticky issues.
in the previous three subsections, we’ve learned about three broad classes of multiple access protocols: channel partitioning protocols, random access protocols, and taking turns protocols. a cable access network will make for an excellent case study here, as we’ll find aspects of each of these three classes of multiple access protocols with the cable access network!
recall from section 1.2.1, that a cable access network typically connects several  thousand  residential  cable  modems  to  a  cable  modem  termination  system (cmts)  at  the  cable  network  headend. the  data-over-cable  service  interface specifications (docsis) [docsis 2011] specifies the cable data network architecture and its protocols. docsis uses fdm to divide the downstream (cmts to modem)  and  upstream  (modem  to  cmts)  network  segments  into  multiple  frequency  channels.  each  downstream  channel  is  6  mhz  wide,  with  a  maximum throughput of approximately 40 mbps per channel (although this data rate is seldom seen at a cable modem in practice); each upstream channel has a maximum channel width of 6.4 mhz, and a maximum upstream throughput of approximately 30 mbps. each upstream and downstream channel is a broadcast channel. frames transmitted on the downstream channel by the cmts are received by all cable modems receiving that channel; since there is just a single cmts transmitting into the downstream channel, however, there is no multiple access problem. the upstream direction, however,  is  more  interesting  and  technically  challenging,  since  multiple  cable modems share the same upstream channel (frequency) to the cmts, and thus collisions can potentially occur.
as illustrated in figure 5.14, each upstream channel is divided into intervals of time  (tdm-like),  each  containing  a  sequence  of  mini-slots  during  which  cable modems can transmit to the cmts. the cmts explicitly grants permission to individual cable modems to transmit during specific mini-slots. the cmts accomplishes  this  by  sending  a  control  message  known  as  a  map message  on  a downstream channel to specify which cable modem (with data to send) can transmit during which mini-slot for the interval of time specified in the control message. since mini-slots are explicitly allocated to cable modems, the cmts can ensure there are no colliding transmissions during a mini-slot.
but how does the cmts know which cable modems have data to send in the first place? this is accomplished by having cable modems send mini-slot-request frames to the cmts during a special set of interval mini-slots that are dedicated
for this purpose, as shown in figure 5.14. these mini-slot-request frames are transmitted in a random access manner and so may collide with each other. a cable modem can neither sense whether the upstream channel is busy nor detect collisions. instead, the cable modem infers that its mini-slot-request frame experienced a collision if it does not receive a response to the requested allocation in the next downstream control message. when a collision is inferred, a cable modem uses  binary  exponential  backoff  to  defer  the  retransmission  of  its  mini-slot -request frame to a future time slot. when there is little traffic on the upstream channel, a cable modem may actually transmit data frames during slots nominally assigned for mini-slot-request frames (and thus avoid having to wait for a mini-slot assignment).
a cable access network thus serves as a terrific example of multiple access protocols in action—fdm, tdm, random access, and centrally allocated time slots all within one network!
5.4 switched local area networks having covered broadcast networks and multiple access protocols in the previous section, let’s turn our attention next to switched local networks. figure 5.15 shows a switched local network connecting three departments, two servers and a router with four switches. because these switches operate at the link layer, they switch link-layer frames (rather than network-layer datagrams), don’t recognize
an optical network terminator (ont), which is connected by dedicated optical fiber to a neighborhood splitter. the splitter combines a number of homes (typically less than 100) onto a single, shared optical fiber, which connects to an optical line terminator (olt) in the telco’s co. the olt, providing conversion between optical and electrical signals, connects to the internet via a telco router. in the home, users connect a home router (typically a wireless router) to the ont and access the internet via this home router. in the pon architecture, all packets sent from olt to the splitter are replicated at the splitter (similar to a cable head end).
ftth can potentially provide internet access rates in the gigabits per second range. however, most ftth isps provide different rate offerings, with the higher rates naturally costing more money. the average downstream speed of us ftth customers was approximately 20 mbps in 2011 (compared with 13 mbps for cable access networks and less than 5 mbps for dsl) [ftth council 2011b].
two other access network technologies are also used to provide internet access to the home. in locations where dsl, cable, and ftth are not available (e.g., in some rural settings), a satellite link can be used to connect a residence to the internet at speeds of more than 1 mbps; starband and hughesnet are two such satellite access providers. dial-up access over traditional phone lines is based on the same model as dsl—a home modem connects over a phone line to a modem in the isp. compared with dsl and other broadband access networks, dial-up access is excruciatingly slow at 56 kbps.
access in the enterprise (and the home): ethernet and wifi on corporate and university campuses, and increasingly in home settings, a local area network (lan) is used to connect an end system to the edge router. although there are many types of lan technologies, ethernet is by far the most prevalent access technology in corporate, university, and home networks. as shown in figure 1.8,  ethernet  users  use  twisted-pair  copper  wire  to  connect  to  an  ethernet  switch,  a
home access: dsl, cable, ftth, dial-up, and satellite in developed countries today, more than 65 percent of the households have internet access, with korea, netherlands, finland, and sweden leading the way with more than 80 percent of households having internet access, almost all via a high-speed broadband connection [itu 2011]. finland and spain have recently declared high-speed internet access to be a “legal right.” given this intense interest in home access, let’s begin our overview of access networks by considering how homes connect to the internet.
today, the two most prevalent types of broadband residential access are digital subscriber line (dsl) and cable. a residence typically obtains dsl internet access from the same local telephone company (telco) that provides its wired local phone access. thus, when dsl is used, a customer’s telco is also its isp. as shown in figure 1.5, each customer’s dsl modem uses the existing telephone line (twistedpair copper wire, which we’ll discuss in section 1.2.2) to exchange data with a digital subscriber line access multiplexer (dslam) located in the telco’s local central office (co). the home’s dsl modem takes digital data and translates it to highfrequency tones for transmission over telephone wires to the co; the analog signals from many such houses are translated back into digital format at the dslam.
nals simultaneously, which are encoded at different frequencies: • a high-speed downstream channel, in the 50 khz to 1 mhz band • a medium-speed upstream channel, in the 4 khz to 50 khz band • an ordinary two-way telephone channel, in the 0 to 4 khz band
this approach makes the single dsl link appear as if there were three separate links, so that a telephone call and an internet connection can share the dsl link at the same time. (we’ll describe this technique of frequency-division multiplexing in
an optical network terminator (ont), which is connected by dedicated optical fiber to a neighborhood splitter. the splitter combines a number of homes (typically less than 100) onto a single, shared optical fiber, which connects to an optical line terminator (olt) in the telco’s co. the olt, providing conversion between optical and electrical signals, connects to the internet via a telco router. in the home, users connect a home router (typically a wireless router) to the ont and access the internet via this home router. in the pon architecture, all packets sent from olt to the splitter are replicated at the splitter (similar to a cable head end).
ftth can potentially provide internet access rates in the gigabits per second range. however, most ftth isps provide different rate offerings, with the higher rates naturally costing more money. the average downstream speed of us ftth customers was approximately 20 mbps in 2011 (compared with 13 mbps for cable access networks and less than 5 mbps for dsl) [ftth council 2011b].
two other access network technologies are also used to provide internet access to the home. in locations where dsl, cable, and ftth are not available (e.g., in some rural settings), a satellite link can be used to connect a residence to the internet at speeds of more than 1 mbps; starband and hughesnet are two such satellite access providers. dial-up access over traditional phone lines is based on the same model as dsl—a home modem connects over a phone line to a modem in the isp. compared with dsl and other broadband access networks, dial-up access is excruciatingly slow at 56 kbps.
access in the enterprise (and the home): ethernet and wifi on corporate and university campuses, and increasingly in home settings, a local area network (lan) is used to connect an end system to the edge router. although there are many types of lan technologies, ethernet is by far the most prevalent access technology in corporate, university, and home networks. as shown in figure 1.8,  ethernet  users  use  twisted-pair  copper  wire  to  connect  to  an  ethernet  switch,  a
figure 1.8  ethernet internet access technology discussed in detail in chapter 5. the ethernet switch, or a network of such interconnected switches, is then in turn connected into the larger internet. with ethernet  access,  users  typically  have  100  mbps  access  to  the  ethernet  switch, whereas servers may have 1 gbps or even 10 gbps access.
increasingly, however, people are accessing the internet wirelessly from laptops, smartphones, tablets, and other devices (see earlier sidebar on “a dizzying array of devices”). in a wireless lan setting, wireless users transmit/receive packets to/from an access point that is connected into the enterprise’s network (most likely including wired ethernet), which in turn is connected to the wired internet. a wireless lan user must typically be within a few tens of meters of the access point. wireless lan access based on ieee 802.11 technology, more colloquially known as wifi, is now just about everywhere—universities, business offices, cafes, airports, homes, and even in airplanes. in many cities, one can stand on a street corner and be within range of ten or twenty base stations (for a browseable global map of 802.11 base stations that have been discovered and logged on a web site by people who take great enjoyment in doing such things, see [wigle.net 2012]). as discussed in detail in chapter 6, 802.11 today provides a shared transmission rate of up to  54 mbps.
even though ethernet and wifi access networks were initially deployed in enterprise (corporate, university) settings, they have recently become relatively common components of home networks. many homes combine broadband residential access (that is, cable modems or dsl) with these inexpensive wireless lan technologies to create powerful home networks [edwards 2011]. figure 1.9 shows a typical home network. this home network consists of a roaming laptop as well as a wired pc; a base station (the wireless access point), which communicates with the wireless pc; a cable modem, providing broadband access to the internet; and a router, which interconnects the base station and the stationary pc with the cable modem. this network allows household members to have broadband access to the internet with one member roaming from the kitchen to the backyard to the bedrooms.
an optical network terminator (ont), which is connected by dedicated optical fiber to a neighborhood splitter. the splitter combines a number of homes (typically less than 100) onto a single, shared optical fiber, which connects to an optical line terminator (olt) in the telco’s co. the olt, providing conversion between optical and electrical signals, connects to the internet via a telco router. in the home, users connect a home router (typically a wireless router) to the ont and access the internet via this home router. in the pon architecture, all packets sent from olt to the splitter are replicated at the splitter (similar to a cable head end).
ftth can potentially provide internet access rates in the gigabits per second range. however, most ftth isps provide different rate offerings, with the higher rates naturally costing more money. the average downstream speed of us ftth customers was approximately 20 mbps in 2011 (compared with 13 mbps for cable access networks and less than 5 mbps for dsl) [ftth council 2011b].
two other access network technologies are also used to provide internet access to the home. in locations where dsl, cable, and ftth are not available (e.g., in some rural settings), a satellite link can be used to connect a residence to the internet at speeds of more than 1 mbps; starband and hughesnet are two such satellite access providers. dial-up access over traditional phone lines is based on the same model as dsl—a home modem connects over a phone line to a modem in the isp. compared with dsl and other broadband access networks, dial-up access is excruciatingly slow at 56 kbps.
access in the enterprise (and the home): ethernet and wifi on corporate and university campuses, and increasingly in home settings, a local area network (lan) is used to connect an end system to the edge router. although there are many types of lan technologies, ethernet is by far the most prevalent access technology in corporate, university, and home networks. as shown in figure 1.8,  ethernet  users  use  twisted-pair  copper  wire  to  connect  to  an  ethernet  switch,  a
figure 1.8  ethernet internet access technology discussed in detail in chapter 5. the ethernet switch, or a network of such interconnected switches, is then in turn connected into the larger internet. with ethernet  access,  users  typically  have  100  mbps  access  to  the  ethernet  switch, whereas servers may have 1 gbps or even 10 gbps access.
increasingly, however, people are accessing the internet wirelessly from laptops, smartphones, tablets, and other devices (see earlier sidebar on “a dizzying array of devices”). in a wireless lan setting, wireless users transmit/receive packets to/from an access point that is connected into the enterprise’s network (most likely including wired ethernet), which in turn is connected to the wired internet. a wireless lan user must typically be within a few tens of meters of the access point. wireless lan access based on ieee 802.11 technology, more colloquially known as wifi, is now just about everywhere—universities, business offices, cafes, airports, homes, and even in airplanes. in many cities, one can stand on a street corner and be within range of ten or twenty base stations (for a browseable global map of 802.11 base stations that have been discovered and logged on a web site by people who take great enjoyment in doing such things, see [wigle.net 2012]). as discussed in detail in chapter 6, 802.11 today provides a shared transmission rate of up to  54 mbps.
even though ethernet and wifi access networks were initially deployed in enterprise (corporate, university) settings, they have recently become relatively common components of home networks. many homes combine broadband residential access (that is, cable modems or dsl) with these inexpensive wireless lan technologies to create powerful home networks [edwards 2011]. figure 1.9 shows a typical home network. this home network consists of a roaming laptop as well as a wired pc; a base station (the wireless access point), which communicates with the wireless pc; a cable modem, providing broadband access to the internet; and a router, which interconnects the base station and the stationary pc with the cable modem. this network allows household members to have broadband access to the internet with one member roaming from the kitchen to the backyard to the bedrooms.
cable internet access requires special modems, called cable modems. as with a dsl modem, the cable modem is typically an external device and connects to the home pc through an ethernet port. (we will discuss ethernet in great detail in chapter 5.) at the cable head end, the cable modem termination system (cmts) serves a similar function as the dsl network’s dslam—turning the analog signal sent from the cable modems in many downstream homes back into digital format. cable modems divide the hfc network into two channels, a downstream and an upstream channel. as with dsl, access is typically asymmetric, with the downstream channel typically allocated a higher transmission rate than the upstream channel. the docsis 2.0 standard defines downstream rates up to 42.8 mbps and upstream rates of up to 30.7 mbps. as in the case of dsl networks, the maximum achievable rate may not be realized due to lower contracted data rates or media impairments.
one  important  characteristic  of  cable  internet  access  is  that  it  is  a  shared broadcast medium. in particular, every packet sent by the head end travels downstream on every link to every home and every packet sent by a home travels on the upstream channel to the head end. for this reason, if several users are simultaneously downloading a video file on the downstream channel, the actual rate at which each user receives its video file will be significantly lower than the aggregate cable downstream rate. on the other hand, if there are only a few active users and they are all web surfing, then each of the users may actually receive web pages at the full cable downstream rate, because the users will rarely request a web page at exactly the same time. because the upstream channel is also shared, a distributed multiple access protocol is needed to coordinate transmissions and avoid collisions. (we’ll discuss this collision issue in some detail in chapter 5.)
although dsl and cable networks currently represent more than 90 percent of residential broadband access in the united states, an up-and-coming technology that promises  even  higher  speeds  is  the  deployment  of  fiber to  the  home  (ftth) [ftth  council  2011a]. as  the  name  suggests,  the  ftth  concept  is  simple— provide an optical fiber path from the co directly to the home. in the united states, verizon has been particularly aggressive with ftth with its fios service [verizon fios 2012].
there are several competing technologies for optical distribution from the co to the homes. the simplest optical distribution network is called direct fiber, with one fiber leaving the co for each home. more commonly, each fiber leaving the central office is actually shared by many homes; it is not until the fiber gets relatively close to the homes that it is split into individual customer-specific fibers. there are two competing optical-distribution network architectures that perform this splitting: active optical networks (aons) and passive optical networks  (pons). aon  is  essentially  switched  ethernet,  which  is  discussed  in chapter 5.
here,  we  briefly  discuss  pon,  which  is  used  in  verizon’s  fios  service. figure 1.7 shows ftth using the pon distribution architecture. each home has
home access: dsl, cable, ftth, dial-up, and satellite in developed countries today, more than 65 percent of the households have internet access, with korea, netherlands, finland, and sweden leading the way with more than 80 percent of households having internet access, almost all via a high-speed broadband connection [itu 2011]. finland and spain have recently declared high-speed internet access to be a “legal right.” given this intense interest in home access, let’s begin our overview of access networks by considering how homes connect to the internet.
today, the two most prevalent types of broadband residential access are digital subscriber line (dsl) and cable. a residence typically obtains dsl internet access from the same local telephone company (telco) that provides its wired local phone access. thus, when dsl is used, a customer’s telco is also its isp. as shown in figure 1.5, each customer’s dsl modem uses the existing telephone line (twistedpair copper wire, which we’ll discuss in section 1.2.2) to exchange data with a digital subscriber line access multiplexer (dslam) located in the telco’s local central office (co). the home’s dsl modem takes digital data and translates it to highfrequency tones for transmission over telephone wires to the co; the analog signals from many such houses are translated back into digital format at the dslam.
nals simultaneously, which are encoded at different frequencies: • a high-speed downstream channel, in the 50 khz to 1 mhz band • a medium-speed upstream channel, in the 4 khz to 50 khz band • an ordinary two-way telephone channel, in the 0 to 4 khz band
this approach makes the single dsl link appear as if there were three separate links, so that a telephone call and an internet connection can share the dsl link at the same time. (we’ll describe this technique of frequency-division multiplexing in
nearly  850  million  end  systems  attached  to  the  internet  [isc  2012],  not  counting smartphones, laptops, and other devices that are only intermittently connected to the internet. overall, more there are an estimated 2 billion internet users [itu 2011].
end systems are connected together by a network of communication links and packet switches. we’ll see in section 1.2 that there are many types of communication links, which are made up of different types of physical media, including coaxial cable, copper wire, optical fiber, and radio spectrum. different links can transmit data at different rates, with the transmission rate of a link measured in bits/second. when one end system has data to send to another end system, the sending end system segments the data and adds header bytes to each segment. the resulting packages of information, known as packets in the jargon of computer networks, are then sent through the network to the destination end system, where they are reassembled into the original data.
a packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links. packet switches come in many shapes and flavors, but the two most prominent types in today’s internet are routers and link-layer switches. both types of switches forward packets toward their ultimate destinations. link-layer switches are typically used in access networks, while routers are typically used in the network core. the sequence of communication links and packet switches traversed by a packet from the sending end system to the receiving end system is known as a route or path through the network. the exact amount of traffic being carried in the internet is difficult to estimate but cisco [cisco vni 2011] estimates global internet traffic will be nearly 40 exabytes per month in 2012.
packet-switched networks (which transport packets) are in many ways similar to transportation networks of highways, roads, and intersections (which transport  vehicles).  consider,  for  example,  a  factory  that  needs  to  move  a  large amount of cargo to some destination warehouse located thousands of kilometers away. at the factory, the cargo is segmented and loaded into a fleet of trucks. each of the trucks then independently travels through the network of highways, roads, and intersections to the destination warehouse. at the destination warehouse, the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment. thus, in many ways, packets are analogous to trucks, communication links are analogous to highways and roads, packet switches are analogous to intersections, and end systems are analogous to buildings. just as a truck takes a path through the transportation network, a packet takes a path through a computer network.
end systems access the internet through internet service providers (isps), including residential isps such as local cable or telephone companies; corporate isps; university isps; and isps that provide wifi access in airports, hotels, coffee shops, and other public places. each isp is in itself a network of packet switches and communication links. isps provide a variety of types of network access to the end systems, including residential broadband access such as cable modem or dsl,
wide-area wireless access: 3g and lte increasingly, devices such as iphones, blackberrys, and android devices are being used to send email, surf the web, tweet, and download music while on the run. these devices employ the same wireless infrastructure used for cellular telephony to send/receive packets through a base station that is operated by the cellular network provider. unlike wifi, a user need only be within a few tens of kilometers (as opposed to a few tens of meters) of the base station.
telecommunications companies have made enormous investments in so-called third-generation (3g) wireless, which provides packet-switched wide-area wireless internet access at speeds in excess of 1 mbps. but even higher-speed wide-area access technologies—a fourth-generation (4g) of wide-area wireless networks—are already being deployed. lte ( for “long-term evolution”—a candidate for bad acronym of the year award) has its roots in 3g technology, and can potentially achieve rates in excess of 10 mbps. lte downstream rates of many tens of mbps have been reported in commercial deployments. we’ll cover the basic principles of wireless networks and mobility, as well as wifi, 3g, and lte technologies (and more!) in chapter 6.
1.2.2 physical media in the previous subsection, we gave an overview of some of the most important network access technologies in the internet. as we described these technologies, we also indicated the physical media used. for example, we said that hfc uses a combination of fiber cable and coaxial cable. we said that dsl and ethernet use copper wire. and we said that mobile access networks use the radio spectrum. 
having discussed how 802.11 uses link-layer acknowledgments, we’re now in a position to describe the 802.11 csma/ca protocol. suppose that a station (wireless station or an ap) has a frame to transmit.
2. otherwise, the station chooses a random backoff value using binary exponential backoff (as we encountered in  section 5.3.2) and counts down this value when the channel is sensed idle. while the channel is sensed busy, the counter value remains frozen.
cable internet access requires special modems, called cable modems. as with a dsl modem, the cable modem is typically an external device and connects to the home pc through an ethernet port. (we will discuss ethernet in great detail in chapter 5.) at the cable head end, the cable modem termination system (cmts) serves a similar function as the dsl network’s dslam—turning the analog signal sent from the cable modems in many downstream homes back into digital format. cable modems divide the hfc network into two channels, a downstream and an upstream channel. as with dsl, access is typically asymmetric, with the downstream channel typically allocated a higher transmission rate than the upstream channel. the docsis 2.0 standard defines downstream rates up to 42.8 mbps and upstream rates of up to 30.7 mbps. as in the case of dsl networks, the maximum achievable rate may not be realized due to lower contracted data rates or media impairments.
one  important  characteristic  of  cable  internet  access  is  that  it  is  a  shared broadcast medium. in particular, every packet sent by the head end travels downstream on every link to every home and every packet sent by a home travels on the upstream channel to the head end. for this reason, if several users are simultaneously downloading a video file on the downstream channel, the actual rate at which each user receives its video file will be significantly lower than the aggregate cable downstream rate. on the other hand, if there are only a few active users and they are all web surfing, then each of the users may actually receive web pages at the full cable downstream rate, because the users will rarely request a web page at exactly the same time. because the upstream channel is also shared, a distributed multiple access protocol is needed to coordinate transmissions and avoid collisions. (we’ll discuss this collision issue in some detail in chapter 5.)
although dsl and cable networks currently represent more than 90 percent of residential broadband access in the united states, an up-and-coming technology that promises  even  higher  speeds  is  the  deployment  of  fiber to  the  home  (ftth) [ftth  council  2011a]. as  the  name  suggests,  the  ftth  concept  is  simple— provide an optical fiber path from the co directly to the home. in the united states, verizon has been particularly aggressive with ftth with its fios service [verizon fios 2012].
there are several competing technologies for optical distribution from the co to the homes. the simplest optical distribution network is called direct fiber, with one fiber leaving the co for each home. more commonly, each fiber leaving the central office is actually shared by many homes; it is not until the fiber gets relatively close to the homes that it is split into individual customer-specific fibers. there are two competing optical-distribution network architectures that perform this splitting: active optical networks (aons) and passive optical networks  (pons). aon  is  essentially  switched  ethernet,  which  is  discussed  in chapter 5.
here,  we  briefly  discuss  pon,  which  is  used  in  verizon’s  fios  service. figure 1.7 shows ftth using the pon distribution architecture. each home has
an optical network terminator (ont), which is connected by dedicated optical fiber to a neighborhood splitter. the splitter combines a number of homes (typically less than 100) onto a single, shared optical fiber, which connects to an optical line terminator (olt) in the telco’s co. the olt, providing conversion between optical and electrical signals, connects to the internet via a telco router. in the home, users connect a home router (typically a wireless router) to the ont and access the internet via this home router. in the pon architecture, all packets sent from olt to the splitter are replicated at the splitter (similar to a cable head end).
ftth can potentially provide internet access rates in the gigabits per second range. however, most ftth isps provide different rate offerings, with the higher rates naturally costing more money. the average downstream speed of us ftth customers was approximately 20 mbps in 2011 (compared with 13 mbps for cable access networks and less than 5 mbps for dsl) [ftth council 2011b].
two other access network technologies are also used to provide internet access to the home. in locations where dsl, cable, and ftth are not available (e.g., in some rural settings), a satellite link can be used to connect a residence to the internet at speeds of more than 1 mbps; starband and hughesnet are two such satellite access providers. dial-up access over traditional phone lines is based on the same model as dsl—a home modem connects over a phone line to a modem in the isp. compared with dsl and other broadband access networks, dial-up access is excruciatingly slow at 56 kbps.
access in the enterprise (and the home): ethernet and wifi on corporate and university campuses, and increasingly in home settings, a local area network (lan) is used to connect an end system to the edge router. although there are many types of lan technologies, ethernet is by far the most prevalent access technology in corporate, university, and home networks. as shown in figure 1.8,  ethernet  users  use  twisted-pair  copper  wire  to  connect  to  an  ethernet  switch,  a
wide-area wireless access: 3g and lte increasingly, devices such as iphones, blackberrys, and android devices are being used to send email, surf the web, tweet, and download music while on the run. these devices employ the same wireless infrastructure used for cellular telephony to send/receive packets through a base station that is operated by the cellular network provider. unlike wifi, a user need only be within a few tens of kilometers (as opposed to a few tens of meters) of the base station.
telecommunications companies have made enormous investments in so-called third-generation (3g) wireless, which provides packet-switched wide-area wireless internet access at speeds in excess of 1 mbps. but even higher-speed wide-area access technologies—a fourth-generation (4g) of wide-area wireless networks—are already being deployed. lte ( for “long-term evolution”—a candidate for bad acronym of the year award) has its roots in 3g technology, and can potentially achieve rates in excess of 10 mbps. lte downstream rates of many tens of mbps have been reported in commercial deployments. we’ll cover the basic principles of wireless networks and mobility, as well as wifi, 3g, and lte technologies (and more!) in chapter 6.
1.2.2 physical media in the previous subsection, we gave an overview of some of the most important network access technologies in the internet. as we described these technologies, we also indicated the physical media used. for example, we said that hfc uses a combination of fiber cable and coaxial cable. we said that dsl and ethernet use copper wire. and we said that mobile access networks use the radio spectrum. 
mobility,  allowing  users  to  maintain  their  tcp sessions  while  traveling,  for example, on a bus or a train. with sufficiently high upstream and downstream bit rates, the user could even maintain video-conferencing sessions while roaming about. this scenario is not that far-fetched. as of 2012, many cellular telephony providers in the u.s. offer their subscribers a cellular internet access service for under $50 per month with typical downstream and upstream bit rates in the hundreds  of  kilobits  per  second.  data  rates  of  several  megabits  per  second  are becoming available as broadband data services such as those we will cover here become more widely deployed.
in this section, we provide a brief overview of current and emerging cellular internet access technologies. our focus here will be on both the wireless first hop as well as the network that connects the wireless first hop into the larger telephone network and/or the internet; in section 6.7 we’ll consider how calls are routed to a user moving between base stations. our brief discussion will necessarily provide only a simplified and high-level description of cellular technologies. modern cellular communications, of course, has great breadth and depth, with many universities offering several courses on the topic. readers seeking a deeper understanding are encouraged to see [goodman 1997; kaaranen 2001; lin 2001; korhonen 2003; schiller 2003; scourias 2012; turner 2012; akyildiz 2010], as well as the particularly excellent and exhaustive reference [mouly 1992].
6.4.1 an overview of cellular network architecture in our description of cellular network architecture in this section, we’ll adopt the terminology of the global system for mobile communications (gsm) standards. (for history buffs, the gsm acronym was originally derived from groupe spécial mobile, until the more anglicized name was adopted, preserving the original acronym letters.) in the 1980s, europeans recognized the need for a pan-european digital cellular telephony system that would replace the numerous incompatible analog cellular telephony systems, leading to the gsm standard [mouly 1992]. europeans deployed gsm technology with great success in the early 1990s, and since then gsm has grown to be the 800-pound gorilla of the cellular telephone world, with more than 80% of all cellular subscribers worldwide using gsm.
when people talk about cellular technology, they often classify the technology as belonging to one of several “generations.” the earliest generations were designed primarily for voice traffic. first generation (1g) systems were analog fdma systems designed exclusively for voice-only communication. these 1g systems are almost extinct now, having been replaced by digital 2g systems. the original 2g systems were also designed for voice, but later extended (2.5g) to support data (i.e., internet) as well as voice service. the 3g systems that currently are being deployed also support voice and data, but with an ever increasing emphasis on data capabilities and higher-speed radio access links.
the particular allocation of time slots to mobile nodes is not mandated by the lte standard. instead, the decision of which mobile nodes will be allowed to transmit in a given time slot on a given frequency is determined by the scheduling algorithms provided by the lte equipment vendor and/or the network operator. with opportunistic scheduling [bender 2000; kolding 2003; kulkarni 2005], matching  the  physical-layer  protocol  to  the  channel  conditions  between  the sender and receiver and choosing the receivers to which packets will be sent based on channel conditions allow the radio network controller to make best use of the wireless medium. in addition, user priorities and contracted levels of service (e.g., silver, gold, or platinum) can be used in scheduling downstream packet transmissions. in addition to the lte capabilities described above, lte-advanced allows for downstream bandwidths of hundreds of mbps by allocating aggregated channels to a mobile node [akyildiz 2010].
an additional 4g wireless technology—wimax (world interoperability for microwave access)—is a family of ieee 802.16 standards that differ significantly from lte. whether lte or wimax becomes the 4g technology of choice is still to be seen, but at the time of this writing (spring 2012), lte appears to have significantly more momentum. a detailed discussion of wimax can be found on this book’s web site.
wide-area wireless access: 3g and lte increasingly, devices such as iphones, blackberrys, and android devices are being used to send email, surf the web, tweet, and download music while on the run. these devices employ the same wireless infrastructure used for cellular telephony to send/receive packets through a base station that is operated by the cellular network provider. unlike wifi, a user need only be within a few tens of kilometers (as opposed to a few tens of meters) of the base station.
telecommunications companies have made enormous investments in so-called third-generation (3g) wireless, which provides packet-switched wide-area wireless internet access at speeds in excess of 1 mbps. but even higher-speed wide-area access technologies—a fourth-generation (4g) of wide-area wireless networks—are already being deployed. lte ( for “long-term evolution”—a candidate for bad acronym of the year award) has its roots in 3g technology, and can potentially achieve rates in excess of 10 mbps. lte downstream rates of many tens of mbps have been reported in commercial deployments. we’ll cover the basic principles of wireless networks and mobility, as well as wifi, 3g, and lte technologies (and more!) in chapter 6.
1.2.2 physical media in the previous subsection, we gave an overview of some of the most important network access technologies in the internet. as we described these technologies, we also indicated the physical media used. for example, we said that hfc uses a combination of fiber cable and coaxial cable. we said that dsl and ethernet use copper wire. and we said that mobile access networks use the radio spectrum. 
figure 1.8  ethernet internet access technology discussed in detail in chapter 5. the ethernet switch, or a network of such interconnected switches, is then in turn connected into the larger internet. with ethernet  access,  users  typically  have  100  mbps  access  to  the  ethernet  switch, whereas servers may have 1 gbps or even 10 gbps access.
increasingly, however, people are accessing the internet wirelessly from laptops, smartphones, tablets, and other devices (see earlier sidebar on “a dizzying array of devices”). in a wireless lan setting, wireless users transmit/receive packets to/from an access point that is connected into the enterprise’s network (most likely including wired ethernet), which in turn is connected to the wired internet. a wireless lan user must typically be within a few tens of meters of the access point. wireless lan access based on ieee 802.11 technology, more colloquially known as wifi, is now just about everywhere—universities, business offices, cafes, airports, homes, and even in airplanes. in many cities, one can stand on a street corner and be within range of ten or twenty base stations (for a browseable global map of 802.11 base stations that have been discovered and logged on a web site by people who take great enjoyment in doing such things, see [wigle.net 2012]). as discussed in detail in chapter 6, 802.11 today provides a shared transmission rate of up to  54 mbps.
even though ethernet and wifi access networks were initially deployed in enterprise (corporate, university) settings, they have recently become relatively common components of home networks. many homes combine broadband residential access (that is, cable modems or dsl) with these inexpensive wireless lan technologies to create powerful home networks [edwards 2011]. figure 1.9 shows a typical home network. this home network consists of a roaming laptop as well as a wired pc; a base station (the wireless access point), which communicates with the wireless pc; a cable modem, providing broadband access to the internet; and a router, which interconnects the base station and the stationary pc with the cable modem. this network allows household members to have broadband access to the internet with one member roaming from the kitchen to the backyard to the bedrooms.
load balancers, each one devoted to a set of specific cloud applications. such a load balancer is sometimes referred to as a “layer-4 switch” since it makes decisions based on the destination port number (layer 4) as well as destination ip address in the packet. upon receiving a request for a particular application, the load balancer forwards it to one of the hosts that handles the application. (a host may then invoke the services of other hosts to help process the request.) when the host finishes processing the request, it sends its response back to the load balancer, which in turn relays the response back to the external client. the load balancer not only balances the work load across hosts, but also provides a nat-like function, translating the public external ip address to the internal ip address of the appropriate host, and then translating back for packets traveling in the reverse direction back to the clients. this prevents clients from contacting hosts directly, which has the security benefit of hiding the internal network structure and preventing clients from directly interacting with the hosts.
hierarchical architecture for a small data center housing only a few thousand hosts, a simple network consisting of a border router, a load balancer, and a few tens of racks all interconnected by a single ethernet switch could possibly suffice. but to scale to tens to hundreds of thousands of hosts, a data center often employs a hierarchy of routers and switches, such as the topology shown in figure 5.30. at the top of the hierarchy, the border router connects to access routers (only two are shown in figure 5.30, but there can be many more). below each access router there are three tiers of switches. each access router connects to a top-tier switch, and each top-tier switch connects to multiple second-tier switches and a load balancer. each second-tier switch in turn connects to multiple racks via the racks’ tor switches (third-tier switches). all links typically use ethernet for their link-layer and physical-layer protocols, with a mix of copper and fiber cabling. with such a hierarchical design, it is possible to scale a data center to hundreds of thousands of hosts.
because it is critical for a cloud application provider to continually provide applications with high availability, data centers also include redundant network equipment  and  redundant  links  in  their  designs  (not  shown  in  figure  5.30).  for  example,  each tor  switch  can  connect  to  two  tier-2  switches,  and  each access router, tier-1 switch, and tier-2 switch can be duplicated and integrated into the design [cisco 2012; greenberg 2009b]. in the hierarchical design in  figure 5.30, observe that the hosts below each access router form a single subnet. in order to localize arp broadcast traffic, each of these subnets is further partitioned into smaller vlan subnets, each comprising a few hundred hosts [greenberg 2009a].
although the conventional hierarchical architecture just described solves the problem of scale, it suffers from limited host-to-host capacity [greenberg 2009b]. to understand this limitation, consider again figure 5.30, and suppose each host connects
the longer-term view of providing acceptable levels of performance in the face of varying traffic demands and occasional network device failures. as with performance  management,  the  snmp protocol  plays  a  central  role  in  fault management.
• configuration management. configuration management allows a network manager to track which devices are on the managed network and the hardware and software configurations of these devices. an overview of configuration management and requirements for ip-based networks can be found in [rfc 3139].
• accounting management. accounting management allows the network manager to specify, log, and control user and device access to network resources. usage quotas, usage-based charging, and the allocation of resource-access privileges all fall under accounting management.
• security management. the goal of security management is to control access to network resources according to some well-defined policy. the key distribution centers that we studied in section 8.3 are components of security management. the use of firewalls to monitor and control external access points to one’s network, a topic we studied in section 8.9, is another crucial component.
in this chapter, we’ll cover only the rudiments of network management. our focus will be purposefully narrow—we’ll examine only the infrastructure for network management—the overall architecture, network management protocols, and information base through which a network administrator keeps the network up and running. we’ll not cover the decision-making processes of the network administrator, who must plan, analyze, and respond to the management information that is conveyed to the noc. in this area, topics such as fault identification and management [katzela 1995; medhi 1997; labovitz 1997; steinder 2002; feamster  2005; wu  2005; teixeira  2006],  anomaly  detection  [lakhina  2004; lakhina 2005; barford 2009], and more come into consideration. nor will we cover the broader topic of service management [saydam 1996; rfc 3052]—the provisioning of resources such as bandwidth, server capacity, and the other computational/communication resources needed to meet the mission-specific service requirements of an enterprise.
an often-asked question is “what is network management?” our discussion above has motivated the need for, and illustrated a few of the uses of, network management. we’ll conclude this section with a single-sentence (albeit a rather long runon sentence) definition of network management from [saydam 1996]:
“network management includes the deployment, integration, and coordination of the hardware, software, and human elements to monitor, test, poll, configure, analyze, evaluate, and control the network and element resources to meet the real-time, operational performance, and quality of service requirements at a reasonable cost.”
how the network/systems and applications (collectively referred to as the ecosystem) are performing with respect to pre-defined measures specific to time of day, day of week, or special events (e.g., storm surges or pay events, such as a boxing match). these predefined performance measures exist throughout the service path, from the customer’s residence or business through the entire network, as well as the interface points to partners and peers. in addition, synthetic transactions are run to ensure the health of the ecosystem on a continual basis. fault management is defined as the ability to detect, log and understand anomalies that may impact customers. comcast utilizes correlation engines to properly determine an event’s severity and act appropriately, eliminating or remediating potential issues before they affect customers. configuration management makes sure appropriate versions of hardware and software are in place across all elements of the ecosystem. keeping these elements at their peak “golden” levels helps them avoid unintended consequences. accounting management ensures that the operations centers have a clear understanding of the provisioning and utilization of the ecosystem. this is especially important to ensure that at all times the operations centers have the ability to reroute traffic effectively. security management ensures that the proper controls exist to ensure the ecosystem is effectively protected against inappropriate access.
and operations personnel are constantly re-evaluating the pre-defined performance  measures and tools to ensure that the customers’ expectations for operational excellence are met.
contrary to what the name snmp (simple network management protocol) might suggest, network management in the internet is much more than just a protocol for moving management data between a management entity and its agents, and has grown to be much more complex than the word “simple” might suggest. the current internet-standard management framework traces its roots back to the simple gateway monitoring protocol, sgmp [rfc 1028]. sgmp was designed by a group of university network researchers, users, and managers, whose experience with sgmp allowed them to design, implement, and deploy snmp in just a few months [lynch 1993]—a far cry from today’s rather drawn-out standardization process. since then, snmp has evolved from snmpv1 through snmpv2 to the most recent version, snmpv3 [rfc 3410], released in april 1999 and updated in december 2002.
problem may be fixed.) unfortunately, our slight oversight is not as innocuous as it may seem. minimally, we will need to add checksum bits to ack/nak packets in order to detect such errors. the more difficult question is how the protocol should recover from errors in ack or nak packets. the difficulty here is that if an ack or nak is corrupted, the sender has no way of knowing whether or not the receiver has correctly received the last piece of transmitted data.
• for  the  first  possibility,  consider  what  a  human  might  do  in  the  messagedictation scenario. if the speaker didn’t understand the “ok” or “please repeat that” reply from the receiver, the speaker would probably ask, “what did you say?” (thus introducing a new type of sender-to-receiver packet to our protocol). the receiver would then repeat the reply. but what if the speaker’s “what did you say?” is corrupted? the receiver, having no idea whether the garbled sentence was part of the dictation or a request to repeat the last reply, would probably then respond with “what did you say?” and then, of course, that response might be garbled. clearly, we’re heading down a difficult path.
• a second alternative is to add enough checksum bits to allow the sender not only to detect, but also to recover from, bit errors. this solves the immediate problem for a channel that can corrupt packets but not lose them.
• a third approach is for the sender simply to resend the current data packet when it receives a garbled ack or nak packet. this approach, however, introduces duplicate packets into the sender-to-receiver channel. the fundamental difficulty with duplicate packets is that the receiver doesn’t know whether the ack or nak it last sent was received correctly at the sender. thus, it cannot know a priori whether an arriving packet contains new data or is a retransmission!
a simple solution to this new problem (and one adopted in almost all existing data transfer protocols, including tcp) is to add a new field to the data packet and have the sender number its data packets by putting a sequence number into this field. the receiver then need only check this sequence number to determine whether or not the received packet is a retransmission. for this simple case of a stop-andwait protocol, a 1-bit sequence number will suffice, since it will allow the receiver to know whether the sender is resending the previously transmitted packet (the sequence number of the received packet has the same sequence number as the most recently received packet) or a new packet (the sequence number changes, moving “forward” in modulo-2 arithmetic). since we are currently assuming a channel that does not lose packets, ack and nak packets do not themselves need to indicate the sequence number of the packet they are acknowledging. the sender knows that a received ack or nak packet (whether garbled or not) was generated in response to its most recently transmitted data packet.
maximum size of a segment’s data field. when tcp sends a large file, such as an image as part of a web page, it typically breaks the file into chunks of size mss (except for the last chunk, which will often be less than the mss). interactive applications, however, often transmit data chunks that are smaller than the mss; for example, with remote login applications like telnet, the data field in the tcp segment is often only one byte. because the tcp header is typically 20 bytes (12 bytes more than the udp header), segments sent by telnet may be only 21 bytes in length. figure 3.29 shows the structure of the tcp segment. as with udp, the header source  and  destination  port  numbers,  which  are  used  for includes multiplexing/demultiplexing data from/to upper-layer applications. also, as with udp, the header includes a checksum field. a tcp segment header also contains the following fields:
• the 32-bit sequence number field and the 32-bit acknowledgment number field are used by the tcp sender and receiver in implementing a reliable data transfer service, as discussed below.
• the 4-bit header length field specifies the length of the tcp header in 32-bit words. the tcp header can be of variable length due to the tcp options field.
telnet: a case study for sequence and acknowledgment numbers telnet,  defined  in  rfc  854,  is  a  popular  application-layer  protocol  used  for remote login. it runs over tcp and is designed to work between any pair of hosts. unlike the bulk data transfer applications discussed in chapter 2, telnet is an interactive application. we discuss a telnet example here, as it nicely illustrates tcp sequence  and  acknowledgment  numbers.  we  note  that  many  users  now  prefer to use the ssh protocol rather than telnet, since data sent in a telnet connection  (including  passwords!)  is  not  encrypted,  making  telnet  vulnerable  to eavesdropping attacks (as discussed in section 8.7).
suppose host a initiates a telnet session with host b. because host a initiates the session, it is labeled the client, and host b is labeled the server. each character typed by the user (at the client) will be sent to the remote host; the remote host will send back a copy of each character, which will be displayed on the telnet user’s screen. this “echo back” is used to ensure that characters seen by the telnet user have already been received and processed at the remote site. each character thus traverses the network twice between the time the user hits the key and the time the character is displayed on the user’s monitor.
now suppose the user types a single letter, ‘c,’ and then grabs a coffee. let’s examine the tcp segments that are sent between the client and server. as shown in figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server, respectively. recall that the sequence number of a segment is the sequence number of the first byte in the data field. thus, the first segment sent from the client will have sequence number 42; the first segment sent from the server will have sequence number 79. recall that the acknowledgment number is the sequence number of the next byte of data that the host is waiting for. after the tcp connection is established but before any data is sent, the client is waiting for byte 79 and the server is waiting for byte 42.
as shown in figure 3.31, three segments are sent. the first segment is sent from the client to the server, containing the 1-byte ascii representation of the letter ‘c’ in its data field. this first segment also has 42 in its sequence number field, as we just described. also, because the client has not yet received any data from the server, this first segment will have 79 in its acknowledgment number field.
the second segment is sent from the server to the client. it serves a dual purpose. first it provides an acknowledgment of the data the server has received. by putting 43 in the acknowledgment field, the server is telling the client that it has successfully received everything up through byte 42 and is now waiting for bytes 43 onward. the second purpose of this segment is to echo back the letter ‘c.’ thus, the second segment has the ascii representation of ‘c’ in its data field. this second segment has the sequence number 79, the initial sequence number of the server-toclient data flow of this tcp connection, as this is the very first byte of data that the server is sending. note that the acknowledgment for client-to-server data is carried in  a  segment  carrying  server-to-client  data;  this  acknowledgment  is  said  to  be piggybacked on the server-to-client data segment.
telnet: a case study for sequence and acknowledgment numbers telnet,  defined  in  rfc  854,  is  a  popular  application-layer  protocol  used  for remote login. it runs over tcp and is designed to work between any pair of hosts. unlike the bulk data transfer applications discussed in chapter 2, telnet is an interactive application. we discuss a telnet example here, as it nicely illustrates tcp sequence  and  acknowledgment  numbers.  we  note  that  many  users  now  prefer to use the ssh protocol rather than telnet, since data sent in a telnet connection  (including  passwords!)  is  not  encrypted,  making  telnet  vulnerable  to eavesdropping attacks (as discussed in section 8.7).
suppose host a initiates a telnet session with host b. because host a initiates the session, it is labeled the client, and host b is labeled the server. each character typed by the user (at the client) will be sent to the remote host; the remote host will send back a copy of each character, which will be displayed on the telnet user’s screen. this “echo back” is used to ensure that characters seen by the telnet user have already been received and processed at the remote site. each character thus traverses the network twice between the time the user hits the key and the time the character is displayed on the user’s monitor.
now suppose the user types a single letter, ‘c,’ and then grabs a coffee. let’s examine the tcp segments that are sent between the client and server. as shown in figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server, respectively. recall that the sequence number of a segment is the sequence number of the first byte in the data field. thus, the first segment sent from the client will have sequence number 42; the first segment sent from the server will have sequence number 79. recall that the acknowledgment number is the sequence number of the next byte of data that the host is waiting for. after the tcp connection is established but before any data is sent, the client is waiting for byte 79 and the server is waiting for byte 42.
as shown in figure 3.31, three segments are sent. the first segment is sent from the client to the server, containing the 1-byte ascii representation of the letter ‘c’ in its data field. this first segment also has 42 in its sequence number field, as we just described. also, because the client has not yet received any data from the server, this first segment will have 79 in its acknowledgment number field.
the second segment is sent from the server to the client. it serves a dual purpose. first it provides an acknowledgment of the data the server has received. by putting 43 in the acknowledgment field, the server is telling the client that it has successfully received everything up through byte 42 and is now waiting for bytes 43 onward. the second purpose of this segment is to echo back the letter ‘c.’ thus, the second segment has the ascii representation of ‘c’ in its data field. this second segment has the sequence number 79, the initial sequence number of the server-toclient data flow of this tcp connection, as this is the very first byte of data that the server is sending. note that the acknowledgment for client-to-server data is carried in  a  segment  carrying  server-to-client  data;  this  acknowledgment  is  said  to  be piggybacked on the server-to-client data segment.
then the ack is acknowledging one or more previously unacknowledged segments. thus the sender updates its sendbase variable; it also restarts the timer if there currently are any not-yet-acknowledged segments.
a few interesting scenarios we have just described a highly simplified version of how tcp provides reliable data transfer. but even this highly simplified version has many subtleties. to get a good feeling for how this protocol works, let’s now walk through a few simple scenarios. figure 3.34 depicts the first scenario, in which host a sends one segment to host b. suppose that this segment has sequence number 92 and contains 8 bytes of data. after sending this segment, host a waits for a segment from b with acknowledgment number 100. although the segment from a is received at b, the acknowledgment from b to a gets lost. in this case, the timeout event occurs, and host  a retransmits  the  same  segment.  of  course,  when  host  b  receives  the retransmission, it observes from the sequence number that the segment contains data that has already been received. thus, tcp in host b will discard the bytes in the retransmitted segment.
telnet: a case study for sequence and acknowledgment numbers telnet,  defined  in  rfc  854,  is  a  popular  application-layer  protocol  used  for remote login. it runs over tcp and is designed to work between any pair of hosts. unlike the bulk data transfer applications discussed in chapter 2, telnet is an interactive application. we discuss a telnet example here, as it nicely illustrates tcp sequence  and  acknowledgment  numbers.  we  note  that  many  users  now  prefer to use the ssh protocol rather than telnet, since data sent in a telnet connection  (including  passwords!)  is  not  encrypted,  making  telnet  vulnerable  to eavesdropping attacks (as discussed in section 8.7).
suppose host a initiates a telnet session with host b. because host a initiates the session, it is labeled the client, and host b is labeled the server. each character typed by the user (at the client) will be sent to the remote host; the remote host will send back a copy of each character, which will be displayed on the telnet user’s screen. this “echo back” is used to ensure that characters seen by the telnet user have already been received and processed at the remote site. each character thus traverses the network twice between the time the user hits the key and the time the character is displayed on the user’s monitor.
now suppose the user types a single letter, ‘c,’ and then grabs a coffee. let’s examine the tcp segments that are sent between the client and server. as shown in figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server, respectively. recall that the sequence number of a segment is the sequence number of the first byte in the data field. thus, the first segment sent from the client will have sequence number 42; the first segment sent from the server will have sequence number 79. recall that the acknowledgment number is the sequence number of the next byte of data that the host is waiting for. after the tcp connection is established but before any data is sent, the client is waiting for byte 79 and the server is waiting for byte 42.
as shown in figure 3.31, three segments are sent. the first segment is sent from the client to the server, containing the 1-byte ascii representation of the letter ‘c’ in its data field. this first segment also has 42 in its sequence number field, as we just described. also, because the client has not yet received any data from the server, this first segment will have 79 in its acknowledgment number field.
the second segment is sent from the server to the client. it serves a dual purpose. first it provides an acknowledgment of the data the server has received. by putting 43 in the acknowledgment field, the server is telling the client that it has successfully received everything up through byte 42 and is now waiting for bytes 43 onward. the second purpose of this segment is to echo back the letter ‘c.’ thus, the second segment has the ascii representation of ‘c’ in its data field. this second segment has the sequence number 79, the initial sequence number of the server-toclient data flow of this tcp connection, as this is the very first byte of data that the server is sending. note that the acknowledgment for client-to-server data is carried in  a  segment  carrying  server-to-client  data;  this  acknowledgment  is  said  to  be piggybacked on the server-to-client data segment.
the third segment is sent from the client to the server. its sole purpose is to acknowledge the data it has received from the server. (recall that the second segment contained data—the letter ‘c’—from the server to the client.) this segment has an empty data field (that is, the acknowledgment is not being piggybacked with any client-to-server data). the segment has 80 in the acknowledgment number field because the client has received the stream of bytes up through byte sequence number 79 and it is now waiting for bytes 80 onward. you might think it odd that this segment  also  has  a  sequence  number  since  the  segment  contains  no  data.  but because  tcp has  a  sequence  number  field,  the  segment  needs  to  have  some sequence number.
3.5.3 round-trip time estimation and timeout tcp, like our rdt protocol in section 3.4, uses a timeout/retransmit mechanism to recover  from  lost  segments. although  this  is  conceptually  simple,  many  subtle issues arise when we implement a timeout/retransmit mechanism in an actual protocol such as tcp. perhaps the most obvious question is the length of the timeout
the actions taken when the event occurs are shown below the horizontal line. when no action is taken on an event, or no event occurs and an action is taken, we’ll use the symbol ⌳ below or above the horizontal, respectively, to explicitly denote the lack of an action or event. the initial state of the fsm is indicated by the dashed arrow. although the fsms in figure 3.9 have but one state, the fsms we will see shortly have multiple states, so it will be important to identify the initial state of each fsm.
the  sending  side  of rdt simply  accepts  data  from  the  upper  layer  via  the rdt_send(data) event, creates a packet containing the data (via the action make_pkt(data))  and  sends  the  packet  into  the  channel.  in  practice,  the rdt_send(data) event would result from a procedure call (for example, to rdt_send()) by the upper-layer application.
on the receiving side, rdt receives a packet from the underlying channel via the rdt_rcv(packet) event, removes the data from the packet (via the action extract (packet, data)) and passes the data up to the upper layer (via the action deliver_data(data)). in practice, the rdt_rcv(packet) event would result from a procedure call (for example, to rdt_rcv()) from the lowerlayer protocol.
in this simple protocol, there is no difference between a unit of data and a packet. also, all packet flow is from the sender to receiver; with a perfectly reliable channel there is no need for the receiver side to provide any feedback to the sender since nothing can go wrong! note that we have also assumed that the receiver is able to receive data as fast as the sender happens to send data. thus, there is no need for the receiver to ask the sender to slow down!
reliable data transfer over a channel with bit errors: rdt2.0 a more realistic model of the underlying channel is one in which bits in a packet may be corrupted. such bit errors typically occur in the physical components of a network as a packet is transmitted, propagates, or is buffered. we’ll continue to assume for the moment that all transmitted packets are received (although their bits may be corrupted) in the order in which they were sent.
before developing a protocol for reliably communicating over such a channel, first consider how people might deal with such a situation. consider how you yourself might dictate a long message over the phone. in a typical scenario, the message taker might say “ok” after each sentence has been heard, understood, and recorded. if the message taker hears a garbled sentence, you’re asked to repeat the garbled sentence. this message-dictation protocol uses both positive acknowledgments (“ok”) and negative acknowledgments (“please repeat that.”). these control messages allow the receiver to let the sender know what has been received correctly, and what has been received in error and thus requires repeating. in a computer network setting, reliable data transfer protocols based on such retransmission are known as arq (automatic repeat request) protocols.
• error detection. first, a mechanism is needed to allow the receiver to detect when bit errors have occurred. recall from the previous section that udp uses the internet checksum field for exactly this purpose. in chapter 5 we’ll examine error-detection and -correction techniques in greater detail; these techniques allow the receiver to detect and possibly correct packet bit errors. for now, we need only know that these techniques require that extra bits (beyond the  bits  of  original  data  to  be  transferred)  be  sent  from  the  sender  to  the receiver;  these  bits  will  be  gathered  into  the  packet  checksum  field  of  the rdt2.0 data packet.
• receiver feedback. since the sender and receiver are typically executing on different end systems, possibly separated by thousands of miles, the only way for the sender to learn of the receiver’s view of the world (in this case, whether or not a packet was received correctly) is for the receiver to provide explicit feedback to the sender. the positive (ack) and negative (nak) acknowledgment replies in the message-dictation scenario are examples of such feedback. our rdt2.0 protocol will similarly send ack and nak packets back from the receiver to the sender. in principle, these packets need only be one bit long; for example, a 0 value could indicate a nak and a value of 1 could indicate an ack.
figure 3.10 shows the fsm representation of rdt2.0, a data transfer protocol employing error detection, positive acknowledgments, and negative acknowledgments. the send side of rdt2.0 has two states. in the leftmost state, the send-side protocol  is  waiting  for  data  to  be  passed  down  from  the  upper  layer.  when  the rdt_send(data) event  occurs,  the  sender  will  create  a  packet  (sndpkt) containing the data to be sent, along with a packet checksum (for example, as discussed in section 3.3.2 for the case of a udp segment), and then send the packet via the udt_send(sndpkt) operation. in the rightmost state, the sender protocol is waiting for an ack or a nak packet from the receiver. if an ack packet is received (the notation rdt_rcv(rcvpkt) && isack (rcvpkt) in figure 3.10 corresponds to this event), the sender knows that the most recently transmitted packet has been received correctly and thus the protocol returns to the state of waiting for data from the upper layer. if a nak is received, the protocol retransmits the last packet and waits for an ack or nak to be returned by the receiver in response to the retransmitted data packet. it is important to note that when the sender is in the wait-for-ack-or-nak state, it cannot get more data from the upper layer; that is, the rdt_send() event can not occur; that will happen only after the sender receives an ack and leaves this state. thus, the sender will not send a new piece of data until it is sure that the receiver has
transfer protocol for a channel with bit errors is rdt2.2, shown in figures 3.13 and 3.14. one subtle change between rtdt2.1 and rdt2.2 is that the receiver must now include the sequence number of the packet being acknowledged by an ack message (this is done by including the ack,0 or ack,1 argument in make_pkt() in the receiver fsm), and the sender must now check the sequence number of the packet being acknowledged by a received ack message (this is done by including the 0 or 1 argument in isack()in the sender fsm).
reliable data transfer over a lossy channel with bit errors: rdt3.0 suppose now that in addition to corrupting bits, the underlying channel can lose packets as well, a not-uncommon event in today’s computer networks (including the internet). two additional concerns must now be addressed by the protocol: how to detect packet loss and what to do when packet loss occurs. the use of checksumming,  sequence  numbers,  ack  packets,  and  retransmissions—the  techniques already developed in rdt2.2—will allow us to answer the latter concern. handling the first concern will require adding a new protocol mechanism.
there are many possible approaches toward dealing with packet loss (several more of which are explored in the exercises at the end of the chapter). here, we’ll put the burden of detecting and recovering from lost packets on the sender. suppose
figure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins sending the packet at t = 0, then at t = l/r = 8 microseconds, the last bit enters the channel at the sender side. the packet then makes its 15-msec cross-country journey, with the last bit of the packet emerging at the receiver at t = rtt/2 + l/r = 15.008 msec. assuming for simplicity that ack packets are extremely small (so that we can ignore their transmission time) and that the receiver can send an ack as soon as the last bit of a data packet is received, the ack emerges back at the sender at t = rtt + l/r = 30.008 msec. at this point, the sender can now transmit the next message. thus, in 30.008 msec, the sender was sending for only 0.008 msec. if we define the utilization of the sender (or the channel) as the fraction of time the sender is actually busy sending bits into the channel, the analysis in figure 3.18(a) shows that the stop-and-wait protocol has a rather dismal sender utilization, usender, of
that is, the sender was busy only 2.7 hundredths of one percent of the time! viewed another way, the sender was able to send only 1,000 bytes in 30.008 milliseconds, an effective throughput of only 267 kbps—even though a 1 gbps link was available! imagine the unhappy network manager who just paid a fortune for a gigabit capacity link but manages to get a throughput of only 267 kilobits per second! this is a graphic example of how network protocols can limit the capabilities
• the  optional  and  variable-length  options  field is  used  when  a  sender  and receiver negotiate the maximum segment size (mss) or as a window scaling factor for use in high-speed networks. a time-stamping option is also defined. see rfc 854 and rfc 1323 for additional details.
• the flag field contains 6 bits. the ack bit is used to indicate that the value carried  in  the  acknowledgment  field  is  valid;  that  is,  the  segment  contains  an acknowledgment for a segment that has been successfully received. the rst, syn, and fin bits are used for connection setup and teardown, as we will discuss at the end of this section. setting the psh bit indicates that the receiver should pass the data to the upper layer immediately. finally, the urg bit is used to indicate that there is data in this segment that the sending-side upper-layer entity has marked as “urgent.” the location of the last byte of this urgent data is indicated by the 16-bit urgent data pointer field. tcp must inform the receiving-side upper-layer entity when urgent data exists and pass it a pointer to the end of the urgent data. (in practice, the psh, urg, and the urgent data pointer are not used. however, we mention these fields for completeness.)
sequence numbers and acknowledgment numbers two of the most important fields in the tcp segment header are the sequence number field and the acknowledgment number field. these fields are a critical part of tcp’s reliable data transfer service. but before discussing how these fields are used to provide reliable data transfer, let us first explain what exactly tcp puts in these fields.
tcp views data as an unstructured, but ordered, stream of bytes. tcp’s use of sequence numbers reflects this view in that sequence numbers are over the stream of transmitted bytes and not over the series of transmitted segments. the sequence number for a segment is therefore the byte-stream number of the first byte in the segment. let’s look at an example. suppose that a process in host a wants to send a stream of data to a process in host b over a tcp connection. the tcp in host a will implicitly number each byte in the data stream. suppose that the data stream consists of a file consisting of 500,000 bytes, that the mss is 1,000 bytes, and that the first byte of the data stream is numbered 0. as shown in figure 3.30, tcp constructs 500 segments out of the data stream. the first segment gets assigned sequence number 0, the second segment gets assigned sequence number 1,000, the third segment gets assigned sequence number 2,000, and so on. each sequence number is inserted in the sequence number field in the header of the appropriate tcp segment.
now let’s consider acknowledgment numbers. these are a little trickier than sequence numbers. recall that tcp is full-duplex, so that host a may be receiving data from host b while it sends data to host b (as part of the same tcp connection). each of the segments that arrive from host b has a sequence number for the data
the syn flood attack we’ve seen in our discussion of tcp’s three-way handshake that a server allocates and initializes connection variables and buffers in response to a received syn. the server then sends a synack in response, and awaits an ack segment from the client. if the client does not send an ack to complete the third step of this 3-way handshake, eventually (often after a minute or more) the server will terminate the halfopen connection and reclaim the allocated resources. 
service (dos) attack known as the syn flood attack. in this attack, the attacker(s) send a large number of tcp syn segments, without completing the third handshake step. with this deluge of syn segments, the server’s connection resources become exhausted as they are allocated (but never used!) for half-open connections; legitimate clients are then denied service. such syn flooding attacks were among the first documented dos attacks [cert syn 1996]. fortunately, an effective defense known as syn cookies [rfc 4987] are now deployed in most major operating systems. syn cookies work as follows:
o  when the server receives a syn segment, it does not know if the segment is coming from a legitimate user or is part of a syn flood attack. so, instead of creating a half-open tcp connection for this syn, the server creates an initial tcp sequence number that is a complicated function (hash function) of source and destination ip addresses and port numbers of the syn segment, as well as a secret number only known to the server. this carefully crafted initial sequence number is the so-called “cookie.” the server then sends the client a synack packet with this special initial sequence number. importantly, the server does not remember the cookie or any other state information corresponding to the syn.
ack, it must verify that the ack corresponds to some syn sent earlier. but how is this done if the server maintains no memory about syn segments? as you may have guessed, it is done with the cookie. recall that for a legitimate ack, the value in the acknowledgment field is equal to the initial sequence number in the synack (the cookie value in this case) plus one (see figure 3.39). the server can then run the same hash function using the source and destination ip address and port numbers in the synack (which are the same as in the original syn) and the secret number. if the result of the function plus one is the same as the acknowledgment (cookie) value in the client’s synack, the server concludes that the ack corresponds to an earlier syn segment and is hence valid. the server then creates a fully open connection along with a socket.
3.5.4 reliable data transfer recall that the internet’s network-layer service (ip service) is unreliable. ip does not guarantee datagram delivery, does not guarantee in-order delivery of datagrams, and does not guarantee the integrity of the data in the datagrams. with ip service, datagrams can overflow router buffers and never reach their destination, datagrams  can  arrive  out  of  order,  and  bits  in  the  datagram  can  get  corrupted (flipped from 0 to 1 and vice versa). because transport-layer segments are carried across the network by ip datagrams, transport-layer segments can suffer from these problems as well.
tcp creates a reliable data transfer service on top of ip’s unreliable besteffort service. tcp’s reliable data transfer service ensures that the data stream that a process reads out of its tcp receive buffer is uncorrupted, without gaps, without duplication, and in sequence; that is, the byte stream is exactly the same byte stream that was sent by the end system on the other side of the connection. how tcp provides a reliable data transfer involves many of the principles that we studied in section 3.4.
in our earlier development of reliable data transfer techniques, it was conceptually easiest to assume that an individual timer is associated with each transmitted but not yet acknowledged segment. while this is great in theory, timer management can require considerable overhead. thus, the recommended tcp timer management procedures [rfc 6298] use only a single retransmission timer, even if there are multiple transmitted but not yet acknowledged segments. the tcp protocol described in this section follows this single-timer recommendation.
we will discuss how tcp provides reliable data transfer in two incremental steps. we first present a highly simplified description of a tcp sender that uses only timeouts to recover from lost segments; we then present a more complete description that uses duplicate acknowledgments in addition to timeouts. in the ensuing discussion, we suppose that data is being sent in only one direction, from host a to host b, and that host a is sending a large file.
figure 3.33 presents a highly simplified description of a tcp sender. we see that there are three major events related to data transmission and retransmission in the tcp sender: data received from application above; timer timeout; and ack receipt. upon the occurrence of the first major event, tcp receives data from the application, encapsulates the data in a segment, and passes the segment to ip. note that each segment includes a sequence number that is the byte-stream number of the first data byte in the segment, as described in section 3.5.2. also note that if the timer is already not running for some other segment, tcp starts the timer when the segment is passed to ip. (it is helpful to think of the timer as being associated with the oldest unacknowledged segment.) the expiration interval for this timer is the timeoutinterval, which is calculated from estimatedrtt and devrtt, as described in section 3.5.3.
of addresses are useful and, in fact, indispensable. we’ll also cover the address resolution protocol (arp), which provides a mechanism to translate ip addresses to link-layer addresses.
mac addresses in truth, it is not hosts and routers that have link-layer addresses but rather their adapters (that is, network interfaces) that have link-layer addresses. a host or router  with  multiple  network  interfaces  will  thus  have  multiple  link-layer addresses associated with it, just as it would also have multiple ip addresses associated with it. it's important to note, however, that link-layer switches do not have link-layer  addresses  associated  with  their  interfaces  that  connect  to  hosts  and routers. this is because the job of the link-layer switch is to carry datagrams between hosts and routers; a switch does this job transparently, that is, without the host or router having to explicitly address the frame to the intervening switch. this is illustrated in figure 5.16. a link-layer address is variously called a lan address, a physical address, or a mac address. because mac address seems to be the most popular term, we’ll henceforth refer to link-layer addresses as mac addresses. for most lans (including ethernet and 802.11 wireless lans), the mac address is 6 bytes long, giving 248 possible mac addresses. as shown in figure 5.16, these 6-byte addresses are typically expressed in hexadecimal notation, with each byte of the address expressed as a pair of hexadecimal numbers. although mac addresses were designed to be permanent, it is now possible to
keeping the layers independent there are several reasons why hosts and router interfaces have mac addresses in addition to network-layer addresses. first, lans are designed for arbitrary network-layer protocols, not just for ip and the internet. if adapters were assigned ip addresses rather than “neutral” mac addresses, then adapters would not easily be able to support other network-layer protocols (for example, ipx or decnet). second, if adapters were to use network-layer addresses instead of mac addresses, the network-layer address would have to be stored in the adapter ram and reconfigured every time the adapter was moved (or powered up). another option is to not use any addresses in the adapters and have each adapter pass the data (typically, an ip datagram) of each frame it receives up the protocol stack. the network layer could then check for a matching network-layer address. one problem with this option is that the host would be interrupted by every frame sent on the lan, including by frames that were destined for other hosts on the same broadcast lan. in summary, in order for the layers to be largely independent building blocks in a network architecture, different layers need to have their own addressing scheme. we have now seen three types of addresses: host names for the application layer, ip addresses for the network layer, and mac addresses for the link layer.
address resolution protocol (arp) because there are both network-layer addresses (for example, internet ip addresses) and  link-layer  addresses  (that  is,  mac  addresses),  there  is  a  need  to  translate between them. for the internet, this is the job of the address resolution protocol (arp) [rfc 826].
to understand the need for a protocol such as arp, consider the network shown in figure 5.17. in this simple example, each host and router has a single ip address and single mac address. as usual, ip addresses are shown in dotted-decimal notation and mac addresses are shown in hexadecimal notation. for the purposes of this discussion, we will assume in this section that the switch broadcasts all frames; that is, whenever a switch receives a frame on one interface, it forwards the frame on all of its other interfaces. in the next section, we will provide a more accurate explanation of how switches operate.
now suppose that the host with ip address 222.222.222.220 wants to send an ip datagram to host 222.222.222.222. in this example, both the source and destination are in the same subnet, in the addressing sense of section 4.4.2. to send a datagram, the source must give its adapter not only the ip datagram but also the mac address for destination 222.222.222.222. the sending adapter will then construct a linklayer  frame  containing  the  destination’s  mac  address  and  send  the  frame  into  the lan.
ethernet frame structure we can learn a lot about ethernet by examining the ethernet frame, which is shown in figure 5.20. to give this discussion about ethernet frames a tangible context, let’s consider sending an ip datagram from one host to another host, with both hosts on the same ethernet lan (for example, the ethernet lan in figure 5.17.) (although the payload of our ethernet frame is an ip datagram, we note that an ethernet frame can carry other network-layer packets as well.) let the sending adapter, adapter a, have the mac address aa-aa-aa-aa-aa-aa and the receiving adapter, adapter b, have the mac address bb-bb-bb-bb-bb-bb. the sending adapter encapsulates the ip datagram within an ethernet frame and passes the frame to the physical layer. the receiving adapter receives the frame from the physical layer, extracts the ip datagram, and passes the ip datagram to the network layer. in this context, let’s now examine the six fields of the ethernet frame, as shown in figure 5.20.
• data field (46 to 1,500 bytes). this field carries the ip datagram. the maximum transmission unit (mtu) of ethernet is 1,500 bytes. this means that if the ip datagram exceeds 1,500 bytes, then the host has to fragment the datagram, as discussed in section 4.4.1. the minimum size of the data field is 46 bytes. this means  that  if  the  ip datagram  is  less  than  46  bytes,  the  data  field  has  to  be “stuffed” to fill it out to 46 bytes. when stuffing is used, the data passed to the network layer contains the stuffing as well as an ip datagram. the network layer uses the length field in the ip datagram header to remove the stuffing.
• destination address (6 bytes). this field contains the mac address of the destination adapter, bb-bb-bb-bb-bb-bb. when adapter b receives an ethernet frame whose destination address is either bb-bb-bb-bb-bb-bb or the mac broadcast address, it passes the contents of the frame’s data field to the network layer; if it receives a frame with any other mac address, it discards the frame.
• type field (2 bytes). the type field permits ethernet to multiplex network-layer protocols. to understand this, we need to keep in mind that hosts can use other network-layer protocols besides ip. in fact, a given host may support multiple
students often wonder if arp is a link-layer protocol or a network-layer protocol. as we’ve seen, an arp packet is encapsulated within a link-layer frame and thus lies architecturally above the link layer. however, an arp packet has fields containing link-layer addresses and thus is arguably a link-layer protocol, but it also contains network-layer addresses and thus is also arguably a networklayer protocol. in the end, arp is probably best considered a protocol that straddles the boundary between the link and network layers—not fitting neatly into the simple layered protocol stack we studied in chapter 1. such are the complexities of real-world protocols!
sending a datagram off the subnet it should now be clear how arp operates when a host wants to send a datagram to another host on the same subnet. but now let’s look at the more complicated situation when a host on a subnet wants to send a network-layer datagram to a host off the subnet (that is, across a router onto another subnet). let’s discuss this issue in the context of figure 5.19, which shows a simple network consisting of two subnets interconnected by a router.
there are several interesting things to note about figure 5.19. each host has exactly one ip address and one adapter. but, as discussed in chapter 4, a router has an ip address for each of its interfaces. for each router interface there is also an arp module (in the router) and an adapter. because the router in figure 5.19 has two interfaces, it has two ip addresses, two arp modules, and two adapters. of course, each adapter in the network has its own mac address.
also note that subnet 1 has the network address 111.111.111/24 and that subnet 2 has the network address 222.222.222/24. thus all of the interfaces connected to subnet 1 have addresses of the form 111.111.111.xxx and all of the interfaces connected to subnet 2 have addresses of the form 222.222.222.xxx.
electronic mail remote terminal access web file transfer remote file server streaming multimedia internet telephony network management routing protocol name translation
although commonly done today, running multimedia applications over udp is controversial. as we mentioned above, udp has no congestion control. but congestion control is needed to prevent the network from entering a congested state in which very little useful work is done. if everyone were to start streaming high-bitrate video without using any congestion control, there would be so much packet overflow at routers that very few udp packets would successfully traverse the source-to-destination path. moreover, the high loss rates induced by the uncontrolled udp senders would cause the tcp senders (which, as we’ll see, do decrease their sending rates in the face of congestion) to dramatically decrease their rates. thus, the lack of congestion control in udp can result in high loss rates between a udp sender and receiver, and the crowding out of tcp sessions—a potentially serious problem [floyd 1999]. many researchers have proposed new mechanisms to force all sources, including udp sources, to perform adaptive congestion control [mahdavi 1997; floyd 2000; kohler 2006: rfc 4340].
before discussing the udp segment structure, we mention that it is possible for an application to have reliable data transfer when using udp. this can be done if reliability is built into the application itself (for example, by adding acknowledgment and retransmission mechanisms, such as those we’ll study in the next section). but this is a nontrivial task that would keep an application developer busy debugging for
adaptive playout delay the previous example demonstrates an important delay-loss trade-off that arises when designing a playout strategy with fixed playout delays. by making the initial playout delay large, most packets will make their deadlines and there will therefore be negligible loss; however, for conversational services such as voip, long delays can become bothersome if not intolerable. ideally, we would like the playout delay to be minimized subject to the constraint that the loss be below a few percent.
the natural way to deal with this trade-off is to estimate the network delay and the variance of the network delay, and to adjust the playout delay accordingly at the beginning of each talk spurt. this adaptive adjustment of playout delays at the beginning of the talk spurts will cause the sender’s silent periods to be compressed and elongated; however, compression and elongation of silence by a small amount is not noticeable in speech.
ti = the timestamp of the ith packet = the time the packet was generated by the sender ri = the time packet i is received by receiver pi = the time packet i is played at receiver
the end-to-end network delay of the ith packet is ri – ti. due to network jitter, this delay will vary from packet to packet. let di denote an estimate of the average
quality in the presence of packet loss. such schemes are called loss recovery schemes. here we define packet loss in a broad sense: a packet is lost either if it never arrives at the receiver or if it arrives after its scheduled playout time. our voip example will again serve as a context for describing loss recovery schemes.
as mentioned at the beginning of this section, retransmitting lost packets may not be feasible in a real-time conversational application such as voip. indeed, retransmitting a packet that has missed its playout deadline serves absolutely no purpose. and retransmitting a packet that overflowed a router queue cannot normally be accomplished quickly enough. because of these considerations, voip applications often use some type of loss anticipation scheme. two types of loss anticipation schemes are forward error correction (fec) and interleaving.
forward error correction (fec) the basic idea of fec is to add redundant information to the original packet stream. for the cost of marginally increasing the transmission rate, the redundant information can be used to reconstruct approximations or exact versions of some of the lost packets. following [bolot 1996] and [perkins 1998], we now outline two simple fec mechanisms. the first mechanism sends a redundant encoded chunk after every n chunks. the redundant chunk is obtained by exclusive or-ing the n original chunks [shacham 1990]. in this manner if any one packet of the group of n + 1 packets is lost, the receiver can fully reconstruct the lost packet. but if two or more packets in a group are lost, the receiver cannot reconstruct the lost packets. by keeping n + 1, the group size, small, a large fraction of the lost packets can be recovered when loss is not excessive. however, the smaller the group size, the greater the relative increase of the transmission rate. in particular, the transmission rate increases by a factor of 1/n, so that, if n = 3, then the transmission rate increases by 33 percent. furthermore, this simple scheme increases the playout delay, as the receiver must wait to receive the entire group of packets before it can begin playout. for more practical details about how fec works for multimedia transport see [rfc 5109].
the second fec mechanism is to send a lower-resolution audio stream as the redundant  information.  for  example,  the  sender  might  create  a  nominal  audio stream and a corresponding low-resolution, low-bit rate audio stream. (the nominal stream could be a pcm encoding at 64 kbps, and the lower-quality stream could be a gsm encoding at 13 kbps.) the low-bit rate stream is referred to as the redundant stream. as shown in figure 7.8, the sender constructs the nth packet by taking the nth chunk from the nominal stream and appending to it the (n – 1)st chunk from the redundant stream. in this manner, whenever there is nonconsecutive packet loss, the receiver can conceal the loss by playing out the low-bit rate encoded chunk that arrives with the subsequent packet. of course, low-bit rate chunks give lower quality than the nominal chunks. however, a stream of mostly high-quality chunks, occasional low-quality chunks, and no missing chunks gives good overall audio quality. note that in this scheme, the receiver only has to receive two packets before playback, so that the increased playout delay is small. furthermore, if the low-bit
early termination and repositioning the video http streaming systems often make use of the http byte-range header in the http get request message, which specifies the specific range of bytes the client currently wants to retrieve from the desired video. this is particularly useful when the user wants to reposition (that is, jump) to a future point in time in the video. when the user repositions to a new position, the client sends a new http request, indicating with the byte-range header from which byte in the file should the server send data. when the server receives the new http request, it can forget about any earlier request and instead send bytes beginning with the byte indicated in the byterange request.
while we are on the subject of repositioning, we briefly mention that when a user repositions  to  a  future  point  in  the  video  or  terminates  the  video  early,  some prefetched-but-not-yet-viewed data transmitted by the server will go unwatched—a waste of network bandwidth and server resources. for example, suppose that the client buffer is full with b bits at some time t0 into the video, and at this time the user repositions to some instant t > t0 + b/r into the video, and then watches the video to completion from that point on. in this case, all b bits in the buffer will be unwatched and the bandwidth and server resources that were used to transmit those b bits have been completely wasted. there is significant wasted bandwidth in the internet due to early termination, which can be quite costly, particularly for wireless links [ihm 2011]. for this reason, many streaming systems use only a moderate-size client application buffer, or will  limit  the  amount  of  prefetched  video  using  the  byte-range  header  in  http requests [rao 2011].
repositioning and early termination are analogous to cooking a large meal, eating only a portion of it, and throwing the rest away, thereby wasting food. so the next time your parents criticize you for wasting food by not eating all your dinner, you can quickly retort by saying they are wasting bandwidth and server resources when they reposition while watching movies over the internet! but, of course, two wrongs do not make a right—both food and bandwidth are not to be wasted!
7.2.3 adaptive streaming and dash although http streaming, as described in the previous subsection, has been extensively deployed in practice (for example, by youtube since its inception), it has a major shortcoming: all clients receive the same encoding of the video, despite the large variations in the amount of bandwidth available to a client, both across different clients and also over time for the same client. this has led to the development of a  new  type  of  http-based  streaming,  often  referred  to  as  dynamic adaptive streaming over http (dash). in dash, the video is encoded into several different versions, with each version having a different bit rate and, correspondingly, a different quality level. the client dynamically requests chunks of video segments of a few seconds in length from the different versions. when the amount of available
bandwidth is high, the client naturally selects chunks from a high-rate version; and when the available bandwidth is low, it naturally selects from a low-rate version. the client selects different chunks one at a time with http get request messages [akhshabi 2011].
on one hand, dash allows clients with different internet access rates to stream in video at different encoding rates. clients with low-speed 3g connections can receive a low bit-rate (and low-quality) version, and clients with fiber connections can receive a high-quality version. on the other hand, dash allows a client to adapt to the available bandwidth if the end-to-end bandwidth changes during the session. this feature is particularly important for mobile users, who typically see their bandwidth availability fluctuate as they move with respect to the base stations. comcast, for example, has deployed an adaptive streaming system in which each video source file is encoded into 8 to 10 different mpeg-4 formats, allowing the highest quality video  format  to  be  streamed  to  the  client,  with  adaptation  being  performed  in response to changing network and device conditions.
with dash, each video version is stored in the http server, each with a different url. the http server also has a manifest file, which provides a url for each version along with its bit rate. the client first requests the manifest file and learns about the various versions. the client then selects one chunk at a time by specifying a url and a byte range in an http get request message for each chunk. while downloading chunks, the client also measures the received bandwidth and runs a rate determination algorithm to select the chunk to request next. naturally, if the client has a lot of video buffered and if the measured receive bandwidth is high, it will choose a chunk from a high-rate version. and naturally if the client has little video buffered and the measured received bandwidth is low, it will choose a chunk from a low-rate version. dash therefore allows the client to freely switch among different quality levels. since a sudden drop in bit rate by changing versions may result in noticeable visual quality degradation, the bit-rate reduction may be achieved using multiple intermediate versions to smoothly transition to a rate where the client’s consumption rate drops below its available receive bandwidth. when the network conditions improve, the client can then later choose chunks from higher bit-rate versions.
by dynamically monitoring the available bandwidth and client buffer level, and adjusting the transmission rate with version switching, dash can often achieve continuous playout at the best possible quality level without frame freezing or skipping. furthermore, since the client (rather than the server) maintains the intelligence to determine which chunk to send next, the scheme also improves server-side scalability. another benefit of this approach is that the client can use the http byte-range request to precisely control the amount of prefetched video that it buffers locally.
we conclude our brief discussion of dash by mentioning that for many implementations, the server not only stores many versions of the video but also separately stores many versions of the audio. each audio version has its own quality level and bit rate and has its own url. in these implementations, the client dynamically selects both video and audio chunks, and locally synchronizes audio and video playout.
characteristics are clearly different from those of elastic data applications such as web browsing, e-mail, social networks, and remote login. for elastic applications, long delays are annoying but not particularly harmful; the completeness and integrity of the transferred data, however, are of paramount importance. we will explore conversational voice and video in more depth in section 7.3, paying particular attention to how adaptive playout, forward error correction, and error concealment can mitigate against network-induced packet loss and delay.
streaming live audio and video this third class of applications is similar to traditional broadcast radio and television, except that transmission takes place over the internet. these applications allow a user to receive a live radio or television transmission—such as a live sporting event or an ongoing news event—transmitted from any corner of the world. today, thousands of radio and television stations around the world are broadcasting content over the internet.
live, broadcast-like applications often have many users who receive the same audio/video program at the same time. although the distribution of live audio/video to many receivers can be efficiently accomplished using the ip multicasting techniques described in section 4.7, multicast distribution is more often accomplished today via application-layer multicast (using p2p networks or cdns) or through multiple separate unicast streams. as with streaming stored multimedia, the network must provide each live multimedia flow with an average throughput that is larger than the video consumption rate. because the event is live, delay can also be an issue, although the timing constraints are much less stringent than those for conversational voice. delays of up to ten seconds or so from when the user chooses to view a live transmission to when playout begins can be tolerated. we will not cover streaming live media in this book because many of the techniques used for streaming live media—initial buffering delay, adaptive bandwidth use, and cdn distribution—are similar to those for streaming stored media.
7.2 streaming stored video  for streaming video applications, prerecorded videos are placed on servers, and users send requests to these servers to view the videos on demand. the user may watch the video from beginning to end without interruption, may stop watching the video well before it ends, or interact with the video by pausing or repositioning to a future or past scene. streaming video systems can be classified into three categories: udp streaming, http streaming, and adaptive http streaming. although all three types of systems are used in practice, the majority of today’s systems employ http streaming and adaptive http streaming.
the internet’s address assignment strategy is known as classless interdomain routing (cidr—pronounced cider) [rfc 4632]. cidr generalizes the notion of subnet addressing. as with subnet addressing, the 32-bit ip address is divided into two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the number of bits in the first part of the address.
the x most significant bits of an address of the form a.b.c.d/x constitute the network portion of the ip address, and are often referred to as the prefix (or network prefix) of the address. an organization is typically assigned a block of contiguous addresses, that is, a range of addresses with a common prefix (see the principles in practice sidebar). in this case, the ip addresses of devices within the organization will share the common prefix. when we cover the internet’s bgp
this example of an isp that connects eight organizations to the internet nicely illustrates how carefully allocated cidrized addresses facilitate routing. suppose, as shown in figure 4.18, that the isp (which we’ll call fly-by-night-isp) advertises to the outside world that it should be sent any datagrams whose first 20 address bits match 200.23.16.0/20. the rest of the world need not know that within the address block 200.23.16.0/20 there are in fact eight other organizations, each with its own subnets. this ability to use a single prefix to advertise multiple networks is often referred to as address aggregation (also route aggregation or route summarization).
isps and then from isps to client organizations. but what happens when addresses are not allocated in such a hierarchical manner? what would happen, for example, if fly-bynight-isp acquires isps-r-us and then has organization 1 connect to the internet through its subsidiary isps-r-us? as shown in figure 4.18, the subsidiary isps-r-us owns the address block 199.31.0.0/16, but organization 1’s ip addresses are unfortunately outside of this address block. what should be done here? certainly, organization 1 could renumber all of its routers and hosts to have addresses within the isps-r-us address block. but this is a costly solution, and organization 1 might well be reassigned to another subsidiary in the future. the solution typically adopted is for organization 1 to keep its ip addresses in 200.23.18.0/23. in this case, as shown in figure 4.19,  fly-by-night-isp continues to advertise the address block 200.23.16.0/20 and isps-r-us continues to advertise 199.31.0.0/16. however, isps-r-us now also advertises the block of addresses for organization 1, 200.23.18.0/23. when other routers in the larger internet see the address blocks 200.23.16.0/20 (from fly-by-night-isp) and 200.23.18.0/23 (from isps-r-us) and want to route to an address in the block 200.23.18.0/23, they will use longest prefix matching (see section 4.2.2), and route toward isps-r-us, as it advertises the longest (most specific) address prefix that matches the destination address.
bulk data transfer (for example, the transfer of a software upgrade from the software developer to users needing the upgrade), streaming continuous media (for example, the transfer of the audio, video, and text of a live lecture to a set of distributed lecture participants), shared data applications (for example, a whiteboard or teleconferencing application that is shared among many distributed participants), data feeds (for example, stock quotes), web cache updating, and interactive gaming (for example, distributed interactive virtual environments or multiplayer games).
in multicast communication, we are immediately faced with two problems— how to identify the receivers of a multicast packet and how to address a packet sent to  these  receivers.  in  the  case  of  unicast  communication,  the  ip address  of  the receiver (destination) is carried in each ip unicast datagram and identifies the single recipient; in the case of broadcast, all nodes need to receive the broadcast packet, so no destination addresses are needed. but in the case of multicast, we now have multiple  receivers.  does  it  make  sense  for  each  multicast  packet  to  carry  the  ip addresses of all of the multiple recipients? while this approach might be workable with a small number of recipients, it would not scale well to the case of hundreds or thousands of receivers; the amount of addressing information in the datagram would swamp the amount of data actually carried in the packet’s payload field. explicit identification of the receivers by the sender also requires that the sender know the identities and addresses of all of the receivers. we will see shortly that there are cases where this requirement might be undesirable.
for these reasons, in the internet architecture (and other network architectures such as atm [black 1995]), a multicast packet is addressed using address indirection. that is, a single identifier is used for the group of receivers, and a copy of the packet that is addressed to the group using this single identifier is delivered to all of the multicast receivers associated with that group. in the internet, the single identifier that represents a group of receivers is a class d multicast ip address. the group of receivers associated with a class d address is referred to as a multicast group. the multicast group abstraction is illustrated in figure 4.47. here, four hosts (shown in shaded color) are associated with the multicast group address of 226.17.30.197 and will receive all datagrams addressed to that multicast address. the difficulty that we must still address is the fact that each host has a unique ip unicast address that is completely independent of the address of the multicast group in which it is participating.
while the multicast group abstraction is simple, it raises a host (pun intended) of questions. how does a group get started and how does it terminate? how is the group address chosen? how are new hosts added to the group (either as senders or receivers)? can anyone join a group (and send to, or receive from, that group) or is group membership restricted and, if so, by whom? do group members know the identities of the other group members as part of the network-layer protocol? how do the network nodes interoperate with each other to deliver a multicast datagram to  all group members? for the internet, the answers to all of these questions involve the internet group management protocol [rfc 3376]. so, let us next briefly consider igmp and then return to these broader questions.
input-queued switch—a queued packet in an input queue must wait for transfer through the fabric (even though its output port is free) because it is blocked by another packet at the head of the line. [karol 1987] shows that due to hol blocking, the input queue will grow to unbounded length (informally, this is equivalent to saying that significant packet loss will occur) under certain assumptions as soon as the packet arrival rate on the input links reaches only 58 percent of their capacity. a number of solutions to hol blocking are discussed in [mckeown 1997b].
4.3.5 the routing control plane in our discussion thus far and in figure 4.6, we’ve implicitly assumed that the routing control plane fully resides and executes in a routing processor within the router. the network-wide routing control plane is thus decentralized—with different pieces (e.g., of a routing algorithm) executing at different routers and interacting by sending control messages to each other. indeed, today’s internet routers and the routing algorithms we’ll study in section 4.6 operate in exactly this manner. additionally, router and switch vendors bundle their hardware data plane and software control plane together into closed (but inter-operable) platforms in a vertically integrated product.
recently, a number of researchers [caesar 2005a, casado 2009, mckeown 2008] have begun exploring new router control plane architectures in which part of the control plane is implemented in the routers (e.g., local measurement/reporting of link state, forwarding table installation and maintenance) along with the data plane, and part of the control plane can be implemented externally to the router (e.g., in a centralized server, which could perform route calculation). a well-defined api dictates  how  these  two  parts  interact  and  communicate  with  each  other.  these researchers argue that separating the software control plane from the hardware data plane (with a minimal router-resident control plane) can simplify routing by replacing distributed routing calculation with centralized routing calculation, and enable network innovation by allowing different customized control planes to operate over fast hardware data planes.
our  discussion  of  network-layer  addressing  and  forwarding  thus  far  has  been without reference to any specific computer network. in this section, we’ll turn our attention to how addressing and forwarding are done in the internet. we’ll see that internet addressing and forwarding are important components of the internet protocol (ip). there are two versions of ip in use today. we’ll first examine the widely deployed ip protocol version 4, which is usually referred to simply as ipv4
two hosts enjoy the security services provided by ipsec. on the sending side, the transport layer passes a segment to ipsec. ipsec then encrypts the segment, appends additional security fields to the segment, and encapsulates the resulting payload in an ordinary ip datagram. (it’s actually a little more complicated than this, as we’ll see in chapter 8.) the sending host then sends the datagram into the internet, which transports it to the destination host. there, ipsec decrypts the segment and passes the unencrypted segment to the transport layer.
• encryption of ip datagram payloads. when the sending host receives a segment from the transport layer, ipsec encrypts the payload. the payload can only be decrypted by ipsec in the receiving host.
• data integrity. ipsec allows the receiving host to verify that the datagram’s header fields and encrypted payload were not modified while the datagram was en route from source to destination.
• origin authentication. when a host receives an ipsec datagram from a trusted source (with a trusted key—see chapter 8), the host is assured that the source ip address in the datagram is the actual source of the datagram.
when two hosts have an ipsec session established between them, all tcp and udp segments sent between them will be encrypted and authenticated. ipsec therefore provides blanket coverage, securing all communication between the two hosts for all network applications.
a company can use ipsec to communicate securely in the nonsecure public internet. for illustrative purposes, we’ll just look at a simple example here. consider a company that has a large number of traveling salespeople, each possessing a company laptop computer. suppose the salespeople need to frequently consult sensitive company information (for example, pricing and product information) that is stored on a server in the company’s headquarters. further suppose that the salespeople also need to send sensitive documents to each other. how can this be done with ipsec? as you might guess, we install ipsec in the server and in all of the salespeople’s laptops. with ipsec installed in these hosts, whenever a salesperson needs to communicate with the server or with another salesperson, the communication session will be secure.
4.5 routing algorithms so far in this chapter, we’ve mostly explored the network layer’s forwarding function. we learned that when a packet arrives to a router, the router indexes a forwarding table and determines the link interface to which the packet is to be directed. we also learned that routing algorithms, operating in network routers, exchange and
addressing processes in order to send postal mail to a particular destination, the destination needs to have an address. similarly, in order for a process running on one host to send packets to a process running on another host, the receiving process needs to have an address.  to identify the receiving process, two pieces of information need to be specified: (1) the address of the host and (2) an identifier that specifies the receiving process in the destination host.
in  the  internet,  the  host  is  identified  by  its  ip address. we’ll  discuss  ip addresses in great detail in chapter 4. for now, all we need to know is that an ip address is a 32-bit quantity that we can think of as uniquely identifying the host. in addition to knowing the address of the host to which a message is destined, the sending process must also identify the receiving process (more specifically, the receiving socket) running in the host. this information is needed because in general a host could be running many network applications. a destination port number serves this purpose. popular applications have been assigned specific port numbers. for example, a web server is identified by port number 80. a mail server process (using the smtp protocol) is identified by port number 25. a list of well-known port numbers for all internet standard protocols can be found at http://www.iana.org. we’ll examine port numbers in detail in chapter 3.
obtaining a block of addresses in order to obtain a block of ip addresses for use within an organization’s subnet, a network administrator might first contact its isp, which would provide addresses from a larger block of addresses that had already been allocated to the isp. for example, the isp may itself have been allocated the address block 200.23.16.0/20. the isp, in turn, could divide its address block into eight equal-sized contiguous address blocks and give one of these address blocks out to each of up to eight organizations that are supported by this isp, as shown below. (we have underlined the subnet part of these addresses for your convenience.)
while obtaining a set of addresses from an isp is one way to get a block of addresses, it is not the only way. clearly, there must also be a way for the isp itself to get a block of addresses. is there a global authority that has ultimate responsibility for managing the ip address space and allocating address blocks to isps and other organizations? indeed there is! ip addresses are managed under the authority of the internet corporation for assigned names and numbers (icann) [icann 2012], based on guidelines set forth in [rfc 2050]. the role of the nonprofit icann organization [ntia 1998] is not only to allocate ip addresses, but also to manage the dns root servers. it also has the very contentious job of assigning domain names and resolving domain name disputes. the icann allocates addresses to regional internet registries (for example, arin, ripe, apnic, and lacnic, which together form the address supporting organization of icann [aso-icann 2012]), and handle the allocation/management of addresses within their regions.
obtaining a host address: the dynamic host configuration protocol once an organization has obtained a block of addresses, it can assign individual ip addresses to the host and router interfaces in its organization. a system administrator  will  typically  manually  configure  the  ip addresses  into  the  router  (often remotely, with a network management tool). host addresses can also be configured manually, but more often this task is now done using the dynamic host configuration protocol (dhcp) [rfc 2131]. dhcp allows a host to obtain (be allocated) an ip address automatically. a network administrator can configure dhcp so that a
base station,  known  as  an access point (ap) in  802.11  parlance.  figure  6.7 shows the ap in each of two bsss connecting to an interconnection device (such as a switch or router), which in turn leads to the internet. in a typical home network, there is one ap and one router (typically integrated together as one unit) that connects the bss to the internet.
as  with  ethernet  devices,  each  802.11  wireless  station  has  a  6-byte  mac address that is stored in the firmware of the station’s adapter (that is, 802.11 network interface card). each ap also has a mac address for its wireless interface. as with ethernet, these mac addresses are administered by ieee and are (in theory) globally unique.
as noted in section 6.1, wireless lans that deploy aps are often referred to as infrastructure wireless lans, with the “infrastructure” being the aps along with the wired ethernet infrastructure that interconnects the aps and a router. figure 6.8 shows that ieee 802.11 stations can also group themselves together to form an ad hoc network—a network with no central control and with no connections to the “outside world.” here, the network is formed “on the fly,” by mobile devices that have found themselves in proximity to each other, that have a need to communicate, and that find no preexisting network infrastructure in their location. an ad hoc network might be formed when people with laptops get together  (for  example,  in  a  conference  room,  a  train,  or  a  car)  and  want  to exchange data in the absence of a centralized ap. there has been tremendous interest in ad hoc networking, as communicating portable devices continue to proliferate. in this section, though, we’ll focus our attention on infrastructure wireless lans.
request that the buffered frames be sent by sending a polling message to the ap. with an inter-beacon time of 100 msec, a wakeup time of 250 microseconds, and a similarly small time to receive a beacon frame and check to ensure that there are no buffered frames, a node that has no frames to send or receive can be asleep 99% of the time, resulting in a significant energy savings.
6.3.6 personal area networks: bluetooth and zigbee as illustrated in figure 6.2, the ieee 802.11 wifi standard is aimed at communication among devices separated by up to 100 meters (except when 802.11 is used in a point-to-point configuration with a directional antenna). two other ieee 802 protocols—bluetooth and zigbee (defined in the ieee 802.15.1 and ieee 802.15.4 standards [ieee 802.15 2012]) and wimax (defined in the ieee 802.16 standard [ieee 802.16d 2004; ieee 802.16e 2005])—are standards for communicating over shorter and longer distances, respectively. we will touch on wimax briefly when we discuss cellular data networks in section 6.4, and so here, we will focus on networks for shorter distances.
bluetooth an ieee 802.15.1 network operates over a short range, at low power, and at low cost. it is essentially a low-power, short-range, low-rate “cable replacement” technology for interconnecting notebooks, peripheral devices, cellular phones, and smartphones,  whereas  802.11  is  a  higher-power,  medium-range,  higher-rate  “access” technology. for this reason, 802.15.1 networks are sometimes referred to as wireless personal area networks (wpans). the link and physical layers of 802.15.1 are based on the earlier bluetooth specification for personal area networks [held 2001, bisdikian 2001]. 802.15.1 networks operate in the 2.4 ghz unlicensed radio band in a tdm manner, with time slots of 625 microseconds. during each time slot, a sender transmits on one of 79 channels, with the channel changing in a known but pseudorandom  manner  from  slot  to  slot.  this  form  of  channel  hopping,  known  as frequency-hopping spread spectrum (fhss), spreads transmissions in time over the frequency spectrum. 802.15.1 can provide data rates up to 4 mbps.
802.15.1 networks are ad hoc networks: no network infrastructure (e.g., an access point) is needed to interconnect 802.15.1 devices. thus, 802.15.1 devices must organize themselves. 802.15.1 devices are first organized into a piconet of up to eight active devices, as shown in figure 6.16. one of these devices is designated as the master, with the remaining devices acting as slaves. the master node truly rules the piconet—its clock determines time in the piconet, it can transmit in each odd-numbered slot, and a slave can transmit only after the master has communicated with it in the previous slot and even then the slave can only transmit to the master. in addition to the slave devices, there can also be up to 255 parked devices in the
distance of the base station, and (2) the host uses that base station to relay data between it (the host) and the larger network. cell towers in cellular networks and access points in 802.11 wireless lans are examples of base stations. in figure 6.1, the base station is connected to the larger network (e.g., the internet, corporate or home network, or telephone network), thus functioning as a link-layer relay between the wireless host and the rest of the world with which the host communicates. hosts  associated  with  a  base  station  are  often  referred  to  as  operating  in infrastructure mode, since all traditional network services (e.g., address assignment and routing) are provided by the network to which a host is connected via the base station. in ad hoc networks, wireless hosts have no such infrastructure with which to connect. in the absence of such infrastructure, the hosts themselves must provide for services such as routing, address assignment, dns-like name translation, and more. when a mobile host moves beyond the range of one base station and into the range of another, it will change its point of attachment into the larger network (i.e., change the base station with which it is associated)—a process referred to as handoff. such mobility raises many challenging questions. if a host can move, how does one find the mobile host’s current location in the network so that data can be forwarded to that mobile host? how is addressing performed, given that a host can be in one of many possible locations? if the host moves during a tcp
• registration with the home agent. mobile ip defines the protocols used by the mobile node and/or foreign agent to register and deregister coas with a mobile node’s home agent. indirect routing of datagrams. the standard also defines the manner in which datagrams are forwarded to mobile nodes by a home agent, including rules for forwarding datagrams, rules for handling error conditions, and several forms of encapsulation [rfc 2003, rfc 2004].
security considerations are prominent throughout the mobile ip standard. for example, authentication of a mobile node is clearly needed to ensure that a malicious user does not register a bogus care-of address with a home agent, which could cause all datagrams addressed to an ip address to be redirected to the malicious user. mobile ip achieves security using many of the mechanisms that we will examine in chapter 8, so we will not address security considerations in our discussion below.
agent discovery a mobile ip node arriving to a new network, whether attaching to a foreign network or returning to its home network, must learn the identity of the corresponding foreign or home agent. indeed it is the discovery of a new foreign agent, with a new network address, that allows the network layer in a mobile node to learn that it has moved into a new foreign network. this process is known as agent discovery. agent discovery can be accomplished in one of two ways: via agent advertisement or via agent solicitation.
with agent advertisement, a foreign or home agent advertises its services using an extension to the existing router discovery protocol [rfc 1256]. the agent periodically broadcasts an icmp message with a type field of 9 (router discovery) on all links to which it is connected. the router discovery message contains the ip address of the router (that is, the agent), thus allowing a mobile node to  learn  the  agent’s  ip address. the  router  discovery  message  also  contains  a mobility  agent  advertisement  extension  that  contains  additional  information needed by the mobile node. among the more important fields in the extension are the following:
• registration required bit (r). indicates that a mobile user in this network must register with a foreign agent. in particular, a mobile user cannot obtain a careof address in the foreign network (for example, using dhcp) and assume the
• care-of address (coa) fields. a list of one or more care-of addresses provided by the foreign agent. in our example below, the coa will be associated with the foreign agent, who will receive datagrams sent to the coa and then forward them to the appropriate mobile node. the mobile user will select one of these addresses as its coa when registering with its home agent.
with agent solicitation, a mobile node wanting to learn about agents without waiting to receive an agent advertisement can broadcast an agent solicitation message, which is simply an icmp message with type value 10. an agent receiving the solicitation will unicast an agent advertisement directly to the mobile node, which can then proceed as if it had received an unsolicited advertisement.
with or without a foreign agent), multiple ways for agents and mobile nodes to discover each other, use of single or multiple coas, and multiple forms of encapsulation. as such, mobile ip is a complex standard, and would require an entire book to describe in detail; indeed one such book is [perkins 1998b]. our modest goal here is to provide an overview of the most important aspects of mobile ip and to illustrate its use in a few common-case scenarios.
the mobile ip architecture contains many of the elements we have considered above, including the concepts of home agents, foreign agents, care-of addresses, and encapsulation/decapsulation. the current standard [rfc 5944] specifies the use of indirect routing to the mobile node.
• agent discovery. mobile ip defines the protocols used by a home or foreign agent to advertise its services to mobile nodes, and protocols for mobile nodes to solicit the services of a foreign or home agent.
• care-of address (coa) fields. a list of one or more care-of addresses provided by the foreign agent. in our example below, the coa will be associated with the foreign agent, who will receive datagrams sent to the coa and then forward them to the appropriate mobile node. the mobile user will select one of these addresses as its coa when registering with its home agent.
with agent solicitation, a mobile node wanting to learn about agents without waiting to receive an agent advertisement can broadcast an agent solicitation message, which is simply an icmp message with type value 10. an agent receiving the solicitation will unicast an agent advertisement directly to the mobile node, which can then proceed as if it had received an unsolicited advertisement.
• care-of address (coa) fields. a list of one or more care-of addresses provided by the foreign agent. in our example below, the coa will be associated with the foreign agent, who will receive datagrams sent to the coa and then forward them to the appropriate mobile node. the mobile user will select one of these addresses as its coa when registering with its home agent.
with agent solicitation, a mobile node wanting to learn about agents without waiting to receive an agent advertisement can broadcast an agent solicitation message, which is simply an icmp message with type value 10. an agent receiving the solicitation will unicast an agent advertisement directly to the mobile node, which can then proceed as if it had received an unsolicited advertisement.
self-learning a switch has the wonderful property (particularly for the already-overworked network  administrator)  that  its  table  is  built  automatically,  dynamically,  and autonomously—without any intervention from a network administrator or from a configuration protocol. in other words, switches are self-learning. this capability is accomplished as follows:
1. the switch table is initially empty. 2. for each incoming frame received on an interface, the switch stores in its table (1) the mac address in the frame’s source address field, (2) the interface from which the frame arrived, and (3) the current time. in this manner the switch records in its table the lan segment on which the sender resides. if every host in the lan eventually sends a frame, then every host will eventually get recorded in the table.
3. the switch deletes an address in the table if no frames are received with that address as the source address after some period of time (the aging time). in this manner, if a pc is replaced by another pc (with a different adapter), the mac address of the original pc will eventually be purged from the switch table.
let’s  walk  through  the  self-learning  property  for  the  uppermost  switch  in  figure 5.15 and its corresponding switch table in figure 5.22. suppose at time 9:39 a frame with source address 01-12-23-34-45-56 arrives from interface 2. suppose that this address is not in the switch table. then the switch adds a new entry to the table, as shown in figure 5.23.
continuing with this same example, suppose that the aging time for this switch is 60 minutes, and no frames with source address 62-fe-f7-11-89-a3 arrive to the switch between 9:32 and 10:32. then at time 10:32, the switch removes this address from its table.
we have just given a high-level overview of how an institution can employ ipsec to create a vpn. to see the forest through the trees, we have brushed aside many important details. let’s now take a closer look.
8.7.2 the ah and esp protocols ipsec is a rather complex animal—it is defined in more than a dozen rfcs. two important rfcs are rfc 4301, which describes the overall ip security architecture, and rfc 6071, which provides an overview of the ipsec protocol suite. our goal in this textbook, as usual, is not simply to re-hash the dry and arcane rfcs, but instead take a more operational and pedagogic approach to describing the protocols.
in the ipsec protocol suite, there are two principal protocols: the authentication header (ah) protocol and the encapsulation security payload (esp) protocol. when a source ipsec entity (typically a host or a router) sends secure datagrams to a destination entity (also a host or a router), it does so with either the ah protocol or the esp protocol. the ah protocol provides source authentication and data integrity but does not provide confidentiality. the esp protocol provides source authentication, data integrity, and confidentiality. because confidentiality is often critical for vpns and other ipsec applications, the esp protocol is much more widely used than the ah protocol. in order to de-mystify ipsec and avoid much of its complication, we will henceforth focus exclusively on the esp protocol. readers wanting to learn also about the ah protocol are encouraged to explore the rfcs and other online resources.
8.7.3 security associations ipsec datagrams are sent between pairs of network entities, such as between two hosts, between two routers, or between a host and router. before sending ipsec datagrams from source entity to destination entity, the source and destination entities create a network-layer logical connection. this logical connection is called a security association (sa). an sa is a simplex logical connection; that is, it is unidirectional from source to destination. if both entities want to send secure datagrams to each other, then two sas (that is, two logical connections) need to be established, one in each direction.
for example, consider once again the institutional vpn in figure 8.27. this institution consists of a headquarters office, a branch office and, say, n traveling salespersons. for the sake of example, let’s suppose that there is bi-directional ipsec traffic between headquarters and the branch office and bi-directional ipsec traffic between headquarters and the salespersons. in this vpn, how many sas are there? to answer this question, note that there are two sas between the headquarters gateway router and the branch-office gateway router (one in each direction); for each salesperson’s laptop, there are two sas between the headquarters gateway router and the laptop (again, one in each direction). so, in total, there are (2 + 2n) sas. keep in mind, however, that not all traffic sent into the internet by the gateway routers or by the laptops will be ipsec secured. for example, a host in headquarters may want to access a web server (such as amazon or google) in the public internet. thus, the gateway router (and the laptops) will emit into the internet both vanilla ipv4 datagrams and secured ipsec datagrams.
0.5 • cwnd = 6 • mss. under tcp reno, the congestion window is set to cwnd = 6 • mss and then grows linearly. under tcp tahoe, the congestion window is set to 1 mss and grows exponentially until it reaches the value of ssthresh, at which point it grows linearly. 
figure  3.52  presents  the  complete  fsm  description  of  tcp’s  congestioncontrol algorithms—slow start, congestion avoidance, and fast recovery. the figure also indicates where transmission of new segments or retransmitted segments can occur. although it is important to distinguish between tcp error control/retransmission and tcp congestion control, it’s also important to appreciate how these two aspects of tcp are inextricably linked.
tcp congestion control: retrospective having delved into the details of slow start, congestion avoidance, and fast recovery, it’s worthwhile to now step back and view the forest from the trees. ignoring the initial slow-start period when a connection begins and assuming that losses are indicated by triple duplicate acks rather than timeouts, tcp’s congestion control consists of linear (additive) increase in cwnd of 1 mss per rtt and then a halving (multiplicative decrease) of cwnd on a triple duplicate-ack event. for this reason, tcp congestion control is often referred to as an additive-increase, multiplicativedecrease (aimd) form of congestion control. aimd congestion control gives rise to the “saw tooth” behavior shown in figure 3.54, which also nicely illustrates our earlier intuition of tcp “probing” for bandwidth—tcp linearly increases its congestion window size (and hence its transmission rate) until a triple duplicate-ack event occurs. it then decreases its congestion window size by a factor of two but then again begins increasing it linearly, probing to see if there is additional available bandwidth.
as noted previously, many tcp implementations use the reno algorithm [padhye 2001]. many variations of the reno algorithm have been proposed [rfc 3782; rfc 2018]. the tcp vegas algorithm [brakmo 1995; ahn 1995] attempts to avoid congestion while maintaining good throughput. the basic idea of vegas is to (1) detect congestion in the routers between source and destination before packet loss occurs, and (2) lower the rate linearly when this imminent packet loss is detected. imminent packet loss is predicted by observing the rtt. the longer the rtt of the packets, the greater the congestion in the routers. linux supports a number of congestion-control algorithms (including tcp reno and tcp vegas) and allows a system administrator to configure which version of tcp will be used. the default version of tcp in linux version 2.6.18 was set to cubic [ha 2008], a version of tcp developed for high-bandwidth applications. for a recent survey of the many flavors of tcp, see [afanasyev 2010].
tcp’s aimd algorithm was developed based on a tremendous amount of engineering insight and experimentation with congestion control in operational networks. ten years after tcp’s development, theoretical analyses showed that tcp’s congestion-control  algorithm  serves  as  a  distributed  asynchronous-optimization algorithm that results in several important aspects of user and network performance being simultaneously optimized [kelly 1998]. a rich theory of congestion control has since been developed [srikant 2004].
macroscopic description of tcp throughput given the saw-toothed behavior of tcp, it’s natural to consider what the average throughput (that is, the average rate) of a long-lived tcp connection might be. in this analysis we’ll ignore the slow-start phases that occur after timeout events. (these phases are typically very short, since the sender grows out of the phase exponentially fast.) during a particular round-trip interval, the rate at which tcp sends data is a function of the congestion window and the current rtt. when the window size is w bytes and the current round-trip time is rtt seconds, then tcp’s transmission rate is roughly w/rtt. tcp then probes for additional bandwidth by increasing w by 1 mss each rtt until a loss event occurs. denote by w the value of w when a loss event occurs. assuming that rtt and w are approximately constant over the duration of the connection, the tcp transmission rate ranges from w/(2 · rtt) to w/rtt.
these  assumptions  lead  to  a  highly  simplified  macroscopic  model  for  the steady-state behavior of tcp. the network drops a packet from the connection when the  rate  increases  to  w/rtt; the  rate  is  then  cut  in  half  and  then  increases  by mss/rtt every rtt until it again reaches w/rtt. this process repeats itself over and over again. because tcp’s throughput (that is, rate) increases linearly between the two extreme values, we have
that for each connection, all the other links along the connection’s path are not congested and have abundant transmission capacity as compared with the transmission capacity of the bottleneck link.) suppose each connection is transferring a large file and there is no udp traffic passing through the bottleneck link. a congestion-control mechanism is said to be fair if the average transmission rate of each connection is approximately r/k; that is, each connection gets an equal share of the link bandwidth.
is tcp’s aimd algorithm fair, particularly given that different tcp connections may start at different times and thus may have different window sizes at a given point in time? [chiu 1989] provides an elegant and intuitive explanation of why tcp congestion control converges to provide an equal share of a bottleneck link’s bandwidth among competing tcp connections.
let’s consider the simple case of two tcp connections sharing a single link with transmission rate r, as shown in figure 3.55. assume that the two connections have the same mss and rtt (so that if they have the same congestion window size, then they have the same throughput), that they have a large amount of data to send, and that no other tcp connections or udp datagrams traverse this shared link. also, ignore the slow-start phase of tcp and assume the tcp connections are operating in ca mode (aimd) at all times.
figure 3.56 plots the throughput realized by the two tcp connections. if tcp is to share the link bandwidth equally between the two connections, then the realized throughput should fall along the 45-degree arrow (equal bandwidth share) emanating from the origin. ideally, the sum of the two throughputs should equal r. (certainly, each connection receiving an equal, but zero, share of the link capacity is not a desirable situation!) so the goal should be to have the achieved throughputs fall somewhere near the intersection of the equal bandwidth share line and the full bandwidth utilization line in figure 3.56.
suppose that the tcp window sizes are such that at a given point in time, connections 1 and 2 realize throughputs indicated by point a in figure 3.56. because the amount of link bandwidth jointly consumed by the two connections is less than
through the use of content distribution networks (cdns), web caches are increasingly playing an important role in the internet. a cdn company installs many geographically distributed caches throughout the internet, thereby localizing much of the traffic. there are shared cdns (such as akamai and limelight) and dedicated cdns (such as google and microsoft). we will discuss cdns in more detail in chapter 7.
2.2.6 the conditional get although caching can reduce user-perceived response times, it introduces a new problem—the copy of an object residing in the cache may be stale. in other words, the object housed in the web server may have been modified since the copy was cached at the client. fortunately, http has a mechanism that allows a cache to verify that its objects are up to date. this mechanism is called the conditional get. an http
dns rotation is also used for e-mail so that multiple mail servers can have the same alias name. also, content distribution companies such as akamai have used dns in more sophisticated ways [dilley 2002] to provide web content distribution (see chapter 7).
the dns is specified in rfc 1034 and rfc 1035, and updated in several additional rfcs. it is a complex system, and we only touch upon key aspects of its operation here. the interested reader is referred to these rfcs and the book by albitz and liu [albitz 1993]; see also the retrospective paper [mockapetris 1988],  which  provides  a  nice  description  of  the  what  and  why  of  dns,  and [mockapetris 2005].
2.5.2 overview of how dns works we now present a high-level overview of how dns works. our discussion will focus on the hostname-to-ip-address translation service.
suppose that some application (such as a web browser or a mail reader) running in a user’s host needs to translate a hostname to an ip address. the application will invoke the client side of dns, specifying the hostname that needs to be translated. (on many unix-based machines, gethostbyname() is the function call that an application calls in order to perform the translation.) dns in the user’s host then takes over, sending a query message into the network. all dns query and reply messages are sent within udp datagrams to port 53. after a delay, ranging from milliseconds to seconds, dns in the user’s host receives a dns reply message that provides the desired mapping. this mapping is then passed to the invoking application. thus, from the perspective of the invoking application in the user’s host, dns is a black box providing a simple, straightforward translation service. but in fact, the black box that implements the service is complex, consisting of a large number of dns servers distributed around the globe, as well as an application-layer protocol that specifies how the dns servers and querying hosts communicate.
a simple design for dns would have one dns server that contains all the mappings. in this centralized design, clients simply direct all queries to the single dns server, and the dns server responds directly to the querying clients. although the simplicity of this design is attractive, it is inappropriate for today’s internet, with its vast  (and  growing)  number  of  hosts.  the  problems  with  a  centralized  design include:
• a single point of failure. if the dns server crashes, so does the entire internet! • traffic volume. a single dns server would have to handle all dns queries (for all the http requests and e-mail messages generated from hundreds of millions of hosts).
accounts and capture credit-card payment information. except for these basic functions, netflix runs its online service by employing machines (or virtual machines) in the amazon cloud. some of the functions taking place in the amazon cloud include:
• content ingestion. before netflix can distribute a movie to its customers, it must first ingest and process the movie. netflix receives studio master versions of movies and uploads them to hosts in the amazon cloud.
• content processing. the machines in the amazon cloud create many different formats for each movie, suitable for a diverse array of client video players running on desktop computers, smartphones, and game consoles connected to televisions. a different version is created for each of these formats and at multiple bit rates, allowing for adaptive streaming over http using dash.
to deliver the movies to its customers on demand, netflix makes extensive use of cdn technology. in fact, as of this writing in 2012, netflix employs not one but three third-party cdn companies at the same time—akamai, limelight, and level-3.
having described the components of the netflix architecture, let’s take a closer look at the interaction between the client and the various servers that are involved in
google’s network infrastructure to support its vast array of cloud services—including search, gmail, calendar, youtube video, maps, documents, and social networks—google has deployed an extensive private network and cdn infrastructure. google’s cdn infrastructure has three tiers of server clusters:
ed in europe [google locations 2012], with each data center having on the order of 100,000 servers. these mega data centers are responsible for serving dynamic (and often personalized) content, including search results and gmail messages.
sisting on the order of 100–500 servers [adhikari 2011a]. the cluster locations are distributed around the world, with each location typically near multiple tier-1 isp pops. these clusters are responsible for serving static content, including youtube videos [adhikari 2011a].
cluster located within an access isp. here a cluster typically consists of tens of servers within a single rack. these enter-deep servers perform tcp splitting (see section 3.7) and serve static content [chen 2011], including the static portions of web pages that embody search results.
all of these data centers and cluster locations are networked together with google’s own private network, as part of one enormous as (as 15169). when a user makes a search query, often the query is first sent over the local isp to a nearby enter-deep cache, from where the static content is retrieved; while providing the static content to the client, the nearby cache also forwards the query over google’s private network to one of the mega data centers, from where the personalized search results are retrieved. for a youtube video, the video itself may come from one of the bring-home caches, whereas portions of the web page surrounding the video may come from the nearby enter-deep cache, and the advertisements surrounding the video come from the data centers. in summary, except for the local isps, the google cloud services are largely provided by a network infrastructure that is independent of the public internet.
akamai takes this approach with clusters in approximately 1,700 locations. the goal is to get close to end users, thereby improving user-perceived delay and throughput by decreasing the number of links and routers between the end user and the cdn cluster from which it receives content. because of this highly distributed design, the task of maintaining and managing the clusters becomes challenging.
• bring home. a second design philosophy, taken by limelight and many other cdn companies, is to bring the isps home by building large clusters at a smaller number (for example, tens) of key locations and connecting these clusters using a private high-speed network. instead of getting inside the access isps, these cdns typically place each cluster at a location that is simultaneously near the pops (see section 1.3) of many tier-1 isps, for example, within a few miles of both at&t and verizon pops in a major city. compared with the enter-deep design philosophy, the bring-home design typically results in lower maintenance and management overhead, possibly at the expense of higher delay and lower throughput to end users.
once its clusters are in place, the cdn replicates content across its clusters. the cdn may not want to place a copy of every video in each cluster, since some videos are rarely viewed or are only popular in some countries. in fact, many cdns do not push  videos  to  their  clusters  but  instead  use  a  simple  pull  strategy:  if  a  client requests a video from a cluster that is not storing the video, then the cluster retrieves the video (from a central repository or from another cluster) and stores a copy locally while streaming the video to the client at the same time. similar to internet caches (see chapter 2), when a cluster’s storage becomes full, it removes videos that are not frequently requested.
cdn operation having identified the two major approaches toward deploying a cdn, let’s now dive down into the nuts and bolts of how a cdn operates. when a browser in  a user’s host is instructed to retrieve a specific video (identified by a url), the cdn  must  intercept  the  request  so  that  it  can  (1)  determine  a  suitable  cdn server cluster for that client at that time, and (2) redirect the client’s request to  a server in that cluster. we’ll shortly discuss how a cdn can determine a suitable cluster. but first let’s examine the mechanics behind intercepting and redirecting a request.
most cdns take advantage of dns to intercept and redirect requests; an interesting discussion of such a use of the dns is [vixie 2009]. let’s consider a simple example to illustrate how dns is typically involved. suppose a content provider, netcinema,  employs  the  third-party  cdn  company,  kingcdn,  to  distribute  its videos to its customers. on the netcinema web pages, each of its videos is assigned a url that includes the string “video” and a unique identifier for the video itself; for example, transformers 7 might be assigned http://video.netcinema.com/6y7b23v. six steps then occur, as shown in figure 7.4:
