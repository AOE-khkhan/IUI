matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
much  of  the  material  in  this  section  falls  within  the  domain  of  numerical  analysis,  and  several  excellent  textbooks  are  available.  one  which  pays particular  attention  to  computational  issues  is  the  1977  book  by  forsythe, malcomb  and  moler.  in  particular,  much  of  the  material  given  here  in  chapters 5,  6,  and  7  is  based  on  the  presentation  given  in  that  book.
the  second  major  reference  for  this  section  is  the  second  volume  of  d.  e. knuth’s  comprehensive  treatment  of  “the  art  of  computer  programming.” knuth  uses  the  term  “seminumerical”  to  describe  algorithms  which  lie  at the  interface  between  numerical  and  symbolic  computation,  such  as  random number  generation  and  polynomial  arithmetic.  among  many  other  topics, knuths  volume  2  covers  in  great  depth  the  material  given  here  in  chapters 1,  3,  and  4.  the  1975  book  by  borodin  and  munro  is  an  additional  reference for  strassen’s  matrix  multiplication  method  and  related  topics.  many  of the  algorithms  that  we’ve  considered  (and  many  others,  principally  symbolic methods  as  mentioned  in  chapter  7)  are  embodied  in  a  computer  system  called macsyma,  which  is  regularly  used  for  serious  mathematical  work.
certainly,  a  reader  seeking  more  information  on  mathematical  algorithms should  expect  to  find  the  topics  treated  at  a  much  more  advanced  mathematical  level  in  the  references  than  the  material  we’ve  considered  here.
chapter  2  is  concerned  with  elementary  data  structures,  as  well  as  polynomials.  beyond  the  references  mentioned  in  the  previous  part,  a  reader  interested  in  learning  more  about  this  subject  might  study  how  elementary  data structures  are  handled  in  modern  programming  languages  such  as  ada,  which have  facilities  for  building  abstract  data  structures.
a.  borodin  and  i.  munro,  the  computational  complexity  of  algebraic  and numerical  problems,  american  elsevier,  new  york,  1975. g.  e.  forsythe,  m.  a.  malcomb,  and  c.  b.  moler,  computer  methods  for mathematical  computations,  prentice-hall,  englewood  cliffs,  nj,  1977. d.  e.  knuth,  the  art  of  computer  programming.  volume  &:   seminumerical algorithms,  addison-wesley,  reading,  ma  (second  edition),  1981. mit  mathlab  group,  macsyma  reference  manual,  laboratory  for  computer  science,  massachusetts  institute  of  technology,  1977. p.  wegner,  programming  with  ada:  an  introduction  by  means  of  graduated examples,  prentice-hall,  englewood  cliffs,  nj,  1980.
not  particularly  random.  this  leads  to  a  common  and  serious  mistake  in  the use  of  linear  congruential  random  number  generators:  the  following  is  a  bad program  for  producing  random  numbers  in  the  range  [0,  r  -  11:
the  non-random  digits  on  the  right  are  the  only  digits  that  are  used, so  the  resulting  sequence  has  few  of  the  desired  properties.  this  problem  is easily  fixed  by  using  the  digits  on  the  left.  we  want  to  compute  a  number between  0  and  r-l  by  computing  a*r   mod  m,  but,  again,  overflow  must  be circumvented,  as  in  the  following  implementation:
another  common  technique  is  to  generate  random  real  numbers  between 0  and  1  by  treating  the  above  numbers  as  fractions  with  the  decimal  point to  the  left.  this  can  be  implemented  by  simply  returning  the  real  value  a/m rather  than  the  integer  a.  then  a  user  could  get  an  integer  in  the  range  [o,r) by  simply  multiplying  this  value  by  r  and  truncating  to  the  nearest  integer. or,  a  random  real  number  between  0  and  1  might  be  exactly  what  is  needed.
additive  congruential  method another  method  for  generating  random  numbers  is  based  on  linear  feedback shift registers  which  were  used  for  early  cryptographic  encryption  machines. the  idea  is  to  start  with  a  register  filled  with  some  arbitrary  pattern,  then shift  it  right  (say)  a  step  at  a  time,  filling  in  vacated  positions  from  the  left with  a  bit  determined  by  the  contents  of  the  register.  the  diagram  below shows  a  simple  4-bit  register,  with  the  new  bit  taken  as  the  “exclusive  or”  of the  two  rightmost  bits.
the  program  maintains  the  55  most  recently  generated  numbers,  with  the  last generated  pointed  to  by  j.  thus,  the  global  variable  a  has  been  replaced  by a  full  table  plus  a  pointer  (j)  into  it.  this  large  amount  of  “global  state”  is  a disadvantage of this generator in some applications, but it is also an advantage because  it  leads  to  an  extremely  long  cycle  even  if  the  modulus  m  is  small.
the  function  randomint  returns  a  random  integer  between  0  and  r-l.  of course,  it  can  easily  be  changed,  just  as  above,  to  a  function  which  returns  a random  real  number  between  0  and  1  (a  b]/m).
testing  randomness one  can  easily  detect  numbers  that  are  not  random,  but  certifying  that  a sequence  of  numbers  is  random  is  a  difficult  task  indeed.  as  mentioned  above, no  sequence  produced  by  a  computer  can  be  random,  but  we  want  a  sequence that  exhibits  many  of  the  properties  of  random  numbers.  unfortunately,  it  is often  not  possible  to  articulate  exactly  which  properties  of  random  numbers are  important  for  a  particular  application.
on  the  other  hand,  it  is  always  a  good  idea  to  perform  some  kind  of  test on  a  random  number  generator  to  be  sure  that  no  degenerate  situations  have turned  up.  random  number  generators  can  be  very,  very  good,  but  when they  are  bad  they  are  horrid.
many  tests  have  been  developed  for  determining  whether  a  sequence shares  various  properties  with  a  truly  random  sequence.  most  of  these  tests have  a  substantial  basis  in  mathematics,  and  it  would  definitely  be  beyond  the scope  of  this  book  to  examine  them  in  detail.  however,  one  statistical  test, the  x2  (chi-square)  test,  is  fundamental  in  nature,  quite  easy  to  implement, and  useful  in  several  applications,  so  we’ll  examine  it  more  carefully.
the  idea  of  the  x2  test  is  to  check  whether  or  not  the  numbers  produced are  spread  out  reasonably.  if  we  generate  n  positive  numbers  less  than  r, then
this  procedure  recursively  determines  the  highest  point  in  the  tree  reachable  (via  a  dotted  link)  from  any  descendant  of  vertex  k  and  uses  this  information  to  determine  if  k  is  an  articulation  point.  normally  this  calculation simply  involves  testing  whether  the  minimum  value  reachable  from  a  son  is higher  up  in  the  tree,  but  we  need  an  extra  test  to  determine  whether  k  is  the root  of  a  depth-first  search  tree  (or,  equivalently,  whether  this  is  the  first  call to visit for the connected component containing k), since we’re using the same recursive  program  for  both  cases.  this  test  is  properly  performed  outside  the recursive  visit,  so  it  does  not  appear  in  the  code  above.
the  program  above  simply  prints  out  the  articulation  points.  of  course, as  before,  it  is  easily  extended  to  do  additional  processing  on  the  articulation points  and  biconnected  components. also,  since  it  is  a  depth-first  search procedure,  the  running  time  is  proportional  to  v  +  e.
besides  the  “reliability”  sort  of  application  mentioned  above,  biconnectedness  can  be  helpful  in  decomposing  large  graphs  into  manageable  pieces.  it  is obvious  that  a  very  large  graph  may  be  processed  one  connected  component at  a  time  for  many  applications;  it  is  somewhat  less  obvious  but  sometimes  as useful  that  a  graph  can  sometimes  be  processed  one  biconnected  component at  a  time.
gorithm.”  note  that  prim’s  algorithm  takes  time  proportional  to  v2   even for  sparse  graphs  (a  factor  of  about  v2/e   1ogv   slower  than  the  priority-first search  solution,  and  that  the  priority-first  search  solution  is  a  factor  of  1ogv slower  than  prim’s  algorithm  for  dense  graphs.
a  completely  different  approach  to  finding  the  minimum  spanning  tree  is to  simply  add  edges  one  at  a  time,  at  each  step  using  the  shortest  edge  that does  not  form  a  cycle.  this  algorithm  gradually  builds  up  the  tree  one  edge  at a  time  from  disconnected  components,  as  illustrated  in  the  following  sequence of  diagrams  for  our  sample  graph:
the  code  for  this  method  can  be  pieced  together  from  programs  that we’ve  already  seen.  a  priority  queue  is  obviously  the  data  structure  to  use  to consider the edges in order of their weight, and the job of testing for cycles can be  obviously  done  with  union-find  structures.  the  appropriate  data  structure to use for the graph is simply an array edge with one entry for each edge. the indirect  priority  queue  procedures  pqconstruct  and  pqremove  from  chapter  11 can be used to maintain the priority queue, using the  weight fields in the edge array  for  priorities.  also,  the  program  uses  the  findinit  and  fastfind   procedures from  chapter  30.  the  program  simply  prints  out  the  edges  which  comprise the  spanning  tree;  with  slightly  more  work  a  dad  array  or  other  representation could  be  computed:
something  else.  this  seems  an  obvious  point  when  stated,  but  it’s  probably the  most  common  mistake  in  recursive  programming.  for  similar  reasons,  one shouldn’t  make  a  recursive  call  for  a  larger  problem,  since  that  might  lead  to a  loop  in  which  the  program  attempts  to  solve  larger  and  larger  problems.
not  all  programming  environments  support  a  general-purpose  recursion facility  because  of  intrinsic  difficulties  involved.  furthermore,  when  recursion is provided and used, it can be a source of unacceptable inefficiency. for these reasons,  we  often  consider  ways  of  removing  recursion.  this  is  quite  easy  to do when there is only one recursive call involved, as in the function above. we simply  replace  the  recursive  call  with  a  goto   to  the  beginning,  after  inserting some  assignment  statements  to  reset  the  values  of  the  parameters  as  directed by  the  recursive  call.  after  cleaning  up  the  program  left  by  these  mechanical transformations,  we  have  the  following  implementation  of  euclid’s  algorithm:
recursion  removal  is  much  more  complicated  when  there  is  more  than one  recursive  call.  the  algorithm  produced  is  sometimes  not  recognizable,  and indeed  is  very  often  useful  as  a  di.fferent   way  of  looking  at  a  fundamental  algorithm.  removing  recursion  almost  always  gives  a  more  efficient  implementation.  we’ll  see  many  examples  of  this  later  on  in  the  book.
analysis of algorithms in  this  short  chapter  we’ve  already  seen  three  different  algorithms  for  the  same problem;  for  most  problems  there  are  many  different  available  algorithms. how  is  one  to  choose  the  best  implementation  from  all  those  available?
this  is  actually  a  well  developed  area  of  study  in  computer  science. frequently,  we’ll  have  occasion  to  call  on  research  results  describing  the  performance  of  fundamental  algorithms.  however,  comparing  algorithms  can  be challenging  indeed,  and  certain  general  guidelines  will  be  useful.
usually  the  problems  that  we  solve  have  a  natural  “size”  (usually  the amount  of  data  to  be  processed;  in  the  above  example  the  magnitude  of the  numbers)  which  we’ll  normally  call  n.  we  would  like  to  know  the resources  used  (most  often  the  amount  of  time  taken)  as  a  function  of  n. we’re  interested  in  the  average  case,  the  amount  of  time  a  program  might  be
one  further  note  on  the  “log”  function.  as  mentioned  above,  the  base of  the  logarithm  changes  things  only  by  a  constant  factor.  since  we  usually deal  with  analytic  results  only  to  within  a  constant  factor,  it  doesn’t  matter much  what  the  base  is,  so  we  refer  to  “logn,”   etc.  on  the  other  hand, it  is  sometimes  the  case  that  concepts  can  be  explained  more  clearly  when some  specific  base  is  used.  in  mathematics,  the  natz~ral   logarithm  (base  e  = 2.718281828..  .)  arises  so  frequently  that  a  special  abbreviation  is  commonly used: log, n  =  in  n.  in  computer  science,  the  binary  logarithm  (base  2)  arises so  frequently  that  the  abbreviation  log,  n  =  lg  n  is  commonly  used.  for example,  lg  n  rounded  up  to  the  nearest  integer  is  the  number  of  bits  required to  represent  n  in  binary.
implementing  algorithms the  algorithms  that  we  will  discuss  in  this  book  are  quite  well  understood, but  for  the  most  part  we’ll  avoid  excessively  detailed  comparisons.  our  goal will  be  to  try  to  identify  those  algorithms  which  are  likely  to  perform  best  for a  given  type  of  input  in  a  given  application.
the  most  common  mistake  made  in  the  selection  of  an  algorithm  is  to ignore  performance  characteristics.  faster  algorithms  are  often  more  complicated,  and  implementors  are  often  willing  to  accept  a  slower  algorithm  to avoid  having  to  deal  with  added  complexity.  but  it  is  often  the  case  that a  faster  algorithm  is  really  not  much  more  complicated,  and  dealing  with slight  added  complexity  is  a  small  price  to  pay  to  avoid  dealing  with  a  slow algorithm.  users  of  a  surprising  number  of  computer  systems  lose  substantial time  waiting  for  simple  quadratic  algorithms  to  finish  when  only  slightly  more complicated  n  log  n  algorithms  are  available  which  could  run  in  a  fraction the  time.
the  second  most  common  mistake  made  in  the  selection  of  an  algorithm is  to  pay  too  much  attention  to  performance  characteristics.  an  n  log  n algorithm  might  be  only  slightly  more  complicated  than  a  quadratic  algorithm for  the  same  problem,  but  a  better  n  log  n  algorithm  might  give  rise  to  a substantial  increase  in  complexity  (and  might  actually  be  faster  only  for  very large  values  of  n).  also,  many  programs  are  really  run  only  a  few  times: the  time  required  to  implement  and  debug  an  optimized  algorithm  might  be substantially  more  than  the  time  required  simply  to  run  a  slightly  slower  one. the  programs  in  this  book  use  only  basic  features  of  pascal,  rather  than taking  advantage  of  more  advanced  capabilities  that  are  available  in  pascal and  other  programming  environments.  our  purpose  is  to  study  algorithms, not  systems  programming  nor  advanced  features  of  programming  languages.
multiplication,  and  division  have  a.  very  long  history,  dating  back  to the  origins  of  algorithm  studies  in  the  work  of  the  arabic  mathematician al-khowdrizmi,  with  roots  going  even  further  back  to  the  greeks  and  the babylonians.
  of  many computer  systems  is  their  capability  for  doing  fast,  accurate  numerical  calculations.  computers  have  built-in  capabilities  to  perform  arithmetic  on  integers  and  floating-point  representations  of  real  numbers;  for  example,  pascal allows  numbers  to  be  of  type  integer  or  re;d,   with  all  of  the  normal  arithmetic operations  defined  on  both  types.  algorithms  come  into  play  when  the  operations  must  be  performed  on  more  complicated  mathematical  objects,  such  as polynomials  or  matrices.
in  this  section,  we’ll  look  at  pascal  implementations  of  some  simple algorithms  for  addition  and  multiplication  of  polynomials  and  matrices.  the algorithms  themselves  are  well-known  and  straightforward;  we’ll  be  examining sophisticated  algorithms  for  these  problems  in  chapter  4.  our  main  purpose in  this  section  is  to  get  used  to  treating  th’ese   mathematical  objects  as  objects for  manipulation  by  pascal  programs.  this  translation  from  abstract  data  to something  which  can  be  processed  by  a  computer  is  fundamental  in  algorithm design.  we’ll  see  many  examples  throughout  this  book  in  which  a  proper representation  can  lead  to  an  efficient  algorithm  and  vice  versa.  in  this chapter,  we’ll  use  two  fundamental  ways  of  structuring  data,  the  array  and the  linked list.  these  data  structures  are  used  by  many  of  the  algorithms  in this  book;  in  later  sections  we’ll  study  some  more  advanced  data  structures.
matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
this  procedure  recursively  determines  the  highest  point  in  the  tree  reachable  (via  a  dotted  link)  from  any  descendant  of  vertex  k  and  uses  this  information  to  determine  if  k  is  an  articulation  point.  normally  this  calculation simply  involves  testing  whether  the  minimum  value  reachable  from  a  son  is higher  up  in  the  tree,  but  we  need  an  extra  test  to  determine  whether  k  is  the root  of  a  depth-first  search  tree  (or,  equivalently,  whether  this  is  the  first  call to visit for the connected component containing k), since we’re using the same recursive  program  for  both  cases.  this  test  is  properly  performed  outside  the recursive  visit,  so  it  does  not  appear  in  the  code  above.
the  program  above  simply  prints  out  the  articulation  points.  of  course, as  before,  it  is  easily  extended  to  do  additional  processing  on  the  articulation points  and  biconnected  components. also,  since  it  is  a  depth-first  search procedure,  the  running  time  is  proportional  to  v  +  e.
besides  the  “reliability”  sort  of  application  mentioned  above,  biconnectedness  can  be  helpful  in  decomposing  large  graphs  into  manageable  pieces.  it  is obvious  that  a  very  large  graph  may  be  processed  one  connected  component at  a  time  for  many  applications;  it  is  somewhat  less  obvious  but  sometimes  as useful  that  a  graph  can  sometimes  be  processed  one  biconnected  component at  a  time.
something  else.  this  seems  an  obvious  point  when  stated,  but  it’s  probably the  most  common  mistake  in  recursive  programming.  for  similar  reasons,  one shouldn’t  make  a  recursive  call  for  a  larger  problem,  since  that  might  lead  to a  loop  in  which  the  program  attempts  to  solve  larger  and  larger  problems.
not  all  programming  environments  support  a  general-purpose  recursion facility  because  of  intrinsic  difficulties  involved.  furthermore,  when  recursion is provided and used, it can be a source of unacceptable inefficiency. for these reasons,  we  often  consider  ways  of  removing  recursion.  this  is  quite  easy  to do when there is only one recursive call involved, as in the function above. we simply  replace  the  recursive  call  with  a  goto   to  the  beginning,  after  inserting some  assignment  statements  to  reset  the  values  of  the  parameters  as  directed by  the  recursive  call.  after  cleaning  up  the  program  left  by  these  mechanical transformations,  we  have  the  following  implementation  of  euclid’s  algorithm:
recursion  removal  is  much  more  complicated  when  there  is  more  than one  recursive  call.  the  algorithm  produced  is  sometimes  not  recognizable,  and indeed  is  very  often  useful  as  a  di.fferent   way  of  looking  at  a  fundamental  algorithm.  removing  recursion  almost  always  gives  a  more  efficient  implementation.  we’ll  see  many  examples  of  this  later  on  in  the  book.
analysis of algorithms in  this  short  chapter  we’ve  already  seen  three  different  algorithms  for  the  same problem;  for  most  problems  there  are  many  different  available  algorithms. how  is  one  to  choose  the  best  implementation  from  all  those  available?
this  is  actually  a  well  developed  area  of  study  in  computer  science. frequently,  we’ll  have  occasion  to  call  on  research  results  describing  the  performance  of  fundamental  algorithms.  however,  comparing  algorithms  can  be challenging  indeed,  and  certain  general  guidelines  will  be  useful.
usually  the  problems  that  we  solve  have  a  natural  “size”  (usually  the amount  of  data  to  be  processed;  in  the  above  example  the  magnitude  of the  numbers)  which  we’ll  normally  call  n.  we  would  like  to  know  the resources  used  (most  often  the  amount  of  time  taken)  as  a  function  of  n. we’re  interested  in  the  average  case,  the  amount  of  time  a  program  might  be
expected  to  take  on  “typical”  input  data,  and  in  the  worst case,  the  amount of  time  a  program  would  take  on  the  worst  possible  input  configuration.
many of the algorithms in this book are very well understood, to the point that  accurate  mathematical  formulas  are  known  for  the  average-  and  worstcase  running  time.  such  formulas  are  developed  first  by  carefully  studying the  program,  to  find  the  running  time  in  terms  of  fundamental  mathematical quantities  and  then  doing  a  mathematical  analysis  of  the  quantities  involved. for  some  algorithms,  it  is  easy  to  hgure  out  the  running  time.  for  example,  the  brute-force  algorithm  above  obviously  requires  min(u,   vu)-gcd(u,   v) iterations  of  the  while  loop,  and  this  quantity  dominates  the  running  time  if the  inputs  are  not  small,  since  all  the  other  statements  are  executed  either 0  or  1  times.  for  other  algorithms,  a  substantial  amount  of  analysis  is  involved.  for  example,  the  running  time  of  the  recursive  euclidean  algorithm obviously  depends  on  the  “overhead”  required  for  each  recursive  call  (which can  be  determined  only  through  detailed1  knowledge  of  the  programming  environment  being  used)  as  well  as  the  number  of  such  calls  made  (which  can be  determined  only  through  extremely  sophisticated  mathematical  analysis). several  important  factors  go  into  this  analysis  which  are  somewhat  outside  a  given  programmer’s  domain  of  influence.  first,  pascal  programs  are translated  into  machine  code  for  a  given  computer,  and  it  can  be  a  challenging task  to  figure  out  exactly  how  long  even  one  pascal  statement  might  take  to execute  (especially  in  an  environment  where  resources  are  being  shared,  so that  even  the  same  program  could  have  varying  performance  characteristics). second,  many  programs  are  extremely  sensitive  to  their  input  data,  and  performance  might  fluctuate  wildly  depending  on  the  input.  the  average  case might  be  a  mathematical  fiction  that  is  not  representative  of  the  actual  data on  which  the  program  is  being  used,  and  the  worst  case  might  be  a  bizarre construction  that  would  never  occur  in  practice.  third,  many  programs  of interest  are  not  well  understood,  and  specific  mathematical  results  may  not be  available.  finally,  it  is  often  the  case  that  programs  are  not  comparable  at all:  one  runs  much  more  efficiently  on  one  particular  kind  of  input,  the  other runs  efficiently  under  other  circumstances.
with  these  caveats  in  mind,  we’ll  use  rough  estimates  for  the  running time  of  our  programs  for  purposes  of  classification,  secure  in  the  knowledge that  a  fuller  analysis  can  be  done  for  important  programs  when  necessary. such  rough  estimates  are  quite  often  easy  to  obtain  via  the  old  programming saw  “90%  of  the  time  is  spent  in  10%  of  the  code.”  (this  has  been  quoted  in the  past  for  many  different  values  of  “go%.“)
the first step in getting a rough estimate of the running time of a program is  to  identify  the  inner  loop.  which  instructions  in  the  program  are  executed most  often?  generally,  it  is  only  a  few  instructions,  nested  deep  within  the
down  and  bottom  up  methods  loses  iml)ortance   for  three  level  trees.  other variations  of  balanced  trees  are  more  iriportant   for  external  searching.  for example,  when  a  node  becomes  full,  sp  itting  (and  the  resultant  half-empty nodes)  can  be  forestalled  by  dumping  some  of  the  contents  of  the  node  into its  “brother”  node  (if  it’s  not  too  full).  this  leads  to  better  space  utilization within  the  nodes,  which  is  likely  to  be  o  ’  central  concern  in  a  large-scale  disk searching  application.
an  alternative  to  b-trees  which  extends  digital  searching  algorithms  to  apply to  external  searching  was  developed  in  1978  by  r.  fagin,  j.  nievergelt,  n. pippenger,  and  r.  strong.  this  method,  called  extendible  hashing,  guarantees that  no  more  than  two  disk  accesses  will  be  used  for  any  search.  as  with  e trees,  our  records  are  stored  on  pages  which  are  split  into  two  pieces  when they  fill  up;  as  with  indexed  sequential  access,  we  maintain  an  index  which we access to find the page containing the records which match our search key. extendible  hashing  combines  these  approaches   by  using  digital  properties  of the  search  keys.
  we’ll  consider  how  it  handles  successive  insertions  of  keys  from  e  x  t  e  r  n  a  l  s  e  a  r  c  h  i  n  g  e  x  a m  p  l  e,  using  pages  with  a  capacity  o  *  up  to  four  records.
we  start  with  an  “index”   with  just  one  entry,  a  pointer  to  the  page which  is  to  hold  the  records.  the  first  four  records  fit  on  the  page,  leaving the  following  trivial  structure:
the  directory  on  disk  1  says  that  all  records  are  on  page  0  of  disk  2,  where they  are  kept  in  sorted  order  of  their  keys.   for  reference,  we  also  give  the binary  value  of  the  keys,  using  our  standard  encoding  of  the  five-bit  binary representation  of  i  for  the  ith  letter  of  the  alphabet.  now  the  page  is  full, and  must  be  split  in  order  to  add  the  ks:y   r=loolo.   the  strategy  is  simple: put  records  with  keys  that  begin  with  0  on  one  page  and  records  with  keys that  begin  with  1  on  another  page.  thi:  necessitates  doubling  the  size  of  the directory,  and  moving  half  the  keys  from  page  0  of  disk  2  to  a  new  page, leaving  the  following  structure:
(calling  itself  recursively  to  place  2  through  n),   then  generating  the  (n  -  l)! permutations  with  the  1  in  the  second  position,  etc.
because  16! >  250.   still,  it  is  important  to  study  because  it  can  form  the  basis for  a  backtracking  program  to  solve  any  problem  involving  reordering  a  set of  elements.
for  example,  consider  the  euclidean  traveling  salesman  problem:  given a  set  of  n  points  in  the  plane,  find  the  shortest  tour  that  connects  them all.  since  each  ordering  of  the  points  corresponds  to  a  legal  tour,  the  above program  can  be  made  to  exhaustively  search  for  the  solution  to  this  problem simply  by  changing  it  to  keep  track  of  the  cost  of  each  tour  and  the  minimum of  the  costs  of  the  full  tours,  in  the  same  manner  as  above. then  the same  branch-and-bound  technique  as  above  can  be  applied,  as  well  as  various backtracking  heuristics  specific  to  the  euclidean  problem.  (for  example,  it  is easy to prove that the optimal tour cannot cross itself, so the search can be cut off  on  all  partial  paths  that  cross  themselves.)  different  search  heuristics  might correspond  to  different  ways  of  ordering  the  permutations.  such  techniques can  save  an  enormous  amount  of  work  but  always  leave  an  enormous  amount of  work  to  be  done.  it  is  not  at  all  a  simple  matter  to  find  an  exact  solution to  the  euclidean  traveling  salesman  problem,  even  for  n  as  low  as  16.
another  reason  that  permutation  generation  is  of  interest  is  that  there are  a  number  of  related  procedures  for  generating  other  combinatorial  objects. in  some  cases,  the  number  of  objects  generated  are  not  quite  so  numerous  are as  permutations,  and  such  procedures  can  be  useful  for  larger  n  in  practice. an example of this is a procedure to generate all ways of choosing a subset of size  k  out  of  a  set  of  n  items.  for  large  n  and  small  k,  the  number  of  ways of  doing  this  is  roughly  proportional  to n k. such  a  procedure  could  be  used as  the  basis  for  a  backtracking  program  to  solve  the  knapsack  problem.
since  finding  the  shortest  tour  seems  to  require  so  much  computation,  it  is reasonable  to  consider  whether  it  might  be  easier  to  find  a  tour  that  is  almost as  short  as  the  shortest.  if  we’re  willing  to  relax  the  restriction  that  we absolutely  must  have  the  shortest  possible  path,  then  it  turns  out  that  we  can deal  with  problems  much  larger  than  is  possible  with  the  techniques  above. for example, it’s relatively easy to find a tour which is longer by at most a factor of two than the optimal tour. the method is based on simply finding the minimum  spanning  tree:  this  not  only,  as  mentioned  above,  provides  a  lower  bound bound  on  the  length  of  the  tour  but  also  turns  out  to  provide  an  upper on  the  length  of  the  tour,  as  follows.  consider  the  tour  produced  by  visiting the  nodes  of  the  minimum  spanning  tree  using  the  following  procedure:  to
or  simply  ax  =  b,  where  a  represents  the  matrix,  z  represents  the  variables, and  b  represents the  rightrhand   sides  of  the  equations.  since  the  rows  of  a are  manipulated  along  with  the  elements  of  b,  it  is  convenient  to  regard  b  as the (n  +  1)st  column of  a  and  use  an  n-by-(n  +  1)  array  to  hold  both.
now  the  forward  elimination  phase  can  be  summarized  as  follows:  first eliminate  the  first  variable  in  all  but  the  first  equation  by  adding  the  appropriate  multiple  of  the  first  equation  to  each  of  the  other  equations,  then eliminate  the  second  variable  in  all  but  the  first  two  equations  by  adding  the appropriate  multiple  of  the  second  equation  to  each  of  the  third  through  the nth  equations,  then  eliminate  the  third  variable  in  all  but  the  first  three equations,  etc.  to  eliminate  the  ith  variable  in  the  jth  equation  (for  j  between  i  $-  1  and  n)  we  multiply  the  ith  equation  by  aji/aii   and  subtract  it from  the  jth  equation.  this  process  is  perhaps  more  succinctly  described  by the  following  program,  which  reads  in  n  followed  by  an  n-by-(  n  +  1)  matrix, performs  the  forward  elimination,  and  writes  out  the  triangulated  result.  in the  input,  and  in  the  output  the  ith  line  contains  the  ith  row  of  the  matrix followed by b,.
(a  call  to  eliminate  should  replace  the  three  nested  for  loops  in  the  program gauss  given  above.)  there  are  some  algorithms  where  it  is  required  that  the pivot  a[i,  i]  be  used  to  eliminate  the  ith  variable  from  every  equation  but  the ith  (not  just  the  (i+l)st   through  the  nth).  this  process  is  called  full  pivoting; for forward elimination we only do part of this work hence the process is called partial pivoting .
after  the  forward  elimination  phase  has  completed,  the  array  a  has all  zeros  below  the  diagonal,  and  the  backward  substitution  phase  can  be executed.  the  code  for  this  is  even  more  straightforward:
a  call  to  eliminate  followed  by  a  call  to  substitute  computes  the  solution  in the  n-element  array  x.  division  by  0  could  still  occur  for  singular  matrices.
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
this  procedure  recursively  determines  the  highest  point  in  the  tree  reachable  (via  a  dotted  link)  from  any  descendant  of  vertex  k  and  uses  this  information  to  determine  if  k  is  an  articulation  point.  normally  this  calculation simply  involves  testing  whether  the  minimum  value  reachable  from  a  son  is higher  up  in  the  tree,  but  we  need  an  extra  test  to  determine  whether  k  is  the root  of  a  depth-first  search  tree  (or,  equivalently,  whether  this  is  the  first  call to visit for the connected component containing k), since we’re using the same recursive  program  for  both  cases.  this  test  is  properly  performed  outside  the recursive  visit,  so  it  does  not  appear  in  the  code  above.
the  program  above  simply  prints  out  the  articulation  points.  of  course, as  before,  it  is  easily  extended  to  do  additional  processing  on  the  articulation points  and  biconnected  components. also,  since  it  is  a  depth-first  search procedure,  the  running  time  is  proportional  to  v  +  e.
besides  the  “reliability”  sort  of  application  mentioned  above,  biconnectedness  can  be  helpful  in  decomposing  large  graphs  into  manageable  pieces.  it  is obvious  that  a  very  large  graph  may  be  processed  one  connected  component at  a  time  for  many  applications;  it  is  somewhat  less  obvious  but  sometimes  as useful  that  a  graph  can  sometimes  be  processed  one  biconnected  component at  a  time.
some  care  must  be  exercised  to  pro.)erly   handle  records  with  equal  keys for  this  algorithm:  the  index  returned  cmluld  fall  in  the  middle  of  a  block  of records  with  key  v,  so  loops  which  scan  in  both  directions  from  that  index should  be  used  to  pick  up  all  the  records. of  course,  in  this  case  the  running time  for  the  search  is  proportional  to  lg)v  plus  the  number  of  records  found. the  sequence  of  comparisons  made  by  the  binary  search  algorithm  is predetermined:  the  specific  sequence  used  is  based  on  the  value  of  the  key being  sought  and  the  value  of  n.   the  comparison  structure  can  be  simply described  by  a  binary  tree  structure.  the  following  binary  tree  describes  the comparison  structure  for  our  example  se,  of  keys:
in  searching  for  the  key  s  for  instance,  it  is  first  compared  to  h.  since  it  is greater,  it  is  next  compared  to  n;  otheruise   it  would  have  been  compared  to c),  etc.  below  we  will  see  algorithms  that  use  an  explicitly  constructed  binary tree  structure  to  guide  the  search.
one  improvement  suggested  for  binary  search  is  to  try  to  guess  more precisely where the key being sought falls tvithin  the current interval of interest (rather  than  blindly  using  the  middle  element  at  each  step).  this  mimics  the way  one  looks  up  a  number  in  the  telephone  directory,  for  example:  if  the name  sought  begins  with  b,  one  looks  r(ear   the  beginning,  but  if  it  begins interpolation  search, with  y,  one  looks  near  the  end.  this  method,  called  requires  only  a  simple  modification  to  the  program  above.  in  the  program above,  the  new  place  to  search  (the  midpoint  of  the  interval)  is  computed with  the  statement  x:=(l+r)   div  2.  this   is  derived  from  the  computation z =  1+  $(r   -  1):  the  middle  of  the  interval  is  computed  by  adding  half  the  size of  the  interval  to  the  left  endpoint.  inte*polation  search  simply  amounts  to replacing  i  in  this  formula  by  an  estima;e   of  where  the  key  might  be  based on  the  values  available:  i  would  be  appropriate  if  v  were  in  the  middle  of  the interval  between  a[i].key   and  a[r].key,   but  we  might  have  better  luck  trying
this  procedure  goes  down  both  subtrees  only  when  the  dividing  line  cuts  the rectangle,  which  should  happen  infrequently  for  relatively  small  rectangles. although  the  method  hasn’t  been  fully  analyzed,  its  running  time  seems  sure to  be  proportional  to  r + log n to retrieve  r  points  from  reasonable  ranges  in a  region  containing  n  points,  which  makes  it  very  competitive  with  the  grid method.
multidimensional  range  searching both  the  grid  method  and  2d  trees  generalize  directly  to  more  than  two  dimensions:  simple,  straightforward  extensions  to  the  above  algorithms  immediately yield  range-searching  methods  which  work  for  more  than  two  dimensions. however,  the  nature  of  multidimensional  space  dictates  that  some  caution  is called  for  and  that  the  performance  characteristics  of  the  algorithms  might be  difficult  to  predict  for  a  particular  application.
to  implement  the  grid  method  for  k-dimensional  searching,  we  simply make  grid  a  k-dimensional  array  and  use  one  index  per  dimension.  the  main problem  is  to  pick  a  reasonable  value  for  size.  this  problem  becomes  quite obvious when large  k  is  considered:  what  type  of  grid  should  we  use  for  lodimensional  search?  the  problem  is  that  even  if  we  use  only  three  divisions per  dimension,  we  need  31°   grid  squares,  most  of  which  will  be  empty,  for reasonable  values  of  n.
the  generalization  from  2d  to  kd  trees  is  also  straightforward:  simply cycle  through  the  dimensions  (as  we  did  for  two  dimensions  by  alternating between  x  and  y)  while  going  down  the  tree.  as  before,  in  a  random  situation, the  resulting  trees  have  the  same  characteristics  as  binary  search  trees.  also as  before,  there  is  a  natural  correspondence  between  the  trees  and  a  simple
as  above,  this  algorithm  requires  time  proportional  to  n3   since  it  works with  a  matrix  of  size  n2  and  spends  time  proportional  to  n  on  each  entry. it  is  actually  possible  in  this  case  to  reduce  the  time  requirement  to  n2   by taking  advantage  of  the  fact  that  the  optimal  position  for  the  root  of  a  tree can’t  be  too  far  from  the  optimal  position  for  the  root  of  a  slightly  smaller tree,  so  that  k  doesn’t  have  to  range  over  all  the  values  from  i  to  i  +  j  in  the program  above.
in  some  cases,  the  dynamic  programming  formulation  of  a  method  to  solve a  problem  produces  a  familiar  algorithm.  for  example,  warshall’s  algorithm (given  in  chapter  32)  for  finding  the  transitive  closure  of  a  directed  graph follows  directly  from  a  dynamic  programming  formulation.  to  show  this, we’ll  consider  the  more  general  all-pairs shortest paths  problem:  given  a  graph with vertices {  1,2,.  . . ,v}  determine  the  shortest  distance  from  each  vertex to  every  other  vertex.
since  the  problem  calls  for  v2  numbers  as  output,  the  adjacency  matrix representation  for  the  graph  is  obviously  appropriate,  as  in  chapters  31  and 32.  thus  we’ll  assume  our  input  to  be  a  v-by-v  array  a  of  edge  weights,  with a[i, j]  :=w  if  there  is  an  edge  from  vertex  i  to  vertex  j  of  weight  w.  if  a[i,  j]= a  b,  i]  for all i and j then this could represent an undirected graph, otherwise it represents  a  directed  graph.  our  task  is  to  find  the  directed  path  of  minimum weight  connecting  each  pair  of  vertices.  one  way  to  solve  this  problem  is  to simply  run  the  shortest  path  algorithm  of  chapter  31  for  each  vertex,  for  a total  running  time  proportional  to v 3. an  even  simpler  algorithm  with  the same  performance  can  be  derived  from  a  dynamic  programming  approach.
the  dynamic  programming  algorithm  for  this  problem  follows  directly from  our  description  of  warshall’s  algorithm  in  chapter  32.  we  compute, 1  5  k  5   n,  the  shortest  path  from  each  vertex  to  each  other  vertex  which uses only vertices from  {1,2,.  . . ,  k}.  the  shortest  path  from  vertex  i  to  vertex j using only vertices from  1,2,  . . . ,  k  is  either  the  shortest  path  from  vertex  i to vertex  j using only vertices from  1,2,.  . . ,  k  -  1  or  a  path  composed  of  the shortest path from vertex i to vertex  k using only vertices from  1,2,  . . . ,  k -  1 and  the  shortest  path  from  vertex  k  to  vertex  j  using  only  vertices  from 1,2,.  . . ,  k  -  1.  this  leads  immediately  to  the  following  program.
that  “al”  be  deleted;  “1c”  means  to  add  cl  to  the  matching,  which  requires that  “c3”  be  deleted;  “3e”  means  to  add  e3  to  the  matching.  thus,  after this  path  is  processed,  we  have  the  matching  a4  b2  cl  d5  e3;  equivalently, the  flow  in  the  network  is  given  by  full  pipes  in  the  edges  connecting  those nodes,  and  all  pipes  leaving  0  and  entering  z  full.
the  proof  that  the  matching  is  exactly  those  edges  which  are  filled  to capacity  by  the  maxflow   algorithm  is  straightforward.  first,  the  network  flow always  gives  a  legal  matching:  since  each  vertex  has  an  edge  of  capacity  1 either  coming  in  (from  the  sink)  or  going  out  (to  the  source),  at  most  one  unit of  flow  can  go  through  each  vertex,  which  implies  that  each  vertex  will  be included  at  most  once  in  the  matching.  second,  no  matching  can  have  more edges,  since  any  such  matching  would  lead  directly  to  a  better  flow   than  that produced  by  the  maxflow   algorithm.
thus,  to  compute  the  maximum  matching  for  a  bipartite  graph  we  simply format  the  graph  so  as  to  be  suitable  for  input  to  the  network  flow  algorithm of  the  previous  chapter.  of  course,  the  graphs  presented  to  the  network  flow algorithm  in  this  case  are  much  simpler  than  the  general  graphs  the  algorithm is  designed  to  handle,  and  it  turns  out  that  the  algorithm  is  somewhat  more efficient  for  this  case.  the  construction  ensures  that  each  call  to  pfs  adds one  edge  to  the  matching,  so  we  know  that  there  are  at  most  v/2   calls  to pfs  during  the  execution  of  the  algorithm.  thus,  for  example,  the  total  time to  find  the  maximum  matching  for  a  dense  bipartite  graph  with  v  vertices (using  the  adjacency  matrix  representation)  is  proportional  to  v3.
stable  marriage  problem the  example  given  at  the  beginning  of  this  chapter,  involving  medical  students and  hospitals,  is  obviously  taken  quite  seriously  by  the  participants.  but the  method  that  we’ll  examine  for  doing  the  matching  is  perhaps  better understood  in  terms  of  a  somewhat  whimsical  model  of  the  situation.  we assume  that  we  have  n  men  and  n  women  who  have  expressed  mutual preferences  (each  man  must  say  exactly  how  he  feels  about  each  of  the  n women  and  vice  versa).  the  problem  is  to  find  a  set  of  n  marriages  that respects everyone’s preferences.
how  should  the  preferences  be  expressed?  one  method  would  be  to  use the  “1-10”   scale,  each  side  assigning  an  absolute  score  to  certain  members  of the  other  side.  this  makes  the  marriage  problem  the  same  as  the  weighted matching  problem,  a  relatively  difficult  problem  to  solve.  furthermore,  use  of absolute  scales  in  itself  can  lead  to  inaccuracies,  since  peoples’  scales  will  be inconsistent  (one  woman’s  10  might  be  another  woman’s  7).  a  more  natural way to express the preferences is to have each person list in order of preference the  following  two  tables  might  show all  the  people  of  the  opposite  sex.
idea  is  to  simply  collapse  the  rows  in  the  table  above  to  just  one  pair  of  rows, and  thus  produce  a  cycling  machine  wired  together  as  follows:
note  carefully  that  this  is  not  quite  “ideal”  parallel  performance:  since we can merge together two files of n elements using one processor in a number of  steps  proportional  to  n,  we  would  hope  to  be  able  to  do  it  in  a  constant number  of  steps  using  n  processors.  in  this  case,  it  has  been  proven  that  it is  not  possible  to  achieve  this  ideal  and  that  the  above  machine  achieves  the best  possible  parallel  performance  for  merging  using  compare-exchange  boxes. the  perfect  shuffle  interconnection  pattern  is  appropriate  for  a  variety  of other  problems.  for  example,  if  a  2n-by-2n  square  matrix  is  kept  in  row-major order,  then  n  perfect  shuffles  will  transpose  the  matrix  (convert  it  to  columnmajor  order).  more  important  examples  include  the  fast  fourier  transform (which  we’ll  examine  in  the  next  chapter);  sorting  (which  can  be  developed  by applying  either  of  the  methods  above  recursively);  polynomial  evaluation;  and a  host  of  others.  each  of  these  problems  can  be  solved  using  a  cycling  perfect shuffle  machine  with  the  same  interconnections  as  the  one  diagramed  above but  with  different  (somewhat  more  complicated)  processors.  some  researchers have even suggested the use of the perfect shuffle interconnection for  “generalpurpose”  parallel  computers.
which  works  in  this  way  is  the  ‘so-called  shift-reduce  parser.  the  idea  is  to maintain  a  pushdown  stack  which  holds  terminal  and  nonterminal  symbols. each  step  in  the  parse  is  either  a  shift  step,  in  which  the  next  input  character is  simply  pushed  onto  the  stack,  or  a  reduce  step,  in  which  the  top  characters on  the  stack  are  matched  to  the  right-hand  side  of  some  production  in  the grammar  and  “reduced  to”  (replaced  by)  the  nonterminal  on  the  left  side of  that  production.  eventually  all  the  input  characters  get  shifted  onto  the stack,  and  eventually  the  stack  gets  reduced  to  a  single  nonterminal  symbol.
the  main  difficulty  in  building  a  shift-reduce  parser  is  deciding  when  to shift  and  when  to  reduce.  this  can  be  a  complicated  decision,  depending on  the  grammar.  various  types  of  shift-reduce  parsers  have  been  studied  in great  detail,  an  extensive  literature  has  been  developed  on  them,  and  they  are quite  often  preferred  over  recursive  descent  parsers  because  they  tend  to  be slightly  more  efficient  and  significantly  more  flexible.  certainly  we  don’t  have space  here  to  do  justice  to  this  field,  and  we’ll  forgo  even  the  details  of  an implementation  for  our  example.
a  compiler  may  be  thought  of  as  a  program  which  translates  from  one  language  to  another.  for  example,  a  pascal  compiler  translates  programs  from the  pascal  language  into  the  machine  language  of  some  particular  computer. we’ll  illustrate  one  way  that  this  might  be  done  by  continuing  with  our regular-expression  pattern-matching  example,  where  we  wish  translate from  the  language  of  regular  expressions  to  a  “language”  for  pattern-matching machines,  the  ch,  nextl,   and  next2  arrays  of  the  match  program  of  the  previous  chapter.
essentially,  the  translation  process  is  “one-to-one”:  for  each  character  in the  pattern  (with  the  exception  of  parentheses)  we  want  to  produce  a  state for  the  pattern-matching  machine  (an  entry  in  each  of  the  arrays).  the  trick is  to  keep  track  of  the  information  necessary  to  fill  in  the  next1   and  next2 arrays. to do so, we’ll convert each of the procedures in our recursive descent parser  into  functions  which  create  pattern-matching  machines.  each  function will  add  new  states  as  necessary  onto  the  end  of  the  ch, nextl,  and  next2 arrays,  and  return  the  index  of  the  initial  state  of  the  machine  created  (the final  state  will  always  be  the  last  entry  in  the  arrays).
problem-solving  techniques.  for  example,  they  form  the  basis  for  many  programs  which  play  games  such  as  chess  or  checkers.  in  this  case,  a  partial solution  is  some  legal  positioning  of  all  the  pieces  on  the  board,  and  the  descendant  of  a  node  in  the  exhaustive  search  tree  is  a  position  that  can  be the  result  of  some  legal  move.  ideally,  it  would  be  best  if  a  program  could exhaustively  search  through  all  possibilities  and  choose  a  move  that  will  lead to  a  win  no  matter  what  the  opponent  does,  but  there  are  normally  far  too many  possibilities  to  do  this,  so  a  backtracking  search  is  typically  done  with quite  sophisticated  pruning  rules  so  that  only  “interesting”  positions  are  examined.  exhaustive  search  techniques  are  also  used  for  other  applications  in artificial 
in  the  next  chapter  we’ll  see  several  other  problems  similar  to  those we’ve  been  studying  that  can  be  attacked  using  these  techniques.  solving a  particular  problem  involves  the  development  of  sophisticated  criteria  which can  be  used  to  limit  the  search.  for  the  traveling  salesman  problem  we’ve given  only  a  few  examples  of  the  many  techniques  that  have  been  tried, and  equally  sophisticated  methods  have  been  developed  for  other  important problems.
however  sophisticated  the  criteria,  it  is  generally  true  that  the  running time  of  backtracking  algorithms  remains  exponential.  roughly,  if  each  node in  the  search  tree  has  cr   sons,  on  the  average,  and  the  length  of  the  solution path  is  n,  then  we  expect  the  number  of  nodes  in  the  tree  to  be  proportional to  on.  different  backtracking  rules  correspond  to  reducing  the  value  of  (y, the  number  of  choices  to  try  at  each  node.  it  is  worthwhile  to  expend  effort to  do  this  because  a  reduction  in  [y  will  lead  to  an  increase  in  the  size  of  the problem  that  can  be  solved.  for  example,  an  algorithm  which  runs  in  time proportional  to  1.1  n can  solve  a  problem  perhaps  eight  times  a  large  as  one which  runs  in  time  proportional  to  2n.
an  interesting  computational  puzzle  is  to  write  a  program  that  generates  all possible  ways  of  rearranging  n  distinct  items.  a  simple  program  for  this permutation  generation  problem  can  be  derived  directly  from  the  exhaustive search program above because, as noted above, if it is run on a complete graph, then  it  must  try  to  visit  the  vertices  of  that  graph  in  all  possible  orders.
generator).  in  fact,  the  linear  feedback  shift  registers  that  we  discussed  in chapter  3  were  first  developed  for  use  in  encryption/decryption  machines such  as  described  here.  however,  key  generators  have  to  be  somewhat  more complicated  than  random  number  generators,  because  there  are  easy  ways  to attack  simple  linear  feedback  shift  registers.  the  problem  is  that  it  might  be easy  for  the  cryptanalyst  to  get  some  plaintext  (for  example,  silence  in  a  voice system),  and  therefore  some  key.  if  the  cryptanalyst  can  get  enough  key  that he  has  the  entire  contents  of  the  shift  register,  then  he  can  get  all  the  key from  that  point  on.
cryptographers  have  several  ways  to  avoid  such  problems.  one  way  is  to make  the  feedback  function  itself  a  cryptovariable.  it  is  usually  assumed  that the  cryptanalyst  knows  everything  about  the  structure  of  the  machine  (maybe he  stole  one)  except  the  cryptovariables,  but  if  some  of  the  cryptovariables  are used  to  “configure”  the  machine,  he  may  have  difficulty  finding  their  values. another  method  commonly  used  to  confuse  the  cryptanalyst  is  the  product cipher,  where  two  different  machines  are  combined  to  produce  a  complicated key  stream  (or  to  drive  each  other).  another  method  is  nonlinear  substitution; here  the  translation  between  plaintext  and  ciphertext  is  done  in  large  chunks, not  bit-by-bit.  the  general  problem  with  such  complex  methods  is  that  they can  be  too  complicated  for  even  the  cryptographer  to  understand  and  that there  always  is  the  possibility  that  things  may  degenerate  badly  for  some choices  of  the  cryptovariables.
in  commercial  applications  such  as  electronic  funds  transfer  and  (real)  computer  mail,  the  key  distribution  problem  is  even  more  onerous  than  in  the traditional  applications  of  cryptography. the  prospect  of  providing  long keys  (which  must  be  changed  often)  to  every  citizen,  while  still  maintaining  both  security  and  cost-effectiveness,  certainly  inhibits  the  development  of such systems. methods have recently been developed, however, which promise to  eliminate  the  key  distribution  problem  completely.  such  systems,  called public-key cryptosystems,  are  likely  to  come  into  widespread  use  in  the  near future.  one  of  the  most  prominent  of  these  systems  is  based  on  some  of  the arithmetic  algorithms  that  we  have  been  studying,  so  we  will  take  a  close  look at  how  it  works.
the idea in public-key cryptosystems is to use a  “phone  book” of encryption  keys.  everyone’s  encryption  key  (denoted  by  p)  is  public  knowledge:  a person’s  key  could  be  listed,  for  example,  next  to  his  number  in  the  telephone book.  everyone  also  has  a  secret  key  used  for  decryption;  this  secret  key (denoted  by  s)  is  not  known  to  anyone  else.  to  transmit  a  message  m,  the sender  looks  up  the  receiver’s  public  key,  uses  it  to  encrypt  the  message,  and then  transmits  the  message.  we’ll  denote  the  encrypted  message  (ciphertext)
each  node  in  this  tree  represents  a  vertical  line  dividing  the  points  in  the  left and  right  subtree.  the  nodes  are  numbered  in  the  order  in  which  the  vertical lines  are  tried  in  the  algorithm.  thus,  first  the  line  between  g  and  0  is  tried and  the  pair  go  is  retained  as  the  closest  so  far.  then  the  line  between  a  and d is tried, but a and d are too far apart to change  min. then the line between 0 and a is tried and the pairs gd  g-4 and oa all are successively closer pairs. it  happens  for  this  example  that  no  closer  pairs  are  found  until  fk,  which  is the  last  pair  checked  for  the  last  dividing  line  tried.  this  diagram  reflects  the difference  between  top-down  and  bottom-up  mergesort.  a  bottom-up  version of the closest-pair problem can be developed in the same way as for mergesort, which  would  be  described  by  a  tree  like  the  one  above,  numbered  left  to  right and  bottom  to  top.
the  general  approach  that  we’ve  used  for  the  closest-pair  problem  can be  used  to  solve  other  geometric  problems.  for  example,  another  problem  of interest  is  the  all-nearest-neighbors  problem:  for  each  point  we  want  to  find the  point  nearest  to  it.  this  problem  can  be  solved  using  a  program  like  the one  above  with  extra  processing  along  the  dividing  line  to  find,  for  each  point, whether  there  is  a  point  on  the  other  side  closer  than  its  closest  point  on  its own  side.  again,  the  “free”  y  sort  is  helpful  for  this  computation.
voronoi diagrams the set of all points closer to a given point in a point set than to all other points in  the  set  is  an  interesting  geometric  structure  called  the  voronoi polygon  for the  point.  the  union  of  all  the  voronoi  polygons  for  a  point  set  is  called  its voronoi  diagram.  this  is  the  ultimate  in  closest-point  computations:  we’ll  see that  most  of  the  problems  involving  distances  between  points  that  we  face have  natural  and  interesting  solutions  based  on  the  voronoi  diagram.  the diagram  for  our  sample  point  set  is  comprised  of  the  thick  lines  in  the  diagram below:
another  convex  hull  algorithm.  we’ll  see  yet  another  example  in  chapter  31 of  a  problem  which  can  be  efficiently  solved  by  first  finding  the  voronoi  dual. the  defining  property  of  the  voronoi  diagram  means  that  it  can  be  used to  solve  the  nearest-neighbor  problem:  to  identify  the  nearest  neighbor  in  a point  set  to  a  given  point,  we  need  only  find  out  which  voronoi  polygon  the point  falls  in.  it  is  possible  to  organize  the  voronoi  polygons  in  a  structure like  a  2d  tree  to  allow  this  search  to  be  done  efficiently.
the  voronoi  diagram  can  be  computed  using  an  algorithm  with  the  same general  structure  as  the  closest-point  algorithm  above.  the  points  are  first sorted  on  their  x  coordinate.  then  that  ordering  is  used  to  split  the  points  in half,  leading  to  two  recursive  calls  to  find  the  voronoi  diagram  of  the  point set  for  each  half.  at  the  same  time,  the  points  are  sorted  on  y;  finally,  the two  voronoi  diagrams  for  the  two  halves  are  merged  together.  as  before,  the merging  together  (done  with  pass=2)   can  make  use  of  the  fact  that  the  points are sorted on x before the recursive calls and that they are sorted on y and the voronoi  diagrams  for  the  two  halves  have  been  built  after  the  recursive  calls. however,  even  with  these  aids,  it  is  quite  a  complicated  task,  and  presentation of  a  full  implementation  would  be  beyond  the  scope  of  this  book.
the  voronoi  diagram  is  certainly  the  natural  structure  for  closest-point problems,  and  understanding  the  characteristics  of  a  problem  in  terms  of the  voronoi  diagram  or  its  dual  is  certainly  a  worthwhile  exercise.  however, for  many  particular  problems,  a  direct  implementation  based  on  the  general schema  given  in  this  chapter  may  be  suitable.  this  is  powerful  enough  to compute  the  voronoi  diagram,  so  it  is  powerful  enough  for  algorithms  based on  the  voronoi  diagram,  and  it  may  admit  to  simpler,  more  efficient  code,  just as  we  saw  for  the  closest-nair  nroblem.
small  index  to  each  key  before  sorting  or  5y  lengthening  the  sort  key  in  some other  way.  it  is  easy  to  take  stability  for  granted:  people  often  react  to  the unpleasant  effects  of  instability  with  disbelief.  actually  there  are  few  methods which  achieve  stability  without  using  significant  extra  time  or  space.
the  following  program,  for  sorting  three  records,  is  intended  to  illustrate the  general  conventions  that  we’ll  be  using.  (in  particular,  the  main  program  is a  peculiar  way  to  exercise  a  program  that  is  known  to  work  only  for  n  =  3:  the point  is  that  most  of  the  sorting  programs  we’ll  consider  could  be  substituted for  sort3  in  this  “driver”  program.)
the  three  assignment  statements  following  each  if  actually  implement  an “exchange”  operation.  we’ll  write  out  the  code  for  such  exchanges  rather  than use  a  procedure  call  because  they’re  fundamental  to  many  sorting  programs and  often  fall  in  the  inner  loop.
in  order  to  concentrate  on  algorithmjc   issues,  we’ll  work  with  algorithms that  simply  sort  arrays  of  integers  into  numerical  order.  it  is  generally  straightforward  to  adapt  such  algorithms  for  use  in  a  practical  application  involving large  keys  or  records.  basically,  sorting  programs  access  records  in  one  of  two ways:  either  keys  are  accessed  for  comparison,  or  entire  records  are  accessed
idea  is  to  simply  collapse  the  rows  in  the  table  above  to  just  one  pair  of  rows, and  thus  produce  a  cycling  machine  wired  together  as  follows:
note  carefully  that  this  is  not  quite  “ideal”  parallel  performance:  since we can merge together two files of n elements using one processor in a number of  steps  proportional  to  n,  we  would  hope  to  be  able  to  do  it  in  a  constant number  of  steps  using  n  processors.  in  this  case,  it  has  been  proven  that  it is  not  possible  to  achieve  this  ideal  and  that  the  above  machine  achieves  the best  possible  parallel  performance  for  merging  using  compare-exchange  boxes. the  perfect  shuffle  interconnection  pattern  is  appropriate  for  a  variety  of other  problems.  for  example,  if  a  2n-by-2n  square  matrix  is  kept  in  row-major order,  then  n  perfect  shuffles  will  transpose  the  matrix  (convert  it  to  columnmajor  order).  more  important  examples  include  the  fast  fourier  transform (which  we’ll  examine  in  the  next  chapter);  sorting  (which  can  be  developed  by applying  either  of  the  methods  above  recursively);  polynomial  evaluation;  and a  host  of  others.  each  of  these  problems  can  be  solved  using  a  cycling  perfect shuffle  machine  with  the  same  interconnections  as  the  one  diagramed  above but  with  different  (somewhat  more  complicated)  processors.  some  researchers have even suggested the use of the perfect shuffle interconnection for  “generalpurpose”  parallel  computers.
the  final  step  in  the  development  >f  a  general  regular  expression  pattern  matching  algorithm  is  to  put  these  procedures  together  with  the  match procedure,  as  follows:
the  program  for  general  regular  expresr:ion  pattern  matching  that  we  have developed  in  this  and  the  previous  chapter  is  efficient  and  quite  useful.  a version  of  this  program  with  a  few  added  capabilities  (for  handling  “don’tcare”  characters  and  other  amenities)  is  likely  to  be  among  the  most  heavily used  utilities  on  many  computer  systems.
it  is  interesting  (some  might  say  confusing)  to  reflect  on  this  algorithm from  a  more  philosophical  point  of  view.  in  this  chapter,  we  have  considered parsers  for  unraveling  the  structure  of  regular  expressions,  based  on  a  formal description  of  regular  expressions  using  a  context-free  grammar.  put  another way,  we  used  the  context-free  gramma]’   to  specify  a  particular  “pattern”: sequences  of  characters  with  legally  balz.nced   parentheses.  the  parser  then checks  to  see  if  the  pattern  occurs  in  the  input  (but  only  considers  a  match legal  if  it  covers  the  entire  input  string).  thus  parsers,  which  check  that  an input  string  is  in  the  set  of  strings  defined  by  some  context-free  grammar, and  pattern  matchers,  which  check  that   an  input  string  is  in  the  set  of strings  defined  by  some  regular  expression,  are  essentially  performing  the  same function!  the  principal  difference  is  that  context-free  grammars  are  capable of  describing  a  much  wider  class  of  strings.   for  example,  the  set  of  all  regular expressions  can’t  be  described  with  regular  expressions.
another  difference  in  the  way  we’ve  implemented  the  programs  is  that  the the  parser,  while  the  match  procedure context-free  grammar  is  “built  in”  to  is  “table-driven”:  the  same  program  wol,ks   for  all  regular  expressions,  once they  have  been  translated  into  the  propel.   format.  it  turns  out  to  be  possible to  build  parsers  which  are  table-driven  in  the  same  way,  so  that  the  same program can be used to parse all language  3 which can be described by contextfree  grammars.  a  parser generator  is  a  program  which  takes  a  grammar  as input  and  produces  a  parser  for  the  language  described  by  that  grammar  as
to  have  a  user-defined  type  for  the  complex  numbers,  it  is  then  necessary to  also  define  procedures  or  functions  for  all  the  arithmetic  operations  on the  numbers,  and  this  obscures  the  algorithm  unnecessarily.  the  following implementation  assumes  a  type  complex  for  which  the  obvious  arithmetic functions  are  defined:
this  program  assumes  that  the  global  variable  outn   has  been  set  to  2n-1, and  that  p,  q,  and  r  are  arrays  indexed  from  0  to  2n  -  1  which  hold  complex numbers.  the  two  polynomials  to  be  multiplied,  p  and  q  are  of  degree  n  -  1, and  the  other  coefficients  in  those  arrays  are  initially  set  to  0.  the  procedure eval   replaces  the  coefficients  of  the  polynomial  given  as  the  first  argument  by the  values  obtained  when  the  polynomial  is  evaluated  at  the  roots  of  unity. the  second  argument  specifies  the  degree  of  the  polynomial  (one  less  than  the number  of  coefficients  and  roots  of  unity)  and  the  third  argument  is  described below.  the  above  code  computes  the  product  of  p  and  q  and  leaves  the  result in  r.
now  we  are  left  with  the  implementation  of  eval.   as  we’ve  seen  before, recursive  programs  involving  arrays  can  be  quite  cumbersome  to  implement.  it turns  out  that  for  this  algorithm  it  is  possible  to  get  around  the  usual  storage management  problem  by  reusing  the  storage  in  a  clever  way.  what  we  would like  to  do  is  have  a  recursive  procedure  that  takes  as  input  a  contiguous  array of  n  +  1  coefficients  and  returns  the  n  +  1  values  in  the  same  array.  but the  recursive  step  involves  processing  two  noncontiguous  arrays:  the  odd  and even  coefficients.  on  reflection,  the  reader  will  see  that  the  “perfect  shuffle” of  the  previous  chapter  is  exactly  what  is  needed  here.  we  can  get  the  odd coefficients  in  a  contiguous  subarray  (the  first  half)  and  the  even  coefficients in  a  contiguous  subarray  (the  second  half)  by  doing  a  “perfect  unshuffle”  of the  input,  as  diagramed  below  for  n  =  15:
note  that  an  extra  scaling  factor  of  n  arises.  this  is  the  “inversion  theorem” for  the  discrete  fourier  transform,  which  says  that  the  same  method  will convert  a  polynomial  both  ways:  between  its  representation  as  coefficients  and its  representation  as  values  at  the  complex  roots  of  unity.
while  the  mathematics  may  seem  complicated,  the  results  indicated  are quite  easy  to  apply:  to  interpolate  a  polynomial  on  the  nth  roots  of  unity, use  the  same  procedure  as  for  evaluation,  using  the  interpolation  values  as polynomial  coefficients,  then  rearrange  and  scale  the  answers.
implementation now  we  have  all  the  pieces  for  a  divide-and-conquer  algorithm  to  multiply two  polynomials  using  only  about  n  lg  n  operations.  the  general  scheme  is to:
evaluate  the  input  polynomials  at  the  (2n  -  1)st   roots  of  unity. multiply  the two values obtained at each point. interpolate  to  find  the  result  by  evaluating  the  polynomial  defined  by the  numbers  just  computed  at  the  (2n  -  1)st   roots  of  unity.
the  description  above  can  be  directly  translated  into  a  program  which  uses  a procedure  that  can  evaluate  a  polynomial  of  degree  n  -  1  at  the  nth  roots of  unity.  unfortunately,  all  the  arithmetic  in  this  algorithm  is  to  be  complex arithmetic,  and  pascal  has  no  built-in  type  complex.  while  it  is  possible
(as  we  found  with  polynomials,  if  we  wtint  to  have  a  program  that  takes  n as  input,  it  is  necessary  in  pascal  to  first  decide  how  large  a  value  of  n  will be  “legal,” and  declare  the  array  suitably.)  note  that  the  code  consists  of three  nested  loops,  so  that  the  total  running  time  is  essentially  proportional to  n3.   the  third  loop  goes  backwards  so  as  to  avoid  destroying  ab,  i]  before it  is  needed  to  adjust  the  values  of  other  #elements  in  the  same  row.
the  program  in  the  above  paragraph  is  too  simple  to  be  quite  right:  a[&   i] might  be  zero,  so  division  by  zero  could  ‘occur.  this  is  easily  fixed,  because we  can  exchange  any  row  (from  i+1   to  n)  with  the  ith  row  to  make  a[i,   i] non-zero  in  the  outer  loop.  if  no  such  row  can  be  found,  then  the  matrix  is singular:  there  is  no  unique  solution.
in  fact,  it  is  advisable  to  do  slightly  more  than  just  find  a  row  with  a non-zero  entry  in  the  ith  column.  it’s  best  to  use  the  row  (from  if1   to  n) whose  entry  in  the  ith  column  is  the  largest  in  absolute  value.  the  reason  for this  is  that  severe  computational  errors  can  arise  if  the  a[&   i]  value  which  is used  to  scale  a  row  is  very  small.  if  a[i,  i]  is  very  small,  then  the  scaling  factor ab,  i]/a[i,   i]  which  is  used  to  eliminate  the  ith  variable  from  the  jth  equation (for  j  from  i+l   to  n)  will  be  very  large.  in  fact,  it  could  get  so  large  as  to dwarf  the  actual  coefficients  ali,  k],  to  the  point  where  the  alj,   k]  value  gets distorted  by  “round-off  error.”
put  simply,  numbers  which  differ  greatly  in  magnitude  can’t  be  accurately added  or  subtracted  in  the  floating  point  number  system  commonly  used  to represent  real  numbers,  but  using  a  small  a[&   i]  value  greatly  increases  the likelihood  that  such  operations  will  have  to  be  performed.  using  the  largest value  in  the  ith  column  from  rows  i+l  to  n  will  ensure  that  the  scaling  factor is always less than 1, and will prevent the occurrence of this type of error. one might  contemplate  looking  beyond  the  ith  column  to  find  a  large  element,  but it  has  been  shown  that  accurate  answers  can  be  obtained  without  resorting  to this  extra  complication.
the  following  code  for  the  forward  elimination  phase  of  gaussian  elimination  is  a  straightforward  implementation  of  this  process.  for  each  i  from  1  to n,  we  scan  down  the  ith  column  to  find  the  largest  element  (in  rows  past  the ith).  the  row  containing  this  element  is  exchanged  with  the  ith  ,  then  the  ith variable  is  eliminated  in  the  equations  i+.l   to  n  exactly  as  before:
an  alternate  way  to  proceed  after  forward  elimination  has  created  all zeros  below  the  diagonal  is  to  use  precisely  the  same  method  to  produce  all zeros  above  the  diagonal:  first  make  the  last   column  zero  except  for  a[n,  n] by  adding  the  appropriate  multiple  of  a[n,  n],  then  do  the  same  for  the  nextto-last  column,  etc.  that  is,  we  do  “partial  pivoting”  again,  but  on  the  other “part”  of  each  column,  working  backwards  through  the  columns.  after  this process,  called  gauss-  jordan  reduction,  is  complete,  only  diagonal  elements are  non-zero,  which  yields  a  trivial  solution.
computational  errors  are  a  prime  source  of  concern  in  gaussian  elimination.  as  mentioned  above,  we  should  be  wary  of  situations  when  the  magnitudes  of  the  coefficients  vastly  differ.  using  the  largest  available  element in  the  column  for  partial  pivoting  ensures  that  large  coefficients  won’t  be  arbitrarily  created  in  the  pivoting  process,  but  it  is  not  always  possible  to  avoid severe  errors.  for  example,  very  small  coefficients  turn  up  when  two  different equations  have  coefficients  which  are  quite  close  to  one  another.  it  is  actually possible  to  determine  in  advance  whether  such  problems  will  cause  inaccurate answers  in  the  solution.  each  matrix  haa   an  associated  numerical  quantity called the  condition number  which  can  ble  used  to  estimate  the  accuracy  of the  computed  answer.  a  good  library  subroutine  for  gaussian  elimination will  compute  the  condition  number  of  the  matrix  as  well  as  the  solution,  so that  the  accuracy  of  the  solution  can  be  lknown.  full  treatment  of  the  issues involved would be beyond the scope of this book.
gaussian  elimination  with  partial  pivoting  using  the  largest  available pivot  is  “guaranteed”  to  produce  results  with  very  small  computational  errors. there  are  quite  carefully  worked  out  mathematical  results  which  show  that  the calculated  answer  is  quite  accurate,  except  for  ill-conditioned  matrices  (which might  be  more  indicative  of  problems  in  .the   system  of  equations  than  in  the method  of  solution).  the  algorithm  has  been  the  subject  of  fairly  detailed theoretical  studies,  and  can  be  recommended  as  a  computational  procedure of  very  wide  applicability.
variations  and  extensions the  method  just  described  is  most  appropriate  for  n-by-n  matrices  with most of the  n2  elements  non-zero.  as  we’ve  seen  for  other  problems,  special techniques  are  appropriate  for  sparse  matrices  where  most  of  the  elements  are 0.  this  situation  corresponds  to  systems  ‘of   equations  in  which  each  equation has  only  a  few  terms.
if  the  non-zero  elements  have  no  particular  structure,  then  the  linked list  representation  discussed  in  chapter  ‘2  is  appropriate,  with  one  node  for each  non-zero  matrix  element,  linked  together  by  both  row  and  column.  the
even if only the worst case is being considered, the analysis of union-find algorithms  is  extremely  complex  and  intricate.  this  can  be  seen  even  from  the nature  of  the  results,  which  do  give  us  clear  indications  of  how  the  algorithms will  perform  in  a  practical  situation. if  either  weight  balancing  or  height balancing  is  used  in  combination  with  either  path  compression,  halving,  or splitting,  then  the  total  number  of  operations  required  to  build  up  a  structure with  e  edges  is  proportional  to  es(e),   where  a(e)  is  a  function  that  is  so slowly  growing  that  o(e)  <  4  unless  e  is  so  large  that  taking  lg  e,  then taking  lg  of  the  result,  then  taking  lg  of  that  result,  and  continuing  16  times still  gives  a  number  bigger  than  1.  this  is  a  stunningly  large  number;  for  all practical  purposes,  it  is  safe  to  assume  that  the  average  amount  of  time  to execute  each  union  and  find  operation  is  constant.  this  result  is  due  to  r.  e. tarjan,   who  further  showed  that  no  algorithm  for  this  problem  (from  a  certain general  class)  can  do  better  that  e&(e),  so  that  this  function  is  intrinsic  to the  problem.
an  important  practical  application  of  union-find  algorithms  is  that  they can  be  used  to  determine  whether  a  graph  with  v  vertices  and  e  edges  is connected  in  space  proportional  to  v  (and  almost  linear  time).  this  is  an advantage  over  depth-first  search  in  some  situations:  here  we  don’t  need  to ever  store  the  edges.  thus  connectivity  for  a  graph  with  thousands  of  vertices and  millions  of  edges  can  be  determined  with  one  quick  pass  through  the edges.
programs.  for  example,  the  following  grammar  describes  a  very  small  subset of  pascal,  arithmetic  expressions  involving  addition  and  multiplication.
again,  w  is  a  special  symbol  which  stands  for  any  letter,  but  in  this  grammar the  letters  are  likely  to  represent  variables  with  numeric  values.  examples  of legal  strings  for  this  grammar  are  a+(b*c)  and  (a+b*c)*d*(a+(b+c)).
as  we  have  defined  things,  some  strings  are  perfectly  legal  both  as  arithmetic  expressions  and  as  regular  expressions.  for  example,  a*(b+c)  might mean  “add  b  to  c  and  multiply  the  result  by  a”   or  “take  any  number  of  a’s followed  by  either  b  or  c.”  this  points  out  the  obvious  fact  that  checking whether  a  string  is  legally  formed  is  one  thing,  but  understanding  what  it means  is  quite  another.  we’ll  return  to  this  issue  after  we’ve  seen  how  to parse  a  string  to  check  whether  or  not  it  is  described  by  some  grammar.
each  regular  expression  is  itself  an  example  of  a  context-free  grammar: any  language  which  can  be  described  by  a  regular  expression  can  also  be described  by  a  context-free  grammar.  the  converse  is  not  true:  for  example, the  concept  of  “balancing”  parentheses  can’t  be  captured  with  regular  expressions. other  types  of  grammars  can  describe  languages  which  can’t  be described  by  context-free  grammars.  for  example,  context-sensitive  grammars are  the  same  as  those  above  except  that  the  left-hand  sides  of  productions need  not  be  single  nonterminals.  the  differences  between  classes  of  languages and  a  hierarchy  of  grammars  for  describing  them  have  been  very  carefully worked  out  and  form  a  beautiful  theory  which  lies  at  the  heart  of  computer science.
top-down  parsing one  parsing  method  uses  recursion  to  recognize  strings  from  the  language described  exactly  as  specified  by  the  grammar.  put  simply,  the  grammar  is such  a  complete  specification  of  the  language  that  it  can  be  turned  directly into  a  program!
terminal  on  the  left-hand  side.  nonterminals  on  the  right-hand  side  of  the input  correspond  to  (possibly  recursive)  procedure  calls;  terminals  correspond to  scanning  the  input  string.  for  example,  the  following  procedure  is  part  of a  top-down  parser  for  our  regular  expression  grammar:
suppose  it  is  known  in  advance  that  the  convex  hull  of  a  set  of  points  is a  triangle.  give  an  easy  algorithm  for  finding  the  triangle.  answer  the same  question  for  a  quadrilateral.
is  it  strictly  necessary  for  the  package-wrapping  method  to  start  with  a point  guaranteed  to  be  on  the  hull?  explain  why  or  why  not.
does  the  graham  scan  work  for  finding  the  convex  hull  of  the  points which  make  up  the  vertices  of  any  simple  polygon?  explain  why  or  give a  counterexample  showing  why  not.
what  four  points  should  be  used  for  the  floyd-eddy  method  if  the  input is  assumed  to  be  randomly  distributed  within  a  circle  (using  random  polar coordinates)?
run  the  package-wrapping  method  for  large  points  sets  with  both  2  and y  equally  likely  to  be  between  0  and  1000.  use  your  curve  fitting  routine to  find  an  approximate  formula  for  the  running  time  of  your  program  for a  point  set  of  size  n.
use  your  curve-fitting  routine  to  find  an  approximate  formula  for  the number  of  points  left  after  the  floyd-eddy  method  is  used  on  point  sets with  x  and  y  equally  likely  to  be  between  0  and  1000.
performance  issues as  mentioned  in  the  previous  chapter,  geometric  algorithms  are  somewhat harder  to  analyze  than  algorithms  from  some  of  the  other  areas  we’ve  studied because  the  input  (and  the  output)  is  more  difficult  to  characterize.  it  often doesn’t  make  sense  to  speak  of  llrandom”   point  sets:  for  example,  as  n gets  large,  the  convex  hull  of  points  drawn  from  a  rectangular  distribution  is extremely  likely  to  be  very  close  to  the  rectangle  defining  the  distribution.  the algorithms  that  we’ve  looked  at  depend  on  different  properties  of  the  point  set distribution  and  are  thus  in  practice  incomparable,  because  to  compare  them analytically  would  require  an  understanding  of  very  complicated  interactions between  little-understood  properties  of  point  sets.  on  the  other  hand,  we can  say  some  things  about  the  performance  of  the  algorithms  that  will  help choosing  one  for  a  particular  application.
the  easiest  of  the  three  to  analyze  is  the  graham  scan.  it  requires  time proportional  to  n  log  n  for  the  sort  and  n  for  the  scan.  a  moment’s  reflection is  necessary  to  convince  oneself  that  the  scan  is  linear,  since  it  does  have a  repeat  “loop-within-a-loop.” however,  it  is  easy  to  see  that  no  point  is “eliminated”  more  than  once,  so  the  total  number  of  times  the  code  within that  repeat  loop  is  iterated  must  be  less  than  n.
the  “package-wrapping”  technique,  on  the  other  hand,  obviously  takes about  mn  steps, where  m  is  the  number  of  vertices  on  the  hull.  to  compare this  with  the  graham  scan  analytically  would  require  a  formula  for  m  in  terms of  n,  a  difficult  problem  in  stochastic  geometry.  for  a  circular  distribution (and  some  others)  the  answer  is  that  m  is  about  n1/3,   and  for  values  of  n which  are  not  large  n‘j3   is  comparable  to  log  n  (which  is  the  expected  value for  a  rectangular  distribution),  so  this  method  will  compete  very  favorably with  the  graham  scan  for  many  practical  problems.  of  course,  the  n2  worst case  should  always  be  taken  into  consideration.
analysis  of  the  floyd-eddy  method  requires  even  more  sophisticated stochastic  geometry,  but  the  general  result  is  the  same  as  that  given  by intuition:  almost  all  the  points  fall  inside  the  quadrilateral  and  are  discarded. this  makes  the  running  time  of  tbe  whole  convex  hull  algorithm  proportional to  n,  since  most  points  are  examined  only  once  (when  they  are  thrown  out). on  the  average,  it  doesn’t  matter  much  which  method  is  used  after  one application  of  the  floyd-eddy  met,hod,   since  so  few  points  are  likely  to  be left.  however,  to  protect  against  the  worst  case  (when  all  points  are  on  the hull),  it  is  prudent  to  use  the  graham  scan.  this  gives  an  algorithm  which  is almost  sure  to  run  in  linear  time  in  practice  and  is  guaranteed  to  run  in  time proportional  to  n  log  n.
the program as given above could fail if there is more than one point with the  lowest  y  coordinate,  unless  theta  is  modified  to  properly  sort  collinear points,  as  described  above.  (this  is  a  subtle  point  which  the  reader  may wish  to  check.)  alternatively,  the  min  computation  could  be  modified  to  find the  point  with  the  lowest  x  coordinate  among  all  points  with  the  lowest  y coordinate,  the  canonical  form  described  in  chapter  24.
one  reason  that  this  method  is  interesting  to  study  is  that  it  is  a  simple form  of  backtracking,  the  algorithm  design  technique  of  “try  something,  if  it doesn’t  work  then  try  something  else”  which  we’ll  see  in  much  more  complicated  forms  in  chapter  39.
hull selection almost any convex hull method can be vastly improved by a method developed independently  by  w.  f.  eddy  and  r.  w.  floyd.  the  general  idea  is  simple: pick  four  points  known  to  be  on  the  hull,  then  throw  out  everything  inside  the quadrilateral  formed  by  those  four  points.  this  leaves  many  fewer  points  to
performance  issues as  mentioned  in  the  previous  chapter,  geometric  algorithms  are  somewhat harder  to  analyze  than  algorithms  from  some  of  the  other  areas  we’ve  studied because  the  input  (and  the  output)  is  more  difficult  to  characterize.  it  often doesn’t  make  sense  to  speak  of  llrandom”   point  sets:  for  example,  as  n gets  large,  the  convex  hull  of  points  drawn  from  a  rectangular  distribution  is extremely  likely  to  be  very  close  to  the  rectangle  defining  the  distribution.  the algorithms  that  we’ve  looked  at  depend  on  different  properties  of  the  point  set distribution  and  are  thus  in  practice  incomparable,  because  to  compare  them analytically  would  require  an  understanding  of  very  complicated  interactions between  little-understood  properties  of  point  sets.  on  the  other  hand,  we can  say  some  things  about  the  performance  of  the  algorithms  that  will  help choosing  one  for  a  particular  application.
the  easiest  of  the  three  to  analyze  is  the  graham  scan.  it  requires  time proportional  to  n  log  n  for  the  sort  and  n  for  the  scan.  a  moment’s  reflection is  necessary  to  convince  oneself  that  the  scan  is  linear,  since  it  does  have a  repeat  “loop-within-a-loop.” however,  it  is  easy  to  see  that  no  point  is “eliminated”  more  than  once,  so  the  total  number  of  times  the  code  within that  repeat  loop  is  iterated  must  be  less  than  n.
the  “package-wrapping”  technique,  on  the  other  hand,  obviously  takes about  mn  steps, where  m  is  the  number  of  vertices  on  the  hull.  to  compare this  with  the  graham  scan  analytically  would  require  a  formula  for  m  in  terms of  n,  a  difficult  problem  in  stochastic  geometry.  for  a  circular  distribution (and  some  others)  the  answer  is  that  m  is  about  n1/3,   and  for  values  of  n which  are  not  large  n‘j3   is  comparable  to  log  n  (which  is  the  expected  value for  a  rectangular  distribution),  so  this  method  will  compete  very  favorably with  the  graham  scan  for  many  practical  problems.  of  course,  the  n2  worst case  should  always  be  taken  into  consideration.
analysis  of  the  floyd-eddy  method  requires  even  more  sophisticated stochastic  geometry,  but  the  general  result  is  the  same  as  that  given  by intuition:  almost  all  the  points  fall  inside  the  quadrilateral  and  are  discarded. this  makes  the  running  time  of  tbe  whole  convex  hull  algorithm  proportional to  n,  since  most  points  are  examined  only  once  (when  they  are  thrown  out). on  the  average,  it  doesn’t  matter  much  which  method  is  used  after  one application  of  the  floyd-eddy  met,hod,   since  so  few  points  are  likely  to  be left.  however,  to  protect  against  the  worst  case  (when  all  points  are  on  the hull),  it  is  prudent  to  use  the  graham  scan.  this  gives  an  algorithm  which  is almost  sure  to  run  in  linear  time  in  practice  and  is  guaranteed  to  run  in  time proportional  to  n  log  n.
one  attractive  feature  of  this  method  is  that  it  generalizes  to  three  (or more)  dimensions. the  convex  hull  of  a  set  of  points  in  3-space  is  a  convex three-dimensional  object  with  flat  faces.  it  can  be  found  by  “sweeping”  a plane  until  the  hull  is  hit,  then  “folding”  faces  of  the  plane,  anchoring  on different  lines  on  the  boundary  of  the  hull,  until  the  “package”  is  “wrapped.” the  program  is  quite  similar  to  selection  sorting,  in  that  we  successively choose  the  “best”  of  the  points  not  yet  chosen,  using  a  brute-force  search  for the  minimum.  the  major  disadvantage  of  the  method  is  that  in  the  worst  case, when  all  the  points  fall  on  the  convex  hull,  the  running  time  is  proportional to  n2.
the graham  scan the  next  method  that  we’ll  examine,  invented  by  r.  l.  graham  in  1972, is  interesting  because  most  of  the  computation  involved  is  for  sorting:  the algorithm  includes  a  sort  followed  by  a  relatively  inexpensive  (though  not  immediately  obvious)  computation.  the  algorithm  starts  with  the  construction of  a  simple  closed  polygon  from  the  points  using  the  method  of  the  previous chapter:  sort  the  points  using  as  keys  the  theta  function  values  corresponding to  the  angle  from  the  horizontal  made  from  the  line  connecting  each  point with  an  ‘anchor’  point  p[l]   (with  the  lowest  y  coordinate)  so  that  tracing p~~l,pk% . . . ,p[n],p[l]   gives  a  closed  polygon.  for  our  example  set  of  points, we  get  the  simple  closed  polygon  of  the  previous  section.  note  that  p[n], p[l],   and  p[2]  are  consecutive  points  on  the  hull;  we’ve  essentially  run  the first  iteration  of  the  package  wrapping  procedure  (in  both  directions).
computation  of  the  convex  hull  is  completed  by  proceeding  around, trying  to  place  each  point  on  the  hull  and  eliminating  previously  placed  points that  couldn’t  possibly  be  on  the  hull.  for  our  example,  we  consider  the  points
it  is  not  entirely  clear  which  branch  of  cryptology  has  been  affected  most by  the  availability  of  computers.  the  cryptographer  now  has  available  a  much more  powerful  encryption  machine  than  before,  but  this  also  gives  him  more room  to  make  a  mistake.  the  cryptanalyst  has  much  more  powerful  tools for  breaking  codes,  but  the  codes  to  be  broken  are  more  complicated  than ever  before.  cryptanalysis  can  place  an  incredible  strain  on  computational resources;  not  only  was  it  among  the  first  applications  areas  for  computers, but  it  still  remains  a  principal  applications  area  for  modern  supercomputers. more  recently,  the  widespread  use  of  computers  has  led  to  the  emergence of  a  variety  of  important  new  applications  for  cryptology,  as  mentioned  above. new cryptographic methods have recently been developed appropriate for such applications,  and  these  have  led  to  the  discovery  of  a  fundamental  relationship between  cryptology  and  an  important  area  of  theoretical  computer  science that  we’ll  examine  briefly  in  chapter  40.
in  this  chapter,  we’ll  examine  some  of  the  basic  characteristics  of  cryptographic  algorithms  because  of  the  importance  of  cryptography  in  modern computer  systems  and  because  of  close  relationships  with  many  of  the  algorithms  we  have  studied.  we’ll  refrain  from  delving  into  detailed  implementations:  cryptography  is  certainly  a  field  that  should  be  left  to  experts.  while it’s  not  difficult  to  “keep  people  honest”  by  encrypting  things  with  a  simple cryptographic  algorithm,  it  is  dangerous  to  rely  upon  a  method  implemented by  a  non-expert. rules  of  the  game all  the  elements  that  go  into  providing  a  means  for  secure  communications between  two  individuals  together  are  called  a  cryptosystem.  the  canonical structure  of  a  typical  cryptosystem  is  diagramed  below:
the  sender  (s)  wishes  to  send  a  message  (called  the  plaintezt)  to  the receiver  (r).  to  do  so,  he  transforms  the  plaintext  into  a  secret  form  suitable
it  is  not  entirely  clear  which  branch  of  cryptology  has  been  affected  most by  the  availability  of  computers.  the  cryptographer  now  has  available  a  much more  powerful  encryption  machine  than  before,  but  this  also  gives  him  more room  to  make  a  mistake.  the  cryptanalyst  has  much  more  powerful  tools for  breaking  codes,  but  the  codes  to  be  broken  are  more  complicated  than ever  before.  cryptanalysis  can  place  an  incredible  strain  on  computational resources;  not  only  was  it  among  the  first  applications  areas  for  computers, but  it  still  remains  a  principal  applications  area  for  modern  supercomputers. more  recently,  the  widespread  use  of  computers  has  led  to  the  emergence of  a  variety  of  important  new  applications  for  cryptology,  as  mentioned  above. new cryptographic methods have recently been developed appropriate for such applications,  and  these  have  led  to  the  discovery  of  a  fundamental  relationship between  cryptology  and  an  important  area  of  theoretical  computer  science that  we’ll  examine  briefly  in  chapter  40.
in  this  chapter,  we’ll  examine  some  of  the  basic  characteristics  of  cryptographic  algorithms  because  of  the  importance  of  cryptography  in  modern computer  systems  and  because  of  close  relationships  with  many  of  the  algorithms  we  have  studied.  we’ll  refrain  from  delving  into  detailed  implementations:  cryptography  is  certainly  a  field  that  should  be  left  to  experts.  while it’s  not  difficult  to  “keep  people  honest”  by  encrypting  things  with  a  simple cryptographic  algorithm,  it  is  dangerous  to  rely  upon  a  method  implemented by  a  non-expert. rules  of  the  game all  the  elements  that  go  into  providing  a  means  for  secure  communications between  two  individuals  together  are  called  a  cryptosystem.  the  canonical structure  of  a  typical  cryptosystem  is  diagramed  below:
the  sender  (s)  wishes  to  send  a  message  (called  the  plaintezt)  to  the receiver  (r).  to  do  so,  he  transforms  the  plaintext  into  a  secret  form  suitable
a  prime  is  found.  one  simple  method  performs  a  calculation  on  a  random number  that,  with  probability  l/2,   will  “prove”  that  the  number  to  be  tested is  not  prime.  (a  number  which  is  not  prime  will  survive  20  applications  of this  test  less  than  one  time  out  of  a  million,  30  applications  less  than  1  time out  of  a  billion.)  the  last  step  is  to  compute  p:  it  turns  out  that  a  variant  of euclid’s  algorithm  (see  chapter  1)  is  just  what  is  needed.
furthermore,  s  seems  to  be  difficult  to  compute  from  knowledge  of  p  (and n),  though  no  one  has  been  able  to  prove  that  to  be  the  case.  apparently, finding p from  s  requires  knowledge  of  x  and  y,   and  apparently  it  is  necessary to  factor  n  to  calculate  x  and  y.  but  factoring  n  is  thought  to  be  very difficult:  the  best  factoring  algorithms  known  would  take  millions  of  years  to factor  a  200-digit  number,  using  current  technology.
an  attractive  feature  of  the  rsa  system  is  that  the  complicated  computations  involving  n,  p,  and  s  are  performed  only  once  for  each  user  who subscribes  to  the  system,  which  the  much  more  frequent  operations  of  encryption  and  decryption  involve  only  breaking  up  the  message  and  applying  the simple  exponentiation  procedure. this  computational  simplicity,  combined with  all  the  convenience  features  provided  by  public-key  cryptosystems,  make this  system  quite  attractive  for  secure  communications,  especially  on  computer systems  and  networks.
the  rsa  method  has  its  drawbacks:  the  exponentiation  procedure  is  actually  expensive  by  cryptographic  standards,  and,  worse,  there  is  the  lingering  possibility  that  it  might  be  possible  to  read  messages  encrypted  using  the method.  this  is  true  with  many  cryptosystems:  a  cryptographic  method  must withstand  serious  cryptanalytic  attacks  before  it  can  be  used  with  confidence. several  other  methods  have  been  suggested  for  implementing  public-key cryptosystems.  some  of  the  most  interesting  are  linked  to  an  important  class of  problems  which  are  generally  thought  to  be  very  hard  (though  this  is  not known  for  sure),  which  we’ll  discuss  in  chapter  40.  these  cryptosystems have  the  interesting  property  that  a  successful  attack  could  provide  insight  on how  to  solve  some  well-known  difficult  unsolved  problems  (as  with  factoring for  the  rsa  method).  this  link  between  cryptology  and  fundamental  topics in  computer  science  research,  along  with  the  potential  for  widespread  use  of public-key  cryptography,  have  made  this  an  active  area  of  current  research.
the  term  curve  fitting  (or  data  fitting)  is  used  to  describe  the  general problem  of  finding  a  function  which  matches  a  set  of  observed  values  at
and  such  that  f(z) assumes  “reasonable”  values  at  other  data  points.  it  could be  that  the  z’s  and  y’s  are  related  by  some  unknown  function,  and  our  goal is  to  find  that  function,  but,  in  general,  the  definition  of  what  is  “reasonable” depends  upon  the  application.  we’ll  see  that  it  is  often  easy  to  identify “unreasonable”  functions.
curve  fitting  has  obvious  application  in  the  analysis  of  experimental  data, and  it  has  many  other  uses.  for  example,,  it  can  be  used  in  computer  graphics to  produce  curves  that  “look  nice”  withlout  the  overhead  of  storing  a  large number  of  points  to  be  plotted.  a  related  application  is  the  use  of  curve  fitting to  provide  a  fast  algorithm  for  computing  the  value  of  a  known  function  at an  arbitrary  point:  keep  a  short  table  of  exact  values,  curve  fit  to  find  other values.
two  principal  methods  are  used  to  approach  this  problem.  the  first  is interpolation:  a  smooth  function  is  to  be  found  which  exactly  matches  the given values at the given points. the second method,  least squares data fitting, is used when the given values may not be exact, and a function is sought which matches  them  as  well  as  possible.
used  for  this  type  of  application. the  idea  is  fundamentally  the  same;  the problem  is  that  the  derivatives  may  not  be  easy  to  compute.  what  is  used is  an  iterative  method:  use  some  estimate  for  the  coefficients,  then  use  these within  the  method  of  least  squares  to  compute  the  derivatives,  thus  producing a  better  estimate  for  the  coefficients.  this  basic  method,  which  is  widely  used today,  was  outlined  by  gauss  in  the  1820s.
the  term  curve  fitting  (or  data  fitting)  is  used  to  describe  the  general problem  of  finding  a  function  which  matches  a  set  of  observed  values  at
and  such  that  f(z) assumes  “reasonable”  values  at  other  data  points.  it  could be  that  the  z’s  and  y’s  are  related  by  some  unknown  function,  and  our  goal is  to  find  that  function,  but,  in  general,  the  definition  of  what  is  “reasonable” depends  upon  the  application.  we’ll  see  that  it  is  often  easy  to  identify “unreasonable”  functions.
curve  fitting  has  obvious  application  in  the  analysis  of  experimental  data, and  it  has  many  other  uses.  for  example,,  it  can  be  used  in  computer  graphics to  produce  curves  that  “look  nice”  withlout  the  overhead  of  storing  a  large number  of  points  to  be  plotted.  a  related  application  is  the  use  of  curve  fitting to  provide  a  fast  algorithm  for  computing  the  value  of  a  known  function  at an  arbitrary  point:  keep  a  short  table  of  exact  values,  curve  fit  to  find  other values.
two  principal  methods  are  used  to  approach  this  problem.  the  first  is interpolation:  a  smooth  function  is  to  be  found  which  exactly  matches  the given values at the given points. the second method,  least squares data fitting, is used when the given values may not be exact, and a function is sought which matches  them  as  well  as  possible.
used  for  this  type  of  application. the  idea  is  fundamentally  the  same;  the problem  is  that  the  derivatives  may  not  be  easy  to  compute.  what  is  used is  an  iterative  method:  use  some  estimate  for  the  coefficients,  then  use  these within  the  method  of  least  squares  to  compute  the  derivatives,  thus  producing a  better  estimate  for  the  coefficients.  this  basic  method,  which  is  widely  used today,  was  outlined  by  gauss  in  the  1820s.
matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
values  (using  some  non-existent  weight  to  represent  false),  or  we  include  a field  for  the  edge  weight  in  adjacency  list  records  in  the  adjacency  structure.
it  is  often  necessary  to  associate  other  information  with  the  vertices or  nodes  of  a  graph  to  allow  it  to  model  more  complicated  objects  or  to save  bookkeeping  information  in  complicated  algorithms.  extra  information associated  with  each  vertex  can  be  accommodated  by  using  auxiliary  arrays indexed  by  vertex  number  (or  by  making  adj  an  array  of  records  in  the adjacency  structure  representation).  extra  information  associated  with  each edge  can  be  put  in  the  adjacency  list  nodes  (or  in  an  array  a  of  records  in the  adjacency  matrix  representation),  or  in  auxiliary  arrays  indexed  by  edge number  (this  requires  numbering  the  edges).
at  the  beginning  of  this  chapter,  we  saw  several  natural  questions  that  arise immediately  when  processing  a  graph.  is  the  graph  connected?  if  not,  what are  its  connected  components?  does  the  graph  have  a  cycle?  these  and  many other  problems  can  be  easily  solved  with  a  technique  called  depth-first search, which is a natural way to “visit” every node and check every edge in the graph systematically.  we’ll  see  in  the  chapters  that  follow  that  simple  variations on  a  generalization  of  this  method  can  be  used  to  solve  a  variety  of  graph problems.
for  now,  we’ll  concentrate  on  the  mechanics  of  examining  every  piece of  the  graph  in  an  organized  way.  below  is  an  implementation  of  depth-first search  which  fills  in  an  array  vaj   [l..vl  as  it  visits  every  vertex  of  the  graph. the  array  is  initially  set  to  all  zeros,  so  vaj[k]=o  indicates  that  vertex  k  has not  yet  been  visited.  the  goal  is  to  systematically  visit  all  the  vertices  of  the graph,  setting  the  vaj   entry  for  the  nowth  vertex  visited  to  now,  for  now= 1,2,..., v.  the  program  uses  a  recursive  procedure  visit  which  visits  all  the vertices  in  the  same  connected  component  as  the  vertex  given  in  the  argument. to  visit  a  vertex,  we  check  all  its  edges  to  see  if  they  lead  to  vertices  which haven’t  yet  been  visited  (as  indicated  by  0  vaj   entries);  if  so,  we  visit  them:
v  letters  of  the  alphabet.)  of  course,  the  order  in  which  the  edges  appear  is not  relevant.  all  orderings  of  the  edges  represent  the  same  graph  and  result in  the  same  adjacency  matrix,  as  computed  by  the  following  program:
the types of  vl and  v2   are  omitted  from  this  program,  as  well  as  the  code  for index.  these  can  be  added  in  a  straightforward  manner,  depending  on  the graph  input  representation  desired.  (for  our  examples,  vl  and  v2   could  be  of type  char  and  index  a  simple  function  which  uses  the  pascal  ord  function.)
the  adjacency  matrix  representation  is  satisfactory  only  if  the  graphs to  be  processed  are  dense:  the  matrix  requires  v2   bits  of  storage  and  v2 steps  just  to  initialize  it.  if  the  number  of  edges  (the  number  of  one  bits in  the  matrix)  is  proportional  to  v2, then  this  may  be  no  problem  because about  v2   steps  are  required  to  read  in  the  edges  in  any  case,  but  if  the  graph is  sparse,  just  initializing  this  matrix  could  be  the  dominant  factor  in  the running  time  of  an  algorithm.  also  this  might  be  the  best  representation  for some  algorithms  which  require  more  than  v2   steps  for  execution.  next  we’ll look at a representation which is more suitable for graphs which are not dense. in  the  adjacency  structure  representation  all  the  vertices  connected  to each  vertex  are  listed  on  an  adjacency  list   for  that  vertex.  this  can  be  easily accomplished  with  linked  lists,  as  shown  in  the  program  below  which  builds the  adjacency  structure  for  our  sample  graph.
values  (using  some  non-existent  weight  to  represent  false),  or  we  include  a field  for  the  edge  weight  in  adjacency  list  records  in  the  adjacency  structure.
it  is  often  necessary  to  associate  other  information  with  the  vertices or  nodes  of  a  graph  to  allow  it  to  model  more  complicated  objects  or  to save  bookkeeping  information  in  complicated  algorithms.  extra  information associated  with  each  vertex  can  be  accommodated  by  using  auxiliary  arrays indexed  by  vertex  number  (or  by  making  adj  an  array  of  records  in  the adjacency  structure  representation).  extra  information  associated  with  each edge  can  be  put  in  the  adjacency  list  nodes  (or  in  an  array  a  of  records  in the  adjacency  matrix  representation),  or  in  auxiliary  arrays  indexed  by  edge number  (this  requires  numbering  the  edges).
at  the  beginning  of  this  chapter,  we  saw  several  natural  questions  that  arise immediately  when  processing  a  graph.  is  the  graph  connected?  if  not,  what are  its  connected  components?  does  the  graph  have  a  cycle?  these  and  many other  problems  can  be  easily  solved  with  a  technique  called  depth-first search, which is a natural way to “visit” every node and check every edge in the graph systematically.  we’ll  see  in  the  chapters  that  follow  that  simple  variations on  a  generalization  of  this  method  can  be  used  to  solve  a  variety  of  graph problems.
for  now,  we’ll  concentrate  on  the  mechanics  of  examining  every  piece of  the  graph  in  an  organized  way.  below  is  an  implementation  of  depth-first search  which  fills  in  an  array  vaj   [l..vl  as  it  visits  every  vertex  of  the  graph. the  array  is  initially  set  to  all  zeros,  so  vaj[k]=o  indicates  that  vertex  k  has not  yet  been  visited.  the  goal  is  to  systematically  visit  all  the  vertices  of  the graph,  setting  the  vaj   entry  for  the  nowth  vertex  visited  to  now,  for  now= 1,2,..., v.  the  program  uses  a  recursive  procedure  visit  which  visits  all  the vertices  in  the  same  connected  component  as  the  vertex  given  in  the  argument. to  visit  a  vertex,  we  check  all  its  edges  to  see  if  they  lead  to  vertices  which haven’t  yet  been  visited  (as  indicated  by  0  vaj   entries);  if  so,  we  visit  them:
down  and  bottom  up  methods  loses  iml)ortance   for  three  level  trees.  other variations  of  balanced  trees  are  more  iriportant   for  external  searching.  for example,  when  a  node  becomes  full,  sp  itting  (and  the  resultant  half-empty nodes)  can  be  forestalled  by  dumping  some  of  the  contents  of  the  node  into its  “brother”  node  (if  it’s  not  too  full).  this  leads  to  better  space  utilization within  the  nodes,  which  is  likely  to  be  o  ’  central  concern  in  a  large-scale  disk searching  application.
an  alternative  to  b-trees  which  extends  digital  searching  algorithms  to  apply to  external  searching  was  developed  in  1978  by  r.  fagin,  j.  nievergelt,  n. pippenger,  and  r.  strong.  this  method,  called  extendible  hashing,  guarantees that  no  more  than  two  disk  accesses  will  be  used  for  any  search.  as  with  e trees,  our  records  are  stored  on  pages  which  are  split  into  two  pieces  when they  fill  up;  as  with  indexed  sequential  access,  we  maintain  an  index  which we access to find the page containing the records which match our search key. extendible  hashing  combines  these  approaches   by  using  digital  properties  of the  search  keys.
  we’ll  consider  how  it  handles  successive  insertions  of  keys  from  e  x  t  e  r  n  a  l  s  e  a  r  c  h  i  n  g  e  x  a m  p  l  e,  using  pages  with  a  capacity  o  *  up  to  four  records.
we  start  with  an  “index”   with  just  one  entry,  a  pointer  to  the  page which  is  to  hold  the  records.  the  first  four  records  fit  on  the  page,  leaving the  following  trivial  structure:
the  directory  on  disk  1  says  that  all  records  are  on  page  0  of  disk  2,  where they  are  kept  in  sorted  order  of  their  keys.   for  reference,  we  also  give  the binary  value  of  the  keys,  using  our  standard  encoding  of  the  five-bit  binary representation  of  i  for  the  ith  letter  of  the  alphabet.  now  the  page  is  full, and  must  be  split  in  order  to  add  the  ks:y   r=loolo.   the  strategy  is  simple: put  records  with  keys  that  begin  with  0  on  one  page  and  records  with  keys that  begin  with  1  on  another  page.  thi:  necessitates  doubling  the  size  of  the directory,  and  moving  half  the  keys  from  page  0  of  disk  2  to  a  new  page, leaving  the  following  structure:
way  of  implementing  binary  search  trees  to  aid  in  searching  large  arrays  of records  is  preferred  for  many  applications,  since  it  avoids  the  extra  expense  of copying  keys  as  in  the  previous  paragraph,  and  it  avoids  the  overhead  of  the storage  allocation  mechanism  implied  by  new.  the  disadvantage  is  that  space is  reserved  with  the  record  array  for  links  which  may  not  be  in  use,  which could  lead  to  problems  with  large  arrays  in  a  dynamic  situation.
the  running  time  of  this  program  obviously  depends  very  heavily  on the  pattern  being  matched.  however,  for  each  of  the  n  input  characters,  it processes  at  most  m  states  of  the  mac:nne,   so  the  worst  case  running  time is  proportional  to  mn.  for  sure,  not  all  nondeterministic  machines  can  be simulated  so  efficiently,  as  discussed  in  more  detail  in  chapter  40,  but  the  use of  a  simple  hypothetical  pattern-matching  machine  in  this  application  leads to  a  quite  reasonable  algorithm  for  a  quite  difficult  problem.  however,  to complete  the  algorithm,  we  need  a  program  which  translates  arbitrary  regular expressions  into  “machines”  for  interpretation  by  the  above  code.  in  the  next chapter,  we’ll  look  at  the  implementation  of  such  a  program  in  the  context  of a  more  general  discussion  of  compilers  a,nd   parsing  techniques.
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
we’ll  prefix  implementations  of  priority  queue  routines  based  on  indirect  heaps with  “pq”  for  indentification   when  they  are  used  in  later  chapters.
now,  to  modify  downheap  to  work  indirectly,  we  need  only  examine  the places  where  it  references  a.  where  it  did  a  comparison  before,  it  must  now access  a  indirectly  through  heap.  where  it  did  a  move  before,  it  must  now make  the  move  in  heap,  not  a,  and  it  must  modify  inv  accordingly.  this  leads to  the  following  implementation:
a  similar  indirect  implementation  can  be  developed  based  on  maintaining heap  as  an  array  of  pointers  to  separately  allocated  records.  in  this  case,  a little  more  work  is  required  to  implement  the  function  of  inv  (find  the  heap position,  given  the  record).
advanced  implementations if  the  join  operation  must  be  done  efficiently,  then  the  implementations  that we have done so far are insufficient and more advanced techniques are needed. although we don’t have space here to go into the details of such methods, we can  discuss  some  of  the  considerations  that  go  into  their  design.
by  “efficiently,”  we  mean  that  a  join  should  be  done  in  about  the  same time  as  the  other  operations.  this  immediately  rules  out  the  linkless  representation  for  heaps  that  we  have  been  using,  since  two  large  heaps  can  be joined  only  by  moving  all  the  elements  in  at  least  one  of  them  to  a  large array.  it  is  easy  to  translate  the  algorithms  we  have  been  examining  to  use linked  representations;  in  fact,  sometimes  there  are  other  reasons  for  doing
employing  linked  lists  in  this  way,  we  use  only  as  many  nodes  as  are required  by  our  program.  as  n  gets  larger,  we  simply  make  more  calls  on  new. by  itself,  this  might  not  be  reason  enough.  to  use  linked  lists  for  this  program, because  it  does  seem  quite  clumsy  comlpared   to  the  array  implementation above.  for  example,  it  uses  twice  as  much  space,  since  a  link  must  be  stored along  with  each  coefficient.  however,  as  suggested  by  the  example  above,  we can take advantage of the possibility that many of the coefficients may be zero. we  can  have  list  nodes  represent  only  the  nonzero   terms  of  the  polynomial  by also  including  the  degree  of  the  term  represented  within  the  list  node,  so  that each list node contains values of c and j to represent cxj.  it is then convenient to  separate  out  the  function  of  creating  a  node  and  adding  it  to  a  list,  as follows:
the  listadd   function  creates  a  new  node,  gives  it  the  specified  fields,  and  links it  into  a  list  after  node  t.   now  the  readlist   routine  can  be  changed  either  to accept  the  same  input  format  as  above  (a:nd  create  list  nodes  only  for  nonzero coefficients)  or  to  input  the  coefficient  and  exponent  directly  for  terms  with nonzero  coefficient.  of  course,  the  write,!ist   function  also  has  to  be  changed suitably.  to  make  it  possible  to  process  the  polynomials  in  an  organized
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
now  the  procedures  from  chapter  14  can  be  used  as  is,  with  a  hash  function used  to  choose  among  the  lists.  for  example,  listinsert(v,  heads[v  mod  m]  )  heads[v  mod  m]) can  be  used  to  add  something  to  the  table,  t:=listsearch(v,  t) until to find the first record with key v, and successively set  t:=listsearch(v, t=z   to  find  subsequent  records  with  key  v.
for  example  if  the  ith  letter  in  the  alphabet  is  represented  with  the number  i  and  we  use  the  hash  function  h(k)  =  kmod  m,  then  we  get  the following  hash  values  for  our  sample  set  of  keys  with  m =  11:
obviously, the amount of time required for a search depends on the length of  the  lists  (and  the  relative  positions  of  the  keys  in  them).  the  lists  could  be left  unordered:  maintaining  sorted  lists  is  not  as  important  for  this  application as it was for the elementary sequential search because the lists are quite short. for  an  “unsuccessful  search”  (for  a  record  with  a  key  not  in  the  table)  we can  assume  that  the  hash  function  scrambles  things  enough  so  that  each  of
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
this  program  differs  from  the  description  above  in  two  important  ways.  first,, rather  than  simply  putting  two  subfiles   on  the  stack  in  some  arbitrary  order, their  sizes  are  checked  and  the  larger  of  the  two  is  put  on  the  stack  first. second, the smaller of the two subfiles  is not put on the stack at all; the values of  the  parameters  are  simply  reset,,  just  as  we  did  for  euclid’s  algorithm.  this technique,  called  “end-recursion  removal”  can  be  applied  to  any  procedure whose  last  action  is  a  recursive  call.  for  quicksort,  the  combination  of  endrecursion  removal  and  a  policy  of  processing  the  smaller  of  the  two  subfiles first  turns  out  to  ensure  that  the  stack  need  only  contain  room  for  about,  lg  n entries, since each entry on the stack after the top one must represent a  subfile less  than  half  the  size  of  the  previous  entry.
this  is  in  sharp  contrast  to  the  size  of  the  stack  in  the  worst  case  in  the recursive  implementation,  which  could  be  as  large  as  n  (for  example,  in  the case  that  the  file  is  already  sorted).  this  is  a  subtle  but  real  difficulty  with a  recursive  implementation  of  quicksort:  there’s  always  an  underlying  stack, and  a  degenerate  case  on  a  large  file  could  cause  the  program  to  terminate abnormally  because  of  lack  of  memory.  this  behavior  is  obviously  undesirable for  a  library  sorting  routine.  below  we’ll  see  ways  to  make  degenerate  cases extremely  unlikely,  but,  there’s  no  way  to  avoid  this  problem  completely  in a  recursive  implementation  (even  switching  the  order  in  which    are processed  doesn’t  help,  without  end-recursion  removal).
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
tree  nodes  are  blackened  in  these  diagrams,  fringe  nodes  are  crossed,  and unseen  nodes  are  blank.  depth-first  search  “explores”  the  graph  by  looking for  new  vertices  far  away  from  the  start  point,  taking  closer  vertices  only  when dead  ends  are  encountered;  breadth-first  search  completely  covers  the  area close  to  the  starting  point,  moving  farther  away  only  when  everything  close has  been  looked  at.  depth-first  search  is  appropriate  for  one  person  looking for  something  in  a  maze  because  the  “next  place  to  look”  is  always  close  by; breadth-first  search  is  more  like  a  group  of  people  looking  for  something  by fanning  out  in  all  directions.
beyond  these  operational  differences,  it  is  interesting  to  reflect  on  the fundamental  differences  in  the  implementations  of  these  methods.  depth-first search  is  very  simply  expressed  recursively  (because  its  underlying  data  structure  is  a  stack),  and  breadth-first  search  admits  to  a  very  simple  nonrecursive implementation  (because  its  underlying  data  structure  is  a  queue).  but  we’ve seen  that  the  true  underlying  data  structure  for  graph  algorithms  is  a  priority queue,  and  this  admits  a  wealth  of  interesting  properties  to  consider.  again, we’ll  see  more  examples  in  the  next  chapter.
union-find  algorithms in  some  applications  we  wish  to  know  simply  whether  a  vertex  x  is  connected to  a  vertex  y  in  a  graph;  the  actual  path  connecting  them  may  not  be relevant.  this  problem  has  been  carefully  studied  in  recent  years;  some efficient  algorithms  have  been  developed  which  are  of  independent  interest because  they  can  also  be  used  for  processing  sets  (collections  of  objects).
graphs  correspond  to  sets  of  objects  in  a  natural  way,  with  vertices corresponding  to  objects  and  edges  have  the  meaning  “is  in  the  same  set  as.” thus,  the  sample  graph  in  the  previous  chapter  corresponds  to  the  sets  {a  b c  d  e  f  g},  {h  i}  and  {j  k  l  m}. eac  connected  component  corresponds
instead  of  indirect  heap).  these  changes  yield  a  worst-case  running  time proportional  to  v2, as  opposed  to  (e  +  v)logv   for  sparsepfs.  that  is,  the running  time  is  linear  for  dense  graphs  (when  e  is  proportional  to  v2),  but sparsepfs  is  likely  to  be  much  faster  for  sparse  graphs.
geometric problems suppose  that  we  are  given  n  points  in  the  plane  and  we  want  to  find  the shortest  set  of  lines  connecting  all  the  points.  this  is  a  geometric  problem, called  the  euclidean  minimum  spanning  tree  problem.  it  can  be  solved  using  the  graph  algorithm  given  above,  but  it  seems  clear  that  the  geometry provides  enough  extra  structure  to  allow  much  more  efficient  algorithms  to  be developed.
the  way  to  solve  the  euclidean  problem  using  the  algorithm  given  above is  to  build  a  complete  graph  with  n  vertices  and  n(n  -  1)/2  edges,  one edge  connecting  each  pair  of  vertices  weighted  with  the  distance  between  the corresponding  points.  then  the  minimum  spanning  tree  can  be  found  with the  algorithm  above  for  dense  graphs  in  time  proportional  to  n2.
it  has  been  proven  that  it,   is  possible  to  do  better.  the  point  is  that  the geometric  structure  makes  most  of  the  edges  in  the  complete  graph  irrelevant to  the  problem,  and  we  can  eliminate  most  of  the  edges  before  even  starting to  construct  the  minimum  spanning  tree.  in  fact,  it  has  been  proven  that the  minimum  spanning  tree  is  a  subset  of  the  graph  derived  by  taking  only the  edges  from  the  dual  of  the  voronoi  diagram  (see  chapter  28).  we  know that  this  graph  has  a  number  of  edges  proportional  to  n,  and  both  kruskal’s algorithm  and  the  priority-first  search  method  work  efficiently  on  such  sparse graphs.  in  principle,  then,  we  could  compute  the  voronoi  dual  (which  takes time  proportional  to  nlog  n),  then  run  either  kruskal’s  algorithm  or  the priority-first  search  method  to  get  a  euclidean  minimum  spanning  tree  algorithm  which  runs  in  time  proportional  to  n  log  n.   but  writing  a  program to  compute  the  voronoi  dual  is  quite  a  challenge  even  for  an  experienced programmer.
another  approach  which  can  be  used  for  random  point  sets  is  to  take advantage  of  the  distribution  of  the  points  to  limit  the  number  of  edges included  in  the  graph,  as  in  the  grid  method  used  in  chapter  26  for  range searching.  if  we  divide  up  the  plane  into  squares  such  that  each  square is  likely  to  contain  about  5  points,  and  then  include  in  the  graph  only  the edges  connecting  each  point  to  the  points  in  the  neighboring  squares,  then  we are  very  likely  (though  not  guaranteed)  to  get  all  the  edges  in  the  minimum spanning  tree,  which  would  mean  that  kruskal’s  algorithm  or  the  priority-first search  method  would  efficiently  finish  the  job.
it  is  interesting  to  reflect  on  the  relationship  between  graph  and  geometric algorithms  brought  out  by  the  problem  posed  in  the  previous  paragraphs.  it
this  program  assumes  that  the  adjacency  matrix  representation  is  being  used for  the  network.  as  long  as  densepfs  can  find  a  path  which  increases  the flow  (by  the  maximum  amount),  we  trace  back  through  the  path  (using  the dad  array  constructed  by  densepfs)  and  increase  the  how   as  indicated.  if  v remains  unseen  after  some  call  to  densepfs,  then  a  mincut  has  been  found  and the  algorithm  terminates.
for  our  example  network,  the  algorithm  first  increases  the  flow  along  the path  abcf,  then  along  adef,  then  along  abcdef.  no  backwards  edges are  used  in  this  example,  since  the  algorithm  does  not  make  the  unwise  choice adebcf  that  we  used  to  illustrate  the  need  for  backwards  edges.  in  the  next chapter  we’ll  see  a  graph  for  which  this  algorithm  uses  backwards  edges  to find  the  maximum  how.
though  this  algorithm  is  easily  implemented  and  is  likely  to  work  well for  networks  that  arise  in  practice,  the  analysis  of  this  method  is  quite  complicated.  first,  as  usual,  densepfs  requires  v2   steps  in  the  worst  case,  or  we could  use  sparsepfs  to  run  in  time  proportional  to  (e  +  v)  log  v  per  iteration, though  the  algorithm  is  likely  to  run  somewhat  faster  than  this,  since  it  stops when  it  reaches  the  sink.  but  how  many  iterations  are  required?  edmonds and  karp  show  the  worst  case  to  be  1  +  logmimp1 f *  where f  *  is  the  cost of the  flow   and  m  is  the  maximum  number  of  edges  in  a  cut  of  the  network. this  is  certainly  complicated  to  compute,  but  it  is  not  likely  to  be  large  for real  networks.  this  formula  is  included  to  give  an  indication  not  of  how  long the  algorithm  might  take  on  an  actual  network,  but  rather  of  the  complexity of  the  analysis.  actually,  this  problem  has  been  quite  widely  studied,  and complicated  algorithms  with  much  better  worst-case  time  bounds  have  been developed.
the  network  flow   problem  can  be  extended  in  several  ways,  and  many variations  have  been  studied  in  some  detail  because  they  are  important  in
perspective in  the  chapters  that  follow  we’ll  consider  a  variety  of  graph  algorithms  largely aimed  at  determining  connectivity  properties  of  both  undirected  and  directed graphs.  these  algorithms  are  fundamental  ones  for  processing  graphs,  but are  only  an  introduction  to  the  subject  of  graph  algorithms.  many  interesting and  useful  algorithms  have  been  developed  which  are  beyond  the  scope  of this  book,  and  many  interesting  problems  have  been  studied  for  which  good algorithms  have  not  yet  been  found.
some  very  efficient  algorithms  have  been  developed  which  are  much  too complicated  to  present  here.  for  example,  it  is  possible  to  determine  efficiently whether  or  not  a  graph  can  be  drawn  on  the  plane  without  any  intersecting lines.  this  problem  is  called  the  planarity  problem,  and  no  efficient  algorithm for solving it was known until 1974, when r. e.  tarjan  developed an ingenious (but  quite  intricate)  algorithm  for  solving  the  problem  in  linear  time,  using depth-first  search.
some  graph  problems  which  arise  naturally  and  are  easy  to  state  seem to  be  quite  difficult,  and  no  good  algorithms  are  known  to  solve  them.  for example,  no  efficient  algorithm  is  known  for  finding  the  minimum-cost  tour which  visits  each  vertex  in  a  weighted  graph.  this  problem,  called  the traveling  salesman problem,  belongs  to  a  large  class  of  difficult  problems  that we’ll  discuss  in  more  detail  in  chapter  40.  most  experts  believe  that  no efficient  algorithms  exist  for  these  problems.
other  graph  problems  may  well  have  efficient  algorithms,  though  none  has been  found.  an  example  of  this  is  the  graph isomorphism  problem:  determine whether  two  graphs  could  be  made  identical  by  renaming  vertices.  efficient algorithms  are  known  for  this  problem  for  many  special  types  of  graphs,  but the  general  problem  remains  open.
in  short,  there  is  a  wide  spectrum  of  problems  and  algorithms  for  dealing with  graphs.  we  certainly  can’t  expect  to  solve  every  problem  which  comes along,  because  even  some  problems  which  appear  to  be  simple  are  still  baffling the  experts.  but  many  problems  which  are  relatively  easy  to  solve  do  arise quite  often,  and  the  graph  algorithms  that  we  will  study  serve  well  in  a  great variety  of  applications.
to  a  different  set.  for  sets,  we’re  interested  in  the  fundamental  question  3s x  in  the  same  set  as  y?”  this  clearly  corresponds  to  the  fundamental  graph question  “is  vertex  x  connected  to  vertex  y?”
given  a  set  of  edges,  we  can  build  an  adjacency  list  representation  of  the corresponding  graph  and  use  depth-first  search  to  assign  to  each  vertex  the index of its connected component, so the questions of the form “is x connected to  y?” can  be  answered  with  just  two  array  accesses  and  a  comparison.  the extra  twist  in  the  methods  that  we  consider  here  is  that  they  are  dynamic:  they can  accept  new  edges  arbitrarily  intermixed  with  questions  and  answer  the questions  correctly  using  the  information  received.  from  the  correspondence with  the  set  problem,  the  addition  of  a  new  edge  is  called  a  union  operation, and  the  queries  are  called  jind  operations.
our  objective  is  to  write  a  function  which  can  check  if  two  vertices  x  and y  are  in  the  same  set  (or,  in  the  graph  representation,  the  same  connected component)  and,  if  not,  can  put  them  in  the  same  set  (put  an  edge  between them  in  the  graph).  instead  of  building  a  direct  adjacency  list  or  other representation  of  the  graph,  we’ll  gain  efficiency  by  using  an  internal  structure specifically  oriented  towards  supporting  the  union  and  find  operations.  the internal  structure  that  we  will  use  is  a  forest  of  trees,  one  for  each  connected component.  we  need  to  be  able  to  find  out  if  two  vertices  belong  to  the  same tree  and  to  be  able  to  combine  two  trees  to  make  one.  it  turns  out  that  both of  these  operations  can  be  implemented  efficiently.
to  illustrate  the  way  this  algorithm  works,  we’ll  look  at  the  forest  constructed  when  the  edges  from  the  sample  graph  that  we’ve  been  using  in this  chapter  are  processed  in  some  arbitrary  order.  initially,  all  nodes  are  in separate  trees.  then  the  edge  ag  causes  a  two  node  tree  to  be  formed,  with a  at  the  root.  (this  choice  is  arbitrary  -we  could  equally  well  have  put  g  at the  root.)  the  edges  ab  and  ac  add  b  and  c  to  this  tree  in  the  same  way, leaving
the  edges  lm,  jm,  jl,  and  jk  build  a  tree  containing  j,  k,  l,  and  m  that has  a  slightly  different  structure  (note  that  jl  doesn’t  contribute  anything, since  lm  and  jm  put  l  and  j  in  the  same  component),  and  the  edges  ed, fd,  and  hi  build  two  more  trees,  leaving  the  forest:
ing  to  those  edges  that  were  actually  used  to  visit  vertices  via  recursive  calls and  dotted  edges  corresponding  to  those  edges  pointing  to  vertices  that  had already  been  visited  at  the  time  the  edge  was  considered.  the  nodes  are visited  in  the  order  a  f  e  d  b  g  j  k  l  m  c  h  i.
note  that  the  directions  on  the  edges  make  this  depth-first  search  forest quite  different  from  the  depth-first  search  forests  that  we  saw  for  undirected graphs.  for  example,  even  though  the  original  graph  was  connected,  the depth-first  search  structure  defined  by  the  solid  edges  is  not  connected:  it  is a  forest,  not  a  tree.
for  undirected  graphs,  we  had  only  one  kind  of  dotted  edge,  one  that connected  a  vertex  with  some  ancestor  in  the  tree.  for  directed  graphs,  there are  three  kinds  of  dotted  edges:  up  edges,  which  point  from  a  vertex  to  some ancestor in the tree, down edges, which point from a vertex to some descendant in  the  tree,  and  cross  edges,  which  point  from  a  vertex  to  some  vertex  which is  neither  a  descendant  nor  an  ancestor  in  the  tree.
as  with  undirected  graphs,  we’re  interested  in  connectivity  properties  of directed  graphs.  we  would  like  to  be  able  to  answer  questions  like  “is  there a directed path  from  vertex  x  to  vertex  y  (a  path  which  only  follows  edges  in the  indicated  direction)?” and  “which  vertices  can  we  get  to  from  vertex  x with  a  directed  path?” and  “is  there  a  directed  path  from  vertex  x  to  vertex y  and  a  directed  path  from  y  to  x.7”  just  as  with  undirected  graphs,  we’ll  be able  to  answer  such  questions  by  appropriately  modifying  the  basic  depth-first search  algorithm,  though  the  various  different  types  of  dotted  edges  make  the modifications  somewhat  more  complicated.
in undirected graphs, simple connectivity gives the vertices that can be reached from  a  given  vertex  by  traversing  edges  from  the  graph:  they  are  all  those  in the  same  connected  component.  similarly,  for  directed  graphs,  we’re  often interested  in  the  set  of  vertices  which  can  be  reached  from  a  given  vertex  by traversing  edges  from  the  graph  in  the  indicated  direction.
it  is  easy  to  prove  that  the  recursive  visit  procedure  from  the  depth-first search  method  in  chapter  29  visits  all  the  nodes  that  can  be  reached  from  the start  node.  thus,  if  we  modify  that  procedure  to  print  out  the  nodes  that  it  is visiting  (say,  by  inserting  write(name(k))   just  upon  entering),  we  are  printing out  all  the  nodes  that  can  be  reached  from  the  start  node.  but  note  carefully that  it  is  not  necessarily  true  that  each  tree  in  the  depth-first  search  forest contains  all  the  nodes  that  can  be  reached  from  the  root  of  that  tree  (in  our example,  all  the  nodes  in  the  graph  can  be  reached  from  h,  not  just  i).  to get  all  the  nodes  that  can  be  visited  from  each  node,  we  simply  call  visit  v times,  once  for  each  node:
ing  to  those  edges  that  were  actually  used  to  visit  vertices  via  recursive  calls and  dotted  edges  corresponding  to  those  edges  pointing  to  vertices  that  had already  been  visited  at  the  time  the  edge  was  considered.  the  nodes  are visited  in  the  order  a  f  e  d  b  g  j  k  l  m  c  h  i.
note  that  the  directions  on  the  edges  make  this  depth-first  search  forest quite  different  from  the  depth-first  search  forests  that  we  saw  for  undirected graphs.  for  example,  even  though  the  original  graph  was  connected,  the depth-first  search  structure  defined  by  the  solid  edges  is  not  connected:  it  is a  forest,  not  a  tree.
for  undirected  graphs,  we  had  only  one  kind  of  dotted  edge,  one  that connected  a  vertex  with  some  ancestor  in  the  tree.  for  directed  graphs,  there are  three  kinds  of  dotted  edges:  up  edges,  which  point  from  a  vertex  to  some ancestor in the tree, down edges, which point from a vertex to some descendant in  the  tree,  and  cross  edges,  which  point  from  a  vertex  to  some  vertex  which is  neither  a  descendant  nor  an  ancestor  in  the  tree.
as  with  undirected  graphs,  we’re  interested  in  connectivity  properties  of directed  graphs.  we  would  like  to  be  able  to  answer  questions  like  “is  there a directed path  from  vertex  x  to  vertex  y  (a  path  which  only  follows  edges  in the  indicated  direction)?” and  “which  vertices  can  we  get  to  from  vertex  x with  a  directed  path?” and  “is  there  a  directed  path  from  vertex  x  to  vertex y  and  a  directed  path  from  y  to  x.7”  just  as  with  undirected  graphs,  we’ll  be able  to  answer  such  questions  by  appropriately  modifying  the  basic  depth-first search  algorithm,  though  the  various  different  types  of  dotted  edges  make  the modifications  somewhat  more  complicated.
in undirected graphs, simple connectivity gives the vertices that can be reached from  a  given  vertex  by  traversing  edges  from  the  graph:  they  are  all  those  in the  same  connected  component.  similarly,  for  directed  graphs,  we’re  often interested  in  the  set  of  vertices  which  can  be  reached  from  a  given  vertex  by traversing  edges  from  the  graph  in  the  indicated  direction.
it  is  easy  to  prove  that  the  recursive  visit  procedure  from  the  depth-first search  method  in  chapter  29  visits  all  the  nodes  that  can  be  reached  from  the start  node.  thus,  if  we  modify  that  procedure  to  print  out  the  nodes  that  it  is visiting  (say,  by  inserting  write(name(k))   just  upon  entering),  we  are  printing out  all  the  nodes  that  can  be  reached  from  the  start  node.  but  note  carefully that  it  is  not  necessarily  true  that  each  tree  in  the  depth-first  search  forest contains  all  the  nodes  that  can  be  reached  from  the  root  of  that  tree  (in  our example,  all  the  nodes  in  the  graph  can  be  reached  from  h,  not  just  i).  to get  all  the  nodes  that  can  be  visited  from  each  node,  we  simply  call  visit  v times,  once  for  each  node:
the  worst  case  for  trees  built  with  digital  searching  will  be  much  better than  for  binary  search  trees.  the  length  of  the  longest  path  in  a  digital search  tree  is  the  length  of  the  longest  match  in  the  leading  bits  between any  two  keys  in  the  tree,  and  this  is  likely  to  be  relatively  short.  and  it  is obvious  that  no  path  will  ever  be  any  longer  than  the  number  of  bits  in  the keys:  for  example,  a  digital  search  tree  built  from  eight-character  keys  with, say,  six  bits  per  character  will  have  no  path  longer  than  48,  even  if  there are  hundreds  of  thousands  of  keys. for  random  keys,  digital  search  trees are  nearly  perfectly  balanced  (the  height  is  about  1gn).   thus,  they  provide an  attractive  alternative  to  standard  binary  search  trees,  provided  that  bit extraction  can  be  done  as  easily  as  key  comparison  (which  is  not  really  the case  in  pascal).
radix  search  tries it  is  quite  often  the  case  that  search  keys  are  very  long,  perhaps  consisting  of twenty  characters  or  more.  in  such  a  situation,  the  cost  of  comparing  a  search key  for  equality  with  a  key  from  the  data  structure  can  be  a  dominant  cost which  cannot  be  neglected.  digital  tree  searching  uses  such  a  comparison  at each  tree  node:  in  this  section  we’ll  see  that  it  is  possible  to  get  by  with  only one  comparison  per  search  in  most  cases.
the  idea  is  to  not  store  keys  in  tree  nodes  at  all,  but  rather  to  put  all the  keys  in  external  nodes  of  the  tree.  that  is,  instead  of  using  a  for  external nodes  of  the  structure,  we  put  nodes  which  contain  the  search  keys.  thus, we  have  two  types  of  nodes:  internal  nodes,  which  just  contain  links  to  other nodes,  and  external  nodes,  which  contain  keys  and  no  links.  (e.  fredkin
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
working.  we’ll  see  methods  that  are  more  efficient  in  the  next  few  chapters, but  they’re  perhaps  only  twice  as  fast  (if  that  much)  except  for  large  n,  and they’re  significantly  more  complicated.  in  short,  if  you  have  a  sorting  problem, use  the  above  program,  then  determine  vvhether  the  extra  effort  required  to replace  it  with  a  sophisticated  method  will  be  worthwhile.  (on  the  other hand,  the  quicksort  algorithm  of  the  next  chapter  is  not  that  much  more difficult  to  implement.  .  .  )
digression: bubble  sort an  elementary  sorting  method  that  is  often  taught  in  introductory  classes  is bubble  sort:  keep  passing  through  the  file,  exchanging  adjacent  elements,  if necessary;  when  no  exchanges  are  required  on  some  pass,  the  file  is  sorted. an  implementation  of  this  method  is  given  below.
it  takes  a  moment’s  reflection  to  convince  oneself  first  that  this  works  at  all, it  is  not  clear  why  this  method second  that  the  running  time  is  quadratic. is  so  often  taught,  since  insertion  sort  seems  simpler  and  more  efficient  by almost  any  measure.  the  inner  loop  of  bubble  sort  has  about  twice  as  many instructions  as  either  insertion  sort  or  selection  sort.
distribution  counting a  very  special  situation  for  which  there  is  a  simple  sorting  algorithm  is  the following:  “sort  a  file  of  n  records  whose  keys  are  distinct  integers  between  1 and  n.”   the  algorithm  for  this  problem  is
for  example,  when  the  4  at  the  end  of  the  file  is  encountered,  it’s  put  into location  12,  since  count[4]  says  that  there  are  12  keys  less  than  or  equal  to 4.  then  count[4]  is  decremented,  since  there’s  now  one  less  key  less  than  or equal  to  4.  the  inner  loop  goes  from  n  down  to  1  so  that  the  sort  will  be stable.  (the  reader  may  wish  to  check  this.)
this  method  will  work  very  well  for  the  type  of  files  postulated.  furthermore,  it  can  be  extended  to  produce  a  much  more  powerful  method  that  we’ll examine  in  chapter  10.
non-random  files we  usually  think  of  sorting  files  that  are  in  some  arbitrarily  scrambled  order. however,  it  is  quite  often  the  case  that  we  have  a  lot  of  information  about  a file  to  be  sorted.  for  example,  one  often  wants  to  add  a  few  elements  to  a sorted file and thus produce a larger sorted file. one way to do so is to simply append  the  new  elements  to  the  end  of  t.he   file,  then  call  a  sorting  algorithm. general-purpose  sorts  are  commonly  mi’sused   for  such  applications;  actually, elementary  methods  can  take  advantage  of  the  order  present  in  the  file.
for  example,  consider  the  operation  of  insertion  sort  on  a  file  which  is already  sorted.  each  element  is  immediately  determined  to  be  in  its  proper  in the  file,  and  the  total  running  time  is  linear.  the  same  is  true  for  bubble  sort, but  selection  sort  is  still  quadratic.  (the  leading  term  in  the  running  time  of selection  sort  does  not  depend  on  the  order  in  the  file  to  be  sorted.)
even  if  a  file  is  not  completely  sorted,  insertion  sort  can  be  quite  useful because  the  running  time  of  insertion  sort  depends  quite  heavily  on  the  order present  in  the  file.  the  running  time  depends  on  the  number  of  inversions:  for each  element  count  up  the  number  of  e:iements   to  its  left  which  are  greater. this  is  the  distance  the  elements  have  to  move  when  inserted  into  the  file during  insertion  sort.  a  file  which  has  some  order  in  it  will  have  fewer inversions  in  it  than  one  which  is  arbitrarily  scrambled.
the  example  cited  above  of  a  file  formed  by  tacking  a  few  new  elenients onto  a  large  sorted  file  is  clearly  a  case  where  the  number  of  the  inversions is  low:  a  file  which  has  only  a  constant  number  of  elements  out  of  place  will have  only  a  linear  number  of  inversions.  another  example  is  a  file  where  each element  is  only  a  constant  distance  front  its  final  position.  files  like  this  can be  created  in  the  initial  stages  of  some  advanced  sorting  methods:  at  a  certain point  it  is  worthwhile  to  switch  over  to  jnsertion   sort.
in  short,  insertion  sort  is  the  method  of  choice  for  “almost  sorted”  files with  few  inversions:  for  such  files,  it  will  outperform  even  the  sophisticated methods  in  the  next  few  chapters.
the  implementation  above  moves  the  file  from  a  to  t  during  each  distribution  counting  phase,  then  back  to  a  in  a  simple  loop.  this  “array  copy” loop  could  be  eliminated  if  desired  by  making  two  copies  of  the  distribution counting  code,  one  to  sort  from  a  into  t,  the  other  to  sort  from  t  into  a. a  linear  sort the  straight  radix  sort  implementation  given  in  the  previous  section  makes b/m   passes  through  the  file.  by  making  rr:  large,  we  get  a  very  efficient  sorting method,  as  long  as  we  have  m =  2m   words  of  memory  available.  a  reasonable choice  is  to  make  m  about  one-fourth  th,e  word-size  (b/4),  so  that  the  radix sort  is  four  distribution  counting  passes.  the  keys  are  treated  as  base-m numbers,  and  each  (base--m)  digit  of  each  key  is  examined,  but  there  are only  four  digits  per  key.  (this  directly  corresponds  with  the  architectural organization  of  many  computers:  one  typical  organization  is  to  have  32-bit words,  each  consisting  of  four  g-bit   bytes.  the  bits  procedure  then  winds  up extracting  particular  bytes  from  words  in  this  case,  which  obviously  can  be done  very  efficiently  on  such  computers.)  now,  each  distribution  counting pass  is  linear,  and  since  there  are  only  four  of  them,  the  entire  sort  is  linear, certainly  the  best  performance  we  could  hope  for  in  a  sort.
in fact, it turns out that we can get  bj, with only two distribution counting passes.  (even  a  careful  reader  is  likely  ‘10  have  difficulty  telling  right  from left  by  this  time,  so  some  caution  is  called  for  in  trying  to  understand  this method.)  this  can  be  achieved  by  taking  advantage  of  the  fact  that  the  file will  be  almost  sorted  if  only  the  leading  b,‘2  bits  of  the  bbit  keys  are  used.  as with  quicksort,  the  sort  can  be  completed  efficiently  by  using  insertion  sort on  the  whole  file  afterwards.  this  method  is  obviously  a  trivial  modification to  the  implementation  above:  to  do  a  right-to-left  sort  using  the  leading  half of  the  keys,  we  simply  start  the  outer  loop  at  pass=b  div  (2*m)   rather  than pass=l.   then  a  conventional  insertion  sol-t   can  be  used  on  the  nearly-ordered file  that  results.  to  become  convinced  that  a  file  sorted  on  its  leading  bits is  quite  well-ordered,  the  reader  should  examine  the  first  few  columns  of  the table  for  radix  exchange  sort  above.  for  example,  insertion  sort  run  on  the the  file  sorted  on  the  first  three  bits  would  require  only  six  exchanges.
using  two  distribution  counting  passes  (with  m  about  one-fourth  the  word size),  then  using  insertion  sort  to  finish  ;he   job  will  yield  a  sorting  method that  is  likely  to  run  faster  than  any  of  the  others  that  we’ve  seen  for  large  files whose  keys  are  random  bits.  its  main  disal,dvantage   is  that  it  requires  an  extra array  of  the  same  size  as  the  array  being  sorted.  it  is  possible  to  eliminate the  extra  array  using  linked-list    but  extra  space  proportional  to n  (for  the  links)  is  still  required.
to  1.  in  the  example,  the  last  two  terms  are  0  when  z  =  1,  the  first  and  last terms  are  0  when  x  =  2,  and  the  first  two  terms  are  0  when  x  =  3.
to  convert  a  polynomial  from  the  form  described  by  lagrange’s  formula to  our  standard  coefficient  representation  is  not  at  all  straightforward.  at least  n2   operations  seem  to  be  required,  since  there  are  n  terms  in  the  sum, each  consisting  of  a  product  with  n  factors.  actually,  it  takes  some  cleverness to  achieve  a  quadratic  algorithm,  since  the  factors  are  not  just  numbers,  but polynomials  of  degree  n.  on  the  other  hand,  each  term  is  very  similar  to the  previous  one.  the  reader  might  be  interested  to  discover  how  to  take advantage  of  this  to  achieve  a  quadratic  algorithm.  this  exercise  leaves  one with  an  appreciation  for  the  non-trivial  nature  of  writing  an  efficient  program to  perform  the  calculation  implied  by  a  mathematical  formula.
as  with  polynomial  evaluation,  there  are  more  sophisticated  methods which  can  solve  the  problem  in  n(log  n)2   steps,  and  in  chapter  36  we’ll  see a  method  that  uses  only  n  log  n  multiplications  for  a  specific  set  of  n  points of  interest.
multiplication our  first  sophisticated  arithmetic  algorithm  is  for  the  problem  of  polynomial multiplication:  given  two  polynomials  p(x)  and  q(x),  compute  their  product p(x)q(x).  as  noted  in  chapter  2,  polynomials  of  degree  n  -  1  could  have n  terms  (including  the  constant)  and  the  product  has  degree  2n  -  2  and  as many  as  2n  -  1  terms.  for  example,
the  naive  algorithm  for  this  problem  that  we  implemented  in  chapter  2 requires  n2  multiplications  for  polynomials  of  degree  n  -  1:  each  of  the  n terms  of  p(x)  must  be  multiplied  by  each  of  the  n  terms  of  q(x).
to  improve  on  the  naive  algorithm,  we’ll  use  a  powerful  technique  for algorithm  design  called  divide-and-conquer:  split  the  problem  into  smaller parts,  solve  them  (recursively),  then  put  the  results  back  together  in  some way.  many  of  our  best  algorithms  are  designed  according  to  this  principle. in  this  section  we’ll  see  how  divide-and-conquer  applies  in  particular  to  the polynomial  multiplication  problem.  in  the  following  section  we’ll  look  at  some analysis  which  gives  a  good  estimate  of  how  much  is  saved.
one  way  to  split  a  polynomial  in  two  is  to  divide  the  coefficients  in  half: given  a  polynomial  of  degree  n-l  (with  n  coefficients)  we  can  split  it  into  two polynomials  with  n/2  coefficients  (assume  that  n  is  even):  by  using  the  n/2 low-order  coefficients  for  one  polynomial  and  the  n/2  high-order  coefficients
why  is  the  divide-and-conquer  method  g:iven   above  an  improvement?  in  this section,  we’ll  look  at  a  few  simple  recurrence  formulas  that  can  be  used  to measure  the  savings  achieved  by  a  divide-and-conquer  algorithm.
from  the  recursive  program,  it  is  clear  that  the  number  of  integer  multiplications  required  to  multiply  two  polynomials  of  size  n  is  the  same  as  the number  of  multiplications  to  multiply  three  pairs  of  polynomials  of  size  n/2. (note  that,  for  example,  no  multiplications  are  required  to  compute  t~(z)z~, just  data  movement.)  if  m(n)  is  the  number  of  multiplications  required  to multiply  two  polynomials  of  size  n,  we  have
for  n  >  1  with  m(1)  =  1.  thus  m(2)  q  =  3,  m(4)  =  9,  m(8)  =  27,  etc.  in general,  if  we  take  n  =  2n,   then  we  can  repeatedly  apply  the  recurrence  to itself  to  find  the  solution:
if  n  =  2n,   then  3% =  2(‘s31n  =  2n1s3  =  n’s3.   although  this  solution  is  exact only  for  n  =  2n,   it  works  out  in  general  that
which  is  a  substantial  savings  over  the  n2   naive  method.  note  that  if  we  were to  have  used  all  four  multiplications  in  the  simple  divide-and-conquer  method, the  recurrence  would  be  m(n)  =  4m(nl/2)   with  the  solution  m(2n)  =  4n  = n2.
the  method  described  in  the  previous  section  nicely  illustrates  the  divideand-conquer technique, but it is seldom  usled   in  practice  because  a  much  better divide-and-conquer  method  is  known,  which  we’ll  study  in  chapter  36.  this method  gets  by  with  dividing  the  original  into  only  two  subproblems,  with a  little  extra  processing.  the  recurrence  describing  the  number  of  multiplications  required  is
though  we  don’t  want  to  dwell  on  the  mathematics  of  solving  such  recurrences,  formulas  of  this  particular  form  arise  so  frequently  that  it  will  be worthwhile  to  examine  the  development  of  an  approximate  solution.  first,  as above,  we  write  n  =  2?
performed.  otherwise  the  procedure  first  shuffles,  then  recursively  calls  itself to  transform  the  two  halves,  then  combines  the  results  of  these  computations as  described  above.  of  course,  the  actual  values  of  the  complex  roots  of  unity are  needed  to  do  the  implementation.  it  is  well  known  that
these  values  are  easily  computed  using  conventional  trigonometric  functions. in  the  above  program,  the  array  w  is  assumed  to  hold  the  (outn+l)st   roots  of unity.  to  get  the  roots  of  unity  needed,  the  program  selects  from  this  array at  an  interval  determined  by  the  variable  i.  for  example,  if  outhj   were  15, the  fourth  roots  of  unity  would  be  found  in  w[o],   w[4],w[8],  and  w[12].   this eliminates  the  need  to  recompute  roots  of  unity  each  time  they  are  used.
as  mentioned  at  the  outset,  the  scope  of  applicability  of  the  fft  is  far greater  than  can  be  indicated  here;  and  the  algorithm  has  been  intensively used  and  studied  in  a  variety  of  domains. nevertheless,  the  fundamental principles  of  operation  in  more  advanced  applications  are  the  same  as  for  the polynomial  multiplication  problem  that  we’ve  discussed  here.  the  fft  is a  classic  example  of  t.he   application  of  the.  “divide-and-conquer”  algorithm design  paradigm  to  achieve  truly  significant  computational  savings.
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
one  advantage  of  this  algorithm  over  the  shortest  paths  algorithm  of chapter  31  is  that  it  works  properly  even  if  negative  edge  weights  are  allowed, as  long  as  there  are  no  cycles  of  negative  weight  in  the  graph  (in  which  case the  shortest  paths  connecting  nodes  on  the  cycle  are  not  defined).  if  a  cycle of  negative  weight  is  present  in  the  graph,  then  the  algorithm  can  detect  that fact,  because  in  that  case  a[i,  i]  will  become  negative  for  some  i  at  some  point during  the  algorithm.
time  and  space  requirements the  above  examples  demonstrate  that  dynamic  programming  applications  can have  quite  different  time  and  space  requirements  depending  on  the  amount  of information  about  small  subproblems  that  must  be  saved.  for  the  shortest paths  algorithm,  no  extra  space  was  required;  for  the  knapsack  problem, space  proportional  to  the  size  of  the  knapsack  was  needed;  and  for  the  other problems  n2  space  was  needed.  for  each  problem,  the  time  required  was  a factor  of  n  greater  than  the  space  required.
the  range  of  possible  applicability  of  dynamic  programming  is  far  larger than  covered  in  the  examples.  from  a  dynamic  programming  point  of  view, divide-and-conquer  recursion  could  be  thought  of  as  a  special  case  in  which a  minimal  amount  of  information  about  small  cases  must  be  computed  and stored,  and  exhaustive  search  (which  we’ll  examine  in  chapter  39)  could  be thought  of  as  a  special  case  in  which  a  maximal  amount  of  information  about small  cases  must  be  computed  and  stored.  dynamic  programming  is  a  natural design  technique  that  appears  in  many  guises  to  solve  problems  throughout this  range.
because  it  combines  a  sequential  key  organization  with  indexed  access, this  organization  is  called  indexed  sequential.  it  is  the  method  of  choice  for applications  where  changes  to  the  database  are  likely  to  be  made  infrequently. the disadvantage of using indexed sequential access is that it is very inflexible. for  example,  adding  b  to  the  configuration  above  requires  that  virtually  the whole  database  be  rebuilt,  with  new  positions  for  many  of  the  keys  and  new values  for  the  indices.
a  better  way  to  handle  searching  in  a  dynamic  situation  is  to  use  balanced trees.  in  order  to  reduce  the  number  of  (relatively  expensive)  disk  accesses,  it is reasonable to allow a large number of keys per node so that the nodes have a  large  branching  factor.  such  trees  were  named  b-trees  by  r.  bayer  and e.  mccreight,  who  were  the  first  to  consider  the  use  of  multiway   balanced trees  for  external  searching.  (many  people  reserve  the  term  “b-tree”  to describe  the  exact  data  structure  built  by  the  algorithm  suggested  by  bayer and  mccreight;  we’ll  use  it  as  a  generic  term  to  mean  “external  balanced trees.“)
the  top-down  algorithm  that  we  used  for  2-3-4  trees  extends  readily  to handle  more  keys  per  node:  assume  that  there  are  anywhere  from  1  to  m  -  1 keys  per  node  (and  so  anywhere  from  2  to  m  links  per  node).  searching proceeds  in  a  way  analogous  to  2-3-4  trees:  to  move  from  one  node  to  the next,  first  find  the  proper  interval  for  the  search  key  in  the  current  node  and then  exit  through  the  corresponding  link  to  get  to  the  next  node.  continue in  this  way  until  an  external  node  is  reached,  then  insert  the  new  key  into the  last  internal  node  reached.  as  with  top-down  2-3-4  trees,  it  is  necessary to  “split”  nodes  that  are  “full”  on  the  way  down  the  tree:  any  time  we  see a  k-node  attached  to  an  m  node,  we  replace  it  by  a  (k  +  1)-node  attached to  two  m/2  nodes.  this  guarantees  that  when  the  bottom  is  reached  there is  room  to  insert  the  new  node.  the  b-tree  constructed  for  m  =  4  and  our sample  keys  is  diagrammed  below:
to  begin,  we’ll  consider  a  pascal  program  which  is  essentially  a  translation  of  the  definition  of  the  concept  of  the  greatest  common  divisor  into  a programming 
the  body  of  the  program  above  is  trivial:  it  reads  two  numbers  from  the input,  then  writes  them  and  their  greatest  common  divisor  on  the  output. the  gcd  function  implements  a  “brute-force”  method:  start  at  the  smaller  of the  two  inputs  and  test  every  integer  (decreasing  by  one  until  1  is  reached) until  an  integer  is  found  that  divides  both  of  the  inputs.  the  built-in  function abs  is  used  to  ensure  that  gcd  is  called  with  positive  arguments.  (the  mod function  is  used  to  test  whether  two  numbers  divide:  u  mod  v  is  the  remainder when  u  is  divided  by  v,   so  a  result  of  0  indicates  that  v  divides  u.)
many  other  similar  examples  are  given  in  the  pascal  user  manual  and report.  the  reader  is  encouraged  to  scan  the  manual,  implement  and  test some  simple  programs  and  then  read  the  manual  carefully  to  become  reasonably  comfortable  with  most  of  the  features  of  pascal.
euclid’s algorithm a  much  more  efficient  method  for  finding  the  greatest  common  divisor  than that  above  was  discovered  by  euclid  over  two  thousand  years  ago.  euclid’s method  is  based  on  the  fact  that  if  u  is  greater  than  v  then  the  greatest common  divisor  of  u  and  v  is  the  same  as  the  greatest  common  divisor  of  v and  u  -  v.  applying  this  rule  successively,  we  can  continue  to  subtract  off multiples  of  v  from  u  until  we  get  a  number  less  than  v.  but  this  number  is
exactly  the  same  as  the  remainder  left  after  dividing  u  by  v,  which  is  what the  mod  function  computes:  the  greatee:t   common  divisor  of  u  and  v  is  the same as the greatest common divisor of  1) and u mod v. if  u mod v is 0, then v divides  u  exactly  and  is  itself  their  greatest  common  divisor,  so  we  are  done. this  mathematical  description  explains  how  to  compute  the  greatest common  divisor  of  two  numbers  by  computing  the  greatest  common  divisor of  two  smaller  numbers.  we  can  implement  this  method  directly  in  pascal simply  by  having  the  gcd  function  call  itself  with  smaller  arguments:
(note  that  if  u  is  less  than  v,  then  u  m’od  v  is  just  u,  and  the  recursive  call just  exchanges  u  and  v  so  things  work  as  described  the  next  time  around.) if  the  two  inputs  are  461952  and  116298,  then  the  following  table  shows  the values  of  u  and  v  each  time  gcd  is  invoked:
recursion a  fundamental  technique  in  the  design  of  efficient  algorithms  is  recursion: solving  a  problem  by  solving  smaller  versions  of  the  same  problem,  as  in  the program  above.  we’ll  see  this  general  approach  used  throughout  this  book, and  we  will  encounter  recursion  many  tirnes.  it  is  important,  therefore,  for  us to  take  a  close  look  at  the  features  of  the  above  elementary  recursive  program. an  essential  feature  is  that  a  recursive  program  must  have  a  termination condition.  it  can’t  always  call  itself,  there  must  be  some  way  for  it  to  do
a  reader  interested  in  learning  more  about  pascal  will  find  a  large  number of  introductory  textbooks  available,  for  example,  the  ones  by  clancy  and cooper  or  holt  and  hune.  someone  with  experience  programming  in  other languages  can  learn  pascal  effectively  directly  from  the  manual  by  wirth  and jensen.  of  course,  the  most  important  thing  to  do  to  learn  about  the  language is  to  implement  and  debug  as  many  programs  as  possible.
many  introductory  pascal  textbooks  contain  some  material  on  data  structures.  though  it  doesn’t  use  pascal,  an  important  reference  for  further  information  on  basic  data  structures  is  volume  one  of  d.e.  knuth’s  series  on  the art  of computer  programming.  not  only  does  this  book  provide  encyclopedic coverage,  but  also  it  and  later  books  in  the  series  are  primary  references  for much  of  the  material  that  we’ll  be  covering  in  this  book.  for  example,  anyone interested  in  learning  more  about  euclid’s  algorithm  will  find  about  fifty  pages devoted  to  it  in  knuth’s  volume  two.
another  reason  to  study  knuth’s  volume  one  is  that  it  covers  in  detail the  mathematical  techniques  needed  for  the  analysis  of  algorithms.  a  reader with  little  mathematical  background  sh’ould   be  warned  that  a  substantial amount  of  discrete  mathematics  is  required  to  properly  analyze  many  algorithms;  a  mathematically  inclined  reader  will  find  much  of  this  material  ably summarized  in  knuth’s  first  book  and  applied  to  many  of  the  methods  we’ll be  studying  in  later  books.
m.  clancy  and  d.  cooper,  oh! pascal,  w. w. norton  &  company,  new  york, 1982. r.  holt  and  j.  p.hume,  programming  standard pascal,  reston   (prentice-hall), reston,  virginia,  1980. d.  e.  knuth,  the  art  of  computer  programming.  volume  1:  fundamental algorithms,  addison-wesley,  reading,  ma,  1968. d.  e.  knuth,  the art  of  computer  programming.  volume  .2:  seminumerical algorithms,  addison-wesley,  reading,  ma,  second  edition,  1981. k.  jensen  and  n.  wirth,  pascal  user  manual  and  report,  springer-verlag, new  york,  1974.
between  points  in  the  plane,  then  a  self-intersecting  tour  can  be  improved  by removing  each  intersection  as  follows.  if  the  line  al3   intersects  the  line  cd, the  situation  can  be  diagramed  as  at  left  below,  without  loss  of  generality. but  it  follows  immediately  that  a  shorter  tour  can  be  formed  by  deleting  ab and  cd  and  adding  ad  and  cb,  as  diagramed  at  right:
applying  this  procedure  successively  will,  given  any  tour,  produce  a  tour  that is  no  longer  and  which  is  not  self-intersecting.  for  example,  the  procedure applied  to  the  tour  produced  from  the  minimum  spanning  tree  in  the  example above  gives  the  shorter  tour  agoenlpkfjmbdhica.  in  fact,  one  of  the most  effective  approaches  to  producing  approximate  solutions  to  the  euclidean traveling  salesman  problem,  developed  by  s.  lin,  is  to  generalize  the  procedure above to improve tours by switching around three or more edges in an existing tour.  very  good  results  have  been  obtained  by  applying  such  a  procedure successively,  until  it  no  longer  leads  to  an  improvement,  to  an  initially  random tour.  one  might  think  that  it  would  be  better  to  start  with  a  tour  that  is already  close  to  the  optimum,  but  lin’s  studies  indicate  that  this  may  not  be the case.
the  various  approaches  to  producing  approximate  solutions  to  the  traveling  salesman  problem  which  are  described  above  are  only  indicative  of  the types  of  techniques  that  can  be  used  in  order  to  avoid  exhaustive  search.  the brief  descriptions  above  do  not  do  justice  to  the  many  ingenious  ideas  that have  been  developed:  the  formulation  and  analysis  of  algorithms  of  this  type is  still  a  quite  active  area  of  research  in  computer  science.
one  might  legitimately  question  why  the  traveling  salesman  problem  and the  other  problems  that  we  have  been  alluding  to  require  exhaustive  search. couldn’t  there  be  a  clever  algorithm  that  finds  the  minimal  tour  as  easily and  quickly  as  we  can  find  the  minimum  spanning  tree?  in  the  next  chapter we’ll  see  why  most  computer  scientists  believe  that  there  is  no  such  algorithm and  why  approximation  algorithms  of  the  type  discussed  in  this  section  must therefore  be  studied.
between  points  in  the  plane,  then  a  self-intersecting  tour  can  be  improved  by removing  each  intersection  as  follows.  if  the  line  al3   intersects  the  line  cd, the  situation  can  be  diagramed  as  at  left  below,  without  loss  of  generality. but  it  follows  immediately  that  a  shorter  tour  can  be  formed  by  deleting  ab and  cd  and  adding  ad  and  cb,  as  diagramed  at  right:
applying  this  procedure  successively  will,  given  any  tour,  produce  a  tour  that is  no  longer  and  which  is  not  self-intersecting.  for  example,  the  procedure applied  to  the  tour  produced  from  the  minimum  spanning  tree  in  the  example above  gives  the  shorter  tour  agoenlpkfjmbdhica.  in  fact,  one  of  the most  effective  approaches  to  producing  approximate  solutions  to  the  euclidean traveling  salesman  problem,  developed  by  s.  lin,  is  to  generalize  the  procedure above to improve tours by switching around three or more edges in an existing tour.  very  good  results  have  been  obtained  by  applying  such  a  procedure successively,  until  it  no  longer  leads  to  an  improvement,  to  an  initially  random tour.  one  might  think  that  it  would  be  better  to  start  with  a  tour  that  is already  close  to  the  optimum,  but  lin’s  studies  indicate  that  this  may  not  be the case.
the  various  approaches  to  producing  approximate  solutions  to  the  traveling  salesman  problem  which  are  described  above  are  only  indicative  of  the types  of  techniques  that  can  be  used  in  order  to  avoid  exhaustive  search.  the brief  descriptions  above  do  not  do  justice  to  the  many  ingenious  ideas  that have  been  developed:  the  formulation  and  analysis  of  algorithms  of  this  type is  still  a  quite  active  area  of  research  in  computer  science.
one  might  legitimately  question  why  the  traveling  salesman  problem  and the  other  problems  that  we  have  been  alluding  to  require  exhaustive  search. couldn’t  there  be  a  clever  algorithm  that  finds  the  minimal  tour  as  easily and  quickly  as  we  can  find  the  minimum  spanning  tree?  in  the  next  chapter we’ll  see  why  most  computer  scientists  believe  that  there  is  no  such  algorithm and  why  approximation  algorithms  of  the  type  discussed  in  this  section  must therefore  be  studied.
doubles,  then  so  does  the  running  time.  this  is  the  optimal  situation for  an  algorithm  that  must  process  n  inputs  (or  produce  n  outputs). nlogn   this  running  time  arises  in  algorithms  which  solve  a  problem  by breaking  it  up  into  smaller  subpr’oblems,   solving  them  independently, and  then  combining  the  solutions.  for  lack  of  a  better  adjective (linearithmic?),   we’ll  say  that  th’e  running  time  of  such  an  algorithm is  “n  log  n.” when  n  is  a  million,  n  log  n  is  perhaps  twenty million.  when  n  doubles,  the  running  time  more  than  doubles  (but not  much  more). when  the  running  time  of  an  algorithm  is  quadratic,  it  is  practical for  use  only  on  relatively  small  problems.  quadratic  running  times typically  arise  in  algorithms  which  process  all  pairs  of  data  items (perhaps  in  a  double  nested  loop).  when  n  is  a  thousand,  the running  time  is  a  million.  whenever  n  doubles,  the  running  time increases  fourfold.   triples  of  data  items  (perhaps similarly,  an  algorithm  which  prlocesses in  a  triple-nested  loop)  has  a  cubic  running  time  and  is  practical  for use  only  on  small  problems.  vvhen   n  is  a  hundred,  the  running time  is  a  million.  whenever  n  doubles,  the  running  time  increases eightfold. few  algorithms  with  exponential  running  time  are  likely  to  be  appropriate  for  practical  use,  though  such  algorithms  arise  naturally  as “brute-force”  solutions  to  problems.  when  n  is  twenty,  the  running time  is  a  million.  whenever  n  doubles,  the  running  time  squares!
the  running  time  of  a  particular  prlogram   is  likely  to  be  some  constant times  one  of  these  terms  (the  “leading  term”)  plus  some  smaller  terms.  the values of the constant coefficient and the terms included depends on the results of  the  analysis  and  on  implementation  details.  roughly,  the  coefficient  of  the leading  term  has  to  do  with  the  number  of  instructions  in  the  inner  loop: at  any  level  of  algorithm  design  it’s  prudent  to  limit  the  number  of  such instructions.  for  large  n  the  effect  of  the  leading  term  dominates;  for  small n  or  for  carefully  engineered  algorithms,  more  terms  may  contribute  and comparisions  of  algorithms  are  more  difficult.  in  most  cases,  we’ll  simply  refer to  the  running  time  of  programs  as  “linear,”  “n  log  n,  ”  “cubic,”   etc.,  with the  implicit  understanding  that  more  detailed  analysis  or  empirical  studies must  be  done  in  cases  where  efficiency  is  very  important.
a  few  other  functions  do  arise.  for  example,  an  algorithm  with  n2 inputs  that  has  a  running  time  that  is  cubic  in  n  is  more  properly  classed as  an  n3j2   algorithm.  also  some  algorithms  have  two  stages  of  subproblem decomposition,  which  leads  to  a  running  time  proportional  to  n(log  n)2.   both
a  more  complicated  problem  is  to  evaluate  a  given  polynomial  at  many different  points.  different  algorithms  are  appropriate  depending  on  how  many evaluations  are  to  be  done  and  whether  or  not  they  are  to  be  done  simultaneously.  if  a  very  large  number  of  evaluations  is  to  be  done,  it  may  be worthwhile  to  do  some  “precomputing”   which  can  slightly  reduce  the  cost for  later  evaluations.  note  that  using  horner’s  method  would  require  about n2   multiplications  to  evaluate  a  degree-n  polynomial  at  n  different  points. much  more  sophisticated  methods  have  been  designed  which  can  solve  the problem  in  n(logn)’   steps,  and  in  chapter  36  we’ll  see  a  method  that  uses only  n  log  n  multiplications  for  a  specific  set  of  n  points  of  interest.
if  the  given  polynomial  has  only  one  term,  then  the  polynomial  evaluation  problem  reduces  to  the  exponentiation  problem:  compute  xn.   horner’s rule  in  this  case  degenerates  to  the  trivial  algorithm  which  requires  n  -  1 multiplications.  for  an  easy  example  of  how  we  can  do  much  better,  consider the  following  sequence  for  computing  x32:
the  “successive  squaring”  method  can  easily  be  extended  to  general  n if  computed  values  are  saved.  for  example,  x55  can  be  computed  from  the above  values  with  four  more  multiphcations:
in  general,  the  binary  representation  of  n  can  be  used  to  choose  which computed  values  to  use.  (in  the  example,  since  55  =  (110111)2,   all  but  x8 are  used.)  the  successive  squares  can  be  computed  and  the  bits  of  n  tested within  the  same  loop.  two  methods  are  available  to  implement  this  using  only
one  “accumulator,” like  horner’s  method.  one  algorithm  involves  scanning the  binary  representation  of  n  from  left  to  right,  starting  with  1  in  the accumulator.  at  each  step,  square  the  accumulator  and  also  multiply  by  z when  there  is  a  1  in  the  binary  representation  of  n.  the  following  sequence of  values  is  computed  by  this  method  for  n  =  55:
another  well-known  alforithm   whks  similarly,  bht   scans  n  from  right  to left.  this  problem  is  a  standard  introductory  programming  exercise,  but  it  is hardly  of  practical  interest.
interpolation the  “inverse”  problem  to  the  problem  of  evaluating  a  polynomial  of  degree  n at  n  points  simultaneously  is  the  problem  of  polynomial  interpolation:  given a  set  of  n  points  x1  ,x2,.  . .  ,xn   and  associated  values  yr,y2,. . .  ,yn,  find  the unique  polynomial  of  degree  n  - 1  which1   has
the  interpolation  problem  is  to  find  the  polynomial,  given  a  set  of  points  and values.  the  evaluation  problem  is  to  find  the  values,  given  the  polynomial and  the  points.  (the  problem  of  finding  the  points,  given  the  polynomial  and the  values,  is  root-finding.)
the  classic  solution  to  the  interpolation  problem  is  given  by  lagrange’s interpolation  formula,  which  is  often  used  as  a  proof  that  a  polynomial  of degree  n  -  1  is  completely  determined  by  n  points:
this  formula  seems  formidable  at  first  but  is  actually  quite  simple.  for example,  the  polynomial  of  degree  2  which  has  p(l)  =  3,  p(2)  =  7,  and  p(3)  = 13 is given by
for x from  xl,  x2,  .  .  .  , xn,   the  formula  is  constructed  so  that  p(xk)  =  yk   for 1  5   k  5   n,  since  the  product  evaluates  to  0  unless  j =  k,  when  it  evaluates
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
there  are  many  other  factors  to  be  t&ken   into  consideration  in  implementing  a  most  efficient  tape-sorting  method.  for  example,  a  major  factor  which we  have  not  considered  at  all  is  the  timt:  that  it  takes  to  rewind  a  tape.  this subject has been studied extensively,  ant  many  fascinating  methods  have  been defined.  however,  as  mentioned  above,  the  savings  achievable  over  the  simple multiway   balanced  merge  are  quite  limited.  even  polyphase  merging  is  only better  than  balanced  merging  for  small  p,  and  then  not  substantially.  for p  >  8,  balanced  merging  is  likely  to  run  j’aster   than  polyphase,  and  for  smaller p  the effect of polyphase is basically to sue  two tapes (a balanced merge with two  extra  tapes  will  run  faster).
an  easier  way many  modern  computer  systems  provide  a  large  virtual  memory  capability which  should  not  be  overlooked  in  imp  ementing  a  method  for  sorting  very large  files.  in  a  good  virtual  memory  syf#tem,   the  programmer  has  the  ability to address a very large amount of data, leaving to the system the responsibility of  making  sure  that  addressed  data  is  lransferred  from  external  to  internal storage when needed. this strategy relict on the fact that many programs have a  relatively  small  “locality  of  reference”  : each  reference  to  memory  is  likely  to be  to  an  area  of  memory  that  is  relatively   close  to  other  recently  referenced areas.  this  implies  that  transfers  from  e:rternal   to  internal  storage  are  needed infrequently.  an  int,ernal   sorting  method  with  a  small  locality  of  reference  can work  very  well  on  a  virtual  memory  system.  (for  example,  quicksort  has  two “localities”  : most  references  are  near  one  of  the  two  partitioning  pointers.) but  check  with  your  systems  programmclr   before  trying  it  on  a  very  large  file: a  method  such  as  radix  sorting,  which  he,s  no  locality  of  reference  whatsoever, would  be  disastrous  on  a  virtual  memory   system,  and  even  quicksort  could cause  problems,  depending  on  how  well  the  available  virtual  memory  system is  implemented.  on  the  other  hand,  th’:   strategy  of  using  a  simple  internal sorting  method  for  sorting  disk  files  desl:rves   serious  consideration  in  a  good virtual  memorv   environment.
variables,  so  we  have  computed  a  feasible  basis  for  the  original  linear  program. in  degenerate  cases,  some  of  the  artificial  variables  may  remain  in  the  basis, so  it  is  necessary  to  do  further  pivoting  to  remove  them  (without  changing the cost).
to summarize, a two-phase process is normally used to solve general linear programs.  first,  we  solve  a  linear  program  involving  the  artificial  s  variables to  get  a  point  on  the  simplex  for  our  original  problem.  then,  we  dispose  of the  s  variables  and  reintroduce  our  original  objective  function  to  proceed  from this  point  to  the  solution.
the  analysis  of  the  running  time  of  the  simplex  method  is  an  extremely complicated  problem,  and  few  results  are  available.  no  one  knows  the  “best” pivot selection strategy, because there are no results to tell us how many pivot steps to expect, for any reasonable class of problems. it is possible to construct artificial  examples  for  which  the  running  time  of  the  simplex  could  be  very large  (an  exponential  function  of  the  number  of  variables).  however,  those who  have  used  the  algorithm  in  practical  settings  are  unanimous  in  testifying to  its  efficiency  in  solving  actual  problems.
the  simple  version  of  the  simplex  algorithm  that  we’ve  considered,  while quite  useful,  is  merely  part  of  a  general  and  beautiful  mathematical  framework providing  a  complete  set  of  tools  which  can  be  used  to  solve  a  variety  of  very important  practical  problems.
the  message:  this  means  that  we  need  to  save  the  tree  along  with  the  message in  order  to  decode  it.  fortunately,  this  does  not  present  any  real  difficulty. it  is  actually  necessary  only  to  store  the  code  array,  because  the  radix  search trie  which  results  from  inserting  the  entries  from  that  array  into  an  initially empty  tree  is  the  decoding  tree.
thus,  the  storage  savings  quoted  above  is  not  entirely  accurate,  because the  message  can’t  be  decoded  without  the  trie  and  we  must  take  into  account the  cost  of  storing  the  trie  (i.e., the  code  array)  along  with  the  message. huffman  encoding is therefore only effective for long files where the savings in the message is enough to offset the cost, or in situations where the coding trie can be  precomputed   and  used  for  a  large  number  of  messages.  for  example,  a trie  based  on  the  frequencies  of  occurrence  of  letters  in  the  english  language could  be  used  for  text  documents. for  that  matter,  a  trie  based  on  the frequency  of  occurrence  of  characters  in  pascal  programs  could  be  used  for encoding  programs  (for  example, “;”  is  likely  to  be  near  the  top  of  such  a trie).  a  huffman   encoding  algorithm  saves  about  23%  when  run  on  the  text for  this  chapter.
as  before,  for  truly  random  files,  even  this  clever  encoding  scheme  won’t work  because  each  character  will  occur  approximately  the  same  number  of times,  which  will  lead  to  a  fully  balanced  coding  tree  and  an  equal  number  of bits  per  letter  in  the  code.
the  message:  this  means  that  we  need  to  save  the  tree  along  with  the  message in  order  to  decode  it.  fortunately,  this  does  not  present  any  real  difficulty. it  is  actually  necessary  only  to  store  the  code  array,  because  the  radix  search trie  which  results  from  inserting  the  entries  from  that  array  into  an  initially empty  tree  is  the  decoding  tree.
thus,  the  storage  savings  quoted  above  is  not  entirely  accurate,  because the  message  can’t  be  decoded  without  the  trie  and  we  must  take  into  account the  cost  of  storing  the  trie  (i.e., the  code  array)  along  with  the  message. huffman  encoding is therefore only effective for long files where the savings in the message is enough to offset the cost, or in situations where the coding trie can be  precomputed   and  used  for  a  large  number  of  messages.  for  example,  a trie  based  on  the  frequencies  of  occurrence  of  letters  in  the  english  language could  be  used  for  text  documents. for  that  matter,  a  trie  based  on  the frequency  of  occurrence  of  characters  in  pascal  programs  could  be  used  for encoding  programs  (for  example, “;”  is  likely  to  be  near  the  top  of  such  a trie).  a  huffman   encoding  algorithm  saves  about  23%  when  run  on  the  text for  this  chapter.
as  before,  for  truly  random  files,  even  this  clever  encoding  scheme  won’t work  because  each  character  will  occur  approximately  the  same  number  of times,  which  will  lead  to  a  fully  balanced  coding  tree  and  an  equal  number  of bits  per  letter  in  the  code.
the  combination  of  the  escape  character,  the  count,  and  the  one  copy of  the  repeated  character  is  called  an  escape  sequence.  note  that  it’s  not worthwhile  to  encode  runs  less  than  four  characters  long  since  at  least  three characters  are  required  to  encode  any  run.
but  what  if  the  escape  character  itself  happens  to  occur  in  the  input? we  can’t  afford  to  simply  ignore  this  possibility,  because  it  might  be  difficult to  ensure  that  any  particular  character  can’t  occur.  (for  example,  someone might  try  to  encode  a  string  that  has  already  been  encoded.)  one  solution  to this problem is to use an escape sequence with a count of zero to represent the escape  character.  thus,  in  our  example,  the  space  character  could  represent zero,  and  the  escape  sequence  “q(space)”  would  be  used  to  represent  any occurrence  of  q  in  the  input.  it  is  interesting  to  note  that  files  which  contain q  are  the  only  files  which  are  made  longer  by  this  compression  method.  if  a file  which  has  already  been  compressed  is  compressed  again,  it  grows  by  at least  the  number  of  characters  equal  to  the  number  of  escape  sequences  used. very  long  runs  can  be  encoded  with  multiple  escape  sequences.  for example, a run of 51 a’s would be encoded as qzaqya using the conventions above.  if  many  very  long  runs  are  expected,  it  would  be  worthwhile  to  reserve more  than  one  character  to  encode  the  counts.
in  practice,  it  is  advisable  to  make  both  the  compression  and  expansion programs  somewhat  sensitive  to  errors.  this  can  be  done  by  including  a  small amount  of  redundancy  in  the  compressed  file  so  that  the  expansion  program can  be  tolerant  of  an  accidental  minor  change  to  the  file  between  compression and  expansion.  for  example,  it  probably  is  worthwhile  to  put  “end-of-line” characters  in  the  compressed  version  of  the  letter  “q”  above,  so  that  the expansion  program  can  resynchronize   itself  in  case  of  an  error.
run-length  encoding  is  not  particularly  effective  for  text  files  because  the only  character  likely  to  be  repeated  is  the  blank,  and  there  are  simpler  ways  to encode  repeated  blanks.  (it  was  used  to  great  advantage  in  the  past  to  compress  text  files  created  by  reading  in  punched-card  decks,  which  necessarily contained  many  blanks.)  in  modern  systems,  repeated  strings  of  blanks  are never  entered,  never  stored:  repeated  strings  of  blanks  at  the  beginning  of lines  are  encoded  as  “tabs,”  blanks  at  the  ends  of  lines  are  obviated  by  the use  of  “end-of-line”  indicators.  a  run-length  encoding  implementation  like the  one  above  (but  modified  to  handle  all  representable  characters)  saves  only about  4%  when  used  on  the  text  file  for  this  chapter  (and  this  savings  all comes  from  the  letter  “q”  example!).
the  running  time  of  this  program  obviously  depends  very  heavily  on the  pattern  being  matched.  however,  for  each  of  the  n  input  characters,  it processes  at  most  m  states  of  the  mac:nne,   so  the  worst  case  running  time is  proportional  to  mn.  for  sure,  not  all  nondeterministic  machines  can  be simulated  so  efficiently,  as  discussed  in  more  detail  in  chapter  40,  but  the  use of  a  simple  hypothetical  pattern-matching  machine  in  this  application  leads to  a  quite  reasonable  algorithm  for  a  quite  difficult  problem.  however,  to complete  the  algorithm,  we  need  a  program  which  translates  arbitrary  regular expressions  into  “machines”  for  interpretation  by  the  above  code.  in  the  next chapter,  we’ll  look  at  the  implementation  of  such  a  program  in  the  context  of a  more  general  discussion  of  compilers  a,nd   parsing  techniques.
notice  that  this  eliminates  zi  from  the  second  equation.  in  a  similar  manner, we  can  eliminate  xi  from  the  third  equation  by  replacing  the  third  equation by  the  sum  of  the  first  and  third:
now  the  variable  zi  is  eliminated  from  all  but  the  first  equation.  by  systematically  proceeding  in  this  way,  we  can  transform  the  original  system  of equations  into  a  system  with  the  same  solution  that  is  much  easier  to  solve. for  the  example,  this  requires  only  one  more  step  which  combines  two  of  the operations  above:  replacing  the  third  equation  by  the  difference  between  the second  and  twice  the  third.  this  makes  all  of  the  elements  below  the  main diagonal  0:  systems  of  equations  of  this  form  are  particularly  easy  to  solve. the  simultaneous  equations  which  result  in  our  example  are:
now  the  third  equation  can  be  solved  immediately:  x3  =  2.  if  we  substitute this  value  into  the  second  equation,  we  can  compute  the  value  of  x2:
this  example  illustrates  the  two  basic  phases  of  gaussian  elimination. the  first  is  the  forward elimination  phase,  where  the  original  system  is  transformed,  by  systematically  eliminating  variables  from  equations,  into  a  system with  all  zeros  below  the  diagonal.  this  process  is  sometimes  called  triangulation.  the  second  phase  is  the  backward  substitution  phase,  where  the  values of  the  variables  are  computed  using  the  t:riangulated   matrix  produced  by  the first  phase. outline of the method in  general,  we  want  to  solve  a  system  of  n  equations  in  n  unknowns:
or  simply  ax  =  b,  where  a  represents  the  matrix,  z  represents  the  variables, and  b  represents the  rightrhand   sides  of  the  equations.  since  the  rows  of  a are  manipulated  along  with  the  elements  of  b,  it  is  convenient  to  regard  b  as the (n  +  1)st  column of  a  and  use  an  n-by-(n  +  1)  array  to  hold  both.
now  the  forward  elimination  phase  can  be  summarized  as  follows:  first eliminate  the  first  variable  in  all  but  the  first  equation  by  adding  the  appropriate  multiple  of  the  first  equation  to  each  of  the  other  equations,  then eliminate  the  second  variable  in  all  but  the  first  two  equations  by  adding  the appropriate  multiple  of  the  second  equation  to  each  of  the  third  through  the nth  equations,  then  eliminate  the  third  variable  in  all  but  the  first  three equations,  etc.  to  eliminate  the  ith  variable  in  the  jth  equation  (for  j  between  i  $-  1  and  n)  we  multiply  the  ith  equation  by  aji/aii   and  subtract  it from  the  jth  equation.  this  process  is  perhaps  more  succinctly  described  by the  following  program,  which  reads  in  n  followed  by  an  n-by-(  n  +  1)  matrix, performs  the  forward  elimination,  and  writes  out  the  triangulated  result.  in the  input,  and  in  the  output  the  ith  line  contains  the  ith  row  of  the  matrix followed by b,.
(a  call  to  eliminate  should  replace  the  three  nested  for  loops  in  the  program gauss  given  above.)  there  are  some  algorithms  where  it  is  required  that  the pivot  a[i,  i]  be  used  to  eliminate  the  ith  variable  from  every  equation  but  the ith  (not  just  the  (i+l)st   through  the  nth).  this  process  is  called  full  pivoting; for forward elimination we only do part of this work hence the process is called partial pivoting .
after  the  forward  elimination  phase  has  completed,  the  array  a  has all  zeros  below  the  diagonal,  and  the  backward  substitution  phase  can  be executed.  the  code  for  this  is  even  more  straightforward:
a  call  to  eliminate  followed  by  a  call  to  substitute  computes  the  solution  in the  n-element  array  x.  division  by  0  could  still  occur  for  singular  matrices.
performed.  otherwise  the  procedure  first  shuffles,  then  recursively  calls  itself to  transform  the  two  halves,  then  combines  the  results  of  these  computations as  described  above.  of  course,  the  actual  values  of  the  complex  roots  of  unity are  needed  to  do  the  implementation.  it  is  well  known  that
these  values  are  easily  computed  using  conventional  trigonometric  functions. in  the  above  program,  the  array  w  is  assumed  to  hold  the  (outn+l)st   roots  of unity.  to  get  the  roots  of  unity  needed,  the  program  selects  from  this  array at  an  interval  determined  by  the  variable  i.  for  example,  if  outhj   were  15, the  fourth  roots  of  unity  would  be  found  in  w[o],   w[4],w[8],  and  w[12].   this eliminates  the  need  to  recompute  roots  of  unity  each  time  they  are  used.
as  mentioned  at  the  outset,  the  scope  of  applicability  of  the  fft  is  far greater  than  can  be  indicated  here;  and  the  algorithm  has  been  intensively used  and  studied  in  a  variety  of  domains. nevertheless,  the  fundamental principles  of  operation  in  more  advanced  applications  are  the  same  as  for  the polynomial  multiplication  problem  that  we’ve  discussed  here.  the  fft  is a  classic  example  of  t.he   application  of  the.  “divide-and-conquer”  algorithm design  paradigm  to  achieve  truly  significant  computational  savings.
an  alternate  way  to  proceed  after  forward  elimination  has  created  all zeros  below  the  diagonal  is  to  use  precisely  the  same  method  to  produce  all zeros  above  the  diagonal:  first  make  the  last   column  zero  except  for  a[n,  n] by  adding  the  appropriate  multiple  of  a[n,  n],  then  do  the  same  for  the  nextto-last  column,  etc.  that  is,  we  do  “partial  pivoting”  again,  but  on  the  other “part”  of  each  column,  working  backwards  through  the  columns.  after  this process,  called  gauss-  jordan  reduction,  is  complete,  only  diagonal  elements are  non-zero,  which  yields  a  trivial  solution.
computational  errors  are  a  prime  source  of  concern  in  gaussian  elimination.  as  mentioned  above,  we  should  be  wary  of  situations  when  the  magnitudes  of  the  coefficients  vastly  differ.  using  the  largest  available  element in  the  column  for  partial  pivoting  ensures  that  large  coefficients  won’t  be  arbitrarily  created  in  the  pivoting  process,  but  it  is  not  always  possible  to  avoid severe  errors.  for  example,  very  small  coefficients  turn  up  when  two  different equations  have  coefficients  which  are  quite  close  to  one  another.  it  is  actually possible  to  determine  in  advance  whether  such  problems  will  cause  inaccurate answers  in  the  solution.  each  matrix  haa   an  associated  numerical  quantity called the  condition number  which  can  ble  used  to  estimate  the  accuracy  of the  computed  answer.  a  good  library  subroutine  for  gaussian  elimination will  compute  the  condition  number  of  the  matrix  as  well  as  the  solution,  so that  the  accuracy  of  the  solution  can  be  lknown.  full  treatment  of  the  issues involved would be beyond the scope of this book.
gaussian  elimination  with  partial  pivoting  using  the  largest  available pivot  is  “guaranteed”  to  produce  results  with  very  small  computational  errors. there  are  quite  carefully  worked  out  mathematical  results  which  show  that  the calculated  answer  is  quite  accurate,  except  for  ill-conditioned  matrices  (which might  be  more  indicative  of  problems  in  .the   system  of  equations  than  in  the method  of  solution).  the  algorithm  has  been  the  subject  of  fairly  detailed theoretical  studies,  and  can  be  recommended  as  a  computational  procedure of  very  wide  applicability.
variations  and  extensions the  method  just  described  is  most  appropriate  for  n-by-n  matrices  with most of the  n2  elements  non-zero.  as  we’ve  seen  for  other  problems,  special techniques  are  appropriate  for  sparse  matrices  where  most  of  the  elements  are 0.  this  situation  corresponds  to  systems  ‘of   equations  in  which  each  equation has  only  a  few  terms.
if  the  non-zero  elements  have  no  particular  structure,  then  the  linked list  representation  discussed  in  chapter  ‘2  is  appropriate,  with  one  node  for each  non-zero  matrix  element,  linked  together  by  both  row  and  column.  the
or  the  accumulation  of  computational  errors.  for  some  types  of  tridiagonal matrices  which  arise  commonly,  it  can  be  proven  that  this  is  not  a  reason  for concern.
gauss-jordan  reduction  can  be  implemented  with  full  pivoting  to  replace a  matrix  by  its  inverse  in  one  sweep    it.  the  inverse  of  a  matrix a,  written  a-‘,   has  the  property  that  a  system  of  equations  ax  =  b  could be  solved  just  by  performing  the  matrix  multiplication  z  =  a-lb.   still,  n3 operations  are  required  to  compute  x  given  b.  however,  there  is  a  way  to preprocess  a  matrix  and  “decompose”  it  into  component  parts  which  make it  possible  to  solve  the  corresponding  system  of  equations  with  any  given rightchand  side  in  time  proportional  to  1v2,   a  savings  of  a  factor  of  n  over using  gaussian  elimination  each  time. roughly,  this  involves  remembering the  operations  that  are  performed  on  the  (n +  1)st   column  during  the  forward elimination  phase,  so  that  the  result  of  forward  elimination  on  a  new  (n  +  1)st column  can  be  computed  efficiently  and  then  back-substitution  performed  as usual.
solving systems of linear equations has been shown to be computationally equivalent  to  multiplying  matrices,  so  tlhere  exist  algorithms  (for  example, strassen’s  matrix  multiplication  algorithm)  which  can  solve  systems  of  n equations  in  n  variables  in  time  proportional  to  n2.*l....   as  with  matrix multiplication,  it  would  not  be  worthwhile  to  use  such  a  method  unless  very large  systems  of  equations  were  to  be  processed  routinely.  as  before,  the actual  running  time  of  gaussian  elimination  in  terms  of  the  number  of  inputs is  n312.   which  is  difficult  to  imnrove   uoon in  nractice.
certainly  one  of  the  most  fundam.ental  scientific  computations  is  the solution  of  systems  of  simultaneous  equations.  the  basic  algorithm  for solving  systems  of  equations,  gaussian  elimination,  is  relatively  simple  and has  changed  little  in  the  150  years  since  it  was  invented.  this  algorithm  has come  to  be  well  understood,  especially  in  the  past  twenty  years,  so  that  it  can be  used  with  some  confidence  that  it  will  efficiently  produce  accurate  results. this  is  an  example  of  an  algorithm  that  will  surely  be  available  in  most computer  installations;  indeed,  it  is  a  primitive  in  several  computer  languages, notably  apl  and  basic.  however,  the  basic  algorithm  is  easy  to  understand and  implement,  and  special  situations  do  arise  where  it  might  be  desirable to  implement  a  modified  version  of  the  algorithm  rather  than  work  with  a standard  subroutine.  also,  the  method  deserves  to  be  learned  as  one  of  the most  important  numeric  methods  in  use  today.
as  with  the  other  mathematical  material  that  we  have  studied  so  far,  our treatment  of  the  method  will  highlight  only  the  basic  principles  and  will  be self-contained.  familiarity  with  linear  algebra  is  not  required  to  understand the  basic  method.  we’ll  develop  a  simple  pascal  implementation  that  might be  easier  to  use  than  a  library  subroutine  for  simple  applications.  however, we’ll  also  see  examples  of  problems  which  could  arise.  certainly  for  a  large  or important  application,  the  use  of  an  expertly  tuned  implementation  is  called for,  as  well  as  some  familiarity  with  the  underlying  mathematics.
or  the  accumulation  of  computational  errors.  for  some  types  of  tridiagonal matrices  which  arise  commonly,  it  can  be  proven  that  this  is  not  a  reason  for concern.
gauss-jordan  reduction  can  be  implemented  with  full  pivoting  to  replace a  matrix  by  its  inverse  in  one  sweep    it.  the  inverse  of  a  matrix a,  written  a-‘,   has  the  property  that  a  system  of  equations  ax  =  b  could be  solved  just  by  performing  the  matrix  multiplication  z  =  a-lb.   still,  n3 operations  are  required  to  compute  x  given  b.  however,  there  is  a  way  to preprocess  a  matrix  and  “decompose”  it  into  component  parts  which  make it  possible  to  solve  the  corresponding  system  of  equations  with  any  given rightchand  side  in  time  proportional  to  1v2,   a  savings  of  a  factor  of  n  over using  gaussian  elimination  each  time. roughly,  this  involves  remembering the  operations  that  are  performed  on  the  (n +  1)st   column  during  the  forward elimination  phase,  so  that  the  result  of  forward  elimination  on  a  new  (n  +  1)st column  can  be  computed  efficiently  and  then  back-substitution  performed  as usual.
solving systems of linear equations has been shown to be computationally equivalent  to  multiplying  matrices,  so  tlhere  exist  algorithms  (for  example, strassen’s  matrix  multiplication  algorithm)  which  can  solve  systems  of  n equations  in  n  variables  in  time  proportional  to  n2.*l....   as  with  matrix multiplication,  it  would  not  be  worthwhile  to  use  such  a  method  unless  very large  systems  of  equations  were  to  be  processed  routinely.  as  before,  the actual  running  time  of  gaussian  elimination  in  terms  of  the  number  of  inputs is  n312.   which  is  difficult  to  imnrove   uoon in  nractice.
or  simply  ax  =  b,  where  a  represents  the  matrix,  z  represents  the  variables, and  b  represents the  rightrhand   sides  of  the  equations.  since  the  rows  of  a are  manipulated  along  with  the  elements  of  b,  it  is  convenient  to  regard  b  as the (n  +  1)st  column of  a  and  use  an  n-by-(n  +  1)  array  to  hold  both.
now  the  forward  elimination  phase  can  be  summarized  as  follows:  first eliminate  the  first  variable  in  all  but  the  first  equation  by  adding  the  appropriate  multiple  of  the  first  equation  to  each  of  the  other  equations,  then eliminate  the  second  variable  in  all  but  the  first  two  equations  by  adding  the appropriate  multiple  of  the  second  equation  to  each  of  the  third  through  the nth  equations,  then  eliminate  the  third  variable  in  all  but  the  first  three equations,  etc.  to  eliminate  the  ith  variable  in  the  jth  equation  (for  j  between  i  $-  1  and  n)  we  multiply  the  ith  equation  by  aji/aii   and  subtract  it from  the  jth  equation.  this  process  is  perhaps  more  succinctly  described  by the  following  program,  which  reads  in  n  followed  by  an  n-by-(  n  +  1)  matrix, performs  the  forward  elimination,  and  writes  out  the  triangulated  result.  in the  input,  and  in  the  output  the  ith  line  contains  the  ith  row  of  the  matrix followed by b,.
this system of equations is a simple “tridiagonal” form which is easily solved with a degenerate version of gaussian elimination as we saw in chapter 5. if we let  ui  =  zi+l  - zi, di =  2(zi+l -xi--i),  and  wi  =  zi   - zi.-1,  we have, for example, the following simultaneous equ.ations  for n = 7:
in  fact,  this  is  a  symmetric  tridiagonal  system,  with  the  diagonal  below  the main diagonal equal to the diagonal above the main diagonal. it turns out that pivoting  on  the  largest  available  element  is  not  necessary  to  get  an  accurate solution for this system of equations.
var  i:  integer; begin readln  (n)  ; for i:=l to n do readln(x[i],y[i]); for  i:=2  to  n-l  do  d[i]:=2*(x[i+l]-x[i-11); for  i:=l  to  n-l  do  u[i]:=x[i+l]-x[i]; for  i:=2  to  n-l  do
used  for  this  type  of  application. the  idea  is  fundamentally  the  same;  the problem  is  that  the  derivatives  may  not  be  easy  to  compute.  what  is  used is  an  iterative  method:  use  some  estimate  for  the  coefficients,  then  use  these within  the  method  of  least  squares  to  compute  the  derivatives,  thus  producing a  better  estimate  for  the  coefficients.  this  basic  method,  which  is  widely  used today,  was  outlined  by  gauss  in  the  1820s.
exactly  the  same  as  the  remainder  left  after  dividing  u  by  v,  which  is  what the  mod  function  computes:  the  greatee:t   common  divisor  of  u  and  v  is  the same as the greatest common divisor of  1) and u mod v. if  u mod v is 0, then v divides  u  exactly  and  is  itself  their  greatest  common  divisor,  so  we  are  done. this  mathematical  description  explains  how  to  compute  the  greatest common  divisor  of  two  numbers  by  computing  the  greatest  common  divisor of  two  smaller  numbers.  we  can  implement  this  method  directly  in  pascal simply  by  having  the  gcd  function  call  itself  with  smaller  arguments:
(note  that  if  u  is  less  than  v,  then  u  m’od  v  is  just  u,  and  the  recursive  call just  exchanges  u  and  v  so  things  work  as  described  the  next  time  around.) if  the  two  inputs  are  461952  and  116298,  then  the  following  table  shows  the values  of  u  and  v  each  time  gcd  is  invoked:
recursion a  fundamental  technique  in  the  design  of  efficient  algorithms  is  recursion: solving  a  problem  by  solving  smaller  versions  of  the  same  problem,  as  in  the program  above.  we’ll  see  this  general  approach  used  throughout  this  book, and  we  will  encounter  recursion  many  tirnes.  it  is  important,  therefore,  for  us to  take  a  close  look  at  the  features  of  the  above  elementary  recursive  program. an  essential  feature  is  that  a  recursive  program  must  have  a  termination condition.  it  can’t  always  call  itself,  there  must  be  some  way  for  it  to  do
much of the material described in this section has actually been developed quite  recently,  so  there  are  many  fewer  available  references  than  for  older, more  central  areas  such  as  sorting  or  mathematical  algorithms.  many  of  the problems  and  solutions  that  we’ve  discussed  were  presented  by  m.  shamos  in 1975.  shamos’  manuscript  treats  a  large  number  of  geometric  algorithms,  and has  stimulated  much  of  the  recent  research.
for  the  most  part,  each  of  the  geometric  algorithms  that  we’ve  discussed is  described  in  its  own  original  reference.  the  convex  hull  algorithms  treated in  chapter  25  may  be  found  in  the  papers  by  jarvis,  graham,  and  eddy.  the range  searching  methods  of  chapter  26  come  from  bentley  and  freidman’s survey  article,  which  contains  many  references  to  original  sources  (of  particular interest  is  bentley’s  own  original  article  on  kd  trees,  written  while  he  was  an undergraduate).  the  treatment  of  the  closest  point  problems  in  chapter  28  is based  on  shamos  and  hoey’s  1976  paper,  and  the  intersection  algorithms  of chapter  27  are  from  their  1975  paper  and  the  article  by  bentley  and  ottmann. but  the  best  route  for  someone  interested  in  learning  more  about  geometric  algorithms  is  to  implement  some,  work  with  them  and  try  to  learn  about their  behavior  on  different  types  of  point  sets.  this  field  is  still  in  its  infancy and  the  best  algorithms  are  yet  to  be  discovered. j.  l.  bentley,  “multidimensional  binary  search  trees  used  for  associative searching,”  communications  of  the  acm,  18,  9  (september,  1975). j.  l.  bentley  and  j.h.  friedman,  “data  structures  for  range  searching,” computing surveys, 11, 4 (december, 1979). j.  l.  bentley  and  t.  ottmann,   “algorithms  for  reporting  and  counting  geometric  intersections,” ieee  transactions  on  computing,  c-28,  9  (september, 1979). w.  f.  eddy,  “a  new  convex  hull  algorithm  for  planar  sets,”  acm  transactions on  mathematical  software,  3  (1977). r.  l.  graham,  “an  efficient  algorithm  for  determining  the  convex  hull  of  a finite  planar  set,”  information  processing  letters,  1  (1972). r.  a.  jarvis, “on  the  identification  of  the  convex  hull  of  a  finite  set  of  points in the plane,” information processing letters, 2 (1973). m.  i.  shamos,  problems in  computational  geometry,  unpublished  manuscript, 1975. m.  i.  shamos  and  d.  hoey,  “closest-point  problems,”  in  16th annual  symposium  on foundations  of  computer science,  ieee,  1975. m. i. shamos and d. hoey, “geometric  intersection  problems,”  in  17th  annual symposium  on  foundations  of  computer  science,  ieee,  1976.
each  node  in  this  tree  represents  a  vertical  line  dividing  the  points  in  the  left and  right  subtree.  the  nodes  are  numbered  in  the  order  in  which  the  vertical lines  are  tried  in  the  algorithm.  thus,  first  the  line  between  g  and  0  is  tried and  the  pair  go  is  retained  as  the  closest  so  far.  then  the  line  between  a  and d is tried, but a and d are too far apart to change  min. then the line between 0 and a is tried and the pairs gd  g-4 and oa all are successively closer pairs. it  happens  for  this  example  that  no  closer  pairs  are  found  until  fk,  which  is the  last  pair  checked  for  the  last  dividing  line  tried.  this  diagram  reflects  the difference  between  top-down  and  bottom-up  mergesort.  a  bottom-up  version of the closest-pair problem can be developed in the same way as for mergesort, which  would  be  described  by  a  tree  like  the  one  above,  numbered  left  to  right and  bottom  to  top.
the  general  approach  that  we’ve  used  for  the  closest-pair  problem  can be  used  to  solve  other  geometric  problems.  for  example,  another  problem  of interest  is  the  all-nearest-neighbors  problem:  for  each  point  we  want  to  find the  point  nearest  to  it.  this  problem  can  be  solved  using  a  program  like  the one  above  with  extra  processing  along  the  dividing  line  to  find,  for  each  point, whether  there  is  a  point  on  the  other  side  closer  than  its  closest  point  on  its own  side.  again,  the  “free”  y  sort  is  helpful  for  this  computation.
voronoi diagrams the set of all points closer to a given point in a point set than to all other points in  the  set  is  an  interesting  geometric  structure  called  the  voronoi polygon  for the  point.  the  union  of  all  the  voronoi  polygons  for  a  point  set  is  called  its voronoi  diagram.  this  is  the  ultimate  in  closest-point  computations:  we’ll  see that  most  of  the  problems  involving  distances  between  points  that  we  face have  natural  and  interesting  solutions  based  on  the  voronoi  diagram.  the diagram  for  our  sample  point  set  is  comprised  of  the  thick  lines  in  the  diagram below:
suppose  it  is  known  in  advance  that  the  convex  hull  of  a  set  of  points  is a  triangle.  give  an  easy  algorithm  for  finding  the  triangle.  answer  the same  question  for  a  quadrilateral.
is  it  strictly  necessary  for  the  package-wrapping  method  to  start  with  a point  guaranteed  to  be  on  the  hull?  explain  why  or  why  not.
does  the  graham  scan  work  for  finding  the  convex  hull  of  the  points which  make  up  the  vertices  of  any  simple  polygon?  explain  why  or  give a  counterexample  showing  why  not.
what  four  points  should  be  used  for  the  floyd-eddy  method  if  the  input is  assumed  to  be  randomly  distributed  within  a  circle  (using  random  polar coordinates)?
run  the  package-wrapping  method  for  large  points  sets  with  both  2  and y  equally  likely  to  be  between  0  and  1000.  use  your  curve  fitting  routine to  find  an  approximate  formula  for  the  running  time  of  your  program  for a  point  set  of  size  n.
use  your  curve-fitting  routine  to  find  an  approximate  formula  for  the number  of  points  left  after  the  floyd-eddy  method  is  used  on  point  sets with  x  and  y  equally  likely  to  be  between  0  and  1000.
suppose  that  we  use  an  arbitrary  point  for  the  “anchor”  in  the  method  for computing  a  simple  closed  polygon  described  in  the  text.  give  conditions which  such  a  point  must  satisfy  for  the  method  to  work.
write  a  program  like  draw  to  “fill  in”  an  arbitrary  triangle.  (your  program should  call  dot  for  all  the  points  inside  the  triangle.)
what  is  the  maximum  value  achievable  by  count  when  inside  is  executed on  a  polygon  with  n  vertices?  give  an  example  supporting  your  answer.
the  running  time  of  this  program  is  proportional  to  the  number  of  grid  squares touched.  since  we  were  careful  to  arrange  things  so  that  each  grid  square contains  a  constant  number  of  points  on  the  average,  this  is  also  proportional, on  the  average,  to  the  number  of  points  examined.  if  the  number  of  points in  the  search  rectangle  is  r,  then  the  number  of  grid  squares  examined  is proportional  to  r.  the  number  of  grid  squares  examined  which  do  not  fall completely  inside  the  search  rectangle  is  certainly  less  than  a  small  constant times  r,  so  the  total  running  time  (on  the  average)  is  linear  in  r,  the  number of  points  sought.  for  large  r,  the  number  of  points  examined  which  don’t  fall in  the  search  rectangle  gets  quite  small:  all  such  points  fall  in  a  grid  square which  intersects  the  edge  of  the  search  rectangle,  and  the  number  of  such squares  is  proportional  to  fi  for  large  r.  note  that  this  argument  falls apart  if  the  grid  squares  are  too  small  (too  many  empty  grid  squares  inside the  search  rectangle)  or  too  large  (too  many  points  in  grid  squares  on  the perimeter  of  the  search  rectangle)  or  if  the  search  rectangle  is  thinner  than the  grid  squares  (it  could  intersect  many  grid  squares,  but  have  few  points inside  it).
the  grid  method  works  well  if  the  points  are  well  distributed  over  the assumed  range  but  badly  if  they  are  clustered  together.  (for  example,  all the  points  could  fall  in  one  grid  box,  which  would  mean  that  all  the  grid machinery  gained  nothing.)  the  next  method  that  we  will  examine  makes this  worst  case  very  unlikely  by  subdividing  the  space  in  a  nonuniform  way,
perspective. from the few examples given, it should be clear that it is easy to underestimate the  difficulty  of  solving  a  particular  geometric  problem  with  a  computer. there  are  many  other  elementary  geometric  computations  that  we  have  not treated  at  all.  for  example,  a  program  to  compute  the  area  of  a  polygon makes  an  interesting  exercise.  however,  the  problems  that  we  have  studied have  provided  some  basic  tools  that  we  will  find  useful  in  later  sections  for solving  the  more  difficult  problems.
some  of  the  algorithms  that  we’ll  study  involve  building  geometric  structures  from  a  given  set  of  points.  the  “simple  closed  polygon”  is  an  elementary example  of  this.  we  will  need  to  decide  upon  appropriate  representations for  such  structures,  develop  algorithms  to  build  them,  and  investigate  their use  for  particular  applications  areas. as  usual,  these  considerations  are  intertwined.  for  example,  the  algorithm  used  in  the  inside  procedure  in  this chapter  depends  in  an  essential  way  on  the  representation  of  the  simple  closed polygon  as  an  ordered  set  of  points  (rather  than  as  an  unordered  set  of  lines). many  of  the  algorithms  that  we’ll  study  involve  geometric  search:  we want to know which points from a given set are close to a given point, or which points fall in a given rectangle, or which points are closest to each other. many of  the  algorithms  appropriate  for  such  search  problems  are  closely  related  to the  search  algorithms  that  we  studied  in  chapters  14-17.  the  parallels  will be  quite  evident.
few  geometric  algorithms  have  been  analyzed  to  the  point  where  precise statements  can  be  made  about  their  relative  performance  characteristics.  as we’ve  already  seen,  the  running  time  of  a  geometric  algorithm  can  depend  on many  things.  the  distribution  of  the  points  themselves,  the  order  in  which they  appear  in  the  input,  and  whether  trigonometric  functions  are  needed or  used  can  all  significantly  affect  the  running  time  of  geometric  algorithms. as  usual  in  such  situations,  we  do  have  empirical  evidence  which  suggests good  algorithms  for  particular  applications.  also,  many  of  the  algorithms  are designed  to  nerform   well  in  the  worst  case.  no  matter  what  the inout
compares  two  rectangles  whose  edges  are  horizontal  and  vertical  according  to the  trivial  rule  that  rectangle  5  is  to  the  left  of  rectangle  y  if  the  right  edge  of x is to the left of the left edge of y, then we can use the above method to test for  intersection  among  a  set  of  such  rectangles.  for  circles,  we  can  use  the  x coordinates  of  the  centers  for  the  ordering,  but  explicitly  test  for  intersection (for  example,  compare  the  distance  between  the  centers  to  the  sum  of  the radii).  again,  if  this  comparison  procedure  is  used  in  the  above  method,  we have  an  algorithm  for  testing  for  intersection  among  a  set  of  circles.  the problem  of  returning  all  intersections  in  such  cases  is  much  more  complicated, though  the  brute-force  method  mentioned  in  the  previous  paragraph  will always  work  if  few  intersections  are  expected.  another  approach  that  will suffice  for  many  applications  is  simply  to  consider  complicated  objects  as  sets of  lines  and  to  use  the  line  intersection  procedure.
algorithms  for  converting  geometric  objects  to  points  in  this  manner  are  called scan-conversion  algorithms.  this  example  illustrates  that  it  is  easy  to  draw nice-looking  diagonal  lines  like  eo  and  il,  but  that  it  is  somewhat  harder  to make  lines  of  arbitrary  slope  look  nice  using  a  coarse  matrix  of  characters.  the recursive  method  given  above  has  the  disadvantages  that  it  is  not  particularly efficient  (some  points  get  plotted  several  times)  and  that  it  doesn’t  draw certain  lines  very  well  (for  example  lines  which  are  nearly  horizontal  and nearly  vertical).  it  has  the  advantages  that  it  is  relatively  simple  and  that it  handles  all  the  cases  of  the  various  orientation  of  the  endpoints  of  the  line in  a  uniform  way.  many  sophisticated  scan-conversion  algorithms  have  been developed  which  are  more  efficient  and  more  accurate  than  this  recursive  one. if  the  array  has  a  very  large  number  of  dots,  then  the  ragged  edges  of the  lines  aren’t  discernible,  but  the  same  types  of  algorithms  are  appropriate. however,  the  very  high  resolution  necessary  to  make  high-quality  lines  can require  very  large  amounts  of  memory  (or  computer  time),  so  sophisticated algorithms  are  called  for,  or  other  technologies  might  be  appropriate.  for example,  the  text  of  this  book  was  printed  on  a  device  capable  of  printing millions of dots per square inch, but most of the lines in the figures were drawn
geometric  process.  in  three  dimensions,  branching  at  each  node  corresponds to  cutting  the  three-dimensional  region  of  interest  with  a  plane;  in  general  we cut  the  k-dimensional  region  of  interest  with  a  (k-  1)-dimensional  hyperplane. if  k  is  very  large,  there  is  likely  to  be  a  significant  amount  of  imbalance in  the  kd  trees,  again  because  practical  point  sets  can’t  be  large  enough  to take  notice  of  randomness  over  a  large  number  of  dimensions.  typically,  all points in a  subtree  will  have  the  same  value  across  several  dimensions,  which leads  to  several  one-way  branches  in  the  trees.  one  way  to  help  alleviate  this problem  is,  rather  than  simply  cycle  through  the  dimensions,  always  to  use  the dimension  that  will  divide  up  the  point  set  in  the  best  way.  this  technique can  also  be  applied  to  2d  trees.  it  requires  that  extra  information  (which dimension  should  be  discriminated  upon)  be  stored  in  each  node,  but  it  does relieve  imbalance,  especially  in  high-dimensional  trees.
in summary, though it is easy to see how to to generalize the programs for range  searching  that  we  have  developed  to  handle  multidimensional  problems, such a step should not be taken lightly for a large application. large databases with  many  attributes  per  record  can  be  quite  complicated  objects  indeed,  and it  is  often  necessary  to  have  a  good  understanding  of  the  characteristics  of the  database  in  order  to  develop  an  efficient  range-searching  method  for  a particular  application.  this  is  a  quite  important  problem  which  is  still  being activelv   studied.
in  this  example,  b  is  used  as  the  anchor.  if  the  points  are  visited  in  the  order b  m  j  l  n  p  k  f  i  e  c  0  a  h  g  d  b  then  a  simple  closed  polygon  will  be traced  out.
if  dx and dy are the delta x and y distances from some point to the anchor point,  then  the  angle  needed  in  this  algorithm  is  tan-’   dyldx.  although the  arctangent  is  a  built-in  function  in  pascal  (and  some  other  programming environments), it is likely to be slow and it leads to at least two annoying extra conditions  to  compute:  whether  dx  is  zero,  and  which  quadrant  the  point  is in.  since  the  angle  is  used  only  for  the  sort  in  this  algorithm,  it  makes  sense to  use  a  function  that  is  much  easier  to  compute  but  has  the  same  ordering properties  as  the  arctangent  (so  that  when  we  sort,  we  get  the  same  result). a  good  candidate  for  such  a  function  is  simply  dy/(dy  +  dz).  testing  for exceptional  conditions  is  still  necessary,  but  simpler.  the  following  program returns  a  number  between  0  and  360  that  is  not  the  angle  made  by  pl  and p2  with  the  horizontal  but  which  has  the  same  order  properties  as  the  true angle.
in  some  programming  environments  it  may  not  be  worthwhile  to  use  such programs  instead  of  standard  trigonometric  functions;  in  others  it  might  lead to  significant  savings.  (in  some  cases  it  might  be  worthwhile  to  change  theta to  have  an  integer  value,  to  avoid  using  real  numbers  entirely.)
inclusion in a polygon the  next  problem  that  we’ll  consider  is  a  natural  one:  given  a  polygon  represented  as  an  array  of  points  and  another  point,  determine  whether  the  point is  inside  or  outside.  a  straightforward  solution  to  this  problem  immediately suggests  itself:  draw  a  long  line  segment  from  the  point  in  any  direction  (long enough so that its other endpoint is guaranteed to be outside the polygon) and
this  procedure  goes  down  both  subtrees  only  when  the  dividing  line  cuts  the rectangle,  which  should  happen  infrequently  for  relatively  small  rectangles. although  the  method  hasn’t  been  fully  analyzed,  its  running  time  seems  sure to  be  proportional  to  r + log n to retrieve  r  points  from  reasonable  ranges  in a  region  containing  n  points,  which  makes  it  very  competitive  with  the  grid method.
multidimensional  range  searching both  the  grid  method  and  2d  trees  generalize  directly  to  more  than  two  dimensions:  simple,  straightforward  extensions  to  the  above  algorithms  immediately yield  range-searching  methods  which  work  for  more  than  two  dimensions. however,  the  nature  of  multidimensional  space  dictates  that  some  caution  is called  for  and  that  the  performance  characteristics  of  the  algorithms  might be  difficult  to  predict  for  a  particular  application.
to  implement  the  grid  method  for  k-dimensional  searching,  we  simply make  grid  a  k-dimensional  array  and  use  one  index  per  dimension.  the  main problem  is  to  pick  a  reasonable  value  for  size.  this  problem  becomes  quite obvious when large  k  is  considered:  what  type  of  grid  should  we  use  for  lodimensional  search?  the  problem  is  that  even  if  we  use  only  three  divisions per  dimension,  we  need  31°   grid  squares,  most  of  which  will  be  empty,  for reasonable  values  of  n.
the  generalization  from  2d  to  kd  trees  is  also  straightforward:  simply cycle  through  the  dimensions  (as  we  did  for  two  dimensions  by  alternating between  x  and  y)  while  going  down  the  tree.  as  before,  in  a  random  situation, the  resulting  trees  have  the  same  characteristics  as  binary  search  trees.  also as  before,  there  is  a  natural  correspondence  between  the  trees  and  a  simple
the program as given above could fail if there is more than one point with the  lowest  y  coordinate,  unless  theta  is  modified  to  properly  sort  collinear points,  as  described  above.  (this  is  a  subtle  point  which  the  reader  may wish  to  check.)  alternatively,  the  min  computation  could  be  modified  to  find the  point  with  the  lowest  x  coordinate  among  all  points  with  the  lowest  y coordinate,  the  canonical  form  described  in  chapter  24.
one  reason  that  this  method  is  interesting  to  study  is  that  it  is  a  simple form  of  backtracking,  the  algorithm  design  technique  of  “try  something,  if  it doesn’t  work  then  try  something  else”  which  we’ll  see  in  much  more  complicated  forms  in  chapter  39.
hull selection almost any convex hull method can be vastly improved by a method developed independently  by  w.  f.  eddy  and  r.  w.  floyd.  the  general  idea  is  simple: pick  four  points  known  to  be  on  the  hull,  then  throw  out  everything  inside  the quadrilateral  formed  by  those  four  points.  this  leaves  many  fewer  points  to
there  are  several  textbooks  on  graph  algorithms,  but  the  reader  should be  forewarned  that  there  is  a  great  deal  to  be  learned  about  graphs,  that they  still  are  not  fully  understood,  and  that  they  are  traditionally  studied from  a  mathematical  (as  opposed  to  an  algorithmic)  standpoint.  thus,  many references  have  more  rigorous  and  deeper  coverage  of  much  more  difficult topics  than  our  treatment  here.
many  of  the  topics  that  we’ve  treated  here  are  covered  in  the  book  by even,  for  example,  our  network  flow  example  in  chapter  33.  another  source for  further  material  is  the  book  by  papadimitriou  and  steiglitz.  though  most of  that  book  is  about  much  more  advanced  topics  (for  example,  there  is  a  full treatment  of  matching  in  general  graphs),  it  has  up-to-date  coverage  of  many of  the  algorithms  that  we’ve  discussed,  including  pointers  to  further  reference material.
the application of depth-first search to solve graph connectivity and other problems  is  the  work  of  r.  e.  tarjan,   whose  original  paper  merits  further study.  the  many  variants  on  algorithms  for  the  union-find  problem  of  chapter 30  are  ably  categorized  and  compared  by  van  leeuwen  and  tarjan.   the algorithms  for  shortest  paths  and  minimum  spanning  trees  in  dense  graphs in  chapter  31  are  quite  old,  but  the  original  papers  by  dijkstra,  prim,  and kruskal  still  make  interesting  reading.  our  treatment  of  the  stable  marriage problem  in  chapter  34  is  based  on  the  entertaining  account  given  by  knuth.
e.  w.  dijkstra,  “a  note  on  two  problems  in  connexion  with  graphs,”  numerishe   muthemutik,  1  (1959). s.  even,  graph algorithms,  computer  science  press,  rockville,  md,  1980. d.  e.  knuth,  marriages  stables,  les  presses  de  l’universite   de  montreal, montreal,  1976. j.  r.  kruskal  jr., “on  the  shortest  spanning  subtree  of  a  graph  and  the traveling  salesman  problem,”  proceedings  ams, 7,  1  (1956). c.  h.  papadimitriou  and  k.  steiglitz,  combinatorial  optimization:  algorithms and  complexity,  prentice-hall, englewood cliffs, nj,  1982. r.  c.  prim,  “shortest  connection  networks  and  some  generalizations,”  bell system  technical  journal,  36  (1957). r.  e.  tarjan,   “depth-first  search  and  linear  graph  algorithms,”  siam  journal on computing, 1, 2 (1972). j.  van  leeuwen  and  r.  e.  tarjan, rithms,”  journal  of  the acm,  to  appear.
this  procedure  recursively  determines  the  highest  point  in  the  tree  reachable  (via  a  dotted  link)  from  any  descendant  of  vertex  k  and  uses  this  information  to  determine  if  k  is  an  articulation  point.  normally  this  calculation simply  involves  testing  whether  the  minimum  value  reachable  from  a  son  is higher  up  in  the  tree,  but  we  need  an  extra  test  to  determine  whether  k  is  the root  of  a  depth-first  search  tree  (or,  equivalently,  whether  this  is  the  first  call to visit for the connected component containing k), since we’re using the same recursive  program  for  both  cases.  this  test  is  properly  performed  outside  the recursive  visit,  so  it  does  not  appear  in  the  code  above.
the  program  above  simply  prints  out  the  articulation  points.  of  course, as  before,  it  is  easily  extended  to  do  additional  processing  on  the  articulation points  and  biconnected  components. also,  since  it  is  a  depth-first  search procedure,  the  running  time  is  proportional  to  v  +  e.
besides  the  “reliability”  sort  of  application  mentioned  above,  biconnectedness  can  be  helpful  in  decomposing  large  graphs  into  manageable  pieces.  it  is obvious  that  a  very  large  graph  may  be  processed  one  connected  component at  a  time  for  many  applications;  it  is  somewhat  less  obvious  but  sometimes  as useful  that  a  graph  can  sometimes  be  processed  one  biconnected  component at  a  time.
that  “al”  be  deleted;  “1c”  means  to  add  cl  to  the  matching,  which  requires that  “c3”  be  deleted;  “3e”  means  to  add  e3  to  the  matching.  thus,  after this  path  is  processed,  we  have  the  matching  a4  b2  cl  d5  e3;  equivalently, the  flow  in  the  network  is  given  by  full  pipes  in  the  edges  connecting  those nodes,  and  all  pipes  leaving  0  and  entering  z  full.
the  proof  that  the  matching  is  exactly  those  edges  which  are  filled  to capacity  by  the  maxflow   algorithm  is  straightforward.  first,  the  network  flow always  gives  a  legal  matching:  since  each  vertex  has  an  edge  of  capacity  1 either  coming  in  (from  the  sink)  or  going  out  (to  the  source),  at  most  one  unit of  flow  can  go  through  each  vertex,  which  implies  that  each  vertex  will  be included  at  most  once  in  the  matching.  second,  no  matching  can  have  more edges,  since  any  such  matching  would  lead  directly  to  a  better  flow   than  that produced  by  the  maxflow   algorithm.
thus,  to  compute  the  maximum  matching  for  a  bipartite  graph  we  simply format  the  graph  so  as  to  be  suitable  for  input  to  the  network  flow  algorithm of  the  previous  chapter.  of  course,  the  graphs  presented  to  the  network  flow algorithm  in  this  case  are  much  simpler  than  the  general  graphs  the  algorithm is  designed  to  handle,  and  it  turns  out  that  the  algorithm  is  somewhat  more efficient  for  this  case.  the  construction  ensures  that  each  call  to  pfs  adds one  edge  to  the  matching,  so  we  know  that  there  are  at  most  v/2   calls  to pfs  during  the  execution  of  the  algorithm.  thus,  for  example,  the  total  time to  find  the  maximum  matching  for  a  dense  bipartite  graph  with  v  vertices (using  the  adjacency  matrix  representation)  is  proportional  to  v3.
stable  marriage  problem the  example  given  at  the  beginning  of  this  chapter,  involving  medical  students and  hospitals,  is  obviously  taken  quite  seriously  by  the  participants.  but the  method  that  we’ll  examine  for  doing  the  matching  is  perhaps  better understood  in  terms  of  a  somewhat  whimsical  model  of  the  situation.  we assume  that  we  have  n  men  and  n  women  who  have  expressed  mutual preferences  (each  man  must  say  exactly  how  he  feels  about  each  of  the  n women  and  vice  versa).  the  problem  is  to  find  a  set  of  n  marriages  that respects everyone’s preferences.
how  should  the  preferences  be  expressed?  one  method  would  be  to  use the  “1-10”   scale,  each  side  assigning  an  absolute  score  to  certain  members  of the  other  side.  this  makes  the  marriage  problem  the  same  as  the  weighted matching  problem,  a  relatively  difficult  problem  to  solve.  furthermore,  use  of absolute  scales  in  itself  can  lead  to  inaccuracies,  since  peoples’  scales  will  be inconsistent  (one  woman’s  10  might  be  another  woman’s  7).  a  more  natural way to express the preferences is to have each person list in order of preference the  following  two  tables  might  show all  the  people  of  the  opposite  sex.
perspective in  the  chapters  that  follow  we’ll  consider  a  variety  of  graph  algorithms  largely aimed  at  determining  connectivity  properties  of  both  undirected  and  directed graphs.  these  algorithms  are  fundamental  ones  for  processing  graphs,  but are  only  an  introduction  to  the  subject  of  graph  algorithms.  many  interesting and  useful  algorithms  have  been  developed  which  are  beyond  the  scope  of this  book,  and  many  interesting  problems  have  been  studied  for  which  good algorithms  have  not  yet  been  found.
some  very  efficient  algorithms  have  been  developed  which  are  much  too complicated  to  present  here.  for  example,  it  is  possible  to  determine  efficiently whether  or  not  a  graph  can  be  drawn  on  the  plane  without  any  intersecting lines.  this  problem  is  called  the  planarity  problem,  and  no  efficient  algorithm for solving it was known until 1974, when r. e.  tarjan  developed an ingenious (but  quite  intricate)  algorithm  for  solving  the  problem  in  linear  time,  using depth-first  search.
some  graph  problems  which  arise  naturally  and  are  easy  to  state  seem to  be  quite  difficult,  and  no  good  algorithms  are  known  to  solve  them.  for example,  no  efficient  algorithm  is  known  for  finding  the  minimum-cost  tour which  visits  each  vertex  in  a  weighted  graph.  this  problem,  called  the traveling  salesman problem,  belongs  to  a  large  class  of  difficult  problems  that we’ll  discuss  in  more  detail  in  chapter  40.  most  experts  believe  that  no efficient  algorithms  exist  for  these  problems.
other  graph  problems  may  well  have  efficient  algorithms,  though  none  has been  found.  an  example  of  this  is  the  graph isomorphism  problem:  determine whether  two  graphs  could  be  made  identical  by  renaming  vertices.  efficient algorithms  are  known  for  this  problem  for  many  special  types  of  graphs,  but the  general  problem  remains  open.
in  short,  there  is  a  wide  spectrum  of  problems  and  algorithms  for  dealing with  graphs.  we  certainly  can’t  expect  to  solve  every  problem  which  comes along,  because  even  some  problems  which  appear  to  be  simple  are  still  baffling the  experts.  but  many  problems  which  are  relatively  easy  to  solve  do  arise quite  often,  and  the  graph  algorithms  that  we  will  study  serve  well  in  a  great variety  of  applications.
perspective in  the  chapters  that  follow  we’ll  consider  a  variety  of  graph  algorithms  largely aimed  at  determining  connectivity  properties  of  both  undirected  and  directed graphs.  these  algorithms  are  fundamental  ones  for  processing  graphs,  but are  only  an  introduction  to  the  subject  of  graph  algorithms.  many  interesting and  useful  algorithms  have  been  developed  which  are  beyond  the  scope  of this  book,  and  many  interesting  problems  have  been  studied  for  which  good algorithms  have  not  yet  been  found.
some  very  efficient  algorithms  have  been  developed  which  are  much  too complicated  to  present  here.  for  example,  it  is  possible  to  determine  efficiently whether  or  not  a  graph  can  be  drawn  on  the  plane  without  any  intersecting lines.  this  problem  is  called  the  planarity  problem,  and  no  efficient  algorithm for solving it was known until 1974, when r. e.  tarjan  developed an ingenious (but  quite  intricate)  algorithm  for  solving  the  problem  in  linear  time,  using depth-first  search.
some  graph  problems  which  arise  naturally  and  are  easy  to  state  seem to  be  quite  difficult,  and  no  good  algorithms  are  known  to  solve  them.  for example,  no  efficient  algorithm  is  known  for  finding  the  minimum-cost  tour which  visits  each  vertex  in  a  weighted  graph.  this  problem,  called  the traveling  salesman problem,  belongs  to  a  large  class  of  difficult  problems  that we’ll  discuss  in  more  detail  in  chapter  40.  most  experts  believe  that  no efficient  algorithms  exist  for  these  problems.
other  graph  problems  may  well  have  efficient  algorithms,  though  none  has been  found.  an  example  of  this  is  the  graph isomorphism  problem:  determine whether  two  graphs  could  be  made  identical  by  renaming  vertices.  efficient algorithms  are  known  for  this  problem  for  many  special  types  of  graphs,  but the  general  problem  remains  open.
in  short,  there  is  a  wide  spectrum  of  problems  and  algorithms  for  dealing with  graphs.  we  certainly  can’t  expect  to  solve  every  problem  which  comes along,  because  even  some  problems  which  appear  to  be  simple  are  still  baffling the  experts.  but  many  problems  which  are  relatively  easy  to  solve  do  arise quite  often,  and  the  graph  algorithms  that  we  will  study  serve  well  in  a  great variety  of  applications.
encountered  is  connected  to  any  vertex  on  the  queue:  that  is,  we’re  entering a  new  connected  component.  this  is  automatically  handled  by  the  priority queue  mechanism,  so  there  is  no  need  for  a  separate  visit  procedure  inside  a main  procedure.  but  note  that  maintaining  the  proper  value  of  now  is  more complicated  than  for  the  recursive  depth-first  search  program  of  the  previous chapter.  the  convention  of  this  program  is  to  leave  the  val  entry  unseen  and zero  for  the  root  of  the  depth-first  search  tree  for  each  connected  component: it  might  be  more  convenient  to  set  it  to  zero  or  some  other  value  (for  example, now)  for  various  applications.
now,  recall  that  now  increases  from  1  to  v  during  the  execution  of  the algorithm  so  it  can  be  used  to  assign  unique  priorities  to  the  vertices.  if  we change the two occurrences of  priority  in  sparsepfs  to  v-now,  we  get  depthfirst  search,  because  newly  encountered  nodes  have  the  highest  priority.  if we  use now for  priority  we  get  breadth-first  search,  because  old  nodes  have the  highest  priority.  these  priority  assignments  make  the  priority  queues operate  like  stacks  and  queues  as  described  above.  (of  course,  if  we  were  only interested  in  using  depth-first  or  breadth-first  search,  we  would  use  a  direct implementation  for  stacks  or  queues,  not  priority  queues  as  in  sparsepfs.) in  the  next  chapter,  we’ll  see  that  other  priority  assignments  lead  to  other classical  graph  algorithms.
the  running  time  for  graph  traversal  when  implemented  in  this  way depends  on  the  method  of  implementing  the  priority  queue.  in  general,  we have to do a priority queue operation for each edge and for each vertex, so the worst  case  running  time  should  be  proportional  to  (e  +  v)  log  v  if  the  priority queue  is  implemented  as  a  heap  as  indicated.  however,  we’ve  already  noted that  for  both  depth-first  and  breadth-first  search  we  can  take  advantage  of the  fact  that  each  new  priority  is  the  highest  or  the  lowest  so  far  encountered to  get  a  running  time  proportional  to  e  +  v.  also,  other  priority  queue implementations  might  sometimes  be  appropriate:  for  example  if  the  graph  is dense  then  we  might  as  well  simply  keep  the  priority  queue  as  an  unordered array.  this  gives  a  worst  case  running  time  proportional  to  e  +  v2   (or  just v2),  since  each  edge  simply  requires  setting  or  resetting  a  priority,  but  each vertex  now  requires  searching  through  the  whole  queue  to  find  the  highest priority  vertex.  an  implementation  which  works  in  this  way  is  given  in  the next  chapter.
the  difference  between  depth-first  and  breadth-first  search  is  quite  evident  when  a  large  graph  is  considered.  the  diagram  at  left  below  shows  the edges  and  nodes  visited  when  depth-first  search  is  halfway  through  the  maze graph  of  the  previous  chapter  starting  at  the  upper  left  corner;  the  diagram at  right  is  the  corresponding  picture  for  breadth-first  search:
first  we  visit  the  closest  vertex  to  a,  which  is  b.  then  both  c  and  f  are distance  2  from  a,  so  we  visit  them  next  (in  whatever  order  the  priority  queue returns  them).  then  d  can  be  attached  at  f  or  at  b  to  get  a  path  of  distance 3  to  a.  (the  algorithm  attaches  it  to  b  because  it  was  put  on  the  tree  before f,  so  d  was  already  on  the  fringe  when  f  was  put  on  the  tree  and  f  didn’t provide  a  shorter  path  to  a.)  finally,  e  and  g  are  visited.  as  usual,  the  tree is  represented  by  the  dad  array  of  father  links.  the  following  table  shows  the array  computed  by  the  priority  graph  traversal  procedure  for  our  example:
thus  the  shortest  path  from  a  to  g  has  a  total  weight  of  5  (found  in  the  val entry  for  g)  and  goes  from  a  to  f  to  e  to  g  (found  by  tracing  backwards in  the  dad  array,  starting  at  g).  note  that  the  correct  operation  of  this program  depends  on  the  val  entry  for  the  root  being  zero,  the  convention  that we  adopted  for  sparsepfs.
as  before,  the  priority  graph  traversal  algorithm  has  a  worst-case  running time  proportional  to  (e  +  v)  log  v,  though  a  different  implementation  of  the priority  queue  can  give  a  v2   algorithm,  which  is  appropriate  for  dense  graphs. below,  we’ll  examine  this  implementation  of  priority  graph  traversal  for  dense graphs  in  full  detail.  for  the  shortest  path  problem,  this  reduces  to  a  method discovered  by  e.  dijkstra  in  1956. though  the  methods  are  the  same  in essence,  we’ll  refer  to  the  sparsepfs  program  of  the  previous  chapter  with priority replaced by val  [k] + tt . weight  as the “priority-first search solution” to the  shortest  paths  problem  and  the  adjacency  matrix  version  given  below  as “dijkstra’s  algorithm.”
dense  graphs as  we’ve  discussed,  when  a  graph  is  represented  with  a  adjacency  matrix,  it  is best  to  use  an  unordered  array  representation  for  the  priority  queue  in  order to  achieve  a  v2   running  time  for  any  priority  graph  traversal  algorithm.  that is,  this  provides  a  linear  algorithm  for  the  priority  first  search  (and  thus  the minimum  spanning  tree  and  shortest  path  problems)  for  dense  graphs.
specifically,  we  maintain  the  priority  queue  in  the  val   array  just  as  in sparsepfs  but  we  implement  the  priority  queue  operations  directly  rather  than using  heaps.  first,  we  adopt  the  convention  that  the  priority  values  in  the val  array  will  be  negated,  so  that  the  sign  of  a  val   entry  tells  whether  the corresponding  vertex  is  on  the  tree  or  the  priority  queue.  to  change  the
another  feature  of  the  algorithm  which  seems  to  be  biased  is  the  order  in which  the  men  become  the  suitor:  is  it  better  to  be  the  first  man  to  propose (and  therefore  be  engaged  at  least  for  a  little  while  to  your  first  choice)  or the  last  (and  therefore  have  a  reduced  chance  to  suffer  the  indignities  of  a broken  engagement)?  the  answer  is  that  this  is  not  a  bias  at  all:  it  doesn’t matter  in  what  order  the  men  become  the  suitor.  as  long  as  each  man  makes proposals  and  each  woman  accepts  according  to  their  lists,  the  same  stable configuration  results.
the  two  special  cases  that  we’ve  examined  give  some  indication  of  how  complicated  the  matching  problem  can  be.  among  the  more  general  problems that  have  been  studied  in  some  detail  are:  the  maximum  matching  problem for  general  (not  necessarily  bipartite)  graphs;  weighted  matching  for  bipartite graphs,  where  edges  have  weights  and  a  matching  with  maximum  total  weight is  sought;  and  weighted  matching  for  general  graphs.  treating  the  many  techniques  that  have  been  tried  for  matching  on  general  graphs  would  fill  an  entire volume:  it  is  one  of  the  most  extensively  studied  problems  in  graph  theory.
component  as  k.  as  usual,  this  program  could  easily  be  modified  to  do  more sophisticated  processing  than  simply  writing  out  the  components.
the  method  is  based  on  two  observations  that  we’ve  actually  already made  in  other  contexts.  first,  once  we  reach  the  end  of  a  call  to  visit  for a  vertex,  then  we  won’t  encounter  any  more  vertices  in  the  same  strongly connected  component  (because  all  the  vertices  which  can  be  reached  from  that vertex have been processed, as we noted above for topological sorting). second, the “up” links in the tree provide a second path from one vertex to another and bind  together  the  strong  components.  as  with  the  algorithm  in  chapter  30  for finding  articulation  points,  we  keep  track  of  the  highest  ancestor  reachable via  one  “up”   link  from  all  descendants  of  each  node.  now,  if  a  vertex  x has  no  descendants  or  “up”   links  in  the  depth-first  search  tree,  or  if  it  has  a descendant  in  the  depth-first  search  tree  with  an  “up”  link  that  points  to  x, and  no  descendants  with  “up”   links  that  point  higher  up  in  the  tree,  then  it and  all  its  descendants  (except  those  vertices  satisfying  the  same  property  and their  descendants)  comprise  a  strongly  connected  component.  in  the  depthfirst  search  tree  at  the  beginning  of  the  chapter,  nodes  b  and  k  satisfy  the first  condition  (so  they  represent  strongly  connected  components  themselves) and  nodes  f(representing   f  e  d),  h  (representing  h  i),  and  a  (representing a  g  j  l  m  c)  satisfy  the  second  condition.  the  members  of  the  component represented  by  a  are  found  by  deleting  b  k  f  and  their  descendants  (they appear  in  previously  discovered  components).  every  descendant  y  of  x  that does  not  satisfy  this  same  property  has  some  descendant  that  has  an  “up” link  that  points  higher  than  y  in  the  tree.  there  is  a  path  from  x  to  y  down through  the  tree;  and  a  path  from  y  to  x  can  be  found  by  going  down  from y  to  the  vertex  with  the  “up”  link  that  reaches  past  y,  then  continuing  the same  process  until  x  is  reached.  a  crucial  extra  twist  is  that  once  we’re  done with  a  vertex,  we  give  it  a  high  val,   so  that  “cross”  links  to  that  vertex  will be  ignored.
this  program  provides  a  deceptively  simple  solution  to  a  relatively  difficult problem.  it  is  certainly  testimony  to  the  subtleties  involved  in  searching directed  graphs,  subtleties  which  can  be  handled  (in  this  case)  by  a  carefully crafted  recursive  program.
the  distinction  here  is  not  crucial:  performing  a  reverse  topological  sort  on  a graph  is  equivalent  to  performing  a  topological  sort  on  the  graph  obtained  by reversing all the edges.
but  we’ve  already  seen  an  algorithm  for  reverse  topological  sorting,  the standard  recursive  depth-first  search  procedure  of  chapter  29!   simply  changing  visit  to  print  out  the  vertex  visited  just  before  exiting,  for  example  by inserting  write(name[k]   )  right  at  the  end,  causes  dfs  to  print  out  the  vertices in  reverse  topological  order,  when  the  input  graph  is  a  dag.  a  simple  induction argument  proves  that  this  works:  we  print  out  the  name  of  each  vertex  after we’ve  printed  out  the  names  of  all  the  vertices  that  it  points  to.  when  visit is  changed  in  this  way  and  run  on  our  example,  it  prints  out  the  vertices  in the  reverse  topological  order  given  above. printing  out  the  vertex  name  on exit  from  this  recursive  procedure  is  exactly  equivalent  to  putting  the  vertex name  on  a  stack  on  entry,  then  popping  it  and  printing  it  on  exit.  it  would be  ridiculous  to  use  an  explicit  stack  in  this  case,  since  the  mechanism  for recursion  provides  it  automatically;  we  mention  this  because  we  do  need  a stack  for  the  more  difficult  problem  to  be  considered  next.
strongly  connected  components if  a  graph  contains  a directed cycle,  (if  we  can  get  from  a  node  back  to  itself by  following  edges  in  the  indicated  direction),  then  it  it  is  not  a  dag  and  it can’t  be  topologically  sorted:  whichever  vertex  on  the  cycle  is  printed  out  first will  have  another  vertex  which  points  to  it  which  hasn’t  yet  been  printed  out. the  nodes  on  the  cycle  are  mutually  accessible  in  the  sense  that  there  is  a way  to  get  from  every  node  on  the  cycle  to  another  node  on  the  cycle  and back.  on  the  other  hand,  even  though  a  graph  may  be  connected,  it  is  not likely  to  be  true  that  any  node  can  be  reached  from  any  other  via  a  directed path.  in  fact,  the  nodes  divide  themselves  into  sets  called  strongly  connected components  with  the  property  that  all  nodes  within  a  componenl  are  mutually accessible, but there is no way to get from a node in one component to a node in  another  component  and  back.  the  strongly  connected  components  of  the directed  graph  at  the  beginning  of  this  chapter  are  two  single  nodes  b  and  k, one pair of nodes h i, one triple of nodes d e f, and one large component with six  nodes  a  c  g  j  l  m.  for  example,  vertex  a  is  in  a  different  component from  vertex  f  because  though  there  is  a  path  from  a  to  f,  there  is  no  way  to get  from  f  to  a.
the  strongly  connected  components  of  a  directed  graph  can  be  found using  a  variant  of  depth-first  search,  as  the  reader  may  have  learned  to  expect. the  method  that  we’ll  examine  was  discovered  by  r.  e.  tarjan   in  1972.  since it  is  based  on  depth-first  search,  it  runs  in  time  proportional  to  v  + e,  but  it  is actually  quite  an  ingenious  method.  it  requires  only  a  few  simple  modifications to  our  basic  visit  procedure,  but  before  tarjan   presented  the  method,  no  linear
for  very  large  graphs,  this  computation  can  be  organized  so  that  the operations  on  bits  can  be  done  a  computer  word  at  a  time,  which  will  lead  to significant  savings  in  many  environments.  (as  we’ve  seen,  it  is  not  intended that  such  optimizations  be  tried  with  pascal.)
for  many  applications  involving  directed  graphs,  cyclic  graphs  do  arise.  if, however,  the  graph  above  modeled  a  manufacturing  line,  then  it  would  imply, say,  that  job  a  must  be  done  before  job  g,  which  must  be  done  before  job c,  which  must  be  done  before  job  a.  but  such  a  situation  is  inconsistent: for  this  and  many  other  applications,  directed  graphs  with  no  directed  cycles (cycles  with  all  edges  pointing  the  same  way)  are  called  for.  such  graphs  are called  directed  acyclic  graphs,  or  just  dags  for  short.  dags  may  have  many cycles  if  the  directions  on  the  edges  are  not  taken  into  account;  their  defining property  is  simply  that  one  should  never  get  in  a  cycle  by  following  edges  in the  indicated  direction.  a  dag  similar  to  the  directed  graph  above,  with  a few  edges  removed  or  directions  switched  in  order  to  remove  cycles,  is  given below.
the  edge  list  for  this  graph  is  the  same  as  for  the  connected  graph  of  chapter 30,  but  here,  again,  the  order  in  which  the  vertices  are  given  when  the  edge is  specified  makes  a  difference.
dags  really  are  quite  different  objects  from  general  directed  graphs:  in a  sense,  they  are  part  tree,  part  graph.  we  can  certainly  take  advantage  of their  special  structure  when  processing  them.  viewed  from  any  vertex,  a  dag looks  like  a  tree;  put  another  way,  the  depth-first  search  forest  for  a  dag  has no  up  edges.  for  example,  the  following  depth-first  search  forest  describes the  operation  of  dfs  on  the  example  dag  above.
even if only the worst case is being considered, the analysis of union-find algorithms  is  extremely  complex  and  intricate.  this  can  be  seen  even  from  the nature  of  the  results,  which  do  give  us  clear  indications  of  how  the  algorithms will  perform  in  a  practical  situation. if  either  weight  balancing  or  height balancing  is  used  in  combination  with  either  path  compression,  halving,  or splitting,  then  the  total  number  of  operations  required  to  build  up  a  structure with  e  edges  is  proportional  to  es(e),   where  a(e)  is  a  function  that  is  so slowly  growing  that  o(e)  <  4  unless  e  is  so  large  that  taking  lg  e,  then taking  lg  of  the  result,  then  taking  lg  of  that  result,  and  continuing  16  times still  gives  a  number  bigger  than  1.  this  is  a  stunningly  large  number;  for  all practical  purposes,  it  is  safe  to  assume  that  the  average  amount  of  time  to execute  each  union  and  find  operation  is  constant.  this  result  is  due  to  r.  e. tarjan,   who  further  showed  that  no  algorithm  for  this  problem  (from  a  certain general  class)  can  do  better  that  e&(e),  so  that  this  function  is  intrinsic  to the  problem.
an  important  practical  application  of  union-find  algorithms  is  that  they can  be  used  to  determine  whether  a  graph  with  v  vertices  and  e  edges  is connected  in  space  proportional  to  v  (and  almost  linear  time).  this  is  an advantage  over  depth-first  search  in  some  situations:  here  we  don’t  need  to ever  store  the  edges.  thus  connectivity  for  a  graph  with  thousands  of  vertices and  millions  of  edges  can  be  determined  with  one  quick  pass  through  the edges.
tree  nodes  are  blackened  in  these  diagrams,  fringe  nodes  are  crossed,  and unseen  nodes  are  blank.  depth-first  search  “explores”  the  graph  by  looking for  new  vertices  far  away  from  the  start  point,  taking  closer  vertices  only  when dead  ends  are  encountered;  breadth-first  search  completely  covers  the  area close  to  the  starting  point,  moving  farther  away  only  when  everything  close has  been  looked  at.  depth-first  search  is  appropriate  for  one  person  looking for  something  in  a  maze  because  the  “next  place  to  look”  is  always  close  by; breadth-first  search  is  more  like  a  group  of  people  looking  for  something  by fanning  out  in  all  directions.
beyond  these  operational  differences,  it  is  interesting  to  reflect  on  the fundamental  differences  in  the  implementations  of  these  methods.  depth-first search  is  very  simply  expressed  recursively  (because  its  underlying  data  structure  is  a  stack),  and  breadth-first  search  admits  to  a  very  simple  nonrecursive implementation  (because  its  underlying  data  structure  is  a  queue).  but  we’ve seen  that  the  true  underlying  data  structure  for  graph  algorithms  is  a  priority queue,  and  this  admits  a  wealth  of  interesting  properties  to  consider.  again, we’ll  see  more  examples  in  the  next  chapter.
union-find  algorithms in  some  applications  we  wish  to  know  simply  whether  a  vertex  x  is  connected to  a  vertex  y  in  a  graph;  the  actual  path  connecting  them  may  not  be relevant.  this  problem  has  been  carefully  studied  in  recent  years;  some efficient  algorithms  have  been  developed  which  are  of  independent  interest because  they  can  also  be  used  for  processing  sets  (collections  of  objects).
graphs  correspond  to  sets  of  objects  in  a  natural  way,  with  vertices corresponding  to  objects  and  edges  have  the  meaning  “is  in  the  same  set  as.” thus,  the  sample  graph  in  the  previous  chapter  corresponds  to  the  sets  {a  b c  d  e  f  g},  {h  i}  and  {j  k  l  m}. eac  connected  component  corresponds
one  advantage  of  this  algorithm  over  the  shortest  paths  algorithm  of chapter  31  is  that  it  works  properly  even  if  negative  edge  weights  are  allowed, as  long  as  there  are  no  cycles  of  negative  weight  in  the  graph  (in  which  case the  shortest  paths  connecting  nodes  on  the  cycle  are  not  defined).  if  a  cycle of  negative  weight  is  present  in  the  graph,  then  the  algorithm  can  detect  that fact,  because  in  that  case  a[i,  i]  will  become  negative  for  some  i  at  some  point during  the  algorithm.
time  and  space  requirements the  above  examples  demonstrate  that  dynamic  programming  applications  can have  quite  different  time  and  space  requirements  depending  on  the  amount  of information  about  small  subproblems  that  must  be  saved.  for  the  shortest paths  algorithm,  no  extra  space  was  required;  for  the  knapsack  problem, space  proportional  to  the  size  of  the  knapsack  was  needed;  and  for  the  other problems  n2  space  was  needed.  for  each  problem,  the  time  required  was  a factor  of  n  greater  than  the  space  required.
the  range  of  possible  applicability  of  dynamic  programming  is  far  larger than  covered  in  the  examples.  from  a  dynamic  programming  point  of  view, divide-and-conquer  recursion  could  be  thought  of  as  a  special  case  in  which a  minimal  amount  of  information  about  small  cases  must  be  computed  and stored,  and  exhaustive  search  (which  we’ll  examine  in  chapter  39)  could  be thought  of  as  a  special  case  in  which  a  maximal  amount  of  information  about small  cases  must  be  computed  and  stored.  dynamic  programming  is  a  natural design  technique  that  appears  in  many  guises  to  solve  problems  throughout this  range.
that  “al”  be  deleted;  “1c”  means  to  add  cl  to  the  matching,  which  requires that  “c3”  be  deleted;  “3e”  means  to  add  e3  to  the  matching.  thus,  after this  path  is  processed,  we  have  the  matching  a4  b2  cl  d5  e3;  equivalently, the  flow  in  the  network  is  given  by  full  pipes  in  the  edges  connecting  those nodes,  and  all  pipes  leaving  0  and  entering  z  full.
the  proof  that  the  matching  is  exactly  those  edges  which  are  filled  to capacity  by  the  maxflow   algorithm  is  straightforward.  first,  the  network  flow always  gives  a  legal  matching:  since  each  vertex  has  an  edge  of  capacity  1 either  coming  in  (from  the  sink)  or  going  out  (to  the  source),  at  most  one  unit of  flow  can  go  through  each  vertex,  which  implies  that  each  vertex  will  be included  at  most  once  in  the  matching.  second,  no  matching  can  have  more edges,  since  any  such  matching  would  lead  directly  to  a  better  flow   than  that produced  by  the  maxflow   algorithm.
thus,  to  compute  the  maximum  matching  for  a  bipartite  graph  we  simply format  the  graph  so  as  to  be  suitable  for  input  to  the  network  flow  algorithm of  the  previous  chapter.  of  course,  the  graphs  presented  to  the  network  flow algorithm  in  this  case  are  much  simpler  than  the  general  graphs  the  algorithm is  designed  to  handle,  and  it  turns  out  that  the  algorithm  is  somewhat  more efficient  for  this  case.  the  construction  ensures  that  each  call  to  pfs  adds one  edge  to  the  matching,  so  we  know  that  there  are  at  most  v/2   calls  to pfs  during  the  execution  of  the  algorithm.  thus,  for  example,  the  total  time to  find  the  maximum  matching  for  a  dense  bipartite  graph  with  v  vertices (using  the  adjacency  matrix  representation)  is  proportional  to  v3.
stable  marriage  problem the  example  given  at  the  beginning  of  this  chapter,  involving  medical  students and  hospitals,  is  obviously  taken  quite  seriously  by  the  participants.  but the  method  that  we’ll  examine  for  doing  the  matching  is  perhaps  better understood  in  terms  of  a  somewhat  whimsical  model  of  the  situation.  we assume  that  we  have  n  men  and  n  women  who  have  expressed  mutual preferences  (each  man  must  say  exactly  how  he  feels  about  each  of  the  n women  and  vice  versa).  the  problem  is  to  find  a  set  of  n  marriages  that respects everyone’s preferences.
how  should  the  preferences  be  expressed?  one  method  would  be  to  use the  “1-10”   scale,  each  side  assigning  an  absolute  score  to  certain  members  of the  other  side.  this  makes  the  marriage  problem  the  same  as  the  weighted matching  problem,  a  relatively  difficult  problem  to  solve.  furthermore,  use  of absolute  scales  in  itself  can  lead  to  inaccuracies,  since  peoples’  scales  will  be inconsistent  (one  woman’s  10  might  be  another  woman’s  7).  a  more  natural way to express the preferences is to have each person list in order of preference the  following  two  tables  might  show all  the  people  of  the  opposite  sex.
even if only the worst case is being considered, the analysis of union-find algorithms  is  extremely  complex  and  intricate.  this  can  be  seen  even  from  the nature  of  the  results,  which  do  give  us  clear  indications  of  how  the  algorithms will  perform  in  a  practical  situation. if  either  weight  balancing  or  height balancing  is  used  in  combination  with  either  path  compression,  halving,  or splitting,  then  the  total  number  of  operations  required  to  build  up  a  structure with  e  edges  is  proportional  to  es(e),   where  a(e)  is  a  function  that  is  so slowly  growing  that  o(e)  <  4  unless  e  is  so  large  that  taking  lg  e,  then taking  lg  of  the  result,  then  taking  lg  of  that  result,  and  continuing  16  times still  gives  a  number  bigger  than  1.  this  is  a  stunningly  large  number;  for  all practical  purposes,  it  is  safe  to  assume  that  the  average  amount  of  time  to execute  each  union  and  find  operation  is  constant.  this  result  is  due  to  r.  e. tarjan,   who  further  showed  that  no  algorithm  for  this  problem  (from  a  certain general  class)  can  do  better  that  e&(e),  so  that  this  function  is  intrinsic  to the  problem.
an  important  practical  application  of  union-find  algorithms  is  that  they can  be  used  to  determine  whether  a  graph  with  v  vertices  and  e  edges  is connected  in  space  proportional  to  v  (and  almost  linear  time).  this  is  an advantage  over  depth-first  search  in  some  situations:  here  we  don’t  need  to ever  store  the  edges.  thus  connectivity  for  a  graph  with  thousands  of  vertices and  millions  of  edges  can  be  determined  with  one  quick  pass  through  the edges.
component  as  k.  as  usual,  this  program  could  easily  be  modified  to  do  more sophisticated  processing  than  simply  writing  out  the  components.
the  method  is  based  on  two  observations  that  we’ve  actually  already made  in  other  contexts.  first,  once  we  reach  the  end  of  a  call  to  visit  for a  vertex,  then  we  won’t  encounter  any  more  vertices  in  the  same  strongly connected  component  (because  all  the  vertices  which  can  be  reached  from  that vertex have been processed, as we noted above for topological sorting). second, the “up” links in the tree provide a second path from one vertex to another and bind  together  the  strong  components.  as  with  the  algorithm  in  chapter  30  for finding  articulation  points,  we  keep  track  of  the  highest  ancestor  reachable via  one  “up”   link  from  all  descendants  of  each  node.  now,  if  a  vertex  x has  no  descendants  or  “up”   links  in  the  depth-first  search  tree,  or  if  it  has  a descendant  in  the  depth-first  search  tree  with  an  “up”  link  that  points  to  x, and  no  descendants  with  “up”   links  that  point  higher  up  in  the  tree,  then  it and  all  its  descendants  (except  those  vertices  satisfying  the  same  property  and their  descendants)  comprise  a  strongly  connected  component.  in  the  depthfirst  search  tree  at  the  beginning  of  the  chapter,  nodes  b  and  k  satisfy  the first  condition  (so  they  represent  strongly  connected  components  themselves) and  nodes  f(representing   f  e  d),  h  (representing  h  i),  and  a  (representing a  g  j  l  m  c)  satisfy  the  second  condition.  the  members  of  the  component represented  by  a  are  found  by  deleting  b  k  f  and  their  descendants  (they appear  in  previously  discovered  components).  every  descendant  y  of  x  that does  not  satisfy  this  same  property  has  some  descendant  that  has  an  “up” link  that  points  higher  than  y  in  the  tree.  there  is  a  path  from  x  to  y  down through  the  tree;  and  a  path  from  y  to  x  can  be  found  by  going  down  from y  to  the  vertex  with  the  “up”  link  that  reaches  past  y,  then  continuing  the same  process  until  x  is  reached.  a  crucial  extra  twist  is  that  once  we’re  done with  a  vertex,  we  give  it  a  high  val,   so  that  “cross”  links  to  that  vertex  will be  ignored.
this  program  provides  a  deceptively  simple  solution  to  a  relatively  difficult problem.  it  is  certainly  testimony  to  the  subtleties  involved  in  searching directed  graphs,  subtleties  which  can  be  handled  (in  this  case)  by  a  carefully crafted  recursive  program.
the  distinction  here  is  not  crucial:  performing  a  reverse  topological  sort  on  a graph  is  equivalent  to  performing  a  topological  sort  on  the  graph  obtained  by reversing all the edges.
but  we’ve  already  seen  an  algorithm  for  reverse  topological  sorting,  the standard  recursive  depth-first  search  procedure  of  chapter  29!   simply  changing  visit  to  print  out  the  vertex  visited  just  before  exiting,  for  example  by inserting  write(name[k]   )  right  at  the  end,  causes  dfs  to  print  out  the  vertices in  reverse  topological  order,  when  the  input  graph  is  a  dag.  a  simple  induction argument  proves  that  this  works:  we  print  out  the  name  of  each  vertex  after we’ve  printed  out  the  names  of  all  the  vertices  that  it  points  to.  when  visit is  changed  in  this  way  and  run  on  our  example,  it  prints  out  the  vertices  in the  reverse  topological  order  given  above. printing  out  the  vertex  name  on exit  from  this  recursive  procedure  is  exactly  equivalent  to  putting  the  vertex name  on  a  stack  on  entry,  then  popping  it  and  printing  it  on  exit.  it  would be  ridiculous  to  use  an  explicit  stack  in  this  case,  since  the  mechanism  for recursion  provides  it  automatically;  we  mention  this  because  we  do  need  a stack  for  the  more  difficult  problem  to  be  considered  next.
strongly  connected  components if  a  graph  contains  a directed cycle,  (if  we  can  get  from  a  node  back  to  itself by  following  edges  in  the  indicated  direction),  then  it  it  is  not  a  dag  and  it can’t  be  topologically  sorted:  whichever  vertex  on  the  cycle  is  printed  out  first will  have  another  vertex  which  points  to  it  which  hasn’t  yet  been  printed  out. the  nodes  on  the  cycle  are  mutually  accessible  in  the  sense  that  there  is  a way  to  get  from  every  node  on  the  cycle  to  another  node  on  the  cycle  and back.  on  the  other  hand,  even  though  a  graph  may  be  connected,  it  is  not likely  to  be  true  that  any  node  can  be  reached  from  any  other  via  a  directed path.  in  fact,  the  nodes  divide  themselves  into  sets  called  strongly  connected components  with  the  property  that  all  nodes  within  a  componenl  are  mutually accessible, but there is no way to get from a node in one component to a node in  another  component  and  back.  the  strongly  connected  components  of  the directed  graph  at  the  beginning  of  this  chapter  are  two  single  nodes  b  and  k, one pair of nodes h i, one triple of nodes d e f, and one large component with six  nodes  a  c  g  j  l  m.  for  example,  vertex  a  is  in  a  different  component from  vertex  f  because  though  there  is  a  path  from  a  to  f,  there  is  no  way  to get  from  f  to  a.
the  strongly  connected  components  of  a  directed  graph  can  be  found using  a  variant  of  depth-first  search,  as  the  reader  may  have  learned  to  expect. the  method  that  we’ll  examine  was  discovered  by  r.  e.  tarjan   in  1972.  since it  is  based  on  depth-first  search,  it  runs  in  time  proportional  to  v  + e,  but  it  is actually  quite  an  ingenious  method.  it  requires  only  a  few  simple  modifications to  our  basic  visit  procedure,  but  before  tarjan   presented  the  method,  no  linear
values  (using  some  non-existent  weight  to  represent  false),  or  we  include  a field  for  the  edge  weight  in  adjacency  list  records  in  the  adjacency  structure.
it  is  often  necessary  to  associate  other  information  with  the  vertices or  nodes  of  a  graph  to  allow  it  to  model  more  complicated  objects  or  to save  bookkeeping  information  in  complicated  algorithms.  extra  information associated  with  each  vertex  can  be  accommodated  by  using  auxiliary  arrays indexed  by  vertex  number  (or  by  making  adj  an  array  of  records  in  the adjacency  structure  representation).  extra  information  associated  with  each edge  can  be  put  in  the  adjacency  list  nodes  (or  in  an  array  a  of  records  in the  adjacency  matrix  representation),  or  in  auxiliary  arrays  indexed  by  edge number  (this  requires  numbering  the  edges).
at  the  beginning  of  this  chapter,  we  saw  several  natural  questions  that  arise immediately  when  processing  a  graph.  is  the  graph  connected?  if  not,  what are  its  connected  components?  does  the  graph  have  a  cycle?  these  and  many other  problems  can  be  easily  solved  with  a  technique  called  depth-first search, which is a natural way to “visit” every node and check every edge in the graph systematically.  we’ll  see  in  the  chapters  that  follow  that  simple  variations on  a  generalization  of  this  method  can  be  used  to  solve  a  variety  of  graph problems.
for  now,  we’ll  concentrate  on  the  mechanics  of  examining  every  piece of  the  graph  in  an  organized  way.  below  is  an  implementation  of  depth-first search  which  fills  in  an  array  vaj   [l..vl  as  it  visits  every  vertex  of  the  graph. the  array  is  initially  set  to  all  zeros,  so  vaj[k]=o  indicates  that  vertex  k  has not  yet  been  visited.  the  goal  is  to  systematically  visit  all  the  vertices  of  the graph,  setting  the  vaj   entry  for  the  nowth  vertex  visited  to  now,  for  now= 1,2,..., v.  the  program  uses  a  recursive  procedure  visit  which  visits  all  the vertices  in  the  same  connected  component  as  the  vertex  given  in  the  argument. to  visit  a  vertex,  we  check  all  its  edges  to  see  if  they  lead  to  vertices  which haven’t  yet  been  visited  (as  indicated  by  0  vaj   entries);  if  so,  we  visit  them:
tree  nodes  are  blackened  in  these  diagrams,  fringe  nodes  are  crossed,  and unseen  nodes  are  blank.  depth-first  search  “explores”  the  graph  by  looking for  new  vertices  far  away  from  the  start  point,  taking  closer  vertices  only  when dead  ends  are  encountered;  breadth-first  search  completely  covers  the  area close  to  the  starting  point,  moving  farther  away  only  when  everything  close has  been  looked  at.  depth-first  search  is  appropriate  for  one  person  looking for  something  in  a  maze  because  the  “next  place  to  look”  is  always  close  by; breadth-first  search  is  more  like  a  group  of  people  looking  for  something  by fanning  out  in  all  directions.
beyond  these  operational  differences,  it  is  interesting  to  reflect  on  the fundamental  differences  in  the  implementations  of  these  methods.  depth-first search  is  very  simply  expressed  recursively  (because  its  underlying  data  structure  is  a  stack),  and  breadth-first  search  admits  to  a  very  simple  nonrecursive implementation  (because  its  underlying  data  structure  is  a  queue).  but  we’ve seen  that  the  true  underlying  data  structure  for  graph  algorithms  is  a  priority queue,  and  this  admits  a  wealth  of  interesting  properties  to  consider.  again, we’ll  see  more  examples  in  the  next  chapter.
union-find  algorithms in  some  applications  we  wish  to  know  simply  whether  a  vertex  x  is  connected to  a  vertex  y  in  a  graph;  the  actual  path  connecting  them  may  not  be relevant.  this  problem  has  been  carefully  studied  in  recent  years;  some efficient  algorithms  have  been  developed  which  are  of  independent  interest because  they  can  also  be  used  for  processing  sets  (collections  of  objects).
graphs  correspond  to  sets  of  objects  in  a  natural  way,  with  vertices corresponding  to  objects  and  edges  have  the  meaning  “is  in  the  same  set  as.” thus,  the  sample  graph  in  the  previous  chapter  corresponds  to  the  sets  {a  b c  d  e  f  g},  {h  i}  and  {j  k  l  m}. eac  connected  component  corresponds
the  running  time  of  this  program  is  proportional  to  the  number  of  grid  squares touched.  since  we  were  careful  to  arrange  things  so  that  each  grid  square contains  a  constant  number  of  points  on  the  average,  this  is  also  proportional, on  the  average,  to  the  number  of  points  examined.  if  the  number  of  points in  the  search  rectangle  is  r,  then  the  number  of  grid  squares  examined  is proportional  to  r.  the  number  of  grid  squares  examined  which  do  not  fall completely  inside  the  search  rectangle  is  certainly  less  than  a  small  constant times  r,  so  the  total  running  time  (on  the  average)  is  linear  in  r,  the  number of  points  sought.  for  large  r,  the  number  of  points  examined  which  don’t  fall in  the  search  rectangle  gets  quite  small:  all  such  points  fall  in  a  grid  square which  intersects  the  edge  of  the  search  rectangle,  and  the  number  of  such squares  is  proportional  to  fi  for  large  r.  note  that  this  argument  falls apart  if  the  grid  squares  are  too  small  (too  many  empty  grid  squares  inside the  search  rectangle)  or  too  large  (too  many  points  in  grid  squares  on  the perimeter  of  the  search  rectangle)  or  if  the  search  rectangle  is  thinner  than the  grid  squares  (it  could  intersect  many  grid  squares,  but  have  few  points inside  it).
the  grid  method  works  well  if  the  points  are  well  distributed  over  the assumed  range  but  badly  if  they  are  clustered  together.  (for  example,  all the  points  could  fall  in  one  grid  box,  which  would  mean  that  all  the  grid machinery  gained  nothing.)  the  next  method  that  we  will  examine  makes this  worst  case  very  unlikely  by  subdividing  the  space  in  a  nonuniform  way,
by a simple path in only one of two ways (corresponding to the truth or falsity of  the  variables).  these  small  components  are  attached  together  as  specified by  the  clauses,  using  more  complicated  subgraphs  which  can  be  traversed  by simple  paths  corresponding  to  the  truth  or  falsity  of  the  clauses.  it  is  quite a  large  step  from  this  brief  description  to  the  full  construction:  the  point is  to  illustrate  that  polynomial  reduction  can  be  applied  to  quite  dissimilar problems.
thus,  if  we  were  to  have  a  polynomial-time  algorithm  for  the  traveling salesman  problem,  then  we  would  have  a  polynomial-time  algorithm  for  the hamilton  cycle  problem,  which  would  also  give  us  a  polynomial-time  algorithm for  the  satisfiability  problem. each  problem  that  is  proven  np-complete provides  another  potential  basis  for  proving  yet  another  future  problem  npcomplete.  the  proof  might  be  as  simple  as  the  reduction  given  above  from  the hamilton  cycle  problem  to  the  traveling  salesman  problem,  or  as  complicated as  the  transformation  sketched  above  from  the  satisfiability  problem  to  the hamilton  cycle  problem,  or  somewhere  in  between.  literally  thousands  of problems  have  been  proven  to  be  np-complete  over  the  last  ten  years  by transforming  one  to  another  in  this  way.
cook’s  theorem reduction  uses  the  np-completeness  of  one  problem  to  imply  the  np-completeness  of  another.  there  is  one  case  where  it  doesn’t  apply:  how  was  the first   problem  proven  to  be  np-complete?  this  was  done  by  s.  a.  cook  in 1971.  cook  gave  a  direct  proof  that  satisfiability  is  np-complete:  that  if there  is  a  polynomial  time  algorithm  for  satisfiability,  then  all  problems  in np  can  be  solved  in  polynomial  time.
the  proof  is  extremely  complicated  but  the  general  method  can  be  explained.  first,  a  full  mathematical  definition  of  a  machine  capable  of  solving any  problem  in  np  is  developed.  this  is  a  simple  model  of  a  general-purpose computer  known  as  a  turing  machine  which  can  read  inputs,  perform  certain operations,  and  write  outputs.  a  turing  machine  can  perform  any  computation  that  any  other  general  purpose  computer  can,  using  the  same  amount  of time  (to  within  a  polynomial  factor),  and  it  has  the  additional  advantage  that it  can  be  concisely  described  mathematically.  endowed  with  the  additional power  of  nondeterminism,  a  turing  machine  can  solve  any  problem  in  np. the  next  step  in  the  proof  is  to  describe  each  feature  of  the  machine,  including  the  way  that  instructions  are  executed,  in  terms  of  logical  formulas  such as  appear  in  the  satisfiability  problem.  in  this  way  a  correspondence  is  established  between  every  problem  in  np  (which  can  be  expressed  as  a  program  on the  nondeterministic  turing  machine)  and  some  instance  of  satisfiability  (the translation  of  that  program  into  a  logical  formula).  now,  the  solution  to  the satisfiability  problem  essentially  corresponds  t,o  a  simulation  of  the  machine
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
the  m  lists  is  equally  likely  to  be  searched  and,  as  in  sequential  list  searching, that  the  list  searched  is  only  traversed  halfway  (on  t,he   average).  the  average length  of  the  list  examined  (not  counting  z)  in  this  example  for  unsuccessful search  is  (0+4+2+2+0+4+0+2+2+1+0)/11  z  1.545.  this  would  be  the average  time  for  an  unsuccessful  search  were  the  lists  unordered;  by  keeping them  ordered  we  cut  the  time  in  half.  for  a  “successful  search”  (for  one  of  the records in the table), we assume that each record is equally likely to be sought: seven  of  the  keys  would  be  found  as  the  first  list  item  examined,  six  would  be found  as  the  second  item  examined,  etc.,  so  the  average  is  (7.1+   6.2 +  2.3  + 2.4)/17)  z  1.941.  (this  count  assumes  that  equal  keys  are  distinguished  with a  unique  identifier  or  some  other  mechanism,  and  the  search  routine  modified appropriately  to  be  able  to  search  for  each  individual  key.)
if  n,  the  number  of  keys  in  the  table,  is  much  larger  than  m  then  a  good approximation  to  the  average  length  of  the  lists  is  n/m.  as  in  chapter  14, unsuccessful  and  successful  searches  would  be  expected  to  go  about  halfway down  some  list.  thus,  hashing  provides  an  easy  way  to  cut  down  the  time required  for  sequential  search  by  a  factor  of  m,  on  the  average.
in  a  separate  chaining  implementation,  m  is  typically  chosen  to  be  relatively  small  so  as  not  to  use  up  a  large  area  of  contiguous  memory.  but  it’s probably  best  to  choose  m  sufficiently  large  that  the  lists  are  short  enough  to make  sequential  search  the  most  efficient  method  for  them:  “hybrid”  methods (such  as  using  binary  trees  instead  of  linked  lists)  are  probably  not  worth  the trouble.
the  implementation  given  above  uses  a  hash  table  of  links  to  headers of  the  lists  containing  the  actual  keys.  maintaining  m  list  header  nodes  is somewhat  wasteful  of  space;  it  is  probably  worthwhile  to  eliminate  them  and make  heads  be  a  table  of  links  to  the  first  keys  in  the  lists.  this  leads  to some  complication  in  the  algorithm.  for  example,  adding  a  new  record  to  the beginning  of  a  list  becomes  a  different  operation  than  adding  a  new  record anywhere  else  in  a  list,  because  it  involves  modifying  an  entry  in  the  table  of links,  not  a  field  of  a  record.  an  alternate  implementation  is  to  put  the  first key  within  the  table.  if  space  is  at  a  premium,  it  is  necessary  to  carefully analyze  the  tradeoff  between  wasting  space  for  a  table  of  links  and  wasting space for a key and a link for each empty list. if n is much bigger than m then the  alternate  method  is  probably  better,  though  m  is  usually  small  enough that  the  extra  convenience  of  using  list  header  nodes  is  probably  justified.
open addressing if  the  number  of  elements  to  be  put  in  the  hash  table  can  be  estimated  in advance,  then  it  is  probably  not  worthwhile  to  use  any  links  at  all  in  the  hash table.  several  methods  have  been  devised  which  store  n  records  in  a  table
the  m  lists  is  equally  likely  to  be  searched  and,  as  in  sequential  list  searching, that  the  list  searched  is  only  traversed  halfway  (on  t,he   average).  the  average length  of  the  list  examined  (not  counting  z)  in  this  example  for  unsuccessful search  is  (0+4+2+2+0+4+0+2+2+1+0)/11  z  1.545.  this  would  be  the average  time  for  an  unsuccessful  search  were  the  lists  unordered;  by  keeping them  ordered  we  cut  the  time  in  half.  for  a  “successful  search”  (for  one  of  the records in the table), we assume that each record is equally likely to be sought: seven  of  the  keys  would  be  found  as  the  first  list  item  examined,  six  would  be found  as  the  second  item  examined,  etc.,  so  the  average  is  (7.1+   6.2 +  2.3  + 2.4)/17)  z  1.941.  (this  count  assumes  that  equal  keys  are  distinguished  with a  unique  identifier  or  some  other  mechanism,  and  the  search  routine  modified appropriately  to  be  able  to  search  for  each  individual  key.)
if  n,  the  number  of  keys  in  the  table,  is  much  larger  than  m  then  a  good approximation  to  the  average  length  of  the  lists  is  n/m.  as  in  chapter  14, unsuccessful  and  successful  searches  would  be  expected  to  go  about  halfway down  some  list.  thus,  hashing  provides  an  easy  way  to  cut  down  the  time required  for  sequential  search  by  a  factor  of  m,  on  the  average.
in  a  separate  chaining  implementation,  m  is  typically  chosen  to  be  relatively  small  so  as  not  to  use  up  a  large  area  of  contiguous  memory.  but  it’s probably  best  to  choose  m  sufficiently  large  that  the  lists  are  short  enough  to make  sequential  search  the  most  efficient  method  for  them:  “hybrid”  methods (such  as  using  binary  trees  instead  of  linked  lists)  are  probably  not  worth  the trouble.
the  implementation  given  above  uses  a  hash  table  of  links  to  headers of  the  lists  containing  the  actual  keys.  maintaining  m  list  header  nodes  is somewhat  wasteful  of  space;  it  is  probably  worthwhile  to  eliminate  them  and make  heads  be  a  table  of  links  to  the  first  keys  in  the  lists.  this  leads  to some  complication  in  the  algorithm.  for  example,  adding  a  new  record  to  the beginning  of  a  list  becomes  a  different  operation  than  adding  a  new  record anywhere  else  in  a  list,  because  it  involves  modifying  an  entry  in  the  table  of links,  not  a  field  of  a  record.  an  alternate  implementation  is  to  put  the  first key  within  the  table.  if  space  is  at  a  premium,  it  is  necessary  to  carefully analyze  the  tradeoff  between  wasting  space  for  a  table  of  links  and  wasting space for a key and a link for each empty list. if n is much bigger than m then the  alternate  method  is  probably  better,  though  m  is  usually  small  enough that  the  extra  convenience  of  using  list  header  nodes  is  probably  justified.
open addressing if  the  number  of  elements  to  be  put  in  the  hash  table  can  be  estimated  in advance,  then  it  is  probably  not  worthwhile  to  use  any  links  at  all  in  the  hash table.  several  methods  have  been  devised  which  store  n  records  in  a  table
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
specifically,  this  heap  is  built  by  first  initializing  the  heap  array  to  point  to the  non-zero  frequency  counts,  then  using  the  pqdownheap  procedure  from chapter  11,  as  follows:
now, the use of this procedure to construct the tree as above is straightforward:  we  take  the  two  smallest  elements  off  the  heap,  add  them  and  put  the result back into the heap. at each step we create one new count, and decrease the  size  of  the  heap  by  one.  this  process  creates  n-l  new  counts,  one  for each  of  the  internal  nodes  of  the  tree  being  created,  as  in  the  following  code:
the  first  two  lines  of  this  loop  are  actually  pqremove;  the  size  of  the  heap  is decreased  by  one.  then  a  new  internal  node  is  “created”  with  index  26+nand given a value equal to the sum of the value at the root and value just removed. then  this  node  is  put  at  the  root,  which  raises  its  priority,  necessitating another  call  on  pqdownheap  to  restore  order  in  the  heap.  the  tree  itself  is represented  with  an  array  of  “father”  links:  dad[t]  is  the  index  of  the  father of  the  node  whose  weight  is  in count  [t].   the  sign  of  dad[t]  indicates  whether the  node  is  a  left  or  right  son  of  its  father.  for  example,  in  the  tree  above   and  count[28]=37 we  might  have  dad[o]=-30,  count[30]=21,
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
as  mentioned  above,  the  primary  reason  that  heapsort   is  of  practical interest is that the number of steps required to sort m  elements is  guaranteed to  be  proportional  to  m  log  m,  no  matter  what  the  input.  unlike  the  other methods  that  we’ve  seen,  there  is  no  “worst-case”  input  that  will  make  heapsort  run  slower.  the  proof  of  this  is  simple:  we  make  about  3m/2   calls  to downheap  (about  m/2   to  construct  the  heap  and  m  for  the  sort),  each  of which  examines  less  than  log  m  heap  elements,  since  the  heap  never  has  more than  m  elements.
actually,  the  above  proof  uses  an  overestimate.  in  fact,  it  can  be  proven that  the  construction  process  takes  linear  time  since  so  many  small  heaps  are processed.  this  is  not  of  particular  importance  to  heapsort,  since  this  time is  still  dominated  by  the  m  log  m  time  for  sorting,  but  it  is  important  for other  priority  queue  applications,  where  a  linear  time  construct  can  lead  to a  linear  time  algorithm.  note  that  constructing  a  heap  with  m  successive inserts  requires  m  log  m  steps  in  the  worst  case  (though  it  turns  out  to  be linear  on  the  average).
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
specifically,  this  heap  is  built  by  first  initializing  the  heap  array  to  point  to the  non-zero  frequency  counts,  then  using  the  pqdownheap  procedure  from chapter  11,  as  follows:
now, the use of this procedure to construct the tree as above is straightforward:  we  take  the  two  smallest  elements  off  the  heap,  add  them  and  put  the result back into the heap. at each step we create one new count, and decrease the  size  of  the  heap  by  one.  this  process  creates  n-l  new  counts,  one  for each  of  the  internal  nodes  of  the  tree  being  created,  as  in  the  following  code:
the  first  two  lines  of  this  loop  are  actually  pqremove;  the  size  of  the  heap  is decreased  by  one.  then  a  new  internal  node  is  “created”  with  index  26+nand given a value equal to the sum of the value at the root and value just removed. then  this  node  is  put  at  the  root,  which  raises  its  priority,  necessitating another  call  on  pqdownheap  to  restore  order  in  the  heap.  the  tree  itself  is represented  with  an  array  of  “father”  links:  dad[t]  is  the  index  of  the  father of  the  node  whose  weight  is  in count  [t].   the  sign  of  dad[t]  indicates  whether the  node  is  a  left  or  right  son  of  its  father.  for  example,  in  the  tree  above   and  count[28]=37 we  might  have  dad[o]=-30,  count[30]=21,
as  mentioned  above,  the  primary  reason  that  heapsort   is  of  practical interest is that the number of steps required to sort m  elements is  guaranteed to  be  proportional  to  m  log  m,  no  matter  what  the  input.  unlike  the  other methods  that  we’ve  seen,  there  is  no  “worst-case”  input  that  will  make  heapsort  run  slower.  the  proof  of  this  is  simple:  we  make  about  3m/2   calls  to downheap  (about  m/2   to  construct  the  heap  and  m  for  the  sort),  each  of which  examines  less  than  log  m  heap  elements,  since  the  heap  never  has  more than  m  elements.
actually,  the  above  proof  uses  an  overestimate.  in  fact,  it  can  be  proven that  the  construction  process  takes  linear  time  since  so  many  small  heaps  are processed.  this  is  not  of  particular  importance  to  heapsort,  since  this  time is  still  dominated  by  the  m  log  m  time  for  sorting,  but  it  is  important  for other  priority  queue  applications,  where  a  linear  time  construct  can  lead  to a  linear  time  algorithm.  note  that  constructing  a  heap  with  m  successive inserts  requires  m  log  m  steps  in  the  worst  case  (though  it  turns  out  to  be linear  on  the  average).
the  methods  for  doing  arithmetic  operations  given  in  chapter  2  are simple  and  straightforward  solutions  to  familiar  problems.  as  such,  they provide  an  excellent  basis  for  applying  allgorithmic   thinking  to  produce  more sophisticated  methods  which  are  substantially  more  efficient.  as  we’ll  see,  it is  one  thing  to  write  down  a  formula  which  implies  a  particular  mathematical calculation;  it  is  quite  another  thing  to  write  a  computer  program  which performs  the  calculation  efficiently.
operations  on  mathematical  objects  are  far  too  diverse  to  be  catalogued here;  we’ll  concentrate  on  a  variety  of  algorithms  for  manipulating  polynomials. the  principal  method  that  we’ll  study  in  this  section  is  a  polyno mial  multiplication  scheme  which  is  of  no  particular  practical  importance  but which  illustrates  a  basic  design  paradigm  called  divide-and-conquer  which  is pervasive  in  algorithm  design.  we’ll  see  in  this  section  how  it  applies  to  matrix multiplication  as  well  as  polynomial  multiplication;  in  later  sections  we’ll  see it  applied  to  most  of  the  problems  that  we  encounter  in  this  book.
then  compute  and  add  3x3,  etc.  this for  any  given  x,  one  could  compute  x4, method  requires  recomputation  of  the  powers  of  x;  an  alternate  method,  which requires  extra  storage,  would  save  the  powers  of  x  as  they  are  computed.
a  simple  method  which  avoids  recomputation  and  uses  no  extra  space is  known  as  homer’s  rule:  by  alternat:ing   the  multiplication  and  addition operations  appropriately,  a  degree-n  polynomial  can  be  evaluated  using  only
a  more  complicated  problem  is  to  evaluate  a  given  polynomial  at  many different  points.  different  algorithms  are  appropriate  depending  on  how  many evaluations  are  to  be  done  and  whether  or  not  they  are  to  be  done  simultaneously.  if  a  very  large  number  of  evaluations  is  to  be  done,  it  may  be worthwhile  to  do  some  “precomputing”   which  can  slightly  reduce  the  cost for  later  evaluations.  note  that  using  horner’s  method  would  require  about n2   multiplications  to  evaluate  a  degree-n  polynomial  at  n  different  points. much  more  sophisticated  methods  have  been  designed  which  can  solve  the problem  in  n(logn)’   steps,  and  in  chapter  36  we’ll  see  a  method  that  uses only  n  log  n  multiplications  for  a  specific  set  of  n  points  of  interest.
if  the  given  polynomial  has  only  one  term,  then  the  polynomial  evaluation  problem  reduces  to  the  exponentiation  problem:  compute  xn.   horner’s rule  in  this  case  degenerates  to  the  trivial  algorithm  which  requires  n  -  1 multiplications.  for  an  easy  example  of  how  we  can  do  much  better,  consider the  following  sequence  for  computing  x32:
the  “successive  squaring”  method  can  easily  be  extended  to  general  n if  computed  values  are  saved.  for  example,  x55  can  be  computed  from  the above  values  with  four  more  multiphcations:
in  general,  the  binary  representation  of  n  can  be  used  to  choose  which computed  values  to  use.  (in  the  example,  since  55  =  (110111)2,   all  but  x8 are  used.)  the  successive  squares  can  be  computed  and  the  bits  of  n  tested within  the  same  loop.  two  methods  are  available  to  implement  this  using  only
the  message:  this  means  that  we  need  to  save  the  tree  along  with  the  message in  order  to  decode  it.  fortunately,  this  does  not  present  any  real  difficulty. it  is  actually  necessary  only  to  store  the  code  array,  because  the  radix  search trie  which  results  from  inserting  the  entries  from  that  array  into  an  initially empty  tree  is  the  decoding  tree.
thus,  the  storage  savings  quoted  above  is  not  entirely  accurate,  because the  message  can’t  be  decoded  without  the  trie  and  we  must  take  into  account the  cost  of  storing  the  trie  (i.e., the  code  array)  along  with  the  message. huffman  encoding is therefore only effective for long files where the savings in the message is enough to offset the cost, or in situations where the coding trie can be  precomputed   and  used  for  a  large  number  of  messages.  for  example,  a trie  based  on  the  frequencies  of  occurrence  of  letters  in  the  english  language could  be  used  for  text  documents. for  that  matter,  a  trie  based  on  the frequency  of  occurrence  of  characters  in  pascal  programs  could  be  used  for encoding  programs  (for  example, “;”  is  likely  to  be  near  the  top  of  such  a trie).  a  huffman   encoding  algorithm  saves  about  23%  when  run  on  the  text for  this  chapter.
as  before,  for  truly  random  files,  even  this  clever  encoding  scheme  won’t work  because  each  character  will  occur  approximately  the  same  number  of times,  which  will  lead  to  a  fully  balanced  coding  tree  and  an  equal  number  of bits  per  letter  in  the  code.
we’ll  prefix  implementations  of  priority  queue  routines  based  on  indirect  heaps with  “pq”  for  indentification   when  they  are  used  in  later  chapters.
now,  to  modify  downheap  to  work  indirectly,  we  need  only  examine  the places  where  it  references  a.  where  it  did  a  comparison  before,  it  must  now access  a  indirectly  through  heap.  where  it  did  a  move  before,  it  must  now make  the  move  in  heap,  not  a,  and  it  must  modify  inv  accordingly.  this  leads to  the  following  implementation:
a  similar  indirect  implementation  can  be  developed  based  on  maintaining heap  as  an  array  of  pointers  to  separately  allocated  records.  in  this  case,  a little  more  work  is  required  to  implement  the  function  of  inv  (find  the  heap position,  given  the  record).
advanced  implementations if  the  join  operation  must  be  done  efficiently,  then  the  implementations  that we have done so far are insufficient and more advanced techniques are needed. although we don’t have space here to go into the details of such methods, we can  discuss  some  of  the  considerations  that  go  into  their  design.
by  “efficiently,”  we  mean  that  a  join  should  be  done  in  about  the  same time  as  the  other  operations.  this  immediately  rules  out  the  linkless  representation  for  heaps  that  we  have  been  using,  since  two  large  heaps  can  be joined  only  by  moving  all  the  elements  in  at  least  one  of  them  to  a  large array.  it  is  easy  to  translate  the  algorithms  we  have  been  examining  to  use linked  representations;  in  fact,  sometimes  there  are  other  reasons  for  doing
the  idea  is  to  pass  the  (unordered)  input  through  a  large  priority  queue, always  writing  out  the  smallest  element  on  the  priority  queue  as  above,  and always  replacing  it  with  the  next  element  from  the  input,  with  one  additional proviso:  if  the  new  element  is  smaller  than  the  last  one  put  out,  then,  since it  could  not  possibly  become  part  of  the  current  sorted  block,  it  should  be marked  as  a  member  of  the  next  block  and  treated  as  greater  than  all  elements in  the  current  block.  when  a  marked  element  makes  it  to  the  top  of  the priority  queue,  the  old  block  is  ended  and  a  new  block  started.  again,  this is  easily  implemented  with  pqinsert   and  pqreplace  from  chapter  11,  again appropriately  modified  so  that  the  smallest  element  is  at  the  top  of  the  heap, and  with  pqreplace  changed  to  treat  marked  elements  as  always  greater  than unmarked  elements.
our  example  file  clearly  demonstrates  the  value  of  replacement  selection. with  an  internal  memory  capable  of  holding  only  three  records,  we  can produce  sorted  blocks  of  size  5,  4,  9,  6,  and  1,  as  illustrated  in  the  following table.  each  step  in  the  diagram  below  shows  the  next  key  to  be  input  (boxed) and  the  contents  of  the  heap  just  before  that  key  is  input.  (as  before,  the order  in  which  the  keys  occupy  the  first  position  in  the  heap  is  the  order  in which  they  are  output.)  asterisks  are  used  to  indicate  which  keys  in  the  heap belong  to  different  blocks:  an  element  marked  the  same  way  as  the  element  at the  root  belongs  to  the  current  sorted  block,  others  belong  to  the  next  sorted block.  always,  the  heap  condition  (first  key  less  than  the  second  and  third)  is maintained,  with  elements  in  the  next  sorted  block  considered  to  be  greater than  elements  in  the  current  sorted  block.
for  example,  when  pqreplace  is  called  for  m,  it  returns  n  for  output  (a  and d  are  considered  greater)  and  then  sifts  down  m  to  make  the  heap  a  m  d.
expected  to  take  on  “typical”  input  data,  and  in  the  worst case,  the  amount of  time  a  program  would  take  on  the  worst  possible  input  configuration.
many of the algorithms in this book are very well understood, to the point that  accurate  mathematical  formulas  are  known  for  the  average-  and  worstcase  running  time.  such  formulas  are  developed  first  by  carefully  studying the  program,  to  find  the  running  time  in  terms  of  fundamental  mathematical quantities  and  then  doing  a  mathematical  analysis  of  the  quantities  involved. for  some  algorithms,  it  is  easy  to  hgure  out  the  running  time.  for  example,  the  brute-force  algorithm  above  obviously  requires  min(u,   vu)-gcd(u,   v) iterations  of  the  while  loop,  and  this  quantity  dominates  the  running  time  if the  inputs  are  not  small,  since  all  the  other  statements  are  executed  either 0  or  1  times.  for  other  algorithms,  a  substantial  amount  of  analysis  is  involved.  for  example,  the  running  time  of  the  recursive  euclidean  algorithm obviously  depends  on  the  “overhead”  required  for  each  recursive  call  (which can  be  determined  only  through  detailed1  knowledge  of  the  programming  environment  being  used)  as  well  as  the  number  of  such  calls  made  (which  can be  determined  only  through  extremely  sophisticated  mathematical  analysis). several  important  factors  go  into  this  analysis  which  are  somewhat  outside  a  given  programmer’s  domain  of  influence.  first,  pascal  programs  are translated  into  machine  code  for  a  given  computer,  and  it  can  be  a  challenging task  to  figure  out  exactly  how  long  even  one  pascal  statement  might  take  to execute  (especially  in  an  environment  where  resources  are  being  shared,  so that  even  the  same  program  could  have  varying  performance  characteristics). second,  many  programs  are  extremely  sensitive  to  their  input  data,  and  performance  might  fluctuate  wildly  depending  on  the  input.  the  average  case might  be  a  mathematical  fiction  that  is  not  representative  of  the  actual  data on  which  the  program  is  being  used,  and  the  worst  case  might  be  a  bizarre construction  that  would  never  occur  in  practice.  third,  many  programs  of interest  are  not  well  understood,  and  specific  mathematical  results  may  not be  available.  finally,  it  is  often  the  case  that  programs  are  not  comparable  at all:  one  runs  much  more  efficiently  on  one  particular  kind  of  input,  the  other runs  efficiently  under  other  circumstances.
with  these  caveats  in  mind,  we’ll  use  rough  estimates  for  the  running time  of  our  programs  for  purposes  of  classification,  secure  in  the  knowledge that  a  fuller  analysis  can  be  done  for  important  programs  when  necessary. such  rough  estimates  are  quite  often  easy  to  obtain  via  the  old  programming saw  “90%  of  the  time  is  spent  in  10%  of  the  code.”  (this  has  been  quoted  in the  past  for  many  different  values  of  “go%.“)
the first step in getting a rough estimate of the running time of a program is  to  identify  the  inner  loop.  which  instructions  in  the  program  are  executed most  often?  generally,  it  is  only  a  few  instructions,  nested  deep  within  the
control  structure  of  a  program,  that  absorb  all  of  the  machine  cycles.  it  is always  worthwhile  for  the  programmer  to  be  aware  of  the  inner  loop,  just  to be  sure  that  unnecessary  expensive  instructions  are  not  put  there.
second,  some  analysis  is  necessary  to  estimate  how  many  times  the  inner loop  is  iterated.  it  would  be  beyond  the  scope  of  this  book  to  describe  the mathematical  mechanisms  which  are  used  in  such  analyses,  but  fortunately the  running  times  many  programs  fall  into  one  of  a  few  distinct  classes.  when possible,  we’ll  give  a  rough  description  of  the  analysis  of  the  programs,  but  it will  often  be  necessary  merely  to  refer  to  the  literature.  (specific  references are  given  at  the  end  of  each  major  section  of  the  book.)  for  example,  the results  of  a  sophisticated  mathematical  argument  show  that  the  number  of recursive steps in euclid’s algorithm when u is chosen at random less than  v is approximately ((12  in  2)/7r2) 1 n  tj.   often,  the  results  of  a  mathematical  analysis are  not  exact,  but  approximate  in  a  precise  technical  sense:  the  result  might be  an  expression  consisting  of  a  sequence  of  decreasing  terms.  just  as  we  are most  concerned  with  the  inner  loop  of  a  program,  we  are  most  concerned  with the  leading  term  (the  largest  term)  of  a  mathematical  expression.
as  mentioned  above,  most  algorithms  have  a  primary  parameter  n, usually  the  number  of  data  items  to  be  processed,  which  affects  the  running time  most  significantly.  the  parameter  n  might  be  the  degree  of  a  polynomial,  the  size  of  a  file  to  be  sorted  or  searched,  the  number  of  nodes  in  a graph,  etc.  virtually  all  of  the  algorithms  in  this  book  have  running  time proportional  to  one  of  the  following  functions:
most  instructions  of  most  programs  are  executed  once  or  at  most only  a  few  times.  if  all  the  instructions  of  a  program  have  this property,  we  say  that  its  running  time  is  constant.  this  is  obviously the  situation  to  strive  for  in  algorithm  design.
log n when  the  running  time  of  a  program  is  logarithmic,  the  program gets  slightly  slower  as  n  grows.this   running  time  commonly  occurs in  programs  which  solve  a  big  problem  by  transforming  it  into  a smaller  problem  by  cutting  the  size  by  some  constant  fraction.  for our  range  of  interest,  the  running  time  can  be  considered  to  be  less than  a  yarge”  constant.  the  base  of  the  logarithm  changes  the constant,  but  not  by  much:  when  n  is  a  thousand,  log  n  is  3  if  the base  is  10,  10  if  the  base  is  2;  when  n  is  a  million,  1ogn  is  twice as  great.  whenever  n  doubles,  log  n  increases  by  a  constant,  but log  n  doesn’t  double  until  n  increases  to  n2. when  the  running  time  of  a  program  is  linear,  it  generally  is  the  case that  a  small  amount  of  processing  is  done  on  each  input  element. when  n  is  a  million,  then  so  is  the  running  time.  whenever  n
the  second  position,  continuing  in  this  way  until  the  entire  array  is  sorted. this  method  is  called  selection  sort  because  it  works  by  repeatedly  “selecting” the  smallest  remaining  element. the  following  program  sorts  a  [1..n]  into numerical  order:
this  is  among  the  simplest  of  sorting  methods,  and  it  will  work  very  well  for small  files.  its  running  time  is  proportional  to  n2:   the  number  of  comparisons between array elements is about  n2/2  since the outer loop (on i) is executed n times  and  the  inner  loop  (on  j)  is  executed  about  n/2  times  on  the  average.  it turns  out  that  the  statement  min:=j  is  executed  only  on  the  order  of  n  log  n times,  so  it  is  not  part  of  the  inner  loop
despite  its  simplicity,  selection  sort  has  a  quite  important  application: it  is  the  method  of  choice  for  sorting  files  with  very  large  records  and  small keys. if the records are  m words long (but the keys are only a few words long), then  the  exchange  takes  time  proportional  to  m,  so  the  total  running  time is  proportional  to  n2   (for  the  comparisons)  plus  nm  (for  the  exchanges).  if m  is  proportional  to  n  then  the  running  time  is  linear  in  the  amount  of  data input,  which  is  difficult  to  beat  even  with  an  advanced  method.  of  course  if it  is  not  absolutely  required  that  the  records  be  actually  rearranged,  then  an “indirect  sort”  can  be  used  to  avoid  the  nm  term  entirely,  so  a  method  which uses  less  comparisons  would  be  justified.  still  selection  sort  is  quite  attractive for  sorting  (say)  a  thousand  looo-word   records  on  one-word  keys.
insertion  sort an  algorithm  almost  as  simple  as  selection  sort  but  perhaps  more  flexible  is insertion sort.  this  is  the  method  often  used  by  people  to  sort  bridge  hands: consider  the  elements  one  at  a  time,  inserting  each  in  its  proper  place  among those  already  considered  (keeping  them  s.orted).   the  element  being  considered is  inserted  merely  by  moving  larger  elements  one  position  to  the  right,  then
as  is,  this  code  doesn’t  work,  because  the  while  will  run  past  the  left  end of  the  array  if  t  is  the  smallest  element  in  the  array.  one  way  to  fix  this  is to  put  a  “sentinel”  key  in  a[o], making  it  at  least  as  small  as  the  smallest element  in  the  array.  using  sentinels  in  situations  like  this  is  common  in sorting  programs  to  avoid  including  a  test  (in  this  case  j>l)  which  almost always  succeeds  within  the  inner  loop.  if  for  some  reason  it  is  inconvenient  to use  a  sentinel  and  the  array  really  must  have  the  bounds  [1..n],  then  standard pascal does not allow a clean alternative, since it does not have a “conditional” and  instruction:  the  test  while  (j>l) and  (a1j-l]>v)  won’t  work  because even  when  j=l,  the  second  part  of  the  and  will  be  evaluated  and  will  cause an  o&of-bounds   array  access.  a  goto  out  of  the  loop  seems  to  be  required. (some  programmers  prefer  to  goto   some  lengths  to  avoid  goto  instructions, for  example  by  performing  an  action  within  the  loop  to  ensure  that  the  loop terminates.  in  this  case,  such  a  solution  seems  hardly  justified,  since  it  makes the  program  no  clearer,  and  it  adds  extra  overhead  everytime  through  the loop  to  guard  against  a  rare  event.)
on  the  average,  the  inner  loop  of  insertion  sort  is  executed  about  n2/2 times:  the  “average”  insertion  goes  about  halfway  into  a  subfile  of  size  n/2. this  is  inherent  in  the  method.  the  point  of  insertion  can  be  found  more efficiently  using  the  searching  techniques  in  chapter  14,  but  n2/2   moves  (to make  room  for  each  element  being  inserted)  are  still  required;  or  the  number of  moves  can  be  lowered  by  using  a  linked  list  instead  of  an  array,  but  then the  methods  of  chapter  14  don’t  apply  and  n2/2   comparisons  are  required (to  find  each  insertion  point).
as  is,  this  code  doesn’t  work,  because  the  while  will  run  past  the  left  end of  the  array  if  t  is  the  smallest  element  in  the  array.  one  way  to  fix  this  is to  put  a  “sentinel”  key  in  a[o], making  it  at  least  as  small  as  the  smallest element  in  the  array.  using  sentinels  in  situations  like  this  is  common  in sorting  programs  to  avoid  including  a  test  (in  this  case  j>l)  which  almost always  succeeds  within  the  inner  loop.  if  for  some  reason  it  is  inconvenient  to use  a  sentinel  and  the  array  really  must  have  the  bounds  [1..n],  then  standard pascal does not allow a clean alternative, since it does not have a “conditional” and  instruction:  the  test  while  (j>l) and  (a1j-l]>v)  won’t  work  because even  when  j=l,  the  second  part  of  the  and  will  be  evaluated  and  will  cause an  o&of-bounds   array  access.  a  goto  out  of  the  loop  seems  to  be  required. (some  programmers  prefer  to  goto   some  lengths  to  avoid  goto  instructions, for  example  by  performing  an  action  within  the  loop  to  ensure  that  the  loop terminates.  in  this  case,  such  a  solution  seems  hardly  justified,  since  it  makes the  program  no  clearer,  and  it  adds  extra  overhead  everytime  through  the loop  to  guard  against  a  rare  event.)
on  the  average,  the  inner  loop  of  insertion  sort  is  executed  about  n2/2 times:  the  “average”  insertion  goes  about  halfway  into  a  subfile  of  size  n/2. this  is  inherent  in  the  method.  the  point  of  insertion  can  be  found  more efficiently  using  the  searching  techniques  in  chapter  14,  but  n2/2   moves  (to make  room  for  each  element  being  inserted)  are  still  required;  or  the  number of  moves  can  be  lowered  by  using  a  linked  list  instead  of  an  array,  but  then the  methods  of  chapter  14  don’t  apply  and  n2/2   comparisons  are  required (to  find  each  insertion  point).
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
computing  the  integral  is  a  fundamental  analytic  operation  often  performed  on  functions  being  processed  on  computers.  one  of  two  completely different approaches can be used, depending on the way the function is represented.  if  an  explicit  representation  of  the  function  is  available,  then  it may  be  possible  to  do  symbolic integrathn   to  compute  a  similar  representation for  the  integral.  at  the  other  extreme,  the  function  may  be  defined  by  a  table, so  that  function  values  are  known  for  only  a  few  points.  the  most  common situation  is  between  these:  the  function  to  be  integrated  is  represented  in  such a  way  that  its  value  at  any  particular  point  can  be  computed.  in  this  case, the goal  is to  compute  a  reasonable  approximation  to  the  integral  of  the  function,  without  performing  an  excessive  number  of  function  evaluations.  this computation  is  often  called  quadrature  by  numerical  analysts.
symbolic  integration if  full  information  is  available  about  a  function,  then  it  may  be  worthwhile to  consider  using  a  method  which  involves  manipulating  some  representation of  the  function  rather  than  working  with  numeric  values. the  goal  is  to transform  a  representation  of  the  function  into  a  representation  of  the  integral, in  much  the  same  way  that  indefinite  integration  is  done  by  hand.
a  simple  example  of  this  is  the  integ,ration  of  polynomials.  in  chapters  2 and  4  we  examined  methods  for  “symbolically”  computing  sums  and  products of  polynomials,  with  programs  that  work.ed   on  a  particular  representation  for the  polynomials  and  produced  the  representation  for  the  answers  from  the  representation  for  the  inputs.  the  operation  of  integration  (and  differentiation) of  polynomials  can  also  be  done  in  this  way.  if  a  polynomial
unlike  our  other  methods,  where  we  decide  how  much  work  we  want to  do  and  then  take  whatever  accuracy  results,  in  adaptive  quadrature  we  do however much work is necessary to achieve a degree of accuracy that we decide upon  ahead  of  time.  this  means  that  tolerance  must  be  chosen  carefully, so  that  the  routine  doesn’t  loop  indefinitely  to  achieve  an  impossibly  high tolerance.  the  number  of  steps  required  depends  very  much  on  the  nature  of the  function  being  integrated.  a  function  which  fluctuates  wildly  will  require a  large  number  of  steps,  but  such  a  function  would  lead  to  a  very  inaccurate answer  for  the  “fixed  interval”  methods.  a  smooth  function  such  as  our example  can  be  handled  with  a  reasonable  number  of  steps.  the  following table  gives,  for  various  values  of  t,  the  value  produced  and  the  number  of recursive  calls  required  by  the  above  routine  to  compute  jrz  dx/x:
the  above  program  can  be  improved  in  several  ways.  first,  there’s certainly  no  need  to  call  intsimp(a,  b,   io)   twice.  in  fact,  the  function  values for  this  call  can  be  shared  by  intsimp(a,  b,  5).  second,  the  tolerance  bound can  be  related  to  the  accuracy  of  the  answer  more  closely  if  the  tolerance  is scaled  by  the  ratio  of  the  size  of  the  current  interval  to  the  size  of  the  full interval.  also,  a  better  routine  can  obviously  be  developed  by  using  an  even better  quadrature  rule  than  simpson’s  (but  it  is  a  basic  law  of  recursion  that another  adaptive  routine  wouldn’t  be  a  good  idea).  a  sophisticated  adaptive quadrature  routine  can  provide  very  accurate  results  for  problems  which  can’t be  handled  any  other  way,  but  careful  attention  must  be  paid  to  the  types  of functions to be processed.
we  will  be  seeing  several  algorithms  that  have  the  same  recursive  structure  as  the  adaptive  quadrature  method  given  above.  the  general  technique of  adapting  simple  methods  to  work  hard  only  on  difficult  parts  of  complex problems  can  be  a  powerful  one  in  algorithm  design.
it  turns  out  that  simpson’s  method  is  exactly  equivalent  to  interpolating the  data  to  a  piecewise  quadratic  function,  then  integrating.  it  is  interesting to  note  that  the  four  methods  we  have  discussed  all  can  be  cast  as  piecewise interpolation  methods:  the  rectangle  rule  interpolates  to  a  constant  (degree-o polynomial);  the  trapezoid  rule  to  a  line  (degree-l  polynomial);  simpson’s  rule to  a  quadratic  polynomial;  and  spline  qua.drature   to  a  cubic  polynomial.
adaptive  a  major  flaw  in  the  methods  that  we  have  discussed  so  far  is  that  the  errors involved  depend  not,  only  upon  the  subinterval  size  used,  but  also  upon  the value  of  the  high-order  derivatives  of  the  function  being  integrated.  this implies  that  the  methods  will  not  work  well  at  all  for  certain  functions  (those with  large  high-order  derivatives).  but  few  functions  have  large  high-order derivatives  everywhere.  it  is  reasonable  to  use  small  intervals  where  the derivatives  are  large  and  large  intervals  where  the  derivatives  are  small.  a method  which  does  this  in  a  systematic  way  is  called  an  adaptive  quadrature routine.
the  general  approach  in  adaptive  quadrature  is  to  use  two  different quadrature  methods  for  each  subinterval,  compare  the  results,  and  subdivide the  interval  further  if  the  difference  is  too  great.  of  course  some  care  should be exercised, since if two equally bad methods are used, they might agree quite closely  on  a  bad  result.  one  way  to  avoid  this  is  to  ensure  that  one  method always  overestimates  the  result  and  that  the  other  always  underestimates  the result,. another way to avoid this is to ensure that one method is more accurate than  the  other.  a  method  of  this  type  is  described  next.
there  is  significant  overhead  involved  in  recursively  subdividing  the  interval,  so  it  pays  to  use  a  good  method  fo:r  estimating  the  integrals,  as  in  the following 
both  estimates  for  the  integral  are  derived  from  simpson’s  method,  one using  twice  as  many  subdivisions  as  the  other.  essentially,  this  amounts  to checking  the  accuracy  of  simpson’s  method  over  the  interval  in  question  and then  subdividing  if  it  is  not  good  enough.
unlike  our  other  methods,  where  we  decide  how  much  work  we  want to  do  and  then  take  whatever  accuracy  results,  in  adaptive  quadrature  we  do however much work is necessary to achieve a degree of accuracy that we decide upon  ahead  of  time.  this  means  that  tolerance  must  be  chosen  carefully, so  that  the  routine  doesn’t  loop  indefinitely  to  achieve  an  impossibly  high tolerance.  the  number  of  steps  required  depends  very  much  on  the  nature  of the  function  being  integrated.  a  function  which  fluctuates  wildly  will  require a  large  number  of  steps,  but  such  a  function  would  lead  to  a  very  inaccurate answer  for  the  “fixed  interval”  methods.  a  smooth  function  such  as  our example  can  be  handled  with  a  reasonable  number  of  steps.  the  following table  gives,  for  various  values  of  t,  the  value  produced  and  the  number  of recursive  calls  required  by  the  above  routine  to  compute  jrz  dx/x:
the  above  program  can  be  improved  in  several  ways.  first,  there’s certainly  no  need  to  call  intsimp(a,  b,   io)   twice.  in  fact,  the  function  values for  this  call  can  be  shared  by  intsimp(a,  b,  5).  second,  the  tolerance  bound can  be  related  to  the  accuracy  of  the  answer  more  closely  if  the  tolerance  is scaled  by  the  ratio  of  the  size  of  the  current  interval  to  the  size  of  the  full interval.  also,  a  better  routine  can  obviously  be  developed  by  using  an  even better  quadrature  rule  than  simpson’s  (but  it  is  a  basic  law  of  recursion  that another  adaptive  routine  wouldn’t  be  a  good  idea).  a  sophisticated  adaptive quadrature  routine  can  provide  very  accurate  results  for  problems  which  can’t be  handled  any  other  way,  but  careful  attention  must  be  paid  to  the  types  of functions to be processed.
we  will  be  seeing  several  algorithms  that  have  the  same  recursive  structure  as  the  adaptive  quadrature  method  given  above.  the  general  technique of  adapting  simple  methods  to  work  hard  only  on  difficult  parts  of  complex problems  can  be  a  powerful  one  in  algorithm  design.
it  turns  out  that  simpson’s  method  is  exactly  equivalent  to  interpolating the  data  to  a  piecewise  quadratic  function,  then  integrating.  it  is  interesting to  note  that  the  four  methods  we  have  discussed  all  can  be  cast  as  piecewise interpolation  methods:  the  rectangle  rule  interpolates  to  a  constant  (degree-o polynomial);  the  trapezoid  rule  to  a  line  (degree-l  polynomial);  simpson’s  rule to  a  quadratic  polynomial;  and  spline  qua.drature   to  a  cubic  polynomial.
adaptive  a  major  flaw  in  the  methods  that  we  have  discussed  so  far  is  that  the  errors involved  depend  not,  only  upon  the  subinterval  size  used,  but  also  upon  the value  of  the  high-order  derivatives  of  the  function  being  integrated.  this implies  that  the  methods  will  not  work  well  at  all  for  certain  functions  (those with  large  high-order  derivatives).  but  few  functions  have  large  high-order derivatives  everywhere.  it  is  reasonable  to  use  small  intervals  where  the derivatives  are  large  and  large  intervals  where  the  derivatives  are  small.  a method  which  does  this  in  a  systematic  way  is  called  an  adaptive  quadrature routine.
the  general  approach  in  adaptive  quadrature  is  to  use  two  different quadrature  methods  for  each  subinterval,  compare  the  results,  and  subdivide the  interval  further  if  the  difference  is  too  great.  of  course  some  care  should be exercised, since if two equally bad methods are used, they might agree quite closely  on  a  bad  result.  one  way  to  avoid  this  is  to  ensure  that  one  method always  overestimates  the  result  and  that  the  other  always  underestimates  the result,. another way to avoid this is to ensure that one method is more accurate than  the  other.  a  method  of  this  type  is  described  next.
there  is  significant  overhead  involved  in  recursively  subdividing  the  interval,  so  it  pays  to  use  a  good  method  fo:r  estimating  the  integrals,  as  in  the following 
both  estimates  for  the  integral  are  derived  from  simpson’s  method,  one using  twice  as  many  subdivisions  as  the  other.  essentially,  this  amounts  to checking  the  accuracy  of  simpson’s  method  over  the  interval  in  question  and then  subdividing  if  it  is  not  good  enough.
is  represented  simply  by  keeping  the  values  of  the  coefficients  in  an  array  p then  the  integral  can  be  easily  computed  as  follows:
obviously a wider class of functions than just polynomials can be handled by  adding  more  symbolic  rules.  the  addition  of  composite  rules  such  as integration  by parts,
can  greatly  expand  the  set  of  functions  which  can  be  handled.  (integration by  parts  requires  a  differentiation  capability.  symbolic  differentiation  is  somewhat  easier  than  symbolic  integration,  since  a  reasonable  set  of  elementary rules  plus  the  composite  chain  rule  will  suffice  for  most  common  functions.) the  large  number  of  rules  available  to  be  applied  to  a  particular  function makes  symbolic  integration  a  difficult  task.  indeed,  it  has  only  recently  been shown  that  there  is  an  algorithm  for  this  task:  a  procedure  which  either returns  the  integral  of  any  given  function  or  says  that  the  answer  cannot  be expressed  in  terms  of  elementary  functions.  a  description  of  this  algorithm in  its  full  generality  would  be  beyond  the  scope  of  this  book.  however, when  the  functions  being  processed  are  from  a  small  restricted  class,  symbolic integration  can  be  a  powerful  tool.
of  course,  symbolic  techniques  have  the  fundamental  limitation  that there  are  a  great  many  integrals  (many  of  which  occur  in  practice)  which  can’t be  evaluated  symbolically.  next,  we’ll  examine  some  techniques  which  have been  developed  to  compute  approximations  to  the  values  of  real  integrals.
perhaps  the  most  obvious  way  to  approximate  the  value  of  an  integral  is  the rectangle  method:  evaluating  an  integral  is  the  same  as  computing  the  area under  a  curve,  and  we  can  estimate  the  area  under  a  curve  by  summing  the areas  of  small  rectangles  which  nearly  fit  under  the  curve,  as  diagrammed below.
when  n  =  1000,  our  answer  is  accurate  to  about  seven  decimal  places. more  sophisticated  quadrature  methods  can  achieve  better  accuracy  with much  less  work.
it  is  not  difficult  to  derive  an  analytic  expression  for  the  error  made  in the  rectangle  method  by  expanding  f(z)  in  a  taylor  series  about  the  midpoint of  each  interval,  integrating,  then  summing  over  all  intervals.  we  won’t  go through  the  details  of  this  calculation:  our  purpose  is  not  to  derive  detailed error  bounds,  but  rather  to  show  error  estimates  for  simple  methods  and  how these  estimates  suggest  more  accurate  methods.  this  can  be  appreciated  even by  a  reader  not  familiar  with  taylor  series.  it  turns  out  that
where  w  is  the  interval  width  ((b  -  a)/n)  and  es  depends  on  the  value  of the  third  derivative  of  f  at  the  interval  midpoints,  etc.  (normally,  this  is a  good  approximation  because  most  “reasonable”  functions  have  small  highorder  derivatives,  though  this  is  not  always  true.)  for  example,  if  we  choose to  make  w  =  .ol   (which  would  correspond  to  n  =  200  in  the  example  above), this  formula  says  the  integral  computed  by  the  procedure  above  should  be accurate  to  about  six  places.
to  be  precise,  suppose  that  we  are  to  compute  jab   f(x)dx,   and  that  the interval  [a,  b]  over  which  the  integral  is  to  be  computed  is  divided  into  n parts,  delimited  by  the  points  x1,  x2,.  .  .  ,xn+l.  then  we  have  n  rectangles, with  the  width  of  the  ith  rectangle  (1  5  i  5 n))  given  by  x,+1   -  x,.  for  the height  of  the  ith  rectangle,  we  could  use  f(x,) or  f(xi+l),   but  it  would  seem that  the  result  would  be  more  accurate  -if  the  value  of  f  at  the  midpoint  of the interval  (f((xi +  xi+l)/2)) is  used,  as  in  the  above  diagram.  this  leads  to the  quadrature  formula
which  estimates  the  value  of  the  integral  ‘of  f(x)  over  the  interval  from  a  =  x1 to  b  =  xn+l.  in  the  common  case  where  all  the  intervals  are  to  be  the  same size,  say  x$+1   -  xi =  20, we have  xi+1  +  zz = (2i +  l)w,  so  the  approximation r  to  the  integral  is  easily  computed.
of  course,  as  n  gets  larger,  the  answer  becomes  more  accurate,  for example,  the  following  table  shows  the  estimate  produced  by  this  function  for j:   dxlx  (which we know to be  in  2  =  0.6931471805599..  .  )  when  invoked  with the call  intrect(l.0,2.o,n)
(recall  that  the  area  of  a  trapezoid  is  one-half  the  product  of  the  height  and the  sum  of  the  lengths  of  the  two  bases.)  the  error  for  this  method  can  be derived  in  a  similar  way  as  for  the  rectangle  method.  it  turns  out  that
thus  the  rectangle  method  is  twice  as  accurate  as  the  trapezoid  method. this  is  borne  out  by  our  example.  the  following  procedure  implements  the trapezoid  method  in  the  common  case  where  all  the  intervals  are  the  same width:
it  may  seem  surprising  at  first  that  the  rectangle  method  is  more  accurate than  the  trapezoid  method:  the  rectangles  tend  to  fall  partly  under  the  curve, partly  over  (so  that  the  error  can  cancel  out  within  an  interval),  while  the trapezoids  tend  to  fall  either  completely  under  or  completely  over  the  curve. another  perfectly  reasonable  method  is  spline  quadrature:  spline  interpolation  is  performed  using  methods  we  have  discussed  and  then  the  integral is  computed  by  piecewise  application  of  the  trivial  symbolic  polynomial  integration  technique  described  above.  bel’ow,   we’ll  see  how  this  relates  to  the other  methods.
compound methods examination  of  the  formulas  given  above  for  the  error  of  the  rectangle  and trapezoid  methods  leads  to  a  simple  method  with  much  greater  accuracy, called  simpson’s  method.  the  idea  is  to  eliminate  the  leading  term  in  the  error
by  combining  the  two  methods.  multiplying  the  formula  for  the  rectangle method  by  2,  adding  the  formula  for  the  trapezoid  method  then  dividing  by 3 gives the equation
the  w3 term has disappeared, so this formula tells us that we can get a method that  is  accurate  to  within  w5   by  combining  the  quadrature  formulas  in  the same way:
if  an  interval  size  of  .ol   is  used  for  simpson’s  rule,  then  the  integral  can be  computed  to  about  ten-place  accuracy.  again,  this  is  borne  out  in  our example.  the  implementation  of  simpson’s  method  is  only  slightly  more complicated  than  the  others  (again,  we  consider  the  case  where  the  intervals are  the  same  width):
this  program  requires  three  “function  evaluations”  (rather  than  two)  in  the inner  loop,  but  it  produces  far  more  accurate  results  than  do  the  previous  two methods.
more  complicated  quadrature  methods  have  been  devised  which  gain accuracy  by  combining  simpler  methods  with  similar  errors.  the  most  wellknown  is  romberg integration,  which  uses  two  different  sets  of  subintervals for  its  two  “methods.”
by  combining  the  two  methods.  multiplying  the  formula  for  the  rectangle method  by  2,  adding  the  formula  for  the  trapezoid  method  then  dividing  by 3 gives the equation
the  w3 term has disappeared, so this formula tells us that we can get a method that  is  accurate  to  within  w5   by  combining  the  quadrature  formulas  in  the same way:
if  an  interval  size  of  .ol   is  used  for  simpson’s  rule,  then  the  integral  can be  computed  to  about  ten-place  accuracy.  again,  this  is  borne  out  in  our example.  the  implementation  of  simpson’s  method  is  only  slightly  more complicated  than  the  others  (again,  we  consider  the  case  where  the  intervals are  the  same  width):
this  program  requires  three  “function  evaluations”  (rather  than  two)  in  the inner  loop,  but  it  produces  far  more  accurate  results  than  do  the  previous  two methods.
more  complicated  quadrature  methods  have  been  devised  which  gain accuracy  by  combining  simpler  methods  with  similar  errors.  the  most  wellknown  is  romberg integration,  which  uses  two  different  sets  of  subintervals for  its  two  “methods.”
it  turns  out  that  simpson’s  method  is  exactly  equivalent  to  interpolating the  data  to  a  piecewise  quadratic  function,  then  integrating.  it  is  interesting to  note  that  the  four  methods  we  have  discussed  all  can  be  cast  as  piecewise interpolation  methods:  the  rectangle  rule  interpolates  to  a  constant  (degree-o polynomial);  the  trapezoid  rule  to  a  line  (degree-l  polynomial);  simpson’s  rule to  a  quadratic  polynomial;  and  spline  qua.drature   to  a  cubic  polynomial.
adaptive  a  major  flaw  in  the  methods  that  we  have  discussed  so  far  is  that  the  errors involved  depend  not,  only  upon  the  subinterval  size  used,  but  also  upon  the value  of  the  high-order  derivatives  of  the  function  being  integrated.  this implies  that  the  methods  will  not  work  well  at  all  for  certain  functions  (those with  large  high-order  derivatives).  but  few  functions  have  large  high-order derivatives  everywhere.  it  is  reasonable  to  use  small  intervals  where  the derivatives  are  large  and  large  intervals  where  the  derivatives  are  small.  a method  which  does  this  in  a  systematic  way  is  called  an  adaptive  quadrature routine.
the  general  approach  in  adaptive  quadrature  is  to  use  two  different quadrature  methods  for  each  subinterval,  compare  the  results,  and  subdivide the  interval  further  if  the  difference  is  too  great.  of  course  some  care  should be exercised, since if two equally bad methods are used, they might agree quite closely  on  a  bad  result.  one  way  to  avoid  this  is  to  ensure  that  one  method always  overestimates  the  result  and  that  the  other  always  underestimates  the result,. another way to avoid this is to ensure that one method is more accurate than  the  other.  a  method  of  this  type  is  described  next.
there  is  significant  overhead  involved  in  recursively  subdividing  the  interval,  so  it  pays  to  use  a  good  method  fo:r  estimating  the  integrals,  as  in  the following 
both  estimates  for  the  integral  are  derived  from  simpson’s  method,  one using  twice  as  many  subdivisions  as  the  other.  essentially,  this  amounts  to checking  the  accuracy  of  simpson’s  method  over  the  interval  in  question  and then  subdividing  if  it  is  not  good  enough.
unlike  our  other  methods,  where  we  decide  how  much  work  we  want to  do  and  then  take  whatever  accuracy  results,  in  adaptive  quadrature  we  do however much work is necessary to achieve a degree of accuracy that we decide upon  ahead  of  time.  this  means  that  tolerance  must  be  chosen  carefully, so  that  the  routine  doesn’t  loop  indefinitely  to  achieve  an  impossibly  high tolerance.  the  number  of  steps  required  depends  very  much  on  the  nature  of the  function  being  integrated.  a  function  which  fluctuates  wildly  will  require a  large  number  of  steps,  but  such  a  function  would  lead  to  a  very  inaccurate answer  for  the  “fixed  interval”  methods.  a  smooth  function  such  as  our example  can  be  handled  with  a  reasonable  number  of  steps.  the  following table  gives,  for  various  values  of  t,  the  value  produced  and  the  number  of recursive  calls  required  by  the  above  routine  to  compute  jrz  dx/x:
the  above  program  can  be  improved  in  several  ways.  first,  there’s certainly  no  need  to  call  intsimp(a,  b,   io)   twice.  in  fact,  the  function  values for  this  call  can  be  shared  by  intsimp(a,  b,  5).  second,  the  tolerance  bound can  be  related  to  the  accuracy  of  the  answer  more  closely  if  the  tolerance  is scaled  by  the  ratio  of  the  size  of  the  current  interval  to  the  size  of  the  full interval.  also,  a  better  routine  can  obviously  be  developed  by  using  an  even better  quadrature  rule  than  simpson’s  (but  it  is  a  basic  law  of  recursion  that another  adaptive  routine  wouldn’t  be  a  good  idea).  a  sophisticated  adaptive quadrature  routine  can  provide  very  accurate  results  for  problems  which  can’t be  handled  any  other  way,  but  careful  attention  must  be  paid  to  the  types  of functions to be processed.
we  will  be  seeing  several  algorithms  that  have  the  same  recursive  structure  as  the  adaptive  quadrature  method  given  above.  the  general  technique of  adapting  simple  methods  to  work  hard  only  on  difficult  parts  of  complex problems  can  be  a  powerful  one  in  algorithm  design.
computing  the  integral  is  a  fundamental  analytic  operation  often  performed  on  functions  being  processed  on  computers.  one  of  two  completely different approaches can be used, depending on the way the function is represented.  if  an  explicit  representation  of  the  function  is  available,  then  it may  be  possible  to  do  symbolic integrathn   to  compute  a  similar  representation for  the  integral.  at  the  other  extreme,  the  function  may  be  defined  by  a  table, so  that  function  values  are  known  for  only  a  few  points.  the  most  common situation  is  between  these:  the  function  to  be  integrated  is  represented  in  such a  way  that  its  value  at  any  particular  point  can  be  computed.  in  this  case, the goal  is to  compute  a  reasonable  approximation  to  the  integral  of  the  function,  without  performing  an  excessive  number  of  function  evaluations.  this computation  is  often  called  quadrature  by  numerical  analysts.
symbolic  integration if  full  information  is  available  about  a  function,  then  it  may  be  worthwhile to  consider  using  a  method  which  involves  manipulating  some  representation of  the  function  rather  than  working  with  numeric  values. the  goal  is  to transform  a  representation  of  the  function  into  a  representation  of  the  integral, in  much  the  same  way  that  indefinite  integration  is  done  by  hand.
a  simple  example  of  this  is  the  integ,ration  of  polynomials.  in  chapters  2 and  4  we  examined  methods  for  “symbolically”  computing  sums  and  products of  polynomials,  with  programs  that  work.ed   on  a  particular  representation  for the  polynomials  and  produced  the  representation  for  the  answers  from  the  representation  for  the  inputs.  the  operation  of  integration  (and  differentiation) of  polynomials  can  also  be  done  in  this  way.  if  a  polynomial
is  represented  simply  by  keeping  the  values  of  the  coefficients  in  an  array  p then  the  integral  can  be  easily  computed  as  follows:
obviously a wider class of functions than just polynomials can be handled by  adding  more  symbolic  rules.  the  addition  of  composite  rules  such  as integration  by parts,
can  greatly  expand  the  set  of  functions  which  can  be  handled.  (integration by  parts  requires  a  differentiation  capability.  symbolic  differentiation  is  somewhat  easier  than  symbolic  integration,  since  a  reasonable  set  of  elementary rules  plus  the  composite  chain  rule  will  suffice  for  most  common  functions.) the  large  number  of  rules  available  to  be  applied  to  a  particular  function makes  symbolic  integration  a  difficult  task.  indeed,  it  has  only  recently  been shown  that  there  is  an  algorithm  for  this  task:  a  procedure  which  either returns  the  integral  of  any  given  function  or  says  that  the  answer  cannot  be expressed  in  terms  of  elementary  functions.  a  description  of  this  algorithm in  its  full  generality  would  be  beyond  the  scope  of  this  book.  however, when  the  functions  being  processed  are  from  a  small  restricted  class,  symbolic integration  can  be  a  powerful  tool.
of  course,  symbolic  techniques  have  the  fundamental  limitation  that there  are  a  great  many  integrals  (many  of  which  occur  in  practice)  which  can’t be  evaluated  symbolically.  next,  we’ll  examine  some  techniques  which  have been  developed  to  compute  approximations  to  the  values  of  real  integrals.
perhaps  the  most  obvious  way  to  approximate  the  value  of  an  integral  is  the rectangle  method:  evaluating  an  integral  is  the  same  as  computing  the  area under  a  curve,  and  we  can  estimate  the  area  under  a  curve  by  summing  the areas  of  small  rectangles  which  nearly  fit  under  the  curve,  as  diagrammed below.
when  n  =  1000,  our  answer  is  accurate  to  about  seven  decimal  places. more  sophisticated  quadrature  methods  can  achieve  better  accuracy  with much  less  work.
it  is  not  difficult  to  derive  an  analytic  expression  for  the  error  made  in the  rectangle  method  by  expanding  f(z)  in  a  taylor  series  about  the  midpoint of  each  interval,  integrating,  then  summing  over  all  intervals.  we  won’t  go through  the  details  of  this  calculation:  our  purpose  is  not  to  derive  detailed error  bounds,  but  rather  to  show  error  estimates  for  simple  methods  and  how these  estimates  suggest  more  accurate  methods.  this  can  be  appreciated  even by  a  reader  not  familiar  with  taylor  series.  it  turns  out  that
where  w  is  the  interval  width  ((b  -  a)/n)  and  es  depends  on  the  value  of the  third  derivative  of  f  at  the  interval  midpoints,  etc.  (normally,  this  is a  good  approximation  because  most  “reasonable”  functions  have  small  highorder  derivatives,  though  this  is  not  always  true.)  for  example,  if  we  choose to  make  w  =  .ol   (which  would  correspond  to  n  =  200  in  the  example  above), this  formula  says  the  integral  computed  by  the  procedure  above  should  be accurate  to  about  six  places.
(recall  that  the  area  of  a  trapezoid  is  one-half  the  product  of  the  height  and the  sum  of  the  lengths  of  the  two  bases.)  the  error  for  this  method  can  be derived  in  a  similar  way  as  for  the  rectangle  method.  it  turns  out  that
thus  the  rectangle  method  is  twice  as  accurate  as  the  trapezoid  method. this  is  borne  out  by  our  example.  the  following  procedure  implements  the trapezoid  method  in  the  common  case  where  all  the  intervals  are  the  same width:
it  may  seem  surprising  at  first  that  the  rectangle  method  is  more  accurate than  the  trapezoid  method:  the  rectangles  tend  to  fall  partly  under  the  curve, partly  over  (so  that  the  error  can  cancel  out  within  an  interval),  while  the trapezoids  tend  to  fall  either  completely  under  or  completely  over  the  curve. another  perfectly  reasonable  method  is  spline  quadrature:  spline  interpolation  is  performed  using  methods  we  have  discussed  and  then  the  integral is  computed  by  piecewise  application  of  the  trivial  symbolic  polynomial  integration  technique  described  above.  bel’ow,   we’ll  see  how  this  relates  to  the other  methods.
compound methods examination  of  the  formulas  given  above  for  the  error  of  the  rectangle  and trapezoid  methods  leads  to  a  simple  method  with  much  greater  accuracy, called  simpson’s  method.  the  idea  is  to  eliminate  the  leading  term  in  the  error
(recall  that  the  area  of  a  trapezoid  is  one-half  the  product  of  the  height  and the  sum  of  the  lengths  of  the  two  bases.)  the  error  for  this  method  can  be derived  in  a  similar  way  as  for  the  rectangle  method.  it  turns  out  that
thus  the  rectangle  method  is  twice  as  accurate  as  the  trapezoid  method. this  is  borne  out  by  our  example.  the  following  procedure  implements  the trapezoid  method  in  the  common  case  where  all  the  intervals  are  the  same width:
it  may  seem  surprising  at  first  that  the  rectangle  method  is  more  accurate than  the  trapezoid  method:  the  rectangles  tend  to  fall  partly  under  the  curve, partly  over  (so  that  the  error  can  cancel  out  within  an  interval),  while  the trapezoids  tend  to  fall  either  completely  under  or  completely  over  the  curve. another  perfectly  reasonable  method  is  spline  quadrature:  spline  interpolation  is  performed  using  methods  we  have  discussed  and  then  the  integral is  computed  by  piecewise  application  of  the  trivial  symbolic  polynomial  integration  technique  described  above.  bel’ow,   we’ll  see  how  this  relates  to  the other  methods.
compound methods examination  of  the  formulas  given  above  for  the  error  of  the  rectangle  and trapezoid  methods  leads  to  a  simple  method  with  much  greater  accuracy, called  simpson’s  method.  the  idea  is  to  eliminate  the  leading  term  in  the  error
x:=j+(v-a[j].jcey)*(r-j)   of  course,  this  assumes numerical  key  values.  suppose  in  our  example  that  the  ith  letter  in  the alphabet  is  represented  by  the  number  i.  then,  in  a  search  for  s,  the  first table  position  examined  would  be  x  =  1  +  (19  -  1)*(17  -  1)/(24   -  1)  =  13.  the search  is  completed  in  just  three  steps:
interpolation  search  manages  to  decrease  the  number  of  elements  examined  to  about  1oglogn.   this  is  a  very  slowly  growing  function  which can  be  thought  of  as  a  constant  for  practical  purposes:  if  n  is  one  billion, 1glgn   <  5.  thus,  any  record  can  be  found  using  only  a  few  accesses,  a  substantial  improvement  over  the  conventional  binary  search  method.  but  this assumes  that  the  keys  are  rather  well  distributed  over  the  interval,  and  it does  require  some  computation:  for  small  n, the  1ogn  cost  of  straight  binary search  is  close  enough  to  log  log  n  that  the  cost  of  interpolating  is  not  likely to  be  worthwhile.  but  interpolation  search  certainly  should  be  considered  for large  files,  for  applications  where  comparisons  are  particularly  expensive,  or for  external  methods  where  very  high  access  costs  are  involved.
binary  tree  search binary  tree  search  is  a  simple,  efficient  dynamic  searching  method  which qualifies  as  one  of  the  most  fundamental  algorithms  in  computer  science.  it’s classified  here  as  an  “elementary”  method  because  it  is  so  simple;  but  in  fact it  is  the  method  of  choice  in  many  situations.
the  idea  is  to  build  up  an  explicit  structure  consisting  of  nodes,  each node  consisting  of  a  record  containing  a  key  and  left  and  right  links.  the left  and  right  links  are  either  null,  or  they  point  to  nodes  called  the  left  son and the  right  son.  the  sons  are  themselves  the  roots  of  trees,  called  the  left subtree  and the  right  subtree   respectively.  for  example,  consider  the  following diagram,  where  nodes  are  represented  as  encircled  key  values  and  the  links  by lines connected to nodes:
polynomial  we’ve  already  seen  one  method  for  solving  the  data-fitting  problem:  if  f  is known  to  be  a  polynomial  of  degree  n  -  1,  then  we  have  the  polynomial  interpolation  problem  of  chapter  4.  even  if  we  have  no  particular  knowledge  about f,  we  could  solve  the  data-fitting  problem  by  letting  f(z)  be  the  interpolating polynomial  of  degree  n  -  1  for  the  given  points  and  values.  this  could  be computed  using  methods  outlined  elsewhere  in  this  book,  but  there  are  many reasons  not  to  use  polynomial  interpolation  for  data  fitting.  for  one  thing, a  fair  amount  of  computation  is  involved  (advanced  n(log  n)2   methods  are available,  but  elementary  techniques  are  quadratic).  computing  a  polynomial of  degree  100  (for  example)  seems  overkill  for  interpolating  a  curve  through 100  points.
the  main  problem  with  polynomial  interpolation  is  that  high-degree polynomials  are  relatively  complicated  functions  which  may  have  unexpected properties  not  well  suited  to  the  function  being  fitted.  a  result  from  classical mathematics  (the  weierstrass  approximation  theorem)  tells  us  that  it  is  possible  to  approximate  any  reasonable  function  with  a  polynomial  (of  sufficiently high  degree).  unfortunately,  polynomials  of  very  high  degree  tend  to  fluctuate wildly.  it  turns  out  that,  even  though  most  functions  are  closely  approximated almost  everywhere  on  a  closed  interval  by  an  interpolation  polynomial,  there are  always  some  places  where  the  approximation  is  terrible.  furthermore, this  theory  assumes  that  the  data  values  are  exact  values  from  some  unknown function  when  it  is  often  the  case  that  the  given  data  values  are  only  approximate.  if  the  y’s  were  approximate  values  from  some  unknown  low-degree polynomial,  we  would  hope  that  the  coefficients  for  the  high-degree  terms  in the  interpolating  polynomial  would  be  0.  it  doesn’t  usually  work  out  this way;  instead  the  interpolating  polynomial  tries  to  use  the  high-degree  terms to  help  achieve  an  exact  fit.  these  effects  make  interpolating  polynomials inappropriate  for  many  curve-fitting  applications. spline  interpolation still,  low-degree  polynomials  are  simple  curves  which  are  easy  to  work  with analytically,  and  they  are  widely  used  for  curve  fitting.  the  trick  is  to  abandon the idea of trying to make one polynomial go through all the points and instead use  different  polynomials  to  connect  adjacent  points,  piecing  them  together smoothly,,  an  elegant  special  case  of  this,  which  also  involves  relatively straightforward  computation,  is  called  spline  interpolation.
a  “spline”  is  a  mechanical  device  used  by  draftsmen  to  draw  aesthetically pleasing curves: the draftsman fixes a set of points (knots) on his drawing, then bends  a  flexible  strip  of  plastic  or  wood  (the  spline)  around  them  and  traces it  to  produce  the  curve.  spline  interpolation  is  the  mathematical  equivalent of  this  process  and  results  in  the  same  curve.
the  arrays  d  and  u  are  the  representation  of  the  tridiagonal  matrix  that  is solved  using  the  program  in  chapter  5.  we  use  d[i]  where  a[i,  i] is used in that program, u[i]  where a [i+l,  i] or a[i, i+l]  is used, and z[i] where  a[i, n+i] is  used.
to evaluate the spline for any value of 2 in the range [zr , zn],  we simply find the interval  [zi, zi+r] containing  z, then compute  t  and  use  the  formula above for si(z) (which, in turn, uses the computed values for pi and pi+r).
this program does not check for the error condition when v is not between x[l]  and  x[ivl.  if there are a large number of spline segments (that is, if  n is  large),  then  there  are  more  efficient  “searching”  methods  for  finding  the interval containing v, which we’ll study in chapter 14.
there are many variations on the idea of curvefitting by piecing together polynomials  in  a  “smooth”  way:  the  computation  of  splines  is  a  quite  welldeveloped  field  of  study.  other  types  of  splines  involve  other  types  of  smoothness criteria as well as changes such as relaxing the condition that the spline must  exactly  touch  each  data  point.  computationally,  they  involve  exactly
compares  two  rectangles  whose  edges  are  horizontal  and  vertical  according  to the  trivial  rule  that  rectangle  5  is  to  the  left  of  rectangle  y  if  the  right  edge  of x is to the left of the left edge of y, then we can use the above method to test for  intersection  among  a  set  of  such  rectangles.  for  circles,  we  can  use  the  x coordinates  of  the  centers  for  the  ordering,  but  explicitly  test  for  intersection (for  example,  compare  the  distance  between  the  centers  to  the  sum  of  the radii).  again,  if  this  comparison  procedure  is  used  in  the  above  method,  we have  an  algorithm  for  testing  for  intersection  among  a  set  of  circles.  the problem  of  returning  all  intersections  in  such  cases  is  much  more  complicated, though  the  brute-force  method  mentioned  in  the  previous  paragraph  will always  work  if  few  intersections  are  expected.  another  approach  that  will suffice  for  many  applications  is  simply  to  consider  complicated  objects  as  sets of  lines  and  to  use  the  line  intersection  procedure.
general  line  intersection when  lines  of  arbitrary  slope  are  allowed,  the  situation  can  become  more complicated,  as  illustrated  by  the  following  example.
first,  the  various  line  orientations  possible  make  it  necessary  to  test  explicitly whether  certain  pairs  of  lines  intersect:  we  can’t  get  by  with  a  simple  interval range  test.  second,  the  ordering  relationship  between  lines  for  the  binary tree  is  more  complicated  than  before,  as  it  depends  on  the  current  y  range of  interest.  third,  any  intersections  which  do  occur  add  new  “interesting”  y values  which  are  likely  to  be  different  from  the  set  of  y  values  that  we  get from  the  line  endpoints.
it  turns  out  that  these  problems  can  be  handled  in  an  algorithm  with  the same  basic  structure  as  given  above.  to  simplify  the  discussion,  we’ll  consider an  algorithm  for  detecting  whether  or  not  there  exists  an  intersecting  pair  in a  set  of  n  lines,  and  then  we’ll  discuss  how  it  can  be  extended  to  return  all intersections.
as  before,  we  first  sort  on  y  to  divide  the  space  into  strips  within  which no  line  endpoints  appear.  just  as  before,  we  proceed  through  the  sorted  list of  points,  adding  each  line  to  a  binary  search  tree  when  its  bottom  point  is encountered  and  deleting  it  when  its  top  point  is  encountered.  just  as  before, the  binary  tree  gives  the  order  in  which  the  lines  appear  in  the  horizontal
general  line  intersection when  lines  of  arbitrary  slope  are  allowed,  the  situation  can  become  more complicated,  as  illustrated  by  the  following  example.
first,  the  various  line  orientations  possible  make  it  necessary  to  test  explicitly whether  certain  pairs  of  lines  intersect:  we  can’t  get  by  with  a  simple  interval range  test.  second,  the  ordering  relationship  between  lines  for  the  binary tree  is  more  complicated  than  before,  as  it  depends  on  the  current  y  range of  interest.  third,  any  intersections  which  do  occur  add  new  “interesting”  y values  which  are  likely  to  be  different  from  the  set  of  y  values  that  we  get from  the  line  endpoints.
it  turns  out  that  these  problems  can  be  handled  in  an  algorithm  with  the same  basic  structure  as  given  above.  to  simplify  the  discussion,  we’ll  consider an  algorithm  for  detecting  whether  or  not  there  exists  an  intersecting  pair  in a  set  of  n  lines,  and  then  we’ll  discuss  how  it  can  be  extended  to  return  all intersections.
as  before,  we  first  sort  on  y  to  divide  the  space  into  strips  within  which no  line  endpoints  appear.  just  as  before,  we  proceed  through  the  sorted  list of  points,  adding  each  line  to  a  binary  search  tree  when  its  bottom  point  is encountered  and  deleting  it  when  its  top  point  is  encountered.  just  as  before, the  binary  tree  gives  the  order  in  which  the  lines  appear  in  the  horizontal
compares  two  rectangles  whose  edges  are  horizontal  and  vertical  according  to the  trivial  rule  that  rectangle  5  is  to  the  left  of  rectangle  y  if  the  right  edge  of x is to the left of the left edge of y, then we can use the above method to test for  intersection  among  a  set  of  such  rectangles.  for  circles,  we  can  use  the  x coordinates  of  the  centers  for  the  ordering,  but  explicitly  test  for  intersection (for  example,  compare  the  distance  between  the  centers  to  the  sum  of  the radii).  again,  if  this  comparison  procedure  is  used  in  the  above  method,  we have  an  algorithm  for  testing  for  intersection  among  a  set  of  circles.  the problem  of  returning  all  intersections  in  such  cases  is  much  more  complicated, though  the  brute-force  method  mentioned  in  the  previous  paragraph  will always  work  if  few  intersections  are  expected.  another  approach  that  will suffice  for  many  applications  is  simply  to  consider  complicated  objects  as  sets of  lines  and  to  use  the  line  intersection  procedure.
var  dx, dy,  dxl,  dx2,  dyl, dy2:  integer; begin dx:=l.p2.x-1.pl.x; dxl :=pl  .x-1.~1.~; dyl :=pl.y-1.~1  .y; dx2:=p2.x-1.p2.x;   dy2:=p2.y-1.p2.y; same:=(dx*dyl-ddy*dxl)*(dx*dy2-dpdx2) end;
in  terms  of  the  variables  in  this  program,  it  is  easy  to  check  that  the  quantity (da:   dyl   -  dy  dzl)   is  0  if  pl  is  on  the  line,  positive  if  pl  is  on  one  side,  and negative  if  it  is  on  the  other  side.  the  same  holds  true  for  the  other  point,  so the  product  of  the  quantities  for  the  two  points  is  positive  if  and  only  if  the points  fall  on  the  same  side  of  the  line,  negative  if  and  only  if  the  points  fall on  different  sides  of  the  line,  and  0  if  and  only  if  one  or  both  points  fall  on the  line.  we’ll  see  that  different  algorithms  need  to  treat  points  which  fall  on lines  in  different  ways,  so  this  three-way  test  is  quite  useful.
this  immediately  gives  an  implementation  of  the  intersect  function.  if the  endpoints  of  both  line  segments  are  on  opposite  sides  of  the  other  then they  must  intersect.
unfortunately,  there  is  one  case  where  this  function  returns  the  wrong  answer: if  the  four  line  endpoints  are  collinear,  it  always  will  report  intersection,  even though  the  lines  may  be  widely  separated.  special  cases  of  this  type  are  the bane  of  geometric  algorithms.  the  reader  may  gain  some  appreciation  for  the kind  of  complications  such  cases  can  lead  to  by  finding  a  clean  way  to  repair intersect  and  same  to  handle  all  cases.
if  many  lines  are  involved,  the  situation  becomes  much  more  complicated. later  on,  we’ll  see  a  sophisticated  algorithm  for  determining  whether  any  pair in  a  set  of  n  lines  intersects.
simple closed path to  get  the  flavor  of  problems  dealing  with  sets  of  points,  let’s  consider  the problem  of  finding  a  path  through  a  set  of  n  given  points  which  doesn’t
if the  suitability  of  suitor  s  can  be  very  quickly  tested  by  the  statement   fiancee[w]] . . . . these  arrays  are  easily  constructed  dirank[w,   s]<rank[w, rectly  from  the  preference  lists.  to  get  things  started,  we  use  a  “sentinel”  man 0  as  the  initial  suitor,  and  put  him  at  the  end  of  all  the  women’s  preference lists.
each  iteration  starts  with  an  unengaged  man  and  ends  with  an  engaged woman.  the  repeat  loop  must  terminate  because  every  man’s  list  contains every  woman  and  each  iteration  of  the  loop  involves  incrementing  some  man’s list,  and  thus  an  unengaged  woman  must  be  encountered  before  any  man’s list  is  exhausted.  the  set  of  engagements  produced  by  the  algorithm  is  stable because  every  woman  whom  any  man  prefers  to  his  fiancee  is  engaged  to someone  that  she  prefers  to  him.
there  are  several  obvious  built-in  biases  in  this  algorithm.  first,  the men  go  through  the  women  on  their  lists  in  order,  while  the  women  must wait  for  the  “right  man”  to  come  along.  this  bias  may  be  corrected  (in  a somewhat  easier  manner  than  in  real  life)  by  interchanging  the  order  in  which the  preference  lists  are  input.  this  produces  the  stable  configuration  1e  2d 3a  4c  5b,  where  every  women  gets  her  first  choice  except  5,  who  gets  her second.  in  general,  there  may  be  many  stable  configurations:  it  can  be  shown that  this  one  is  “optimal”  for  the  women,  in  the  sense  that  no  other  stable configuration  will  give  any  woman  a  better  choice  from  her  list.  (of  course, the  first  stable  configuration  for  our  example  is  optimal  for  the  men.)
people  believe  that  no  good  solution  exists.  to  appreciate  the  difficulty  of  the problem,  the  reader  might  wish  to  try  solving  the  case  where  the  values  are all  1,  the  size  of  the  jth  item  is  &  and  m  is  n/2.
but  when  capacities,  sizes  and  values  are  all  integers,  we  have  the  fundamental  principle  that  optimal  decisions,  once  made,  do  not  need  to  be changed.  once  we  know  the  best  way  to  pack  knapsacks  of  any  size  with  the first  j  items,  we  do  not  need  to  reexamine  those  problems,  regardless  of  what the  next  items  are.  any  time  this  general  principle  can  be  made  to  work, dynamic  programming  is  applicable.
in  this  algorithm,  only  a  small  amount  of  information  about  previous optimal  decisions  needs  to  be  saved.  different  dynamic  programming  applications  have  widely  different  requirements  in  this  regard:  we’ll  see  other  examples below.
are  to  be  multiplied  together.  of  course,  for  the  multiplications  to  be  valid, the  number  of  columns  in  one  matrix  must  be  the  same  as  the  number  of  rows in  the  next.  but  the  total  number  of  scalar  multiplications  involved  depends on  the  order  in  which  the  matrices  are  multiplied.  for  example,  we  could proceed  from  left  to  right:  multiplying  a  by  b,  we  get  a  4-by-3  matrix  after using  24  scalar  multiplications. multiplying  this  result  by  c  gives  a  4-by-1 matrix  after  12  more  scalar  multiplications.  multiplying  this  result  by  d  gives a  4-by-2  matrix  after  8  more  scalar  multiplications.  continuing  in  this  way, we  get  a  4-by-3  result  after  a  grand  total  of  84  scalar  multiplications.  but  if we  proceed  from  right  to  left  instead,  we  get  the  same  4-by-3  result  with  only 69  scalar  multiplications.
many  other  orders  are  clearly  possible.  the  order  of  multiplication  can  be expressed  by  parenthesization:  for  example  the  left-wright   order  described above  is  the  ordering  (((((a*b)*c)*d)*e)*f),  and  the  right-to-left  order  is (a*(b*(c*(d*(e*f))))).  any  legal  parenthesization  will  lead  to  the  correct answer,  but  which  leads  to  the  fewest  scalar  multiplications?
very substantial savings can be achieved when large matrices are involved: for  example,  if  matrices  b,  c,  and  f  in  the  example  above  were  to  each  have a  dimension  of  300  where  their  dimension  is  3,  then  the  left-to-right  order will  require  6024  scalar  multiplications  but  the  right-to-left  order  will  use  an
another  way  to  see  this  is  to  look  at  the  version  of  the  program  with  the  next table  “wired  in”: at  label  4  we  go  to  2  if  a[i]   is  not  0,  but  at  label  2  we  go to  1  if  a[i]  is  not  0.  why  not  just  go  to  1  directly?  fortunately,  it  is  easy to  put  this  change  into  the  algorithm.  we  need  only  replace  the  statement next[i]   :=j  in  the  initnext  program  by
with  this  change,  we  either  increment  j  cr  reset  it  from  the  next  table  at  most once  for  each  value  of  i,  so  the  algorithm  is  clearly  linear.
the  knuth-morris-pratt  algorithm  ls  not  likely  to  be  significantly  faster than  the  brute-force  method  in  most  actual  applications,  because  few  applications  involve  searching  for  highly  self-repetitive  patterns  in  highly  selfrepetitive  text.  however,  the  method  does   have  a  major  virtue  from  a  practical  point  of  view:  it  proceeds  sequentially  through  the  input  and  never  “backs up”  in  the  input.  this  makes  the  method  convenient  for  use  on  a  large  file being  read  in  from  some  external  device.  (algorithms  which  require  backup require  some  complicated  buffering  in  this  situation.)
boyer-moore  algorithm if  “backing  up”  is  not  a  problem,  then  a  significantly  faster  string  searching method  can  be  developed  by  scanning  .,he   pattern  from  right  to  left  when trying  to  match  it  against  the  text.  when  searching  for  our  sample  pattern 10100111,  if  we  find  matches  on  the  eighth,  seventh,  and  sixth  character  but not  on  the  fifth,  then  we  can  immediatelyi   slide  the  pattern  seven  positions  to the  right,  and  check  the  fifteenth  character  next,  because  our  partial  match found  111,  which  might  appear  elsewhm?re   in  the  pattern.  of  course,  the pattern  at  the  end  does  appear  elsewhere:   in  general,  so  we  need  a  next  table as  above.  for  example,  the  following  is  a  right-to-left  version  of  the  next  table for  the  pattern  10110101:
priority-first  search  method  will  be  faster  for  some  graphs,  prim’s  for  some others,  kruskal’s  for  still  others.  as  mentioned  above,  the  worst  case  for  the priority-first  search  method  is  (e  +  v)logv   while  the  worst  case  for  prim’s is  v2  and  the  worst  case  for  kruskal’s  is  elog  e.  but  it  is  unwise  to  choose between  the  algorithms  on  the  basis  of  these  formulas  because  “worstrcase” graphs  are  unlikely  to  occur  in  practice. in  fact,  the  priority-first  search method  and  kruskal’s  method  are  both  likely  to  run  in  time  proportional  to e  for  graphs  that  arise  in  practice:  the  first  because  most  edges  do  not  really require  a  priority  queue  adjustment  that  takes  1ogv   steps  and  the  second because  the  longest  edge  in  the  minimum  spanning  tree  is  probably  sufficiently short  that  not  many  edges  are  taken  off  the  priority  queue.  of  course,  prim’s method  also  runs  in  time  proportional  to  about  e  for  dense  graphs  (but  it shouldn’t  be  used  for  sparse  graphs).
the  shortest path  problem  is  to  find  the  path  in  a  weighted  graph  connecting two  given  vertices  x  and  y  with  the  property  that  the  sum  of  the  weights  of all  the  edges  is  minimized  over  all  such  paths.
if  the  weights  are  all  1,  then  the  problem  is  still  interesting:  it  is  to  find the  path  containing  the  minimum  number  of  edges  which  connects  x  and  y. moreover,  we’ve  already  considered  an  algorithm  which  solves  the  problem: breadth-first  search.  it  is  easy  to  prove  by  induction  that  breadth-first  search starting  at  x  will  first  visit  all  vertices  which  can  be  reached  from  z  with  1 edge,  then  al:  vertices  which  can  be  reached  from  x  with  2  edges,  etc.,  visiting all  vertices  which  can  be  reached  with  k  edges  before  encountering  any  that require  k  +  1  edges.  thus,  when  y  is  first  encountered,  the  shortest  path  from x  has  been  found  (because  no  shorter  paths  reached  y).
consider  the  problem  of  finding  the  shortest  paths  connecting  a  given  vertex x  with  each  of  the  other  vertices  in  the  graph.  again,  it  turns  out  that  the problem  is  simple  to  solve  with  the  priority  graph  traversal  algorithm  of  the previous  chapter.
if  we  draw  the  shortest  path  from  x  to  each  other  vertex  in  the  graph, then  we  clearly  get  no  cycles,  and  we  have  a  spanning  tree.  each  vertex  leads to  a  different  spanning  tree;  for  example,  the  following  three  diagrams  show the  shortest  path  spanning  trees  for  vertices  a,  b,  and  e  in  the  example  graph that  we’ve  been  using.
control  structure  of  a  program,  that  absorb  all  of  the  machine  cycles.  it  is always  worthwhile  for  the  programmer  to  be  aware  of  the  inner  loop,  just  to be  sure  that  unnecessary  expensive  instructions  are  not  put  there.
second,  some  analysis  is  necessary  to  estimate  how  many  times  the  inner loop  is  iterated.  it  would  be  beyond  the  scope  of  this  book  to  describe  the mathematical  mechanisms  which  are  used  in  such  analyses,  but  fortunately the  running  times  many  programs  fall  into  one  of  a  few  distinct  classes.  when possible,  we’ll  give  a  rough  description  of  the  analysis  of  the  programs,  but  it will  often  be  necessary  merely  to  refer  to  the  literature.  (specific  references are  given  at  the  end  of  each  major  section  of  the  book.)  for  example,  the results  of  a  sophisticated  mathematical  argument  show  that  the  number  of recursive steps in euclid’s algorithm when u is chosen at random less than  v is approximately ((12  in  2)/7r2) 1 n  tj.   often,  the  results  of  a  mathematical  analysis are  not  exact,  but  approximate  in  a  precise  technical  sense:  the  result  might be  an  expression  consisting  of  a  sequence  of  decreasing  terms.  just  as  we  are most  concerned  with  the  inner  loop  of  a  program,  we  are  most  concerned  with the  leading  term  (the  largest  term)  of  a  mathematical  expression.
as  mentioned  above,  most  algorithms  have  a  primary  parameter  n, usually  the  number  of  data  items  to  be  processed,  which  affects  the  running time  most  significantly.  the  parameter  n  might  be  the  degree  of  a  polynomial,  the  size  of  a  file  to  be  sorted  or  searched,  the  number  of  nodes  in  a graph,  etc.  virtually  all  of  the  algorithms  in  this  book  have  running  time proportional  to  one  of  the  following  functions:
most  instructions  of  most  programs  are  executed  once  or  at  most only  a  few  times.  if  all  the  instructions  of  a  program  have  this property,  we  say  that  its  running  time  is  constant.  this  is  obviously the  situation  to  strive  for  in  algorithm  design.
log n when  the  running  time  of  a  program  is  logarithmic,  the  program gets  slightly  slower  as  n  grows.this   running  time  commonly  occurs in  programs  which  solve  a  big  problem  by  transforming  it  into  a smaller  problem  by  cutting  the  size  by  some  constant  fraction.  for our  range  of  interest,  the  running  time  can  be  considered  to  be  less than  a  yarge”  constant.  the  base  of  the  logarithm  changes  the constant,  but  not  by  much:  when  n  is  a  thousand,  log  n  is  3  if  the base  is  10,  10  if  the  base  is  2;  when  n  is  a  million,  1ogn  is  twice as  great.  whenever  n  doubles,  log  n  increases  by  a  constant,  but log  n  doesn’t  double  until  n  increases  to  n2. when  the  running  time  of  a  program  is  linear,  it  generally  is  the  case that  a  small  amount  of  processing  is  done  on  each  input  element. when  n  is  a  million,  then  so  is  the  running  time.  whenever  n
the  same  steps  of  determining  the  coefficients  for  each  of  the  spline  pieces  by solving  the  system  of  linear  equations  derived  from  imposing  constraints  on how  they  are  joined.
method  of  least squares a  very  common  experimental  situation  is  that,  while  the  data  values  that  we have  are  not  exact,  we  do  have  some  idea  of  the  form  of  the  function  which is  to  fit  the  data.  the  function  might  depend  on  some  parameters
and  the  curve  fitting  procedure  is  to  find  the  choice  of  parameters  that  “best” matches  the  observed  values  at  the  given  points.  if  the  function  were  a  polynomial  (with  the  parameters  being  the  coefficients)  and  the  values  were  exact, then  this  would  be  interpolation.  but  now  we  are  considering  more  general functions  and  inaccurate  data.  to  simplify  the  discussion,  we’ll  concentrate on  fitting  to  functions  which  are  expressed  as  a  linear  combination  of  simpler functions,  with  the  unknown  parameters  being  the  coefficients:
this  includes  most  of  the  functions  that  we’ll  be  interested  in.  after  studying this  case,  we’ll  consider  more  general  functions.
a  common  way  of  measuring  how  well  a  function  fits  is  the  least-squares criterion:  the  error  is  calculated  by  adding  up  the  squares  of  the  errors  at each  of  the  observation  points:
this  is  a  very  natural  measure:  the  squaring  is  done  to  stop  cancellations among  errors  with  different  signs.  obviously,  it  is  most  desirable  to  find  the choice  of  parameters  that  minimizes  e.  it  turns  out  that  this  choice  can  be computed  efficiently:  this  is  the  so-called  method  of  least squares.
the  method  follows  quite  directly  from  the  definition.  to  simplify  the derivation,  we’ll  do  the  case  m = 2, n =  3,  but  the  general  method  will  follow directly. suppose that we have three points xi,  x2,  x3 and corresponding values yi,  ys,   ys   which  are  to  be  fitted  to  a  function  of  the  form  f(x) =  cl  fi(x)  + cz   f2(x). our job is to find the choice of the coefficients cl,  cz  which minimizes the  least-squares  error
used  for  this  type  of  application. the  idea  is  fundamentally  the  same;  the problem  is  that  the  derivatives  may  not  be  easy  to  compute.  what  is  used is  an  iterative  method:  use  some  estimate  for  the  coefficients,  then  use  these within  the  method  of  least  squares  to  compute  the  derivatives,  thus  producing a  better  estimate  for  the  coefficients.  this  basic  method,  which  is  widely  used today,  was  outlined  by  gauss  in  the  1820s.
algorithms  for  converting  geometric  objects  to  points  in  this  manner  are  called scan-conversion  algorithms.  this  example  illustrates  that  it  is  easy  to  draw nice-looking  diagonal  lines  like  eo  and  il,  but  that  it  is  somewhat  harder  to make  lines  of  arbitrary  slope  look  nice  using  a  coarse  matrix  of  characters.  the recursive  method  given  above  has  the  disadvantages  that  it  is  not  particularly efficient  (some  points  get  plotted  several  times)  and  that  it  doesn’t  draw certain  lines  very  well  (for  example  lines  which  are  nearly  horizontal  and nearly  vertical).  it  has  the  advantages  that  it  is  relatively  simple  and  that it  handles  all  the  cases  of  the  various  orientation  of  the  endpoints  of  the  line in  a  uniform  way.  many  sophisticated  scan-conversion  algorithms  have  been developed  which  are  more  efficient  and  more  accurate  than  this  recursive  one. if  the  array  has  a  very  large  number  of  dots,  then  the  ragged  edges  of the  lines  aren’t  discernible,  but  the  same  types  of  algorithms  are  appropriate. however,  the  very  high  resolution  necessary  to  make  high-quality  lines  can require  very  large  amounts  of  memory  (or  computer  time),  so  sophisticated algorithms  are  called  for,  or  other  technologies  might  be  appropriate.  for example,  the  text  of  this  book  was  printed  on  a  device  capable  of  printing millions of dots per square inch, but most of the lines in the figures were drawn
var  dx, dy,  dxl,  dx2,  dyl, dy2:  integer; begin dx:=l.p2.x-1.pl.x; dxl :=pl  .x-1.~1.~; dyl :=pl.y-1.~1  .y; dx2:=p2.x-1.p2.x;   dy2:=p2.y-1.p2.y; same:=(dx*dyl-ddy*dxl)*(dx*dy2-dpdx2) end;
in  terms  of  the  variables  in  this  program,  it  is  easy  to  check  that  the  quantity (da:   dyl   -  dy  dzl)   is  0  if  pl  is  on  the  line,  positive  if  pl  is  on  one  side,  and negative  if  it  is  on  the  other  side.  the  same  holds  true  for  the  other  point,  so the  product  of  the  quantities  for  the  two  points  is  positive  if  and  only  if  the points  fall  on  the  same  side  of  the  line,  negative  if  and  only  if  the  points  fall on  different  sides  of  the  line,  and  0  if  and  only  if  one  or  both  points  fall  on the  line.  we’ll  see  that  different  algorithms  need  to  treat  points  which  fall  on lines  in  different  ways,  so  this  three-way  test  is  quite  useful.
this  immediately  gives  an  implementation  of  the  intersect  function.  if the  endpoints  of  both  line  segments  are  on  opposite  sides  of  the  other  then they  must  intersect.
unfortunately,  there  is  one  case  where  this  function  returns  the  wrong  answer: if  the  four  line  endpoints  are  collinear,  it  always  will  report  intersection,  even though  the  lines  may  be  widely  separated.  special  cases  of  this  type  are  the bane  of  geometric  algorithms.  the  reader  may  gain  some  appreciation  for  the kind  of  complications  such  cases  can  lead  to  by  finding  a  clean  way  to  repair intersect  and  same  to  handle  all  cases.
if  many  lines  are  involved,  the  situation  becomes  much  more  complicated. later  on,  we’ll  see  a  sophisticated  algorithm  for  determining  whether  any  pair in  a  set  of  n  lines  intersects.
simple closed path to  get  the  flavor  of  problems  dealing  with  sets  of  points,  let’s  consider  the problem  of  finding  a  path  through  a  set  of  n  given  points  which  doesn’t
var  dx, dy,  dxl,  dx2,  dyl, dy2:  integer; begin dx:=l.p2.x-1.pl.x; dxl :=pl  .x-1.~1.~; dyl :=pl.y-1.~1  .y; dx2:=p2.x-1.p2.x;   dy2:=p2.y-1.p2.y; same:=(dx*dyl-ddy*dxl)*(dx*dy2-dpdx2) end;
in  terms  of  the  variables  in  this  program,  it  is  easy  to  check  that  the  quantity (da:   dyl   -  dy  dzl)   is  0  if  pl  is  on  the  line,  positive  if  pl  is  on  one  side,  and negative  if  it  is  on  the  other  side.  the  same  holds  true  for  the  other  point,  so the  product  of  the  quantities  for  the  two  points  is  positive  if  and  only  if  the points  fall  on  the  same  side  of  the  line,  negative  if  and  only  if  the  points  fall on  different  sides  of  the  line,  and  0  if  and  only  if  one  or  both  points  fall  on the  line.  we’ll  see  that  different  algorithms  need  to  treat  points  which  fall  on lines  in  different  ways,  so  this  three-way  test  is  quite  useful.
this  immediately  gives  an  implementation  of  the  intersect  function.  if the  endpoints  of  both  line  segments  are  on  opposite  sides  of  the  other  then they  must  intersect.
unfortunately,  there  is  one  case  where  this  function  returns  the  wrong  answer: if  the  four  line  endpoints  are  collinear,  it  always  will  report  intersection,  even though  the  lines  may  be  widely  separated.  special  cases  of  this  type  are  the bane  of  geometric  algorithms.  the  reader  may  gain  some  appreciation  for  the kind  of  complications  such  cases  can  lead  to  by  finding  a  clean  way  to  repair intersect  and  same  to  handle  all  cases.
if  many  lines  are  involved,  the  situation  becomes  much  more  complicated. later  on,  we’ll  see  a  sophisticated  algorithm  for  determining  whether  any  pair in  a  set  of  n  lines  intersects.
simple closed path to  get  the  flavor  of  problems  dealing  with  sets  of  points,  let’s  consider  the problem  of  finding  a  path  through  a  set  of  n  given  points  which  doesn’t
the  table  size  is  greater  than  before,  since  we  must  have  m  >  n,  but  the total  amount  of  memory  space  used  is  less,  since  no  links  are  used.  the average  number  of  items  that  must  be  examined  for  a  successful  search  for this  example  is:  33/17   z  1.941.
linear  probing  (indeed,  any  hashing  method)  works  because  it  guarantees that,  when  searching  for  a  particular  key,  we  look  at  every  key  that  hashes to  the  same  table  address  (in  particular,  the  key  itself  if  it’s  in  the  table). unfortunately,  in  linear  probing,  other  keys  are  also  examined,  especially when  the  table  begins  to  fill  up:  in  the  example  above,  the  search  for  x involved  looking  at  g,  h,  and  i  which  did  not  have  the  same  hash  value. what’s  worse,  insertion  of  a  key  with  one  hash  value  can  drastically  increase the  search  times  for  keys  with  other  hash  values:  in  the  example,  an  insertion at  position  17  would  cause  greatly  increased  search  times  for  position  16.  this phenomenon,  called  clustering,  can  make  linear  probing  run  very  slowly  for nearly  full  tables.
fortunately,  there  is  an  easy  way  to  virtually  eliminate  the  clustering problem:  double hashing.  the  basic  strategy  is  the  same;  the  only  difference  is that,  instead  of  examining  each  successive  entry  following  a  collided  position, we  use  a  second  hash  function  to  get  a  fixed  increment  to  use  for  the  “probe” sequence.  this  is  easily  implemented  by  inserting  u:=h2(v)   at  the  beginning of  the  procedure  and  changing  x:=(x+1)  mod  m to  x:=(x+u)   mod  m  within the  while  loop.  the  second  hash  function  h2  must  be  chosen  with  some  care, otherwise  the  program  might  not  work  at  all.
first,  we  obviously  don’t  want  to  have  u=o,   since  that  would  lead  to  an infinite  loop  on  collision.  second,  it  is  important  that  m  and  u  be  relatively prime  here,  since  otherwise  some  of  the  probe  sequences  could  be  very  short (for  example,  consider  the  case  m=2u).   this  is  easily  enforced  by  making m  prime  and  u<m.  third,  the  second  hash  function  should  be  “different” from  the  first,  otherwise  a  slightly  more  complicated  clustering  could  occur. a function such as hz(k)  = m - 2  - k mod (m  - 2) will produce a good range of “second” hash values.
variables,  so  we  have  computed  a  feasible  basis  for  the  original  linear  program. in  degenerate  cases,  some  of  the  artificial  variables  may  remain  in  the  basis, so  it  is  necessary  to  do  further  pivoting  to  remove  them  (without  changing the cost).
to summarize, a two-phase process is normally used to solve general linear programs.  first,  we  solve  a  linear  program  involving  the  artificial  s  variables to  get  a  point  on  the  simplex  for  our  original  problem.  then,  we  dispose  of the  s  variables  and  reintroduce  our  original  objective  function  to  proceed  from this  point  to  the  solution.
the  analysis  of  the  running  time  of  the  simplex  method  is  an  extremely complicated  problem,  and  few  results  are  available.  no  one  knows  the  “best” pivot selection strategy, because there are no results to tell us how many pivot steps to expect, for any reasonable class of problems. it is possible to construct artificial  examples  for  which  the  running  time  of  the  simplex  could  be  very large  (an  exponential  function  of  the  number  of  variables).  however,  those who  have  used  the  algorithm  in  practical  settings  are  unanimous  in  testifying to  its  efficiency  in  solving  actual  problems.
the  simple  version  of  the  simplex  algorithm  that  we’ve  considered,  while quite  useful,  is  merely  part  of  a  general  and  beautiful  mathematical  framework providing  a  complete  set  of  tools  which  can  be  used  to  solve  a  variety  of  very important  practical  problems.
also,  the  declaration  of  r has to be  suita.bly   changed  to  accomodate   twice  as many  coefficients  for  the  product.  each  of  the  n  coefficients  of  p  is  multiplied by  each  of  the  n  coefficients  of  q,  so  this  is  clearly  a  quadratic  algorithm.
an  advantage  of  representing  a  polynomial  by  an  array  containing  its coefficients is that it’s easy to reference any coefficient directly; a disadvantage is  that  space  may  have  to  be  saved  for  more  numbers  than  necessary.  for example,  the  program  above  couldn’t  reasonably  be  used  to  multiply
 and the output only three.   is  to  use  a  linked  list.  this involves  storing  items  in  noncontiguous  memory  locations,  with  each  item containing  the  address  of  the  next.  the  pascal  mechanisms  for  linked  lists  are somewhat  more  complicated  than  for  arrays.  for  example,  the  following  program  computes  the  sum  of  two  polynomials  using  a  linked  list  representation (the  bodies  of  the  readlist   and  add  functions  and  the  writelist  procedure  are given  in  the  text  following):
the  polynomials  are  represented  by  linked  lists  which  are  built  by  the readlist  procedure.  the  format  of  these  is  described  in  the  type  statement: the  lists  are  made  up  of  nodes,  each  node  containing  a  coefficient  and  a  link to  the  next  node  on  the  list.  if  we  have  a  link  to  the  first  node  on  a  list,  then we  can  examine  the  coefficients  in  order,  by  following  links.  the  last  node on  each  list  contains  a  link  to  a  special  (dummy  node  called  a:  if  we  reach  z when  scanning  through  a  list,  we  know  we’re  at  the  end.  (it  is  possible  to  get by  without  such  dummy  nodes,  but  they  do  make  certain  manipulations  on the  lists  somewhat  simpler.)  the  type  statement  only  describes  the  formats of  the  nodes;  nodes  can  be  created  only  when  the  builtin   procedure  new  is called.  for  example,  the  call  new(z)  creates  a  new  node,  putting  a  pointer  to
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
employing  linked  lists  in  this  way,  we  use  only  as  many  nodes  as  are required  by  our  program.  as  n  gets  larger,  we  simply  make  more  calls  on  new. by  itself,  this  might  not  be  reason  enough.  to  use  linked  lists  for  this  program, because  it  does  seem  quite  clumsy  comlpared   to  the  array  implementation above.  for  example,  it  uses  twice  as  much  space,  since  a  link  must  be  stored along  with  each  coefficient.  however,  as  suggested  by  the  example  above,  we can take advantage of the possibility that many of the coefficients may be zero. we  can  have  list  nodes  represent  only  the  nonzero   terms  of  the  polynomial  by also  including  the  degree  of  the  term  represented  within  the  list  node,  so  that each list node contains values of c and j to represent cxj.  it is then convenient to  separate  out  the  function  of  creating  a  node  and  adding  it  to  a  list,  as follows:
the  listadd   function  creates  a  new  node,  gives  it  the  specified  fields,  and  links it  into  a  list  after  node  t.   now  the  readlist   routine  can  be  changed  either  to accept  the  same  input  format  as  above  (a:nd  create  list  nodes  only  for  nonzero coefficients)  or  to  input  the  coefficient  and  exponent  directly  for  terms  with nonzero  coefficient.  of  course,  the  write,!ist   function  also  has  to  be  changed suitably.  to  make  it  possible  to  process  the  polynomials  in  an  organized
the  procedure  to  write  out  what’s  on  a  list  is  the  simplest.  it  simply steps  through  the  list,  writing  out  the  value  of  the  coefficient  in  each  node encountered, until z is found:
building  a  list  involves  first  calling  new  to  create  a  node,  then  filling  in the coefficient, and then linking the node to the end of the partial list built so far.  the  following  function  reads  the  same  format as  before,  and  constructs  the  linked  list  which  represents  the  corresponding polynomial:
the dummy node z is used here to hold the link which points to the first node on  the  list  while  the  list  is  being  constructed.  after  this  list  is  built,  a  is  set to  link  to  itself.  this  ensures  that  once  we  reach  the  end  of  a  list,  we  stay there.  another  convention  which  leave  z pointing to the beginning, to provide a way to get from the back to the front. finally,  the  program  which  adds  two  polynomials  constructs  a  new  list in  a  manner  similar  to  readlist,  calculating  the  coefficients  for  the  result by  stepping  through  the  argument  lists  and  adding  together  corresponding coefficients:
as  quicksort  for  random  files).  the  main  advantage  of  mergesort  over  these methods  is  that  it  is  stable;  the  main  disadvantage  of  mergesort  over  these methods  is  that  extra  space  proportional  to  n  (for  the  links)  is  required.  it is  also  possible  to  develop  a  nonrecursive  implementation  of  mergesort  using arrays,  switching  to  a  different  array  for  each  pass  in  the  same  way  that  we discussed  in  chapter  10  for  straight  radix  sort.
recursion  revisited the  programs  of  this  chapter  (together  with  quicksort)  are  typical  of  implementations  of  divide-and-conquer  algorithms.  we’ll  see  several  algorithms with  similar  structure  in  later  chapters,  so  it’s  worthwhile  to  take  a  more detailed  look  at  some  basic  characteristics  of  these  implementations.
quicksort  is  actually  a  “conquer-and-divide”  algorithm:  in  a  recursive implementation,  most  of  the  work  is  done  before  the  recursive  calls.  on  the other  hand,  the  recursive  mergesort  is  more  in  the  spirit  of  divide-and-conquer: first  the  file  is  divided  into  two  parts,  then  each  part  is  conquered  individually. the  first  problem  for  which  mergesort  does  actual  processing  is  a  small  one; at  the  finish  the  largest  subfile  is  processed.  quicksort  starts  with  actual processing  on  the  largest  subfile,
this  difference  manifests  itself  in  the  non-recursive  implementations  of the  two  methods.  quicksort  must  maintain  a  stack,  since  it  has  to  save  large subproblems  which  are  divided  up  in  a  data-dependent  manner.  mergesort admits  to  a  simple  non-recursive  version  because  the  way  in  which  it  divides the  file  is  independent  of  the  data,  so  the  order  in  which  it  processes  subproblems  can  be  rearranged  somewhat  to  give  a  simpler  program.
another  practical  difference  which  manifests  itself  is  that  mergesort  is stable  (if  properly  implemented);  quicksort  is  not  (without  going  to  extra trouble).  for  mergesort,  if  we  assume  (inductively)  that  the  subfiles   have  been sorted  stably,  then  we  need  only  be  sure  that  the  merge  is  done  in  a  stable manner,  which  is  easily  arranged.  but  for  quicksort,  no  easy  way  of  doing the  partitioning  in  a  stable  manner  suggests  itself,  so  the  possibility  of  being stable  is  foreclosed  even  before  the  recursion  comes  into  play.
many  algorithms  are  quite  simply  expressed  in  a  recursive  formulation. in  modern  programming  environments,  recursive  programs  implementing  such algorithms  can  be  quite  useful.  however,  it  is  always  worthwhile  to  study  the nature  of  the  recursive  structure  of  the  program  and  the  possibility  of  removing  the  recursion.  if  the  result  is  not  a  simpler,  more  efficient  implementation of  the  algorithm,  such  study  will  at  least  lead  to  better  understanding  of  the method.
another  feature  of  the  algorithm  which  seems  to  be  biased  is  the  order  in which  the  men  become  the  suitor:  is  it  better  to  be  the  first  man  to  propose (and  therefore  be  engaged  at  least  for  a  little  while  to  your  first  choice)  or the  last  (and  therefore  have  a  reduced  chance  to  suffer  the  indignities  of  a broken  engagement)?  the  answer  is  that  this  is  not  a  bias  at  all:  it  doesn’t matter  in  what  order  the  men  become  the  suitor.  as  long  as  each  man  makes proposals  and  each  woman  accepts  according  to  their  lists,  the  same  stable configuration  results.
the  two  special  cases  that  we’ve  examined  give  some  indication  of  how  complicated  the  matching  problem  can  be.  among  the  more  general  problems that  have  been  studied  in  some  detail  are:  the  maximum  matching  problem for  general  (not  necessarily  bipartite)  graphs;  weighted  matching  for  bipartite graphs,  where  edges  have  weights  and  a  matching  with  maximum  total  weight is  sought;  and  weighted  matching  for  general  graphs.  treating  the  many  techniques  that  have  been  tried  for  matching  on  general  graphs  would  fill  an  entire volume:  it  is  one  of  the  most  extensively  studied  problems  in  graph  theory.
multiplication,  and  division  have  a.  very  long  history,  dating  back  to the  origins  of  algorithm  studies  in  the  work  of  the  arabic  mathematician al-khowdrizmi,  with  roots  going  even  further  back  to  the  greeks  and  the babylonians.
  of  many computer  systems  is  their  capability  for  doing  fast,  accurate  numerical  calculations.  computers  have  built-in  capabilities  to  perform  arithmetic  on  integers  and  floating-point  representations  of  real  numbers;  for  example,  pascal allows  numbers  to  be  of  type  integer  or  re;d,   with  all  of  the  normal  arithmetic operations  defined  on  both  types.  algorithms  come  into  play  when  the  operations  must  be  performed  on  more  complicated  mathematical  objects,  such  as polynomials  or  matrices.
in  this  section,  we’ll  look  at  pascal  implementations  of  some  simple algorithms  for  addition  and  multiplication  of  polynomials  and  matrices.  the algorithms  themselves  are  well-known  and  straightforward;  we’ll  be  examining sophisticated  algorithms  for  these  problems  in  chapter  4.  our  main  purpose in  this  section  is  to  get  used  to  treating  th’ese   mathematical  objects  as  objects for  manipulation  by  pascal  programs.  this  translation  from  abstract  data  to something  which  can  be  processed  by  a  computer  is  fundamental  in  algorithm design.  we’ll  see  many  examples  throughout  this  book  in  which  a  proper representation  can  lead  to  an  efficient  algorithm  and  vice  versa.  in  this chapter,  we’ll  use  two  fundamental  ways  of  structuring  data,  the  array  and the  linked list.  these  data  structures  are  used  by  many  of  the  algorithms  in this  book;  in  later  sections  we’ll  study  some  more  advanced  data  structures.
much  of  the  material  in  this  section  falls  within  the  domain  of  numerical  analysis,  and  several  excellent  textbooks  are  available.  one  which  pays particular  attention  to  computational  issues  is  the  1977  book  by  forsythe, malcomb  and  moler.  in  particular,  much  of  the  material  given  here  in  chapters 5,  6,  and  7  is  based  on  the  presentation  given  in  that  book.
the  second  major  reference  for  this  section  is  the  second  volume  of  d.  e. knuth’s  comprehensive  treatment  of  “the  art  of  computer  programming.” knuth  uses  the  term  “seminumerical”  to  describe  algorithms  which  lie  at the  interface  between  numerical  and  symbolic  computation,  such  as  random number  generation  and  polynomial  arithmetic.  among  many  other  topics, knuths  volume  2  covers  in  great  depth  the  material  given  here  in  chapters 1,  3,  and  4.  the  1975  book  by  borodin  and  munro  is  an  additional  reference for  strassen’s  matrix  multiplication  method  and  related  topics.  many  of the  algorithms  that  we’ve  considered  (and  many  others,  principally  symbolic methods  as  mentioned  in  chapter  7)  are  embodied  in  a  computer  system  called macsyma,  which  is  regularly  used  for  serious  mathematical  work.
certainly,  a  reader  seeking  more  information  on  mathematical  algorithms should  expect  to  find  the  topics  treated  at  a  much  more  advanced  mathematical  level  in  the  references  than  the  material  we’ve  considered  here.
chapter  2  is  concerned  with  elementary  data  structures,  as  well  as  polynomials.  beyond  the  references  mentioned  in  the  previous  part,  a  reader  interested  in  learning  more  about  this  subject  might  study  how  elementary  data structures  are  handled  in  modern  programming  languages  such  as  ada,  which have  facilities  for  building  abstract  data  structures.
a.  borodin  and  i.  munro,  the  computational  complexity  of  algebraic  and numerical  problems,  american  elsevier,  new  york,  1975. g.  e.  forsythe,  m.  a.  malcomb,  and  c.  b.  moler,  computer  methods  for mathematical  computations,  prentice-hall,  englewood  cliffs,  nj,  1977. d.  e.  knuth,  the  art  of  computer  programming.  volume  &:   seminumerical algorithms,  addison-wesley,  reading,  ma  (second  edition),  1981. mit  mathlab  group,  macsyma  reference  manual,  laboratory  for  computer  science,  massachusetts  institute  of  technology,  1977. p.  wegner,  programming  with  ada:  an  introduction  by  means  of  graduated examples,  prentice-hall,  englewood  cliffs,  nj,  1980.
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
be  achieved  by  multiplying  a  through  c  in  the  optimal  way,  then  multiplying  d  through  f  in  the  optimal  way,  then  multiplying  the  resulting  matrices together.  (only  d  is  actually  in  the  best  array:  the  optimal  splits  are  indicated by  pairs  of  letters  in  the  table  for  clarity.)  to  find  how  to  multiply  a  through c  in  the  optimal  way,  we  look  in  row  a  and  column  c,  etc.  the  following program  implements  this  process  of  extracting  the  optimal  parenthesization from  the  cost  and  best  arrays  computed  by  the  program  above:
for  our  example,  the  parenthesization  computed  is  ((a*(b*c))*((d*e)*f)) which,  as  mentioned  above,  requires  only  36  scalar  multiplications.  for  the example  cited  earlier  with  the  dimensions  cf  3  in  b,  c  and  f  changed  to  300, the  same  parenthesization  is  optimal,  requiring  2412  scalar  multiplications. the  triple  loop  in  the  dynamic  programming  code  leads  to  a  running  time proportional  to  n3  and  the  space  required  is  proportional  to  n2,   substantially more  than  we  used  for  the  knapsack  problem.  but  this  is  quite  palatable compared  to  the  alternative  of  trying  all  4n-‘/na  possibilities.
optimal  binary  search  trees in  many  applications  of  searching,  it  is  known  that  the  search  keys  may  occur with  widely  varying  frequency.  for  example,  a  program  which  checks  the spelling of words in english text is likely to look up words like “and” and “the” far  more  often  than  words  like  “dynamic”  and  “programming.”  similarly, a  pascal  compiler  is  likely  to  see  keywords  like  “end”  and  “do”  far  more often  than  “label”  or  “downto.”  if  binary  tree  searching  is  used,  it  is  clearly advantageous to have the most frequently sought keys near the top of the tree. a  dynamic  programming  algorithm  can  be  used  to  determine  how  to  arrange the  keys  in  the  tree  so  that  the  total  cost  of  searching  is  minimized.
each  node  in  the  following  binary  search  tree  on  the  keys  a  through  g  is labeled  with  an  integer  which  is  assumed  to  be  proportional  to  its  frequency of  access:
readln   (n)  ; for  i:=o  to  n-l  do  for  j:=o  to  n-l  do  read(p[i,   j]); for  i:=o  to  n-l  do  for  j:=o  to  n-l  do  read(q[i,  j]); for  i:=o  to  n-l  do  for  j:=o  to  n-l  do  r[i,   j]:=p[i,   j]+q[i,   j]; for  i:=o  to  n-l  do  for  j:=o  to  n  do
element  r[i,   j]  is  the  dot   product  of  the  ith  row  of  p  with  the  jth  column of  q.  the  dot  product  is  simply  the  sum  of  the  n  term-by-term  multiplications  p[i, l]*q[l,  j]+p[i,  2]*q[2, j]+...  p[i, n-l]*q[n-i,  j] as in the following program:
each  of  the  n2   elements  in  the  result  matrix  is  computed  with  n  multiplications,  so  about  n3   operations  are  required  to  multiply  two  n  by  n matrices  together.  (as  noted  in  the  previous  chapter,  this  is  not  really  a  cubic algorithm,  since  the  number  of  data  items  in  this  case  is  about  n2,   not  n.)
as  with  polynomials,  sparse  matrices  (those  with  many  zero  elements)  can be processed in a much more efficient manner using a linked list representation. to  keep  the  two-dimensional  structure  intact,  each  nonzero   matrix  element is  represented  by  a  list  node  containing  ,a  value  and  two  links:  one  pointing to  the  next  nonzero   element  in  the  same  row  and  the  other  pointing  to  the next  nonzero  element  in  the  same  column.  implementing  addition  for  sparse
such relationships can be challenging to solve precisely, they are often easy to solve  for  some  particular  values  of  n  to  get  solutions  which  give  reasonable estimates  for  all  values  of  n.  our  purpo,se   in  this  discussion  is  to  gain  some intuitive  feeling  for  how  divide-and-conquer  algorithms  achieve  efficiency,  not to  do  detailed  analysis  of  the  algorithms.  indeed,  the  particular  recurrences that  we’ve  just  solved  are  sufficient  to  describe  the  performance  of  most  of the  algorithms  that  we’ll  be  studying,  and  we’ll  simply  be  referring  back  to them.
matrix  multiplication the  most  famous  application  of  the  divide-and-conquer  technique  to  an  arithmetic  problem  is  strassen’s  method  for  matrix  multiplication.  we  won’t  go into  the  details  here,  but  we  can  sketch  the  method,  since  it  is  very  similar  to the  polynomial  multiplication  method  that  we  have  just  studied.
the  straightforward  method  for  multiplying  two  n-by-n  matrices  requires  n3   scalar  multiplications,  since  each  of  the  n2   elements  in  the  product matrix  is  obtained  by  n  multiplications.
strassen’s  method  is  to  divide  the  size  of  the  problem  in  half;  this  corresponds  to  dividing  each  of  the  matrice;s  into  quarters,  each  n/2  by  n/2. the  remaining  problem  is  equivalent  to  multiplying  2-by-2  matrices.  just  as we  were  able  to  reduce  the  number  of  multiplications  required  from  four  to three  by  combining  terms  in  the  polynomial  multiplication  problem,  strassen was  able  to  find  a  way  to  combine  terms  to  reduce  the  number  of  multiplications  required  for  the  2-by-2  matrix  multiplication  problem  from  8  to  7.  the rearrangement  and  the  terms  required  are  quite  complicated.
this  result  was  quite  surprising  when  it  first  appeared,  since  it  had  previously been  thought  that  n3  multiplications  were  absolutely  necessary  for  matrix multiplication.  the  problem  has  been  studied  very  intensively  in  recent  years, and  slightly  better  methods  than  strassen’s  have  been  found.  the  “best” algorithm  for  matrix  multiplication  has  still  not  been  found,  and  this  is  one of  the  most  famous  outstanding  problems  of  computer  science.
it  is  important  to  note  that  we  have  been  counting  multiplications  only. before  choosing  an  algorithm  for  a  practical  application,  the  costs  of  the extra  additions  and  subtractions  for  combining  terms  and  the  costs  of  the
recursive  calls  must  be  considered.  these  costs  may  depend  heavily  on  the particular  implementation  or  computer  used.  but  certainly,  this  overhead makes  strassen’s  method  less  efficient  than  the  standard  method  for  small matrices.  even  for  large  matrices,  in  terms  of  the  number  of  data  items  input, strassen’s  method  really  represents  an  improvement  only  from  n’.5   to  n1.41. this  improvement  is  hard  to  notice  except  for  very  large  n.  for  example,  n would  have  to  be  more  than  a  million  for  strassen’s  method  to  use  four  times as  few  multiplications  as  the  standard  method,  even  though  the  overhead  per multiplication  is  likely  to  be  four  times  as  large.  thus  the  algorithm  is  a theoretical,  not  practical,  contribution.
this  illustrates  a  general  tradeoff  which  appears  in  all  applications  (though the  effect,  is  not  always  so  dramatic):  simple  algorithms  work  best  for  small problems,  but  sophisticated  algorithms  can  reap  tremendous  savings  for  large problems.
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
such relationships can be challenging to solve precisely, they are often easy to solve  for  some  particular  values  of  n  to  get  solutions  which  give  reasonable estimates  for  all  values  of  n.  our  purpo,se   in  this  discussion  is  to  gain  some intuitive  feeling  for  how  divide-and-conquer  algorithms  achieve  efficiency,  not to  do  detailed  analysis  of  the  algorithms.  indeed,  the  particular  recurrences that  we’ve  just  solved  are  sufficient  to  describe  the  performance  of  most  of the  algorithms  that  we’ll  be  studying,  and  we’ll  simply  be  referring  back  to them.
matrix  multiplication the  most  famous  application  of  the  divide-and-conquer  technique  to  an  arithmetic  problem  is  strassen’s  method  for  matrix  multiplication.  we  won’t  go into  the  details  here,  but  we  can  sketch  the  method,  since  it  is  very  similar  to the  polynomial  multiplication  method  that  we  have  just  studied.
the  straightforward  method  for  multiplying  two  n-by-n  matrices  requires  n3   scalar  multiplications,  since  each  of  the  n2   elements  in  the  product matrix  is  obtained  by  n  multiplications.
strassen’s  method  is  to  divide  the  size  of  the  problem  in  half;  this  corresponds  to  dividing  each  of  the  matrice;s  into  quarters,  each  n/2  by  n/2. the  remaining  problem  is  equivalent  to  multiplying  2-by-2  matrices.  just  as we  were  able  to  reduce  the  number  of  multiplications  required  from  four  to three  by  combining  terms  in  the  polynomial  multiplication  problem,  strassen was  able  to  find  a  way  to  combine  terms  to  reduce  the  number  of  multiplications  required  for  the  2-by-2  matrix  multiplication  problem  from  8  to  7.  the rearrangement  and  the  terms  required  are  quite  complicated.
this  result  was  quite  surprising  when  it  first  appeared,  since  it  had  previously been  thought  that  n3  multiplications  were  absolutely  necessary  for  matrix multiplication.  the  problem  has  been  studied  very  intensively  in  recent  years, and  slightly  better  methods  than  strassen’s  have  been  found.  the  “best” algorithm  for  matrix  multiplication  has  still  not  been  found,  and  this  is  one of  the  most  famous  outstanding  problems  of  computer  science.
it  is  important  to  note  that  we  have  been  counting  multiplications  only. before  choosing  an  algorithm  for  a  practical  application,  the  costs  of  the extra  additions  and  subtractions  for  combining  terms  and  the  costs  of  the
recursive  calls  must  be  considered.  these  costs  may  depend  heavily  on  the particular  implementation  or  computer  used.  but  certainly,  this  overhead makes  strassen’s  method  less  efficient  than  the  standard  method  for  small matrices.  even  for  large  matrices,  in  terms  of  the  number  of  data  items  input, strassen’s  method  really  represents  an  improvement  only  from  n’.5   to  n1.41. this  improvement  is  hard  to  notice  except  for  very  large  n.  for  example,  n would  have  to  be  more  than  a  million  for  strassen’s  method  to  use  four  times as  few  multiplications  as  the  standard  method,  even  though  the  overhead  per multiplication  is  likely  to  be  four  times  as  large.  thus  the  algorithm  is  a theoretical,  not  practical,  contribution.
this  illustrates  a  general  tradeoff  which  appears  in  all  applications  (though the  effect,  is  not  always  so  dramatic):  simple  algorithms  work  best  for  small problems,  but  sophisticated  algorithms  can  reap  tremendous  savings  for  large problems.
or  the  accumulation  of  computational  errors.  for  some  types  of  tridiagonal matrices  which  arise  commonly,  it  can  be  proven  that  this  is  not  a  reason  for concern.
gauss-jordan  reduction  can  be  implemented  with  full  pivoting  to  replace a  matrix  by  its  inverse  in  one  sweep    it.  the  inverse  of  a  matrix a,  written  a-‘,   has  the  property  that  a  system  of  equations  ax  =  b  could be  solved  just  by  performing  the  matrix  multiplication  z  =  a-lb.   still,  n3 operations  are  required  to  compute  x  given  b.  however,  there  is  a  way  to preprocess  a  matrix  and  “decompose”  it  into  component  parts  which  make it  possible  to  solve  the  corresponding  system  of  equations  with  any  given rightchand  side  in  time  proportional  to  1v2,   a  savings  of  a  factor  of  n  over using  gaussian  elimination  each  time. roughly,  this  involves  remembering the  operations  that  are  performed  on  the  (n +  1)st   column  during  the  forward elimination  phase,  so  that  the  result  of  forward  elimination  on  a  new  (n  +  1)st column  can  be  computed  efficiently  and  then  back-substitution  performed  as usual.
solving systems of linear equations has been shown to be computationally equivalent  to  multiplying  matrices,  so  tlhere  exist  algorithms  (for  example, strassen’s  matrix  multiplication  algorithm)  which  can  solve  systems  of  n equations  in  n  variables  in  time  proportional  to  n2.*l....   as  with  matrix multiplication,  it  would  not  be  worthwhile  to  use  such  a  method  unless  very large  systems  of  equations  were  to  be  processed  routinely.  as  before,  the actual  running  time  of  gaussian  elimination  in  terms  of  the  number  of  inputs is  n312.   which  is  difficult  to  imnrove   uoon in  nractice.
standard  method  can  be  implemented  for  this  representation,  with  the  usual extra  complications  due  to  the  need  to  create  and  destroy  non-zero  elements. this  technique  is  not  likely  to  be  worthwhile  if  one  can  afford  the  memory  to hold  the  whole  matrix,  since  it  is  much  more  complicated  than  the  standard method.  also,  sparse  matrices  become  substantially  less  sparse  during  the gaussian  elimination  process.
some  matrices  not  only  have  just  a  few  non-zero  elements  but  also  have a  simple  structure,  so  that  linked  lists  are  not  necessary.  the  most  common example  of  this  is  a  “band))  matrix,  where  the  non-zero  elements  all  fall  very close to the diagonal. in such cases, the inner loops of the gaussian elimination algorithms  need  only  be  iterated  a  few  times,  so  that  the  total  running  time (and  storage  requirement)  is  proportional  to  n,  not  n3.
an  interesting  special  case  of  a  band  matrix  is  a  “tridiagonal”  matrix, where  only  elements  directly  on,  directly  above,  or  directly  below  the  diagonal are  non-zero.  for  example,  below  is  the  general  form  of  a  tridiagonal  matrix for  n  =  !j:
for forward elimination, only the case  j=i+l   and  k=i+l   needs  to  be  included, since  a[i,  k]=o  for  k>i+l.   (the case  k =i  can  be  skipped  since  it  sets  to  0 an  array  element  which  is  never  examined  again  -this  same  change  could  be made  to  straight  gaussian  elimination.)  of  course,  a  two-dimensional  array of size  n2   wouldn’t  be  used  for  a  tridiagonal  matrix.  the  storage  required  for the  above  program  can  be  reduced  to  be  linear  in  n  by  maintaining  four  arrays instead  of  the  a  matrix:  one  for  each  of  the  three  nonzero  diagonals  and  one for the (n  +  l)st  column.  note  that  this  program  doesn’t  necessarily  pivot  on the  largest  available  element,  so  there  is  no  insurance  against  division  by  zero
for  any  cut  of  the  network  into  two  parts,  we  can  measure  the  flow  “across” the cut: the total of the  flow  on the edges which go from the source to the sink minus  the  total  of  the  flow  on  the  edges  which  go  the  other  way.  our  example cut  has  a  value  of  8,  which  is  equal  to  the  total  flow  for  the  network.  it  turns out  that  whenever  the  cut  flow  equals  the  total  flow,  we  know  not  only  that the  flow  is  maximal,  but  also  that  the  cut  is  minimal  (that  is,  every  other cut  has  at  least  as  high  a  flow  “across”).  this  is  called  the  maxfiow-mincut theorem:  the  flow   couldn’t  be  any  larger  (otherwise  the  cut  would  have  to  be larger  also);  and  no  smaller  cuts  exist  (otherwise  the  flow  would  have  to  be smaller  also).
network  searching the  ford-fulkerson  method  described  above  may  be  summarized  as  follows: “start  with  zero  flow  everywhere  and  increase  the  flow  along  any  path  from source to sink with no full forward edges or empty backward edges, continuing until  there  are  no  such  paths  in  the  network.”  but  this  is  not  an  algorithm  in the  sense  to  which  we  have  become  accustomed,  since  the  method  for  finding paths  is  not  specified.  the  example  above  is  based  on  the  intuition  that  the longer  the  path,  the  more  the  network  is  filled  up,  and  thus  that  long  paths should  be  preferred.  but  the  following  classical  example  shows  that  some  care should  be  exercised:
now, if the first path chosen is abdc, then the flow is increased by only one. then  the  second  path  chosen  might  be  adbc,  again  increasing  the  flow  by
mazes this  systematic  way  of  examining  every  vertex  and  edge  of  a  graph  has  a distinguished  history:  depth-first  search  was  first  stated  formally  hundreds of  years  ago  as  a  method  for  traversing  mazes.  for  example,  at  left  in  the diagram  below  is  a  popular  maze,  and  at  right  is  the  graph  constructed  by putting  a  vertex  at  each  point  where  there  is  more  than  one  path  to  take, then  connecting  the  vertices  according  to  the  paths:
this  is  significantly  more  complicated  than  early  english  garden  mazes, which  were  constructed  as  paths  through  tall  hedges.  in  these  mazes,  all walls  were  connected  to  the  outer  walls,  so  that  gentlemen  and  ladies  could stroll  in  and  clever  ones  could  find  their  way  out  by  simply  keeping  their right  hand  on  the  wall  (laboratory  mice  have  reportedly  learned  this  trick). when  independent  inside  walls  can  occur,  it  is  necessary  to  resort  to  a  more sophisticated  strategy  to  get  around  in  a  maze,  which  leads  to  depth-first search.
to  use  depth-first  search  to  get  from  one  place  to  another  in  a  maze,  we use  visit,  starting  at  the  vertex  on  the  graph  corresponding  to  our  starting point.  each  time  visit  “follows”  an  edge  via  a  recursive  call,  we  walk  along the  corresponding  path  in  the  maze.  the  trick  in  getting  around  is  that  we must  walk  back  along  the  path  that  we  used  to  enter  each  vertex  when  visit finishes  for  that  vertex.  this  puts  us  back  at  the  vertex  one  step  higher  up in  the  depth-first  search  tree,  ready  to  follow  its  next  edge.
which the reader might be amused to use as input for some of the algorithms in later  chapters.  to  fully  capture  the  correspondence  with  the  maze,  a  weighted
as  quicksort  for  random  files).  the  main  advantage  of  mergesort  over  these methods  is  that  it  is  stable;  the  main  disadvantage  of  mergesort  over  these methods  is  that  extra  space  proportional  to  n  (for  the  links)  is  required.  it is  also  possible  to  develop  a  nonrecursive  implementation  of  mergesort  using arrays,  switching  to  a  different  array  for  each  pass  in  the  same  way  that  we discussed  in  chapter  10  for  straight  radix  sort.
recursion  revisited the  programs  of  this  chapter  (together  with  quicksort)  are  typical  of  implementations  of  divide-and-conquer  algorithms.  we’ll  see  several  algorithms with  similar  structure  in  later  chapters,  so  it’s  worthwhile  to  take  a  more detailed  look  at  some  basic  characteristics  of  these  implementations.
quicksort  is  actually  a  “conquer-and-divide”  algorithm:  in  a  recursive implementation,  most  of  the  work  is  done  before  the  recursive  calls.  on  the other  hand,  the  recursive  mergesort  is  more  in  the  spirit  of  divide-and-conquer: first  the  file  is  divided  into  two  parts,  then  each  part  is  conquered  individually. the  first  problem  for  which  mergesort  does  actual  processing  is  a  small  one; at  the  finish  the  largest  subfile  is  processed.  quicksort  starts  with  actual processing  on  the  largest  subfile,
this  difference  manifests  itself  in  the  non-recursive  implementations  of the  two  methods.  quicksort  must  maintain  a  stack,  since  it  has  to  save  large subproblems  which  are  divided  up  in  a  data-dependent  manner.  mergesort admits  to  a  simple  non-recursive  version  because  the  way  in  which  it  divides the  file  is  independent  of  the  data,  so  the  order  in  which  it  processes  subproblems  can  be  rearranged  somewhat  to  give  a  simpler  program.
another  practical  difference  which  manifests  itself  is  that  mergesort  is stable  (if  properly  implemented);  quicksort  is  not  (without  going  to  extra trouble).  for  mergesort,  if  we  assume  (inductively)  that  the  subfiles   have  been sorted  stably,  then  we  need  only  be  sure  that  the  merge  is  done  in  a  stable manner,  which  is  easily  arranged.  but  for  quicksort,  no  easy  way  of  doing the  partitioning  in  a  stable  manner  suggests  itself,  so  the  possibility  of  being stable  is  foreclosed  even  before  the  recursion  comes  into  play.
many  algorithms  are  quite  simply  expressed  in  a  recursive  formulation. in  modern  programming  environments,  recursive  programs  implementing  such algorithms  can  be  quite  useful.  however,  it  is  always  worthwhile  to  study  the nature  of  the  recursive  structure  of  the  program  and  the  possibility  of  removing  the  recursion.  if  the  result  is  not  a  simpler,  more  efficient  implementation of  the  algorithm,  such  study  will  at  least  lead  to  better  understanding  of  the method.
tapes  2  and  3  to  tape  1  leaves  one  file  on  tape  2,  one  file  on  tape  1.  then  a twoway   merge  from  tapes  1  and  2  to  tape  3  leaves  the  entire  sorted  file  on tape  3.
this  “merge  until  empty”  strategy  can  be  extended  to  work  for  an  arbitrary  number  of  tapes.  for  example,  if  we  have  four  tape  units  tl,  t2, t3,  and  t4  and  we  start  out  with  tl   being  the  output  tape,  t2  having  13 initial  runs,  t3  having  11  initial  runs,  and  t4  having  7  initial  runs,  then  after running  a  3-way  “merge  until  empty,” we  have  t4  empty,  tl   with  7  (long) runs,  t2  with  6  runs,  and  t3  with  4  runs.  at  this  point,  we  can  rewind tl   and  make  it  an  input  tape,  and  rewind  t4  and  make  it  an  output  tape. continuing  in  this  way,  we  eventually  get  the  whole  sorted  file  onto  tl:
the  main  difficulty  in  implementing  a  polyphase  merge  is  to  determine how  to  distribute  the  initial  runs.  it  is  not  difficult  to  see  how  to  build  the table  above  by  working  backwards:  take  the  largest  number  on  each  line,  make it  zero,  and  add  it  to  each  of  the  other  numbers  to  get  the  previous  line.  this corresponds  to  defining  the  highest-order  merge  for  the  previous  line  which could  give  the  present  line.  this  technique  works  for  any  number  of  tapes (at  least  three):  the  numbers  which  arise  are  “generalized  fibonacci  numbers” which  have  many  interesting  properties.  of  course,  the  number  of  initial  runs may  not  be  known  in  advance,  and  it  probably  won’t  be  exactly  a  generalized fibonacci  number.  thus  a  number  of  “dummy”  runs  must  be  added  to  make the  number  of  initial  runs  exactly  what  is  needed  for  the  table.
the  analysis  of  polyphase  merging  is  complicated,  interesting,  and  yields surprising  results.  for  example,  it  turns  out  that  the  very  best  method  for distributing  dummy  runs  among  the  tapes  involves  using  extra  phases  and more  dummy  runs  than  would  seem  to  be  needed.  the  reason  for  this  is  that some  runs  are  used  in  merges  much  more  often  than  others.
(it is convenient to pass the list length as  e. parameter to the recursive program: alternatively,  it  could  be  stored  with  the  list  or  the  program  could  scan  the list  to  find  its  length.)
this program sorts by splitting the list po: nted to by c into two halves, pointed to by a and b,  sorting the two halves recursively, then using merge to produce the  final  result.  again,  this  program  adheres  to  the  convention  that  all  lists end  with  z:  the  input  list  must  end  with  z  (and  therefore  so  does  the  b  list); and  the  explicit  instruction  cf.next:=z   puts  z  at  the  end  of  the  a  list.  this program  is  quite  simple  to  understand  in  a  recursive  formulation  even  though it  actually  is  a  rather  sophisticated  algorithm.
the  running  time  of  the  program  fits   the  standard  “divide-and-conquer” recurrence  m(n)  =  2m(n/2)   +  n.  the  program  is  thus  guaranteed  to  run in  time  proportional  to  nlogn.   (see  chapter  4).
our  file  of  sample  sorting  keys  is  processed  as  shown  in  the  following table.  each  line  in  the  table  shows  the  result  of  a  call  on  merge.  first  we merge  0  and  s  to  get  0  s,  then  we  merge  this  with  a  to  get  a  0  s.  eventually we  merge  r  t  with  i  n  to  get  i  n  r  t,  then  this  is  merged  with  a  0  s  to get  a  i  n  0  r  s  t,  etc.:
the  input  device  refills  the  buffer  with  the  data  already  used  by  the  processor. the  same  technique  works  for  output,  with  the  roles  of  the  processor  and the  device  reversed.  usually  the  i/o  time  is  far  greater  than  the  processing time  and  so  the  effect  of  double-buffering  is  to  overlap  the  computation  time entirely;  thus  the  buffers  should  be  as  large  as  possible.
a  difficulty  with  double-buffering  is  that  it  really  uses  only  about  half the  available  memory  space.  this  can  lead  to  inefficiency  if  a  large  number of  buffers  are  involved,  as  is  the  case  in  p-way  merging  when  p  is  not  small. this  problem  can  be  dealt  with  using  a  technique  called  forecasting,  which requires the use of only one extra buffer (not  p) during the merging process. forecasting  works  as  follows.  certainly  the  best  way  to  overlap  input  with computation  during  the  replacement  selection  process  is  to  overlap  the  input of  the  buffer  that  needs  to  be  filled  next  with  the  processing  part  of  the algorithm.  but  it  is  easy  to  determine  which  buffer  this  is:  the  next  input buffer  to  be  emptied  is  the  one  whose  lust  item  is  smallest.  for  example,  when merging  a  0  s  with  i  r  t  and  a  g  n  we  know  that  the  third  buffer  will  be the  first  to  empty,  then  the  first.  a  simple  way  to  overlap  processing  with input for  multiway   merging  is  therefore  to  keep  one  extra  buffer  which  is  filled by  the  input  device  according  to  this  rule.  when  the  processor  encounters  an empty  buffer,  it  waits  until  the  input  buffer  is  filled  (if  it  hasn’t  been  filled already),  then  switches  to  begin  using  that  buffer  and  directs  the  input  device to  begin  filling  the  buffer  just  emptied  according  to  the  forecasting  rule.
the  most  important  decision  to  be  made  in  the  implementation  of  the multiway   merge  is  the  choice  of  the  value  of  p,  the  “order”  of  the  merge.  for tape  sorting,  when  only  sequential  access  is  allowed,  this  choice  is  easy:  p must  be  chosen  to  be  one  less  than  the  number  of  tape  units  available:  the multiway   merge  uses  p  input  tapes  and  one  output  tape.  obviously,  there should  be  at  least  two  input  tapes,  so  it  doesn’t  make  sense  to  try  to  do  tape sorting  with  less  than  three  tapes.
for  disk  sorting,  when  access  to  arbitrary  positions  is  allowed  but  is somewhat  more  expensive  than  sequential  access,  it  is  also  reasonable  to choose  p  to  be  one  less  than  the  number  of  disks  available,  to  avoid  the higher  cost  of  non-sequential  access  that  would  be  involved,  for  example,  if two  different  input  files  were  on  the  same  disk.  another  alternative  commonly used is to pick  p large enough so that the sort will be complete in two merging phases: it is usually unreasonable to try to do the sort in one pass, but a  twopass  sort  can  often  be  done  with  a  reasonably  small  p.  since  replacement selection  produces  about  n/2m   runs  and  each  merging  pass  divides  the number of runs by p, this means p should be chosen to be the smallest integer with  p2   >  n/2m.  for  our  example  of  sorting  a  200-million-word  file  on  a computer  with  a  l-million-word  memory,  this  implies  that  p  =  11  would  be  a safe choice to ensure a two-pass sort. (the right value of  p could be computed
between  points  in  the  plane,  then  a  self-intersecting  tour  can  be  improved  by removing  each  intersection  as  follows.  if  the  line  al3   intersects  the  line  cd, the  situation  can  be  diagramed  as  at  left  below,  without  loss  of  generality. but  it  follows  immediately  that  a  shorter  tour  can  be  formed  by  deleting  ab and  cd  and  adding  ad  and  cb,  as  diagramed  at  right:
applying  this  procedure  successively  will,  given  any  tour,  produce  a  tour  that is  no  longer  and  which  is  not  self-intersecting.  for  example,  the  procedure applied  to  the  tour  produced  from  the  minimum  spanning  tree  in  the  example above  gives  the  shorter  tour  agoenlpkfjmbdhica.  in  fact,  one  of  the most  effective  approaches  to  producing  approximate  solutions  to  the  euclidean traveling  salesman  problem,  developed  by  s.  lin,  is  to  generalize  the  procedure above to improve tours by switching around three or more edges in an existing tour.  very  good  results  have  been  obtained  by  applying  such  a  procedure successively,  until  it  no  longer  leads  to  an  improvement,  to  an  initially  random tour.  one  might  think  that  it  would  be  better  to  start  with  a  tour  that  is already  close  to  the  optimum,  but  lin’s  studies  indicate  that  this  may  not  be the case.
the  various  approaches  to  producing  approximate  solutions  to  the  traveling  salesman  problem  which  are  described  above  are  only  indicative  of  the types  of  techniques  that  can  be  used  in  order  to  avoid  exhaustive  search.  the brief  descriptions  above  do  not  do  justice  to  the  many  ingenious  ideas  that have  been  developed:  the  formulation  and  analysis  of  algorithms  of  this  type is  still  a  quite  active  area  of  research  in  computer  science.
one  might  legitimately  question  why  the  traveling  salesman  problem  and the  other  problems  that  we  have  been  alluding  to  require  exhaustive  search. couldn’t  there  be  a  clever  algorithm  that  finds  the  minimal  tour  as  easily and  quickly  as  we  can  find  the  minimum  spanning  tree?  in  the  next  chapter we’ll  see  why  most  computer  scientists  believe  that  there  is  no  such  algorithm and  why  approximation  algorithms  of  the  type  discussed  in  this  section  must therefore  be  studied.
to  begin,  we’ll  consider  a  pascal  program  which  is  essentially  a  translation  of  the  definition  of  the  concept  of  the  greatest  common  divisor  into  a programming 
the  body  of  the  program  above  is  trivial:  it  reads  two  numbers  from  the input,  then  writes  them  and  their  greatest  common  divisor  on  the  output. the  gcd  function  implements  a  “brute-force”  method:  start  at  the  smaller  of the  two  inputs  and  test  every  integer  (decreasing  by  one  until  1  is  reached) until  an  integer  is  found  that  divides  both  of  the  inputs.  the  built-in  function abs  is  used  to  ensure  that  gcd  is  called  with  positive  arguments.  (the  mod function  is  used  to  test  whether  two  numbers  divide:  u  mod  v  is  the  remainder when  u  is  divided  by  v,   so  a  result  of  0  indicates  that  v  divides  u.)
many  other  similar  examples  are  given  in  the  pascal  user  manual  and report.  the  reader  is  encouraged  to  scan  the  manual,  implement  and  test some  simple  programs  and  then  read  the  manual  carefully  to  become  reasonably  comfortable  with  most  of  the  features  of  pascal.
euclid’s algorithm a  much  more  efficient  method  for  finding  the  greatest  common  divisor  than that  above  was  discovered  by  euclid  over  two  thousand  years  ago.  euclid’s method  is  based  on  the  fact  that  if  u  is  greater  than  v  then  the  greatest common  divisor  of  u  and  v  is  the  same  as  the  greatest  common  divisor  of  v and  u  -  v.  applying  this  rule  successively,  we  can  continue  to  subtract  off multiples  of  v  from  u  until  we  get  a  number  less  than  v.  but  this  number  is
something  else.  this  seems  an  obvious  point  when  stated,  but  it’s  probably the  most  common  mistake  in  recursive  programming.  for  similar  reasons,  one shouldn’t  make  a  recursive  call  for  a  larger  problem,  since  that  might  lead  to a  loop  in  which  the  program  attempts  to  solve  larger  and  larger  problems.
not  all  programming  environments  support  a  general-purpose  recursion facility  because  of  intrinsic  difficulties  involved.  furthermore,  when  recursion is provided and used, it can be a source of unacceptable inefficiency. for these reasons,  we  often  consider  ways  of  removing  recursion.  this  is  quite  easy  to do when there is only one recursive call involved, as in the function above. we simply  replace  the  recursive  call  with  a  goto   to  the  beginning,  after  inserting some  assignment  statements  to  reset  the  values  of  the  parameters  as  directed by  the  recursive  call.  after  cleaning  up  the  program  left  by  these  mechanical transformations,  we  have  the  following  implementation  of  euclid’s  algorithm:
recursion  removal  is  much  more  complicated  when  there  is  more  than one  recursive  call.  the  algorithm  produced  is  sometimes  not  recognizable,  and indeed  is  very  often  useful  as  a  di.fferent   way  of  looking  at  a  fundamental  algorithm.  removing  recursion  almost  always  gives  a  more  efficient  implementation.  we’ll  see  many  examples  of  this  later  on  in  the  book.
analysis of algorithms in  this  short  chapter  we’ve  already  seen  three  different  algorithms  for  the  same problem;  for  most  problems  there  are  many  different  available  algorithms. how  is  one  to  choose  the  best  implementation  from  all  those  available?
this  is  actually  a  well  developed  area  of  study  in  computer  science. frequently,  we’ll  have  occasion  to  call  on  research  results  describing  the  performance  of  fundamental  algorithms.  however,  comparing  algorithms  can  be challenging  indeed,  and  certain  general  guidelines  will  be  useful.
usually  the  problems  that  we  solve  have  a  natural  “size”  (usually  the amount  of  data  to  be  processed;  in  the  above  example  the  magnitude  of the  numbers)  which  we’ll  normally  call  n.  we  would  like  to  know  the resources  used  (most  often  the  amount  of  time  taken)  as  a  function  of  n. we’re  interested  in  the  average  case,  the  amount  of  time  a  program  might  be
the  program  maintains  the  55  most  recently  generated  numbers,  with  the  last generated  pointed  to  by  j.  thus,  the  global  variable  a  has  been  replaced  by a  full  table  plus  a  pointer  (j)  into  it.  this  large  amount  of  “global  state”  is  a disadvantage of this generator in some applications, but it is also an advantage because  it  leads  to  an  extremely  long  cycle  even  if  the  modulus  m  is  small.
the  function  randomint  returns  a  random  integer  between  0  and  r-l.  of course,  it  can  easily  be  changed,  just  as  above,  to  a  function  which  returns  a random  real  number  between  0  and  1  (a  b]/m).
testing  randomness one  can  easily  detect  numbers  that  are  not  random,  but  certifying  that  a sequence  of  numbers  is  random  is  a  difficult  task  indeed.  as  mentioned  above, no  sequence  produced  by  a  computer  can  be  random,  but  we  want  a  sequence that  exhibits  many  of  the  properties  of  random  numbers.  unfortunately,  it  is often  not  possible  to  articulate  exactly  which  properties  of  random  numbers are  important  for  a  particular  application.
on  the  other  hand,  it  is  always  a  good  idea  to  perform  some  kind  of  test on  a  random  number  generator  to  be  sure  that  no  degenerate  situations  have turned  up.  random  number  generators  can  be  very,  very  good,  but  when they  are  bad  they  are  horrid.
many  tests  have  been  developed  for  determining  whether  a  sequence shares  various  properties  with  a  truly  random  sequence.  most  of  these  tests have  a  substantial  basis  in  mathematics,  and  it  would  definitely  be  beyond  the scope  of  this  book  to  examine  them  in  detail.  however,  one  statistical  test, the  x2  (chi-square)  test,  is  fundamental  in  nature,  quite  easy  to  implement, and  useful  in  several  applications,  so  we’ll  examine  it  more  carefully.
the  idea  of  the  x2  test  is  to  check  whether  or  not  the  numbers  produced are  spread  out  reasonably.  if  we  generate  n  positive  numbers  less  than  r, then
a  prime  is  found.  one  simple  method  performs  a  calculation  on  a  random number  that,  with  probability  l/2,   will  “prove”  that  the  number  to  be  tested is  not  prime.  (a  number  which  is  not  prime  will  survive  20  applications  of this  test  less  than  one  time  out  of  a  million,  30  applications  less  than  1  time out  of  a  billion.)  the  last  step  is  to  compute  p:  it  turns  out  that  a  variant  of euclid’s  algorithm  (see  chapter  1)  is  just  what  is  needed.
furthermore,  s  seems  to  be  difficult  to  compute  from  knowledge  of  p  (and n),  though  no  one  has  been  able  to  prove  that  to  be  the  case.  apparently, finding p from  s  requires  knowledge  of  x  and  y,   and  apparently  it  is  necessary to  factor  n  to  calculate  x  and  y.  but  factoring  n  is  thought  to  be  very difficult:  the  best  factoring  algorithms  known  would  take  millions  of  years  to factor  a  200-digit  number,  using  current  technology.
an  attractive  feature  of  the  rsa  system  is  that  the  complicated  computations  involving  n,  p,  and  s  are  performed  only  once  for  each  user  who subscribes  to  the  system,  which  the  much  more  frequent  operations  of  encryption  and  decryption  involve  only  breaking  up  the  message  and  applying  the simple  exponentiation  procedure. this  computational  simplicity,  combined with  all  the  convenience  features  provided  by  public-key  cryptosystems,  make this  system  quite  attractive  for  secure  communications,  especially  on  computer systems  and  networks.
the  rsa  method  has  its  drawbacks:  the  exponentiation  procedure  is  actually  expensive  by  cryptographic  standards,  and,  worse,  there  is  the  lingering  possibility  that  it  might  be  possible  to  read  messages  encrypted  using  the method.  this  is  true  with  many  cryptosystems:  a  cryptographic  method  must withstand  serious  cryptanalytic  attacks  before  it  can  be  used  with  confidence. several  other  methods  have  been  suggested  for  implementing  public-key cryptosystems.  some  of  the  most  interesting  are  linked  to  an  important  class of  problems  which  are  generally  thought  to  be  very  hard  (though  this  is  not known  for  sure),  which  we’ll  discuss  in  chapter  40.  these  cryptosystems have  the  interesting  property  that  a  successful  attack  could  provide  insight  on how  to  solve  some  well-known  difficult  unsolved  problems  (as  with  factoring for  the  rsa  method).  this  link  between  cryptology  and  fundamental  topics in  computer  science  research,  along  with  the  potential  for  widespread  use  of public-key  cryptography,  have  made  this  an  active  area  of  current  research.
geometric  process.  in  three  dimensions,  branching  at  each  node  corresponds to  cutting  the  three-dimensional  region  of  interest  with  a  plane;  in  general  we cut  the  k-dimensional  region  of  interest  with  a  (k-  1)-dimensional  hyperplane. if  k  is  very  large,  there  is  likely  to  be  a  significant  amount  of  imbalance in  the  kd  trees,  again  because  practical  point  sets  can’t  be  large  enough  to take  notice  of  randomness  over  a  large  number  of  dimensions.  typically,  all points in a  subtree  will  have  the  same  value  across  several  dimensions,  which leads  to  several  one-way  branches  in  the  trees.  one  way  to  help  alleviate  this problem  is,  rather  than  simply  cycle  through  the  dimensions,  always  to  use  the dimension  that  will  divide  up  the  point  set  in  the  best  way.  this  technique can  also  be  applied  to  2d  trees.  it  requires  that  extra  information  (which dimension  should  be  discriminated  upon)  be  stored  in  each  node,  but  it  does relieve  imbalance,  especially  in  high-dimensional  trees.
in summary, though it is easy to see how to to generalize the programs for range  searching  that  we  have  developed  to  handle  multidimensional  problems, such a step should not be taken lightly for a large application. large databases with  many  attributes  per  record  can  be  quite  complicated  objects  indeed,  and it  is  often  necessary  to  have  a  good  understanding  of  the  characteristics  of the  database  in  order  to  develop  an  efficient  range-searching  method  for  a particular  application.  this  is  a  quite  important  problem  which  is  still  being activelv   studied.
employing  linked  lists  in  this  way,  we  use  only  as  many  nodes  as  are required  by  our  program.  as  n  gets  larger,  we  simply  make  more  calls  on  new. by  itself,  this  might  not  be  reason  enough.  to  use  linked  lists  for  this  program, because  it  does  seem  quite  clumsy  comlpared   to  the  array  implementation above.  for  example,  it  uses  twice  as  much  space,  since  a  link  must  be  stored along  with  each  coefficient.  however,  as  suggested  by  the  example  above,  we can take advantage of the possibility that many of the coefficients may be zero. we  can  have  list  nodes  represent  only  the  nonzero   terms  of  the  polynomial  by also  including  the  degree  of  the  term  represented  within  the  list  node,  so  that each list node contains values of c and j to represent cxj.  it is then convenient to  separate  out  the  function  of  creating  a  node  and  adding  it  to  a  list,  as follows:
the  listadd   function  creates  a  new  node,  gives  it  the  specified  fields,  and  links it  into  a  list  after  node  t.   now  the  readlist   routine  can  be  changed  either  to accept  the  same  input  format  as  above  (a:nd  create  list  nodes  only  for  nonzero coefficients)  or  to  input  the  coefficient  and  exponent  directly  for  terms  with nonzero  coefficient.  of  course,  the  write,!ist   function  also  has  to  be  changed suitably.  to  make  it  possible  to  process  the  polynomials  in  an  organized
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
why  is  the  divide-and-conquer  method  g:iven   above  an  improvement?  in  this section,  we’ll  look  at  a  few  simple  recurrence  formulas  that  can  be  used  to measure  the  savings  achieved  by  a  divide-and-conquer  algorithm.
from  the  recursive  program,  it  is  clear  that  the  number  of  integer  multiplications  required  to  multiply  two  polynomials  of  size  n  is  the  same  as  the number  of  multiplications  to  multiply  three  pairs  of  polynomials  of  size  n/2. (note  that,  for  example,  no  multiplications  are  required  to  compute  t~(z)z~, just  data  movement.)  if  m(n)  is  the  number  of  multiplications  required  to multiply  two  polynomials  of  size  n,  we  have
for  n  >  1  with  m(1)  =  1.  thus  m(2)  q  =  3,  m(4)  =  9,  m(8)  =  27,  etc.  in general,  if  we  take  n  =  2n,   then  we  can  repeatedly  apply  the  recurrence  to itself  to  find  the  solution:
if  n  =  2n,   then  3% =  2(‘s31n  =  2n1s3  =  n’s3.   although  this  solution  is  exact only  for  n  =  2n,   it  works  out  in  general  that
which  is  a  substantial  savings  over  the  n2   naive  method.  note  that  if  we  were to  have  used  all  four  multiplications  in  the  simple  divide-and-conquer  method, the  recurrence  would  be  m(n)  =  4m(nl/2)   with  the  solution  m(2n)  =  4n  = n2.
the  method  described  in  the  previous  section  nicely  illustrates  the  divideand-conquer technique, but it is seldom  usled   in  practice  because  a  much  better divide-and-conquer  method  is  known,  which  we’ll  study  in  chapter  36.  this method  gets  by  with  dividing  the  original  into  only  two  subproblems,  with a  little  extra  processing.  the  recurrence  describing  the  number  of  multiplications  required  is
though  we  don’t  want  to  dwell  on  the  mathematics  of  solving  such  recurrences,  formulas  of  this  particular  form  arise  so  frequently  that  it  will  be worthwhile  to  examine  the  development  of  an  approximate  solution.  first,  as above,  we  write  n  =  2?
now,  applying  this  same  formula  to  itself  n  times  ends  up  simply  giving  n copies  of  the  “1,”  from  which  it  follows  immediately  that  m(2n)  =  712~.   again, it  turns  out  that  this  holds  true  (roughly)  for  all  n,  and  we  have  the  solution
we’ll  see  several  algorithms  from  different  applications  areas  whose  performance  characteristics  are  described  by  recurrences  of  this  type.  fortunately, many  of  the  recurrences  that  come  up  are  so  similar  to  those  above  that  the same  techniques  can  be  used.
for  another  example,  consider  the  situation  when  an  algorithm  divides the problem to be solved in half, then is able to ignore one half and (recursively) solve  the  other.  the  running  time  of  such  an  algorithm  might  be  described by  the  recurrence
this  is  easier  to  solve  than  the  one  in  the  previous  paragraph.  we  immediately have  i14(2~)  =  n  and,  again,  it  turns  out  that  m(n)  z  1gn.
of  course,  it’s  not  always  possible  to  get  by  with  such  trivial  manipulations.  for  a  slightly  more  difficult  example,  consider  an  algorithm  of  the  type described  in  the  previous  paragraph  which  must  somehow  examine  each  element  before  or  after  the  recursive  step.  the  running  time  of  such  an  algorithm is  described  by  the  recurrence
to  summarize,  many  of  the  most  interesting  algorithms  that  we  will encounter  are  based  on  the  divide-and-conquer  technique  of  combining  the solutions  of  recursively  solved  smaller  subproblems.  the  running  time  of  such algorithms  can  usually  be  described  by  recurrence  relationships  which  are  a direct  mathematical  translation  of  the  structure  of  the  algorithm.  though
to  1.  in  the  example,  the  last  two  terms  are  0  when  z  =  1,  the  first  and  last terms  are  0  when  x  =  2,  and  the  first  two  terms  are  0  when  x  =  3.
to  convert  a  polynomial  from  the  form  described  by  lagrange’s  formula to  our  standard  coefficient  representation  is  not  at  all  straightforward.  at least  n2   operations  seem  to  be  required,  since  there  are  n  terms  in  the  sum, each  consisting  of  a  product  with  n  factors.  actually,  it  takes  some  cleverness to  achieve  a  quadratic  algorithm,  since  the  factors  are  not  just  numbers,  but polynomials  of  degree  n.  on  the  other  hand,  each  term  is  very  similar  to the  previous  one.  the  reader  might  be  interested  to  discover  how  to  take advantage  of  this  to  achieve  a  quadratic  algorithm.  this  exercise  leaves  one with  an  appreciation  for  the  non-trivial  nature  of  writing  an  efficient  program to  perform  the  calculation  implied  by  a  mathematical  formula.
as  with  polynomial  evaluation,  there  are  more  sophisticated  methods which  can  solve  the  problem  in  n(log  n)2   steps,  and  in  chapter  36  we’ll  see a  method  that  uses  only  n  log  n  multiplications  for  a  specific  set  of  n  points of  interest.
multiplication our  first  sophisticated  arithmetic  algorithm  is  for  the  problem  of  polynomial multiplication:  given  two  polynomials  p(x)  and  q(x),  compute  their  product p(x)q(x).  as  noted  in  chapter  2,  polynomials  of  degree  n  -  1  could  have n  terms  (including  the  constant)  and  the  product  has  degree  2n  -  2  and  as many  as  2n  -  1  terms.  for  example,
the  naive  algorithm  for  this  problem  that  we  implemented  in  chapter  2 requires  n2  multiplications  for  polynomials  of  degree  n  -  1:  each  of  the  n terms  of  p(x)  must  be  multiplied  by  each  of  the  n  terms  of  q(x).
to  improve  on  the  naive  algorithm,  we’ll  use  a  powerful  technique  for algorithm  design  called  divide-and-conquer:  split  the  problem  into  smaller parts,  solve  them  (recursively),  then  put  the  results  back  together  in  some way.  many  of  our  best  algorithms  are  designed  according  to  this  principle. in  this  section  we’ll  see  how  divide-and-conquer  applies  in  particular  to  the polynomial  multiplication  problem.  in  the  following  section  we’ll  look  at  some analysis  which  gives  a  good  estimate  of  how  much  is  saved.
one  way  to  split  a  polynomial  in  two  is  to  divide  the  coefficients  in  half: given  a  polynomial  of  degree  n-l  (with  n  coefficients)  we  can  split  it  into  two polynomials  with  n/2  coefficients  (assume  that  n  is  even):  by  using  the  n/2 low-order  coefficients  for  one  polynomial  and  the  n/2  high-order  coefficients
performed.  otherwise  the  procedure  first  shuffles,  then  recursively  calls  itself to  transform  the  two  halves,  then  combines  the  results  of  these  computations as  described  above.  of  course,  the  actual  values  of  the  complex  roots  of  unity are  needed  to  do  the  implementation.  it  is  well  known  that
these  values  are  easily  computed  using  conventional  trigonometric  functions. in  the  above  program,  the  array  w  is  assumed  to  hold  the  (outn+l)st   roots  of unity.  to  get  the  roots  of  unity  needed,  the  program  selects  from  this  array at  an  interval  determined  by  the  variable  i.  for  example,  if  outhj   were  15, the  fourth  roots  of  unity  would  be  found  in  w[o],   w[4],w[8],  and  w[12].   this eliminates  the  need  to  recompute  roots  of  unity  each  time  they  are  used.
as  mentioned  at  the  outset,  the  scope  of  applicability  of  the  fft  is  far greater  than  can  be  indicated  here;  and  the  algorithm  has  been  intensively used  and  studied  in  a  variety  of  domains. nevertheless,  the  fundamental principles  of  operation  in  more  advanced  applications  are  the  same  as  for  the polynomial  multiplication  problem  that  we’ve  discussed  here.  the  fft  is a  classic  example  of  t.he   application  of  the.  “divide-and-conquer”  algorithm design  paradigm  to  achieve  truly  significant  computational  savings.
the  input  device  refills  the  buffer  with  the  data  already  used  by  the  processor. the  same  technique  works  for  output,  with  the  roles  of  the  processor  and the  device  reversed.  usually  the  i/o  time  is  far  greater  than  the  processing time  and  so  the  effect  of  double-buffering  is  to  overlap  the  computation  time entirely;  thus  the  buffers  should  be  as  large  as  possible.
a  difficulty  with  double-buffering  is  that  it  really  uses  only  about  half the  available  memory  space.  this  can  lead  to  inefficiency  if  a  large  number of  buffers  are  involved,  as  is  the  case  in  p-way  merging  when  p  is  not  small. this  problem  can  be  dealt  with  using  a  technique  called  forecasting,  which requires the use of only one extra buffer (not  p) during the merging process. forecasting  works  as  follows.  certainly  the  best  way  to  overlap  input  with computation  during  the  replacement  selection  process  is  to  overlap  the  input of  the  buffer  that  needs  to  be  filled  next  with  the  processing  part  of  the algorithm.  but  it  is  easy  to  determine  which  buffer  this  is:  the  next  input buffer  to  be  emptied  is  the  one  whose  lust  item  is  smallest.  for  example,  when merging  a  0  s  with  i  r  t  and  a  g  n  we  know  that  the  third  buffer  will  be the  first  to  empty,  then  the  first.  a  simple  way  to  overlap  processing  with input for  multiway   merging  is  therefore  to  keep  one  extra  buffer  which  is  filled by  the  input  device  according  to  this  rule.  when  the  processor  encounters  an empty  buffer,  it  waits  until  the  input  buffer  is  filled  (if  it  hasn’t  been  filled already),  then  switches  to  begin  using  that  buffer  and  directs  the  input  device to  begin  filling  the  buffer  just  emptied  according  to  the  forecasting  rule.
the  most  important  decision  to  be  made  in  the  implementation  of  the multiway   merge  is  the  choice  of  the  value  of  p,  the  “order”  of  the  merge.  for tape  sorting,  when  only  sequential  access  is  allowed,  this  choice  is  easy:  p must  be  chosen  to  be  one  less  than  the  number  of  tape  units  available:  the multiway   merge  uses  p  input  tapes  and  one  output  tape.  obviously,  there should  be  at  least  two  input  tapes,  so  it  doesn’t  make  sense  to  try  to  do  tape sorting  with  less  than  three  tapes.
for  disk  sorting,  when  access  to  arbitrary  positions  is  allowed  but  is somewhat  more  expensive  than  sequential  access,  it  is  also  reasonable  to choose  p  to  be  one  less  than  the  number  of  disks  available,  to  avoid  the higher  cost  of  non-sequential  access  that  would  be  involved,  for  example,  if two  different  input  files  were  on  the  same  disk.  another  alternative  commonly used is to pick  p large enough so that the sort will be complete in two merging phases: it is usually unreasonable to try to do the sort in one pass, but a  twopass  sort  can  often  be  done  with  a  reasonably  small  p.  since  replacement selection  produces  about  n/2m   runs  and  each  merging  pass  divides  the number of runs by p, this means p should be chosen to be the smallest integer with  p2   >  n/2m.  for  our  example  of  sorting  a  200-million-word  file  on  a computer  with  a  l-million-word  memory,  this  implies  that  p  =  11  would  be  a safe choice to ensure a two-pass sort. (the right value of  p could be computed
actual applications. for example, the  multicommodity  flow  problem  involves introducing  multiple  sources,  sinks,  and  types  of  material  in  the  network.  this makes  the  problem  much  more  difficult  and  requires  more  advanced  algorithms than  those  considered  here:  for  example,  no  analogue  to  the  max-flow  mincut  theorem  is  known  to  hold  for  the  general  case.  other  extensions  to  the network  flow  problem  include  placing  capacity  constraints  on  vertices  (easily handled  by  introducing  artificial  edges  to  handle  these  capacities),  allowing undirected  edges  (also  easily  handled  by  replacing  undirected  edges  by  pairs of  directed  edges),  and  introducing  lower  bounds  on  edge  flows  (not  so  easily handled).  if  we  make  the  realistic  assumption  that  pipes  have  associated  costs as  well  as  capacities,  then  we  have  the  min-cost  flow  problem,  a  quite  difficult nroblem  from  onerations  research. 1
that  “al”  be  deleted;  “1c”  means  to  add  cl  to  the  matching,  which  requires that  “c3”  be  deleted;  “3e”  means  to  add  e3  to  the  matching.  thus,  after this  path  is  processed,  we  have  the  matching  a4  b2  cl  d5  e3;  equivalently, the  flow  in  the  network  is  given  by  full  pipes  in  the  edges  connecting  those nodes,  and  all  pipes  leaving  0  and  entering  z  full.
the  proof  that  the  matching  is  exactly  those  edges  which  are  filled  to capacity  by  the  maxflow   algorithm  is  straightforward.  first,  the  network  flow always  gives  a  legal  matching:  since  each  vertex  has  an  edge  of  capacity  1 either  coming  in  (from  the  sink)  or  going  out  (to  the  source),  at  most  one  unit of  flow  can  go  through  each  vertex,  which  implies  that  each  vertex  will  be included  at  most  once  in  the  matching.  second,  no  matching  can  have  more edges,  since  any  such  matching  would  lead  directly  to  a  better  flow   than  that produced  by  the  maxflow   algorithm.
thus,  to  compute  the  maximum  matching  for  a  bipartite  graph  we  simply format  the  graph  so  as  to  be  suitable  for  input  to  the  network  flow  algorithm of  the  previous  chapter.  of  course,  the  graphs  presented  to  the  network  flow algorithm  in  this  case  are  much  simpler  than  the  general  graphs  the  algorithm is  designed  to  handle,  and  it  turns  out  that  the  algorithm  is  somewhat  more efficient  for  this  case.  the  construction  ensures  that  each  call  to  pfs  adds one  edge  to  the  matching,  so  we  know  that  there  are  at  most  v/2   calls  to pfs  during  the  execution  of  the  algorithm.  thus,  for  example,  the  total  time to  find  the  maximum  matching  for  a  dense  bipartite  graph  with  v  vertices (using  the  adjacency  matrix  representation)  is  proportional  to  v3.
stable  marriage  problem the  example  given  at  the  beginning  of  this  chapter,  involving  medical  students and  hospitals,  is  obviously  taken  quite  seriously  by  the  participants.  but the  method  that  we’ll  examine  for  doing  the  matching  is  perhaps  better understood  in  terms  of  a  somewhat  whimsical  model  of  the  situation.  we assume  that  we  have  n  men  and  n  women  who  have  expressed  mutual preferences  (each  man  must  say  exactly  how  he  feels  about  each  of  the  n women  and  vice  versa).  the  problem  is  to  find  a  set  of  n  marriages  that respects everyone’s preferences.
how  should  the  preferences  be  expressed?  one  method  would  be  to  use the  “1-10”   scale,  each  side  assigning  an  absolute  score  to  certain  members  of the  other  side.  this  makes  the  marriage  problem  the  same  as  the  weighted matching  problem,  a  relatively  difficult  problem  to  solve.  furthermore,  use  of absolute  scales  in  itself  can  lead  to  inaccuracies,  since  peoples’  scales  will  be inconsistent  (one  woman’s  10  might  be  another  woman’s  7).  a  more  natural way to express the preferences is to have each person list in order of preference the  following  two  tables  might  show all  the  people  of  the  opposite  sex.
instance  of  the  network  flow  problem. the  point  of  this  example  is  not that  linear  programming  will  provide  a  better  algorithm  for  this  problem, but  rather  that  linear  programming  is  a  quite  general  technique  that  can  be applied  to  a  variety  of  problems.  for  example,  if  we  were  to  generalize  the network  flow  problem  to  include  costs  as  well  as  capacities,  or  whatever,  the linear  programming  formulation  would  not  look  much  different,  even  though the  problem  might  be  significantly  more  difficult  to  solve  directly.
not  only  are  linear  programs  richly  expressive  but  also  there  exists  an algorithm  for  solving  them  (the  simplex  algorithm)  which  has  proven  to  be quite  efficient  for  many  problems  arising  in  practice.  for  some  problems (such  as  network  flow)  there  may  be  an  algorithm  specifically  oriented  to  that problem  which  can  perform  better  than  linear  programming/simplex;  for  other problems  (including  various  extensions  of  network  flow),  no  better  algorithms are  known.  even  if  there  is  a  better  algorithm,  it  may  be  complicated  or difficult  to  implement,  while  the  procedure  of  developing  a  linear  program and  solving  it  with  a  simplex  library  routine  is  often  quite  straightforward. this  “general-purpose”  aspect  of  the  method  is  quite  attractive  and  has  led to  its  widespread  use.  the  danger  in  relying  upon  it  too  heavily  is  that  it  may lead  to  inefficient  solutions  for  some  simple  problems  (for  example,  many  of those  for  which  we  have  studied  algorithms  in  this  book).
linear  programs  can  be  cast  in  a  geometric  setting.  the  following  linear program  is  easy  to  visualize  because  only  two  variables  are  involved.
these  transformations  are  purely  “local”:  no  part  of  the  tree  need  be  examined or  modified  other  than  what  is  diagrammed.  each  of  the  transformations passes up one of the keys from a 4-node  to its father in the tree, restructuring links  accordingly.  note  that  we  don’t  have  to  worry  explicitly  about  the  father being  a  4-node  since  our  through  each node  in  the  tree,  we  come  out  on  a  node  that  is  not  a  4-node.  in  particular, when  we  come  out  the  bottom  of  the  tree,  we  are  not  on  a  4-node,  and  we can directly insert the new node either by transforming a 2-node to a 3-node or  a  3-node  to  a  4-node.  actually,  it  is  convenient  to  treat  the  insertion  as  a split of an imaginary 4-node at the bottom which passes up the new key to be inserted.  whenever  the  root  of  the  tree  becomes  a  4-node,  we’ll  split  it  into three  2-nodes,  as  we  did  for  our  first  node  split  in  the  example  above.  this (and only this) makes the tree grow one level “higher.”
the  algorithm  sketched  in  the  previous  paragraph  gives  a  way  to  do searches  and  insertions  in  2-3-4  trees;  since  the  4-nodes  are  split  up  on  the way  from  the  top  down,  the  trees  are  called  top-down  2-s-4 trees.  what’s interesting is that, even though we haven’t been worrying about balancing at all,  the  resulting  trees  are  perfectly  balanced!  the  distance  from  the  root  to every  external  node  is  the  same,  which  implies  that  the  time  required  by  a search  or  an  insertion  is  always  proportional  to  log  n.   the  proof  that  the  trees are always perfectly balanced is simple: the transformations that we perform have  no  effect  on  the  distance  from  any  node  to  the  root,  except  when  we  split the root, and in this case the distance from all nodes to the root is increased by  one.
the  description  given  above  is  sufficient  to  define  an  algorithm  for  searching  using  binary  trees  which  has  guaranteed  worst-case  performance.  however, we  are  only  halfway  towards  an  actual  implementation.  while  it  would  be possible  to  write  algorithms  which  actually  perform  transformations  on  distinct data types representing 2-, 3-, and  4-nodes,  most of the things that need to  be  done  are  very  inconvenient  in  this  direct  representation.  (one  can  become  convinced  of  this  by  trying  to  implement  even  the  simpler  of  the  two node  transformations.)  furthermore,  the  overhead  incurred  in  manipulating the  more  complex  node  structures  is  likely  to  make  the  algorithms  slower  than standard  binary  tree  search.  the  primary  purpose  of  balancing  is  to  provide “insurance”  against  a  bad  worst  case,  but  it  would  be  unfortunate  to  have to  pay  the  overhead  cost  for  that  insurance  on  every  run  of  the  algorithm. fortunately,  as  we’ll  see  below,  there  is  a  relatively  simple  representation  of 2-,  3-,  and  4-nodes  that  allows  the  transformations  to  be  done  in  a  uniform way  with  very  little  overhead  beyond  the  costs  incurred  by  standard  binary tree 
the  running  time  of  this  program  obviously  depends  very  heavily  on the  pattern  being  matched.  however,  for  each  of  the  n  input  characters,  it processes  at  most  m  states  of  the  mac:nne,   so  the  worst  case  running  time is  proportional  to  mn.  for  sure,  not  all  nondeterministic  machines  can  be simulated  so  efficiently,  as  discussed  in  more  detail  in  chapter  40,  but  the  use of  a  simple  hypothetical  pattern-matching  machine  in  this  application  leads to  a  quite  reasonable  algorithm  for  a  quite  difficult  problem.  however,  to complete  the  algorithm,  we  need  a  program  which  translates  arbitrary  regular expressions  into  “machines”  for  interpretation  by  the  above  code.  in  the  next chapter,  we’ll  look  at  the  implementation  of  such  a  program  in  the  context  of a  more  general  discussion  of  compilers  a,nd   parsing  techniques.
with  “efficient”  exponential  algorithms,  using  the  backtracking  techniques described  in  the  previous  chapter.  finally,  there  is  quite  a  large  gap  between polynomial  and  exponential  time  which  is  not  addressed  by  the  theory.  what about  an  algorithm  that  runs  in  time  proportional  to  nl”sn   or  ‘2m?
all  of  the  application  areas  that  we’ve  studied  in  this  book  are  touched by  np-completeness:  there  are  np-complete  problems  in  numerical  applications,  in  sorting  and  searching,  in  string  processing,  in  geometry,  and  in  graph processing.  the  most  important  practical  contribution  of  the  theory  of  npcompleteness  is  that  it  provides  a  mechanism  to  discover  whether  a  new  problem  from  any  of  these  diverse  areas  is  “easy”  or  “hard.”  if  one  can  find  an efficient  algorithm  to  solve  a  new  problem,  then  there  is  no  difficulty.  if  not, a  proof  that  the  problem  is  np-complete  at  least  gives  the  information  that the  development  of  an  efficient  algorithm  would  be  a  stunning  achievement (and  suggests  that  a  different  approach  should  perhaps  be  tried).  the  scores of  efficient  algorithms  that  we’ve  examined  in  this  book  are  testimony  that  we have  learned  a  great  deal  about  efficient  computational  methods  since  euclid, but  the  theory  of  np-completeness  shows  that,  indeed,  we  still  have  a  great deal  to  learn.
surprisingly,  in  this  representation  each  “split-and-interleave”  operation  reduces  to  precisely  the  same  interconnection  pattern.  this  pattern  is  called the  perfect  shufle   because  the  wires  are  exactly  interleaved,  in  the  same  way that  cards  from  the  two  halves  would  be  interleaved  in  an  ideal  mix  of  a  deck of cards.
this  method  was  named  the  odd-even  merge  by  k.  e.  batcher,  who invented  it  in  1968.  the  essential  feature  of  the  method  is  that  all  of  the compare-exchange  operations  in  each  stage  can  be  done  in  parallel.  it  clearly demonstrates  that  two  files  of  n  elements  can  be  merged  together  in  1ogn parallel  steps  (the  number  of  rows  in  the  table  is  halved  at  every  step),  using less  than  n  log  n  compare-exchange  boxes.  from  the  description  above,  this might  seem  like  a  straightforward  result:  actually,  the  problem  of  finding  such a  machine  had  stumped  researchers  for  quite  some  time.
batcher  also  developed  a  closely  related  (but  more  difficult  to  understand) merging algorithm, the  bitonic  merge,  which  leads  to  an  even  simpler  machine.
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
one  attractive  feature  of  this  method  is  that  it  generalizes  to  three  (or more)  dimensions. the  convex  hull  of  a  set  of  points  in  3-space  is  a  convex three-dimensional  object  with  flat  faces.  it  can  be  found  by  “sweeping”  a plane  until  the  hull  is  hit,  then  “folding”  faces  of  the  plane,  anchoring  on different  lines  on  the  boundary  of  the  hull,  until  the  “package”  is  “wrapped.” the  program  is  quite  similar  to  selection  sorting,  in  that  we  successively choose  the  “best”  of  the  points  not  yet  chosen,  using  a  brute-force  search  for the  minimum.  the  major  disadvantage  of  the  method  is  that  in  the  worst  case, when  all  the  points  fall  on  the  convex  hull,  the  running  time  is  proportional to  n2.
the graham  scan the  next  method  that  we’ll  examine,  invented  by  r.  l.  graham  in  1972, is  interesting  because  most  of  the  computation  involved  is  for  sorting:  the algorithm  includes  a  sort  followed  by  a  relatively  inexpensive  (though  not  immediately  obvious)  computation.  the  algorithm  starts  with  the  construction of  a  simple  closed  polygon  from  the  points  using  the  method  of  the  previous chapter:  sort  the  points  using  as  keys  the  theta  function  values  corresponding to  the  angle  from  the  horizontal  made  from  the  line  connecting  each  point with  an  ‘anchor’  point  p[l]   (with  the  lowest  y  coordinate)  so  that  tracing p~~l,pk% . . . ,p[n],p[l]   gives  a  closed  polygon.  for  our  example  set  of  points, we  get  the  simple  closed  polygon  of  the  previous  section.  note  that  p[n], p[l],   and  p[2]  are  consecutive  points  on  the  hull;  we’ve  essentially  run  the first  iteration  of  the  package  wrapping  procedure  (in  both  directions).
computation  of  the  convex  hull  is  completed  by  proceeding  around, trying  to  place  each  point  on  the  hull  and  eliminating  previously  placed  points that  couldn’t  possibly  be  on  the  hull.  for  our  example,  we  consider  the  points
.  i:.: .  .  .  .  .  : .  *..  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . .  . . .
. .  . . .  .  . .  . .  .  . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . ..~.....................
:..:...:...:............::.......:.....::...::.~:.:..:.*...:...:...:...:~..::.:::. .  . . . . . . . .  . . . . . . .  .  .  .  .  .  .  .  .  .  .  .  . .  .  .
.:.:.........:..:.:..........:.:......:....::.:..:.....:.....::....::::..*.:.,:..: .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .n.
.  -.:  :..  ..:.  .::.  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . *
-:.: .  .  .  . . .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..~ .  . . . . . . .  . . . . . . .  . .  . . .  . . .  .  .  .  .  -  ::  ‘...  .
.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ,. i. .  .  . . .  :  :. . 
.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . .  . .
.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . .  . . . . .  . . . .  . . . .  . .  . .  .
certainly,  the  study  of  the  perfect  shuffle  and  systolic  machines  illustrates that hardware design can have a significant effect on algorithm design, suggesting  changes  that  can  provide  interesting  new  algorithms  and  fresh  challenges for  the  algorithm  designer.  while  this  is  an  interesting  and  fruitful  area  for further  research,  we  must  conclude  with  a  few  sobering  notes.  first,  a  great deal  of  engineering  effort  is  required  to  translate  general  schemes  for  parallel computation  such  as  those  sketched  above  to  actual  algorithm  machines  with good  performance.  for  many  applications,  the  resource  expenditure  required is  simply  not  justified,  and  a  simple  “algorithm  machine”  consisting  of  a  conventional  (inexpensive)  microprocessor  running  a  conventional  algorithm  will do  quite  well.  for  example,  if  one  has  many  instances  of  the  same  problem to  solve  and  several  microprocessors  with  which  to  solve  them,  then  ideal parallel  performance  can  be  achieved  by  having  each  microprocessor  (using  a conventional  algorithm)  working  on  a  different  instance  of  the  problem,  with no  interconnection  at  all  required.  if  one  has  n  files  to  sort  and  n  processors  available  on  which  to  sort  them,  why  not  simply  use  one  processor  for each  sort,  rather  than  having  all  n  processors  labor  together  on  all  n  sorts? techniques  such  as  those  discussed  in  this  chapter  are  currently  justified  only for  applications  with  very  special  time  or  space  requirements.  in  studying various  parallel  computation  schemes  and  their  effects  on  the  performance  of various  algorithms,  we  can  look  forward  to  the  development  of  general-purpose parallel  computers  that  will  provide  improved  performance  for  a  wide  variety of  algorithms.
output.  this  can  be  carried  one  step  further:  it  is  possible  to  build  compilers which  are  table-driven  in  terms  of  both  the  input  and  the  output  languages.  a compiler-compiler  is  a  program  which  takes  two  grammars  (and  some  formal specification  of  the  relationships  between  them)  as  input  and  produces  a compiler  which  translates  strings  from  one  language  to  the  other  as  output. parser  generators  and  compiler-compilers  are  available  for  general  use  in many  computing  environments,  and  are  quite  useful  tools  which  can  be  used to  produce  efficient  and  reliable  parsers  and  compilers  with  a  relatively  small amount of effort. on the other hand, top-down recursive descent parsers of the type  considered  here  are  quite  serviceable  for  simple  grammars  which  arise  in many  applications.  thus,  as  with  many  of  the  algorithms  we  have  considered, we  have  a  straightforward  method  which  can  be  used  for  applications  where a  great  deal  of  implementation  effort  might  not  be  justified,  and  several  advanced  methods  which  can  lead  to  significant  performance  improvements  for large-scale  applications.  of  course,  in  this  case,  this  is  significantly  understating  the  point:  we’ve  only  scratched  the  surface  of  this  extensively  researched
which  works  in  this  way  is  the  ‘so-called  shift-reduce  parser.  the  idea  is  to maintain  a  pushdown  stack  which  holds  terminal  and  nonterminal  symbols. each  step  in  the  parse  is  either  a  shift  step,  in  which  the  next  input  character is  simply  pushed  onto  the  stack,  or  a  reduce  step,  in  which  the  top  characters on  the  stack  are  matched  to  the  right-hand  side  of  some  production  in  the grammar  and  “reduced  to”  (replaced  by)  the  nonterminal  on  the  left  side of  that  production.  eventually  all  the  input  characters  get  shifted  onto  the stack,  and  eventually  the  stack  gets  reduced  to  a  single  nonterminal  symbol.
the  main  difficulty  in  building  a  shift-reduce  parser  is  deciding  when  to shift  and  when  to  reduce.  this  can  be  a  complicated  decision,  depending on  the  grammar.  various  types  of  shift-reduce  parsers  have  been  studied  in great  detail,  an  extensive  literature  has  been  developed  on  them,  and  they  are quite  often  preferred  over  recursive  descent  parsers  because  they  tend  to  be slightly  more  efficient  and  significantly  more  flexible.  certainly  we  don’t  have space  here  to  do  justice  to  this  field,  and  we’ll  forgo  even  the  details  of  an implementation  for  our  example.
a  compiler  may  be  thought  of  as  a  program  which  translates  from  one  language  to  another.  for  example,  a  pascal  compiler  translates  programs  from the  pascal  language  into  the  machine  language  of  some  particular  computer. we’ll  illustrate  one  way  that  this  might  be  done  by  continuing  with  our regular-expression  pattern-matching  example,  where  we  wish  translate from  the  language  of  regular  expressions  to  a  “language”  for  pattern-matching machines,  the  ch,  nextl,   and  next2  arrays  of  the  match  program  of  the  previous  chapter.
essentially,  the  translation  process  is  “one-to-one”:  for  each  character  in the  pattern  (with  the  exception  of  parentheses)  we  want  to  produce  a  state for  the  pattern-matching  machine  (an  entry  in  each  of  the  arrays).  the  trick is  to  keep  track  of  the  information  necessary  to  fill  in  the  next1   and  next2 arrays. to do so, we’ll convert each of the procedures in our recursive descent parser  into  functions  which  create  pattern-matching  machines.  each  function will  add  new  states  as  necessary  onto  the  end  of  the  ch, nextl,  and  next2 arrays,  and  return  the  index  of  the  initial  state  of  the  machine  created  (the final  state  will  always  be  the  last  entry  in  the  arrays).
term, we  used lookahead to avoid such a loop; in this case the proper way to get  around  the  problem  is  to  switch  the  grammar  to  say  (term)+(expression). the  occurrence  of  a  nonterminal  as  the  first  thing  on  the  right  hand  side  of a  replacement  rule  for  itself  is  called  left  recursion.  actually,  the  problem is  more  subtle,  because  the  left  recursion  can  arise  indirectly:  for  example if  we  were  to  have  the  productions  (expression)  ::=  (term)  and  (term)  ::= v  1  (expression)  +  (term).  recursive  descent  parsers  won’t  work  for  such grammars:  they  have  to  be  transformed  to  equivalent  grammars  without  left recursion,  or  some  other  parsing  method  has  to  be  used.  in  general,  there is  an  intimate  and  very  widely  studied  connection  between  parsers  and  the grammars  they  recognize.  the  choice  of  a  parsing  technique  is  often  dictated by  the  characteristics  of  the  grammar  to  be  parsed.
bottom-  up  parsing though  there  are  several  recursive  calls  in  the  programs  above,  it  is  an  instructive  exercise  to  remove  the  recursion  systematically.  recall  from  chapter 9  (where  we  removed  the  recursion  from  quicksort)  that  each  procedure  call can  be  replaced  by  a  stack  push  and  each  procedure  return  by  a  stack  pop, mimicking  what  the  pascal  system  does  to  implement  recursion.  a  reason for  doing  this  is  that  many  of  the  calls  which  seem  recursive  are  not  truly recursive.  when  a  procedure  call  is  the  last  action  of  a  procedure,  then  a simple  goto   can  be  used.  this  turns  expression  and  term  into  simple  loops, which  can  be  incorporated  together  and  combined  with  factor  to  produce  a single  procedure  with  one  true  recursive  call  (the  call  to  expression  within factor).
this  view  leads  directly  to  a  quite  simple  way  to  check  whether  regular expressions  are  legal.  once  all  the  procedure  calls  are  removed,  we  see  that each  terminal  symbol  is  simply  scanned  as  it  is  encountered.  the  only  real processing  done  is  to  check  whether  there  is  a  right  parenthesis  to  match  each left  parenthesis  and  whether  each  ‘i+”  is  followed  by  either  a  letter  or  a  “(i’. that  is,  checking  whether  a  regular  expression  is  legal  is  essentially  equivalent to  checking  for  balanced  parentheses. this  can  be  simply  implemented  by keeping  a  counter,  initialized  to  0,  which  is  incremented  when  a  left  parenthesis  is  encountered,  decremented  when  a  right  parenthesis  is  encountered. if  the  counter  is  zero  when  the  end  of  the  expression  is  reached,  and  each  ‘i+” of  the  expression  is  followed  by  either  a  letter  or  a  “(“,  then  the  expression was legal.
of  course,  there  is  more  to  parsing  than  simply  checking  whether  the input  string  is  legal:  the  main  goal  is  to  build  the  parse  tree  (even  if  in  an implicit  way,  as  in  the  top-down  parser)  for  other  processing.  it  turns  out  to be  possible  to  do  this  with  programs  with  the  same  essential  structure  as  the parenthesis  checker  described  in  the  previous  paragraph.  one  type  of  parser
term, we  used lookahead to avoid such a loop; in this case the proper way to get  around  the  problem  is  to  switch  the  grammar  to  say  (term)+(expression). the  occurrence  of  a  nonterminal  as  the  first  thing  on  the  right  hand  side  of a  replacement  rule  for  itself  is  called  left  recursion.  actually,  the  problem is  more  subtle,  because  the  left  recursion  can  arise  indirectly:  for  example if  we  were  to  have  the  productions  (expression)  ::=  (term)  and  (term)  ::= v  1  (expression)  +  (term).  recursive  descent  parsers  won’t  work  for  such grammars:  they  have  to  be  transformed  to  equivalent  grammars  without  left recursion,  or  some  other  parsing  method  has  to  be  used.  in  general,  there is  an  intimate  and  very  widely  studied  connection  between  parsers  and  the grammars  they  recognize.  the  choice  of  a  parsing  technique  is  often  dictated by  the  characteristics  of  the  grammar  to  be  parsed.
bottom-  up  parsing though  there  are  several  recursive  calls  in  the  programs  above,  it  is  an  instructive  exercise  to  remove  the  recursion  systematically.  recall  from  chapter 9  (where  we  removed  the  recursion  from  quicksort)  that  each  procedure  call can  be  replaced  by  a  stack  push  and  each  procedure  return  by  a  stack  pop, mimicking  what  the  pascal  system  does  to  implement  recursion.  a  reason for  doing  this  is  that  many  of  the  calls  which  seem  recursive  are  not  truly recursive.  when  a  procedure  call  is  the  last  action  of  a  procedure,  then  a simple  goto   can  be  used.  this  turns  expression  and  term  into  simple  loops, which  can  be  incorporated  together  and  combined  with  factor  to  produce  a single  procedure  with  one  true  recursive  call  (the  call  to  expression  within factor).
this  view  leads  directly  to  a  quite  simple  way  to  check  whether  regular expressions  are  legal.  once  all  the  procedure  calls  are  removed,  we  see  that each  terminal  symbol  is  simply  scanned  as  it  is  encountered.  the  only  real processing  done  is  to  check  whether  there  is  a  right  parenthesis  to  match  each left  parenthesis  and  whether  each  ‘i+”  is  followed  by  either  a  letter  or  a  “(i’. that  is,  checking  whether  a  regular  expression  is  legal  is  essentially  equivalent to  checking  for  balanced  parentheses. this  can  be  simply  implemented  by keeping  a  counter,  initialized  to  0,  which  is  incremented  when  a  left  parenthesis  is  encountered,  decremented  when  a  right  parenthesis  is  encountered. if  the  counter  is  zero  when  the  end  of  the  expression  is  reached,  and  each  ‘i+” of  the  expression  is  followed  by  either  a  letter  or  a  “(“,  then  the  expression was legal.
of  course,  there  is  more  to  parsing  than  simply  checking  whether  the input  string  is  legal:  the  main  goal  is  to  build  the  parse  tree  (even  if  in  an implicit  way,  as  in  the  top-down  parser)  for  other  processing.  it  turns  out  to be  possible  to  do  this  with  programs  with  the  same  essential  structure  as  the parenthesis  checker  described  in  the  previous  paragraph.  one  type  of  parser
its  final  position.  next,  scan  from  the  left   end  of  the  array  until  finding an  element  greater  than  a[r]  and  scan  from  the  right  end  of  the  array  until finding  an  element  less  than  a[r].   the  two  elements  which  stopped  the  scans are  obviously  out  of  place  in  the  final  p,srtitioned   array,  so  exchange  them. (actually, it turns out, for reasons described below, to be best to also stop the scans  for  elements  equal  to  a[r],   even  though  this  might  seem  to  involve  some unnecessary exhanges.)  cont,inuing   in  this  way  ensures  that  all  array  elements to  the  left  of  the  left  pointer  are  less  than  a[r],   and  array  elements  to  the  right of  the  right  pointer  are  greater  than  a  [r]  .  when  the  scan  pointers  cross,  the partitioning  process  is  nearly  complete:  all  that  remains  is  to  exchange  a[r] with  the  leftmost  element  of  the  right  subfile.
the  rightmost  element,  e,  is  chosen  as  the  partitioning  element.  first the  scan  from  the  left  stops  at  the  s,  then  the  scan  from  the  right  stops  at  the a,  then  these  two  are  exchanged,  as  shown  on  the  second  line  of  the  table. next  the  scan  from  the  left  stops  at  the  0,  then  the  scan  from  the  right  stops at  the  e,  then  these  two  are  exchanged,  as  shown  on  the  third  line  of  the table.  next  the  pointers  cross.  the  scan  from  the  left  stops  at  the  r,  and the  scan  from  the  right  stops  at  the  e.  the  proper  move  at  this  point  is  to exchange  the  e  at  the  right  with  the  r,  leaving  the  partitioned  file  shown  on the  last  line  of  the  table.  the  sort  is  finished  by  sorting  the  two  subfiles   on either  side  of  the  partitioning  element  (recursively).
to  introduce  the  general  approach  that  we’ll  be  taking  to  studying algorithms,  we’ll  examine  a  classic  elementary  problem:  “reduce  a  given fraction  to  lowest  terms.” we  want  to  write  213, not  416,  200/300,  or  178468/   to  finding  the  greatest  common 267702.  solving  this  problem  is  equival.ent divisor  (gcd)  of  the  numerator  and  the  denominator:  the  largest  integer  which divides  them  both.  a  fraction  is  reduced  to  lowest  terms  by  dividing  both numerator  and  denominator  by  their  greatest  common  divisor.
pascal a  concise  description  of  the  pascal  language  is  given  in  the  wirth  and  jensen pascal  user manual and  report that serves as the definition for the language. our  purpose  here  is  not  to  repeat  information  from  that  book  but  rather  to examine  the  implementation  of  a  few  simple  algorithms  which  illustrate  some of  the  basic  features  of  the  language  and.  the  style  that  we’ll  be  using.
pascal  has  a  rigorous  high-level  syntax  which  allows  easy  identification  of the  main  features  of  the  program.  the  variables  (var)  and  functions  (function) used  by  the  program  are  declared  first,  f~ollowed   by  the  body  of  the  program. (other  major  program  parts,  not  used  in  the  program  below  which  are  declared before  the  program  body  are  constants  and  types.)  functions  have  the  same format  as  the  main  program  except  that  they  return  a  value,  which  is  set  by assigning  something  to  the  function  name  within  the  body  of  the  function. (functions  that  return  no  value  are  called  procedures.)
the  built-in  function  readln  reads  a.  line  from  the  input  and  assigns  the values found to the variables given as arguments; writeln  is similar. a standard built-in  predicate,  eof,  is  set  to  true  when  there  is  no  more  input.  (input  and output  within  a  line  are  possible  with  read,  write,  and  eoln.)  the  declaration of  input  and  output  in  the  program  statement  indicates  that  the  program  is using  the  “standard”  input  and  output  &reams.
a  reader  interested  in  learning  more  about  pascal  will  find  a  large  number of  introductory  textbooks  available,  for  example,  the  ones  by  clancy  and cooper  or  holt  and  hune.  someone  with  experience  programming  in  other languages  can  learn  pascal  effectively  directly  from  the  manual  by  wirth  and jensen.  of  course,  the  most  important  thing  to  do  to  learn  about  the  language is  to  implement  and  debug  as  many  programs  as  possible.
many  introductory  pascal  textbooks  contain  some  material  on  data  structures.  though  it  doesn’t  use  pascal,  an  important  reference  for  further  information  on  basic  data  structures  is  volume  one  of  d.e.  knuth’s  series  on  the art  of computer  programming.  not  only  does  this  book  provide  encyclopedic coverage,  but  also  it  and  later  books  in  the  series  are  primary  references  for much  of  the  material  that  we’ll  be  covering  in  this  book.  for  example,  anyone interested  in  learning  more  about  euclid’s  algorithm  will  find  about  fifty  pages devoted  to  it  in  knuth’s  volume  two.
another  reason  to  study  knuth’s  volume  one  is  that  it  covers  in  detail the  mathematical  techniques  needed  for  the  analysis  of  algorithms.  a  reader with  little  mathematical  background  sh’ould   be  warned  that  a  substantial amount  of  discrete  mathematics  is  required  to  properly  analyze  many  algorithms;  a  mathematically  inclined  reader  will  find  much  of  this  material  ably summarized  in  knuth’s  first  book  and  applied  to  many  of  the  methods  we’ll be  studying  in  later  books.
m.  clancy  and  d.  cooper,  oh! pascal,  w. w. norton  &  company,  new  york, 1982. r.  holt  and  j.  p.hume,  programming  standard pascal,  reston   (prentice-hall), reston,  virginia,  1980. d.  e.  knuth,  the  art  of  computer  programming.  volume  1:  fundamental algorithms,  addison-wesley,  reading,  ma,  1968. d.  e.  knuth,  the art  of  computer  programming.  volume  .2:  seminumerical algorithms,  addison-wesley,  reading,  ma,  second  edition,  1981. k.  jensen  and  n.  wirth,  pascal  user  manual  and  report,  springer-verlag, new  york,  1974.
programs.  for  example,  the  following  grammar  describes  a  very  small  subset of  pascal,  arithmetic  expressions  involving  addition  and  multiplication.
again,  w  is  a  special  symbol  which  stands  for  any  letter,  but  in  this  grammar the  letters  are  likely  to  represent  variables  with  numeric  values.  examples  of legal  strings  for  this  grammar  are  a+(b*c)  and  (a+b*c)*d*(a+(b+c)).
as  we  have  defined  things,  some  strings  are  perfectly  legal  both  as  arithmetic  expressions  and  as  regular  expressions.  for  example,  a*(b+c)  might mean  “add  b  to  c  and  multiply  the  result  by  a”   or  “take  any  number  of  a’s followed  by  either  b  or  c.”  this  points  out  the  obvious  fact  that  checking whether  a  string  is  legally  formed  is  one  thing,  but  understanding  what  it means  is  quite  another.  we’ll  return  to  this  issue  after  we’ve  seen  how  to parse  a  string  to  check  whether  or  not  it  is  described  by  some  grammar.
each  regular  expression  is  itself  an  example  of  a  context-free  grammar: any  language  which  can  be  described  by  a  regular  expression  can  also  be described  by  a  context-free  grammar.  the  converse  is  not  true:  for  example, the  concept  of  “balancing”  parentheses  can’t  be  captured  with  regular  expressions. other  types  of  grammars  can  describe  languages  which  can’t  be described  by  context-free  grammars.  for  example,  context-sensitive  grammars are  the  same  as  those  above  except  that  the  left-hand  sides  of  productions need  not  be  single  nonterminals.  the  differences  between  classes  of  languages and  a  hierarchy  of  grammars  for  describing  them  have  been  very  carefully worked  out  and  form  a  beautiful  theory  which  lies  at  the  heart  of  computer science.
top-down  parsing one  parsing  method  uses  recursion  to  recognize  strings  from  the  language described  exactly  as  specified  by  the  grammar.  put  simply,  the  grammar  is such  a  complete  specification  of  the  language  that  it  can  be  turned  directly into  a  program!
terminal  on  the  left-hand  side.  nonterminals  on  the  right-hand  side  of  the input  correspond  to  (possibly  recursive)  procedure  calls;  terminals  correspond to  scanning  the  input  string.  for  example,  the  following  procedure  is  part  of a  top-down  parser  for  our  regular  expression  grammar:
ing  to  those  edges  that  were  actually  used  to  visit  vertices  via  recursive  calls and  dotted  edges  corresponding  to  those  edges  pointing  to  vertices  that  had already  been  visited  at  the  time  the  edge  was  considered.  the  nodes  are visited  in  the  order  a  f  e  d  b  g  j  k  l  m  c  h  i.
note  that  the  directions  on  the  edges  make  this  depth-first  search  forest quite  different  from  the  depth-first  search  forests  that  we  saw  for  undirected graphs.  for  example,  even  though  the  original  graph  was  connected,  the depth-first  search  structure  defined  by  the  solid  edges  is  not  connected:  it  is a  forest,  not  a  tree.
for  undirected  graphs,  we  had  only  one  kind  of  dotted  edge,  one  that connected  a  vertex  with  some  ancestor  in  the  tree.  for  directed  graphs,  there are  three  kinds  of  dotted  edges:  up  edges,  which  point  from  a  vertex  to  some ancestor in the tree, down edges, which point from a vertex to some descendant in  the  tree,  and  cross  edges,  which  point  from  a  vertex  to  some  vertex  which is  neither  a  descendant  nor  an  ancestor  in  the  tree.
as  with  undirected  graphs,  we’re  interested  in  connectivity  properties  of directed  graphs.  we  would  like  to  be  able  to  answer  questions  like  “is  there a directed path  from  vertex  x  to  vertex  y  (a  path  which  only  follows  edges  in the  indicated  direction)?” and  “which  vertices  can  we  get  to  from  vertex  x with  a  directed  path?” and  “is  there  a  directed  path  from  vertex  x  to  vertex y  and  a  directed  path  from  y  to  x.7”  just  as  with  undirected  graphs,  we’ll  be able  to  answer  such  questions  by  appropriately  modifying  the  basic  depth-first search  algorithm,  though  the  various  different  types  of  dotted  edges  make  the modifications  somewhat  more  complicated.
in undirected graphs, simple connectivity gives the vertices that can be reached from  a  given  vertex  by  traversing  edges  from  the  graph:  they  are  all  those  in the  same  connected  component.  similarly,  for  directed  graphs,  we’re  often interested  in  the  set  of  vertices  which  can  be  reached  from  a  given  vertex  by traversing  edges  from  the  graph  in  the  indicated  direction.
it  is  easy  to  prove  that  the  recursive  visit  procedure  from  the  depth-first search  method  in  chapter  29  visits  all  the  nodes  that  can  be  reached  from  the start  node.  thus,  if  we  modify  that  procedure  to  print  out  the  nodes  that  it  is visiting  (say,  by  inserting  write(name(k))   just  upon  entering),  we  are  printing out  all  the  nodes  that  can  be  reached  from  the  start  node.  but  note  carefully that  it  is  not  necessarily  true  that  each  tree  in  the  depth-first  search  forest contains  all  the  nodes  that  can  be  reached  from  the  root  of  that  tree  (in  our example,  all  the  nodes  in  the  graph  can  be  reached  from  h,  not  just  i).  to get  all  the  nodes  that  can  be  visited  from  each  node,  we  simply  call  visit  v times,  once  for  each  node:
same  technique  as  used  in  patricia  can  be   used  in  binary  radix  trie  searching to  eliminate  one-way  branching,  but  this   only  exacerbates  the  multiple  node type  problem.
unlike  standard  binary  tree  search,  the  radix  methods  are  insensitive  to the  order  in  which  keys  are  inserted;  thtty   depend  only  upon  the  structure  of the  keys  themselves.  for  patricia  the  pl,icement   of  the  upwards  links  depend on  the  order  of  insertion,  but  the  tree  structure  depends  only  on  the  bits  in the  keys,  as  for  the  other  methods.  this,   even  patricia  would  have  trouble with  a  set  of  keys  like  001,  0001,  00001,  300001,  etc.,  but  for  normal  key  sets, the  tree  should  be  relatively  well-balanced  so  the  number  of  bit  inspections, even  for  very  long  keys,  will  be  roughly  proportional  to  1gn  when  there  are n  nodes  in  the  tree.
the  most  useful  feature  of  radix  trie  searching  is  that  it  can  be  done efficiently  with  keys  of  varying  length.  in  all  of  the  other  searching  methods we  have  seen  the  length  of  the  key  is  “built  into”  the  searching  procedure  in some  way,  so  that  the  running  time  is  dependent   on  the  length  of  the  keys as  well  as  the  number  of  keys.  the  spetific   savings  available  depends  on  the method  of  bit  access  used.  for  example,  suppose  we  have  a  computer  which can  efficiently  access  g-bit   “bytes”  of  tlata,  and  we  have  to  search  among hundreds  of  looo-bit   keys.  then  patricia  would  require  access  of  only  about thl:  search,  plus  one  125-byte  equality 9  or  10  bytes  of  the  search  key  for  comparison  while  hashing  requires  accest   of  all  125-bytes  of  the  search  key  for computing the hash function plus a few  elluality  comparisons, and comparisonbased  methods  require  several  long  comparisons.  this  effect  makes  patricia (or  radix  trie  searching  with  one-way  branching  removed)  the  search  method of choice when very long keys are involved.
the  running  time  of  this  program  obviously  depends  very  heavily  on the  pattern  being  matched.  however,  for  each  of  the  n  input  characters,  it processes  at  most  m  states  of  the  mac:nne,   so  the  worst  case  running  time is  proportional  to  mn.  for  sure,  not  all  nondeterministic  machines  can  be simulated  so  efficiently,  as  discussed  in  more  detail  in  chapter  40,  but  the  use of  a  simple  hypothetical  pattern-matching  machine  in  this  application  leads to  a  quite  reasonable  algorithm  for  a  quite  difficult  problem.  however,  to complete  the  algorithm,  we  need  a  program  which  translates  arbitrary  regular expressions  into  “machines”  for  interpretation  by  the  above  code.  in  the  next chapter,  we’ll  look  at  the  implementation  of  such  a  program  in  the  context  of a  more  general  discussion  of  compilers  a,nd   parsing  techniques.
idea  is  to  simply  collapse  the  rows  in  the  table  above  to  just  one  pair  of  rows, and  thus  produce  a  cycling  machine  wired  together  as  follows:
note  carefully  that  this  is  not  quite  “ideal”  parallel  performance:  since we can merge together two files of n elements using one processor in a number of  steps  proportional  to  n,  we  would  hope  to  be  able  to  do  it  in  a  constant number  of  steps  using  n  processors.  in  this  case,  it  has  been  proven  that  it is  not  possible  to  achieve  this  ideal  and  that  the  above  machine  achieves  the best  possible  parallel  performance  for  merging  using  compare-exchange  boxes. the  perfect  shuffle  interconnection  pattern  is  appropriate  for  a  variety  of other  problems.  for  example,  if  a  2n-by-2n  square  matrix  is  kept  in  row-major order,  then  n  perfect  shuffles  will  transpose  the  matrix  (convert  it  to  columnmajor  order).  more  important  examples  include  the  fast  fourier  transform (which  we’ll  examine  in  the  next  chapter);  sorting  (which  can  be  developed  by applying  either  of  the  methods  above  recursively);  polynomial  evaluation;  and a  host  of  others.  each  of  these  problems  can  be  solved  using  a  cycling  perfect shuffle  machine  with  the  same  interconnections  as  the  one  diagramed  above but  with  different  (somewhat  more  complicated)  processors.  some  researchers have even suggested the use of the perfect shuffle interconnection for  “generalpurpose”  parallel  computers.
performed.  otherwise  the  procedure  first  shuffles,  then  recursively  calls  itself to  transform  the  two  halves,  then  combines  the  results  of  these  computations as  described  above.  of  course,  the  actual  values  of  the  complex  roots  of  unity are  needed  to  do  the  implementation.  it  is  well  known  that
these  values  are  easily  computed  using  conventional  trigonometric  functions. in  the  above  program,  the  array  w  is  assumed  to  hold  the  (outn+l)st   roots  of unity.  to  get  the  roots  of  unity  needed,  the  program  selects  from  this  array at  an  interval  determined  by  the  variable  i.  for  example,  if  outhj   were  15, the  fourth  roots  of  unity  would  be  found  in  w[o],   w[4],w[8],  and  w[12].   this eliminates  the  need  to  recompute  roots  of  unity  each  time  they  are  used.
as  mentioned  at  the  outset,  the  scope  of  applicability  of  the  fft  is  far greater  than  can  be  indicated  here;  and  the  algorithm  has  been  intensively used  and  studied  in  a  variety  of  domains. nevertheless,  the  fundamental principles  of  operation  in  more  advanced  applications  are  the  same  as  for  the polynomial  multiplication  problem  that  we’ve  discussed  here.  the  fft  is a  classic  example  of  t.he   application  of  the.  “divide-and-conquer”  algorithm design  paradigm  to  achieve  truly  significant  computational  savings.
in  this  example,  b  is  used  as  the  anchor.  if  the  points  are  visited  in  the  order b  m  j  l  n  p  k  f  i  e  c  0  a  h  g  d  b  then  a  simple  closed  polygon  will  be traced  out.
if  dx and dy are the delta x and y distances from some point to the anchor point,  then  the  angle  needed  in  this  algorithm  is  tan-’   dyldx.  although the  arctangent  is  a  built-in  function  in  pascal  (and  some  other  programming environments), it is likely to be slow and it leads to at least two annoying extra conditions  to  compute:  whether  dx  is  zero,  and  which  quadrant  the  point  is in.  since  the  angle  is  used  only  for  the  sort  in  this  algorithm,  it  makes  sense to  use  a  function  that  is  much  easier  to  compute  but  has  the  same  ordering properties  as  the  arctangent  (so  that  when  we  sort,  we  get  the  same  result). a  good  candidate  for  such  a  function  is  simply  dy/(dy  +  dz).  testing  for exceptional  conditions  is  still  necessary,  but  simpler.  the  following  program returns  a  number  between  0  and  360  that  is  not  the  angle  made  by  pl  and p2  with  the  horizontal  but  which  has  the  same  order  properties  as  the  true angle.
in  some  programming  environments  it  may  not  be  worthwhile  to  use  such programs  instead  of  standard  trigonometric  functions;  in  others  it  might  lead to  significant  savings.  (in  some  cases  it  might  be  worthwhile  to  change  theta to  have  an  integer  value,  to  avoid  using  real  numbers  entirely.)
inclusion in a polygon the  next  problem  that  we’ll  consider  is  a  natural  one:  given  a  polygon  represented  as  an  array  of  points  and  another  point,  determine  whether  the  point is  inside  or  outside.  a  straightforward  solution  to  this  problem  immediately suggests  itself:  draw  a  long  line  segment  from  the  point  in  any  direction  (long enough so that its other endpoint is guaranteed to be outside the polygon) and
perspective. from the few examples given, it should be clear that it is easy to underestimate the  difficulty  of  solving  a  particular  geometric  problem  with  a  computer. there  are  many  other  elementary  geometric  computations  that  we  have  not treated  at  all.  for  example,  a  program  to  compute  the  area  of  a  polygon makes  an  interesting  exercise.  however,  the  problems  that  we  have  studied have  provided  some  basic  tools  that  we  will  find  useful  in  later  sections  for solving  the  more  difficult  problems.
some  of  the  algorithms  that  we’ll  study  involve  building  geometric  structures  from  a  given  set  of  points.  the  “simple  closed  polygon”  is  an  elementary example  of  this.  we  will  need  to  decide  upon  appropriate  representations for  such  structures,  develop  algorithms  to  build  them,  and  investigate  their use  for  particular  applications  areas. as  usual,  these  considerations  are  intertwined.  for  example,  the  algorithm  used  in  the  inside  procedure  in  this chapter  depends  in  an  essential  way  on  the  representation  of  the  simple  closed polygon  as  an  ordered  set  of  points  (rather  than  as  an  unordered  set  of  lines). many  of  the  algorithms  that  we’ll  study  involve  geometric  search:  we want to know which points from a given set are close to a given point, or which points fall in a given rectangle, or which points are closest to each other. many of  the  algorithms  appropriate  for  such  search  problems  are  closely  related  to the  search  algorithms  that  we  studied  in  chapters  14-17.  the  parallels  will be  quite  evident.
few  geometric  algorithms  have  been  analyzed  to  the  point  where  precise statements  can  be  made  about  their  relative  performance  characteristics.  as we’ve  already  seen,  the  running  time  of  a  geometric  algorithm  can  depend  on many  things.  the  distribution  of  the  points  themselves,  the  order  in  which they  appear  in  the  input,  and  whether  trigonometric  functions  are  needed or  used  can  all  significantly  affect  the  running  time  of  geometric  algorithms. as  usual  in  such  situations,  we  do  have  empirical  evidence  which  suggests good  algorithms  for  particular  applications.  also,  many  of  the  algorithms  are designed  to  nerform   well  in  the  worst  case.  no  matter  what  the inout
the  methods  for  doing  arithmetic  operations  given  in  chapter  2  are simple  and  straightforward  solutions  to  familiar  problems.  as  such,  they provide  an  excellent  basis  for  applying  allgorithmic   thinking  to  produce  more sophisticated  methods  which  are  substantially  more  efficient.  as  we’ll  see,  it is  one  thing  to  write  down  a  formula  which  implies  a  particular  mathematical calculation;  it  is  quite  another  thing  to  write  a  computer  program  which performs  the  calculation  efficiently.
operations  on  mathematical  objects  are  far  too  diverse  to  be  catalogued here;  we’ll  concentrate  on  a  variety  of  algorithms  for  manipulating  polynomials. the  principal  method  that  we’ll  study  in  this  section  is  a  polyno mial  multiplication  scheme  which  is  of  no  particular  practical  importance  but which  illustrates  a  basic  design  paradigm  called  divide-and-conquer  which  is pervasive  in  algorithm  design.  we’ll  see  in  this  section  how  it  applies  to  matrix multiplication  as  well  as  polynomial  multiplication;  in  later  sections  we’ll  see it  applied  to  most  of  the  problems  that  we  encounter  in  this  book.
then  compute  and  add  3x3,  etc.  this for  any  given  x,  one  could  compute  x4, method  requires  recomputation  of  the  powers  of  x;  an  alternate  method,  which requires  extra  storage,  would  save  the  powers  of  x  as  they  are  computed.
a  simple  method  which  avoids  recomputation  and  uses  no  extra  space is  known  as  homer’s  rule:  by  alternat:ing   the  multiplication  and  addition operations  appropriately,  a  degree-n  polynomial  can  be  evaluated  using  only
recursive  calls  must  be  considered.  these  costs  may  depend  heavily  on  the particular  implementation  or  computer  used.  but  certainly,  this  overhead makes  strassen’s  method  less  efficient  than  the  standard  method  for  small matrices.  even  for  large  matrices,  in  terms  of  the  number  of  data  items  input, strassen’s  method  really  represents  an  improvement  only  from  n’.5   to  n1.41. this  improvement  is  hard  to  notice  except  for  very  large  n.  for  example,  n would  have  to  be  more  than  a  million  for  strassen’s  method  to  use  four  times as  few  multiplications  as  the  standard  method,  even  though  the  overhead  per multiplication  is  likely  to  be  four  times  as  large.  thus  the  algorithm  is  a theoretical,  not  practical,  contribution.
this  illustrates  a  general  tradeoff  which  appears  in  all  applications  (though the  effect,  is  not  always  so  dramatic):  simple  algorithms  work  best  for  small problems,  but  sophisticated  algorithms  can  reap  tremendous  savings  for  large problems.
in  general,  suppose  we  wish  our  program  to  be  able  to  compute  r(x)  =  p(x)  + q(x),  where  p  and  q  are  polynomials  with  n  coefficients.  the  following program  is  a  straightforward  implementation  of  polynomial  addition:
readln  (n) ; for  i:=o  to  n-l  do  read(p[i]); for  i:=o  to  n-l  do  read(q[i]); for  i:=o  to  n-j  do  r[i]   :=p[i]+q[i]; for  i:=o  to  n-l  do  write(r[i]); wri teln
in  this  program,  the  polynomial  p(z)   =  pc   +  pix   +  a..  +  pr\r-isn-’   is represented by the  array p [o..n-l] with p [j] = pj, etc. a polynomial of degree n-l  is  defined  by  n  coefficients.  the  input  is  assumed  to  be  n,  followed  by the  p  coefficients,  followed  by  the  q  coefficients.  in  pascal,  we  must  decide ahead  of  time  how  large  n  might  get;  this  program  will  handle  polynomials up  to  degree  100.  obviously,  maxn  should  be  set  to  the  maximum  degree anticipated.  this  is  inconvenient  if  the  program  is  to  be  used  at  different times  for  various  sizes  from  a  wide  range:  many  programming  environments allow  “dynamic  arrays”  which,  in  this  case,  could  be  set  to  the  size  n.  we’ll see  another  technique  for  handling  this  situation  below.
the  program  above  shows  that  addition  is  quite  trivial  once  this  representation  for  polynomials  has  been  chosen;  other  operations  are  also  easily coded.  for  example,  to  multiply  we  can  replace  the  third  for  loop  by
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
the  methods  for  doing  arithmetic  operations  given  in  chapter  2  are simple  and  straightforward  solutions  to  familiar  problems.  as  such,  they provide  an  excellent  basis  for  applying  allgorithmic   thinking  to  produce  more sophisticated  methods  which  are  substantially  more  efficient.  as  we’ll  see,  it is  one  thing  to  write  down  a  formula  which  implies  a  particular  mathematical calculation;  it  is  quite  another  thing  to  write  a  computer  program  which performs  the  calculation  efficiently.
operations  on  mathematical  objects  are  far  too  diverse  to  be  catalogued here;  we’ll  concentrate  on  a  variety  of  algorithms  for  manipulating  polynomials. the  principal  method  that  we’ll  study  in  this  section  is  a  polyno mial  multiplication  scheme  which  is  of  no  particular  practical  importance  but which  illustrates  a  basic  design  paradigm  called  divide-and-conquer  which  is pervasive  in  algorithm  design.  we’ll  see  in  this  section  how  it  applies  to  matrix multiplication  as  well  as  polynomial  multiplication;  in  later  sections  we’ll  see it  applied  to  most  of  the  problems  that  we  encounter  in  this  book.
then  compute  and  add  3x3,  etc.  this for  any  given  x,  one  could  compute  x4, method  requires  recomputation  of  the  powers  of  x;  an  alternate  method,  which requires  extra  storage,  would  save  the  powers  of  x  as  they  are  computed.
a  simple  method  which  avoids  recomputation  and  uses  no  extra  space is  known  as  homer’s  rule:  by  alternat:ing   the  multiplication  and  addition operations  appropriately,  a  degree-n  polynomial  can  be  evaluated  using  only
a  more  complicated  problem  is  to  evaluate  a  given  polynomial  at  many different  points.  different  algorithms  are  appropriate  depending  on  how  many evaluations  are  to  be  done  and  whether  or  not  they  are  to  be  done  simultaneously.  if  a  very  large  number  of  evaluations  is  to  be  done,  it  may  be worthwhile  to  do  some  “precomputing”   which  can  slightly  reduce  the  cost for  later  evaluations.  note  that  using  horner’s  method  would  require  about n2   multiplications  to  evaluate  a  degree-n  polynomial  at  n  different  points. much  more  sophisticated  methods  have  been  designed  which  can  solve  the problem  in  n(logn)’   steps,  and  in  chapter  36  we’ll  see  a  method  that  uses only  n  log  n  multiplications  for  a  specific  set  of  n  points  of  interest.
if  the  given  polynomial  has  only  one  term,  then  the  polynomial  evaluation  problem  reduces  to  the  exponentiation  problem:  compute  xn.   horner’s rule  in  this  case  degenerates  to  the  trivial  algorithm  which  requires  n  -  1 multiplications.  for  an  easy  example  of  how  we  can  do  much  better,  consider the  following  sequence  for  computing  x32:
the  “successive  squaring”  method  can  easily  be  extended  to  general  n if  computed  values  are  saved.  for  example,  x55  can  be  computed  from  the above  values  with  four  more  multiphcations:
in  general,  the  binary  representation  of  n  can  be  used  to  choose  which computed  values  to  use.  (in  the  example,  since  55  =  (110111)2,   all  but  x8 are  used.)  the  successive  squares  can  be  computed  and  the  bits  of  n  tested within  the  same  loop.  two  methods  are  available  to  implement  this  using  only
tells  us  immediately  how  to  evaluate  p(z)   at  the  eighth  roots  of  unity  from these sequences. first, we evaluate  p,(x) and po(x)  at the fourth roots of unity. then  we  substitute  each  of  the  eighth  roots  of  unity  for  x  in  the  equation above,  which  requires  adding  the  appropriate  p,   value  to  the  product  of  the appropriate  p,  value  and  the  eighth  root  of  unity:
p(4) =  p&4,   +  w:po(w:), pm =  p&4  +  w;p&j:), p(4) =  pe(w,“, +  w:po(w.f), pm3  =  p&3 +  w;po(w:), p(4) =  pew2  - w:po(w:), po-4  =  pebi,   - w;p&j~), p(4) =  p&4,   -  w;po(w:), p(4) =  p&3  -  w:pob4
in  general,  to  evaluate  p(x)  on  the  nth  roots  of  unity,  we  recursively  evaluate p,(x)  and  po(x)   on  the  ;nth   roots  of  unity  and  perform  the  n  multiplications as  above.  this  only  works  when  n  is  even,  so  we’ll  assume  from  now  on  that n  is  a  power  of  two,  so  that  it  remains  even  throughout  the  recursion.  the recursion  stops  when  n  =  2  and  we  have  po   +  pix   to  be  evaluated  at  1  and -1,  with  the  results  po   +  pi  and  pc   -pi.
which  has  the  solution  m(n)  =  n  lg  n.  this  is  a  substantial  improvement over  the  straightforward  n2  method  for  interpolation  but,  of  course,  it  works only  at  the  roots  of  unity.
this  gives  a  method  for  transforming  a  polynomial  from  its  representation as  n  coefficients  in  the  conventional  manner  to  its  representation  in  terms  of its  values  at  the  roots  of  unity.  this  conversion  of  the  polynomial  from  the first  representation  to  the  second  is  the  fourier  transform,  and  the  efficient recursive  calculation  procedure  that  we  have  described  is  called  the  “fast” fourier  transform  (fft).  (these  same  techniques  apply  to  more  general functions  than  polynomials.  more  precisely  we’re  doing  the  “discrete”  fourier transform.)
one  “accumulator,” like  horner’s  method.  one  algorithm  involves  scanning the  binary  representation  of  n  from  left  to  right,  starting  with  1  in  the accumulator.  at  each  step,  square  the  accumulator  and  also  multiply  by  z when  there  is  a  1  in  the  binary  representation  of  n.  the  following  sequence of  values  is  computed  by  this  method  for  n  =  55:
another  well-known  alforithm   whks  similarly,  bht   scans  n  from  right  to left.  this  problem  is  a  standard  introductory  programming  exercise,  but  it  is hardly  of  practical  interest.
interpolation the  “inverse”  problem  to  the  problem  of  evaluating  a  polynomial  of  degree  n at  n  points  simultaneously  is  the  problem  of  polynomial  interpolation:  given a  set  of  n  points  x1  ,x2,.  . .  ,xn   and  associated  values  yr,y2,. . .  ,yn,  find  the unique  polynomial  of  degree  n  - 1  which1   has
the  interpolation  problem  is  to  find  the  polynomial,  given  a  set  of  points  and values.  the  evaluation  problem  is  to  find  the  values,  given  the  polynomial and  the  points.  (the  problem  of  finding  the  points,  given  the  polynomial  and the  values,  is  root-finding.)
the  classic  solution  to  the  interpolation  problem  is  given  by  lagrange’s interpolation  formula,  which  is  often  used  as  a  proof  that  a  polynomial  of degree  n  -  1  is  completely  determined  by  n  points:
this  formula  seems  formidable  at  first  but  is  actually  quite  simple.  for example,  the  polynomial  of  degree  2  which  has  p(l)  =  3,  p(2)  =  7,  and  p(3)  = 13 is given by
for x from  xl,  x2,  .  .  .  , xn,   the  formula  is  constructed  so  that  p(xk)  =  yk   for 1  5   k  5   n,  since  the  product  evaluates  to  0  unless  j =  k,  when  it  evaluates
to  1.  in  the  example,  the  last  two  terms  are  0  when  z  =  1,  the  first  and  last terms  are  0  when  x  =  2,  and  the  first  two  terms  are  0  when  x  =  3.
to  convert  a  polynomial  from  the  form  described  by  lagrange’s  formula to  our  standard  coefficient  representation  is  not  at  all  straightforward.  at least  n2   operations  seem  to  be  required,  since  there  are  n  terms  in  the  sum, each  consisting  of  a  product  with  n  factors.  actually,  it  takes  some  cleverness to  achieve  a  quadratic  algorithm,  since  the  factors  are  not  just  numbers,  but polynomials  of  degree  n.  on  the  other  hand,  each  term  is  very  similar  to the  previous  one.  the  reader  might  be  interested  to  discover  how  to  take advantage  of  this  to  achieve  a  quadratic  algorithm.  this  exercise  leaves  one with  an  appreciation  for  the  non-trivial  nature  of  writing  an  efficient  program to  perform  the  calculation  implied  by  a  mathematical  formula.
as  with  polynomial  evaluation,  there  are  more  sophisticated  methods which  can  solve  the  problem  in  n(log  n)2   steps,  and  in  chapter  36  we’ll  see a  method  that  uses  only  n  log  n  multiplications  for  a  specific  set  of  n  points of  interest.
multiplication our  first  sophisticated  arithmetic  algorithm  is  for  the  problem  of  polynomial multiplication:  given  two  polynomials  p(x)  and  q(x),  compute  their  product p(x)q(x).  as  noted  in  chapter  2,  polynomials  of  degree  n  -  1  could  have n  terms  (including  the  constant)  and  the  product  has  degree  2n  -  2  and  as many  as  2n  -  1  terms.  for  example,
the  naive  algorithm  for  this  problem  that  we  implemented  in  chapter  2 requires  n2  multiplications  for  polynomials  of  degree  n  -  1:  each  of  the  n terms  of  p(x)  must  be  multiplied  by  each  of  the  n  terms  of  q(x).
to  improve  on  the  naive  algorithm,  we’ll  use  a  powerful  technique  for algorithm  design  called  divide-and-conquer:  split  the  problem  into  smaller parts,  solve  them  (recursively),  then  put  the  results  back  together  in  some way.  many  of  our  best  algorithms  are  designed  according  to  this  principle. in  this  section  we’ll  see  how  divide-and-conquer  applies  in  particular  to  the polynomial  multiplication  problem.  in  the  following  section  we’ll  look  at  some analysis  which  gives  a  good  estimate  of  how  much  is  saved.
one  way  to  split  a  polynomial  in  two  is  to  divide  the  coefficients  in  half: given  a  polynomial  of  degree  n-l  (with  n  coefficients)  we  can  split  it  into  two polynomials  with  n/2  coefficients  (assume  that  n  is  even):  by  using  the  n/2 low-order  coefficients  for  one  polynomial  and  the  n/2  high-order  coefficients
evaluate  the  input  polynomials  at  2n  -  1  distinct  points. multiply  the  two  values  obtained  at  each  point. interpolate  to  find  the  unique  result  polynomial  that  has  the  given  value at  the  given  points. for  example,  to  compute  t(z)  =  p(x)q(x)  with  p(z)  =  1+x+x2 and  q(x)  =
x - 2 x+1 r(x)  =  24- - - -2+1   - 2 - o   - 2 - l   - 2 - 2 x - 2 x+2 +4----1+2   - 1 - o   - 1 - 1   - 1 - 2 x + 2   x+1  z - 1   x - 2 +2-  ~  -  0+2   0+1   o - 1   o - 2 x + 2   x+1  x - o   x - 2 +6-  -  -  1+2   1sl  1 - o   l - 2 x + 2   x+1  x - o   x - l +28-   -  -  2+2   2fl  2 - o   2 - l ’
as  described  so  far,  this  method  is  not  a  good  algorithm  for  polynomial  multiplication since the best algorithms we have so far for both evaluation (repeated application  of  horner’s  method)  and  interpolation  (lagrange  formula)  require n2  operations.  however,  there  is  some  hope  of  finding  a  better  algorithm  because  the  method  works  for  any  choice  of  2n  -  1  distinct  points  whatsoever, and  it  is  reasonable  to  expect  that  evaluation  and  interpolation  will  be  easier for  some  sets  of  points  than  for  others.
in  general,  suppose  we  wish  our  program  to  be  able  to  compute  r(x)  =  p(x)  + q(x),  where  p  and  q  are  polynomials  with  n  coefficients.  the  following program  is  a  straightforward  implementation  of  polynomial  addition:
readln  (n) ; for  i:=o  to  n-l  do  read(p[i]); for  i:=o  to  n-l  do  read(q[i]); for  i:=o  to  n-j  do  r[i]   :=p[i]+q[i]; for  i:=o  to  n-l  do  write(r[i]); wri teln
in  this  program,  the  polynomial  p(z)   =  pc   +  pix   +  a..  +  pr\r-isn-’   is represented by the  array p [o..n-l] with p [j] = pj, etc. a polynomial of degree n-l  is  defined  by  n  coefficients.  the  input  is  assumed  to  be  n,  followed  by the  p  coefficients,  followed  by  the  q  coefficients.  in  pascal,  we  must  decide ahead  of  time  how  large  n  might  get;  this  program  will  handle  polynomials up  to  degree  100.  obviously,  maxn  should  be  set  to  the  maximum  degree anticipated.  this  is  inconvenient  if  the  program  is  to  be  used  at  different times  for  various  sizes  from  a  wide  range:  many  programming  environments allow  “dynamic  arrays”  which,  in  this  case,  could  be  set  to  the  size  n.  we’ll see  another  technique  for  handling  this  situation  below.
the  program  above  shows  that  addition  is  quite  trivial  once  this  representation  for  polynomials  has  been  chosen;  other  operations  are  also  easily coded.  for  example,  to  multiply  we  can  replace  the  third  for  loop  by
also,  the  declaration  of  r has to be  suita.bly   changed  to  accomodate   twice  as many  coefficients  for  the  product.  each  of  the  n  coefficients  of  p  is  multiplied by  each  of  the  n  coefficients  of  q,  so  this  is  clearly  a  quadratic  algorithm.
an  advantage  of  representing  a  polynomial  by  an  array  containing  its coefficients is that it’s easy to reference any coefficient directly; a disadvantage is  that  space  may  have  to  be  saved  for  more  numbers  than  necessary.  for example,  the  program  above  couldn’t  reasonably  be  used  to  multiply
 and the output only three.   is  to  use  a  linked  list.  this involves  storing  items  in  noncontiguous  memory  locations,  with  each  item containing  the  address  of  the  next.  the  pascal  mechanisms  for  linked  lists  are somewhat  more  complicated  than  for  arrays.  for  example,  the  following  program  computes  the  sum  of  two  polynomials  using  a  linked  list  representation (the  bodies  of  the  readlist   and  add  functions  and  the  writelist  procedure  are given  in  the  text  following):
the  polynomials  are  represented  by  linked  lists  which  are  built  by  the readlist  procedure.  the  format  of  these  is  described  in  the  type  statement: the  lists  are  made  up  of  nodes,  each  node  containing  a  coefficient  and  a  link to  the  next  node  on  the  list.  if  we  have  a  link  to  the  first  node  on  a  list,  then we  can  examine  the  coefficients  in  order,  by  following  links.  the  last  node on  each  list  contains  a  link  to  a  special  (dummy  node  called  a:  if  we  reach  z when  scanning  through  a  list,  we  know  we’re  at  the  end.  (it  is  possible  to  get by  without  such  dummy  nodes,  but  they  do  make  certain  manipulations  on the  lists  somewhat  simpler.)  the  type  statement  only  describes  the  formats of  the  nodes;  nodes  can  be  created  only  when  the  builtin   procedure  new  is called.  for  example,  the  call  new(z)  creates  a  new  node,  putting  a  pointer  to
to  1.  in  the  example,  the  last  two  terms  are  0  when  z  =  1,  the  first  and  last terms  are  0  when  x  =  2,  and  the  first  two  terms  are  0  when  x  =  3.
to  convert  a  polynomial  from  the  form  described  by  lagrange’s  formula to  our  standard  coefficient  representation  is  not  at  all  straightforward.  at least  n2   operations  seem  to  be  required,  since  there  are  n  terms  in  the  sum, each  consisting  of  a  product  with  n  factors.  actually,  it  takes  some  cleverness to  achieve  a  quadratic  algorithm,  since  the  factors  are  not  just  numbers,  but polynomials  of  degree  n.  on  the  other  hand,  each  term  is  very  similar  to the  previous  one.  the  reader  might  be  interested  to  discover  how  to  take advantage  of  this  to  achieve  a  quadratic  algorithm.  this  exercise  leaves  one with  an  appreciation  for  the  non-trivial  nature  of  writing  an  efficient  program to  perform  the  calculation  implied  by  a  mathematical  formula.
as  with  polynomial  evaluation,  there  are  more  sophisticated  methods which  can  solve  the  problem  in  n(log  n)2   steps,  and  in  chapter  36  we’ll  see a  method  that  uses  only  n  log  n  multiplications  for  a  specific  set  of  n  points of  interest.
multiplication our  first  sophisticated  arithmetic  algorithm  is  for  the  problem  of  polynomial multiplication:  given  two  polynomials  p(x)  and  q(x),  compute  their  product p(x)q(x).  as  noted  in  chapter  2,  polynomials  of  degree  n  -  1  could  have n  terms  (including  the  constant)  and  the  product  has  degree  2n  -  2  and  as many  as  2n  -  1  terms.  for  example,
the  naive  algorithm  for  this  problem  that  we  implemented  in  chapter  2 requires  n2  multiplications  for  polynomials  of  degree  n  -  1:  each  of  the  n terms  of  p(x)  must  be  multiplied  by  each  of  the  n  terms  of  q(x).
to  improve  on  the  naive  algorithm,  we’ll  use  a  powerful  technique  for algorithm  design  called  divide-and-conquer:  split  the  problem  into  smaller parts,  solve  them  (recursively),  then  put  the  results  back  together  in  some way.  many  of  our  best  algorithms  are  designed  according  to  this  principle. in  this  section  we’ll  see  how  divide-and-conquer  applies  in  particular  to  the polynomial  multiplication  problem.  in  the  following  section  we’ll  look  at  some analysis  which  gives  a  good  estimate  of  how  much  is  saved.
one  way  to  split  a  polynomial  in  two  is  to  divide  the  coefficients  in  half: given  a  polynomial  of  degree  n-l  (with  n  coefficients)  we  can  split  it  into  two polynomials  with  n/2  coefficients  (assume  that  n  is  even):  by  using  the  n/2 low-order  coefficients  for  one  polynomial  and  the  n/2  high-order  coefficients
for  i:=o   to  n2-i  do  ti[i]:=pl[i]+ph[i]; for i:=o  to n2-1 do t2[i]:=ql[i]+qh[i]; rm:=mult(tl, rl:=mult(pl,   41,  n2); rh:=mult(ph, for i:=o  to n-2 do mult [i] :=rl[i] mult   [n-l]  :=o; for i:=o  to n-2 do mult  [n+i] :=rh  [i] for i:=o to n-2 do
although the above code is a succinct description of this method, it is (unfortunately)  not  a  legal  pascal  program  because  functions  can’t  dynamically  declare arrays.  this  problem  could  be  handled  in  pascal  by  representing  the  polync+ mials  as  linked  lists,  as  we  did  in  chapter  2.  this  program  assumes  that  n  is  a power  of  two,  though  the  details  for  general  n  can  be  worked  out  easily.  the main  complications  are  to  make  sure  that  the  recursion  terminates  properly and  that  the  polynomials  are  divided  properly  when  n  is  odd.
the  same  method  can  be  used  for  multiplying  integers,  though  care  must be  taken  to  treat  “carries”  properly  during  the  subtractions  after  the  recursive calls.
as  with  polynomial  evaluation  and  interpolation,  there  are  sophisticated methods  for  polynomial  multiplication,  and  in  chapter  36  we’ll  see  a  method that  works  in  time  proportional  to  n  log  n.
performed.  otherwise  the  procedure  first  shuffles,  then  recursively  calls  itself to  transform  the  two  halves,  then  combines  the  results  of  these  computations as  described  above.  of  course,  the  actual  values  of  the  complex  roots  of  unity are  needed  to  do  the  implementation.  it  is  well  known  that
these  values  are  easily  computed  using  conventional  trigonometric  functions. in  the  above  program,  the  array  w  is  assumed  to  hold  the  (outn+l)st   roots  of unity.  to  get  the  roots  of  unity  needed,  the  program  selects  from  this  array at  an  interval  determined  by  the  variable  i.  for  example,  if  outhj   were  15, the  fourth  roots  of  unity  would  be  found  in  w[o],   w[4],w[8],  and  w[12].   this eliminates  the  need  to  recompute  roots  of  unity  each  time  they  are  used.
as  mentioned  at  the  outset,  the  scope  of  applicability  of  the  fft  is  far greater  than  can  be  indicated  here;  and  the  algorithm  has  been  intensively used  and  studied  in  a  variety  of  domains. nevertheless,  the  fundamental principles  of  operation  in  more  advanced  applications  are  the  same  as  for  the polynomial  multiplication  problem  that  we’ve  discussed  here.  the  fft  is a  classic  example  of  t.he   application  of  the.  “divide-and-conquer”  algorithm design  paradigm  to  achieve  truly  significant  computational  savings.
multiplication,  and  division  have  a.  very  long  history,  dating  back  to the  origins  of  algorithm  studies  in  the  work  of  the  arabic  mathematician al-khowdrizmi,  with  roots  going  even  further  back  to  the  greeks  and  the babylonians.
  of  many computer  systems  is  their  capability  for  doing  fast,  accurate  numerical  calculations.  computers  have  built-in  capabilities  to  perform  arithmetic  on  integers  and  floating-point  representations  of  real  numbers;  for  example,  pascal allows  numbers  to  be  of  type  integer  or  re;d,   with  all  of  the  normal  arithmetic operations  defined  on  both  types.  algorithms  come  into  play  when  the  operations  must  be  performed  on  more  complicated  mathematical  objects,  such  as polynomials  or  matrices.
in  this  section,  we’ll  look  at  pascal  implementations  of  some  simple algorithms  for  addition  and  multiplication  of  polynomials  and  matrices.  the algorithms  themselves  are  well-known  and  straightforward;  we’ll  be  examining sophisticated  algorithms  for  these  problems  in  chapter  4.  our  main  purpose in  this  section  is  to  get  used  to  treating  th’ese   mathematical  objects  as  objects for  manipulation  by  pascal  programs.  this  translation  from  abstract  data  to something  which  can  be  processed  by  a  computer  is  fundamental  in  algorithm design.  we’ll  see  many  examples  throughout  this  book  in  which  a  proper representation  can  lead  to  an  efficient  algorithm  and  vice  versa.  in  this chapter,  we’ll  use  two  fundamental  ways  of  structuring  data,  the  array  and the  linked list.  these  data  structures  are  used  by  many  of  the  algorithms  in this  book;  in  later  sections  we’ll  study  some  more  advanced  data  structures.
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
gorithm.”  note  that  prim’s  algorithm  takes  time  proportional  to  v2   even for  sparse  graphs  (a  factor  of  about  v2/e   1ogv   slower  than  the  priority-first search  solution,  and  that  the  priority-first  search  solution  is  a  factor  of  1ogv slower  than  prim’s  algorithm  for  dense  graphs.
a  completely  different  approach  to  finding  the  minimum  spanning  tree  is to  simply  add  edges  one  at  a  time,  at  each  step  using  the  shortest  edge  that does  not  form  a  cycle.  this  algorithm  gradually  builds  up  the  tree  one  edge  at a  time  from  disconnected  components,  as  illustrated  in  the  following  sequence of  diagrams  for  our  sample  graph:
the  code  for  this  method  can  be  pieced  together  from  programs  that we’ve  already  seen.  a  priority  queue  is  obviously  the  data  structure  to  use  to consider the edges in order of their weight, and the job of testing for cycles can be  obviously  done  with  union-find  structures.  the  appropriate  data  structure to use for the graph is simply an array edge with one entry for each edge. the indirect  priority  queue  procedures  pqconstruct  and  pqremove  from  chapter  11 can be used to maintain the priority queue, using the  weight fields in the edge array  for  priorities.  also,  the  program  uses  the  findinit  and  fastfind   procedures from  chapter  30.  the  program  simply  prints  out  the  edges  which  comprise the  spanning  tree;  with  slightly  more  work  a  dad  array  or  other  representation could  be  computed:
this  program  assumes  that  the  adjacency  matrix  representation  is  being  used for  the  network.  as  long  as  densepfs  can  find  a  path  which  increases  the flow  (by  the  maximum  amount),  we  trace  back  through  the  path  (using  the dad  array  constructed  by  densepfs)  and  increase  the  how   as  indicated.  if  v remains  unseen  after  some  call  to  densepfs,  then  a  mincut  has  been  found  and the  algorithm  terminates.
for  our  example  network,  the  algorithm  first  increases  the  flow  along  the path  abcf,  then  along  adef,  then  along  abcdef.  no  backwards  edges are  used  in  this  example,  since  the  algorithm  does  not  make  the  unwise  choice adebcf  that  we  used  to  illustrate  the  need  for  backwards  edges.  in  the  next chapter  we’ll  see  a  graph  for  which  this  algorithm  uses  backwards  edges  to find  the  maximum  how.
though  this  algorithm  is  easily  implemented  and  is  likely  to  work  well for  networks  that  arise  in  practice,  the  analysis  of  this  method  is  quite  complicated.  first,  as  usual,  densepfs  requires  v2   steps  in  the  worst  case,  or  we could  use  sparsepfs  to  run  in  time  proportional  to  (e  +  v)  log  v  per  iteration, though  the  algorithm  is  likely  to  run  somewhat  faster  than  this,  since  it  stops when  it  reaches  the  sink.  but  how  many  iterations  are  required?  edmonds and  karp  show  the  worst  case  to  be  1  +  logmimp1 f *  where f  *  is  the  cost of the  flow   and  m  is  the  maximum  number  of  edges  in  a  cut  of  the  network. this  is  certainly  complicated  to  compute,  but  it  is  not  likely  to  be  large  for real  networks.  this  formula  is  included  to  give  an  indication  not  of  how  long the  algorithm  might  take  on  an  actual  network,  but  rather  of  the  complexity of  the  analysis.  actually,  this  problem  has  been  quite  widely  studied,  and complicated  algorithms  with  much  better  worst-case  time  bounds  have  been developed.
the  network  flow   problem  can  be  extended  in  several  ways,  and  many variations  have  been  studied  in  some  detail  because  they  are  important  in
priority  of  a  vertex,  we  simply  assign  the  new  priority  to  the  vaj   entry  for  that vertex.  to  remove  the  highest  priority  vertex,  we  simply  scan  through  the vaj   array  to  find  the  vertex  with  the  largest  negative  (closest  to  0)  vaj   value (then  complement  its  vaj   entry).  after  making  these  mechanical  changes  to the  sparsepfs  program  of  the  previous  chapter,  we  are  left  with  the  following compact  program.
note  that,  the  loop  to  update  the  priorities  and  the  loop  to  find  the  minimum are  combined:  each  time  we  remove  a  vertex  from  the  fringe,  we  pass  through all  the  vertices,  updating  their  priority  if  necessary,  and  keeping  track  of  the minimum  value  found.  (also,  note  that  unseen  must  be  slightly  less  than maxint  since  a  value  one  higher  is  used  as  a  sentinel  to  find  the  minimum, and  the  negative  of  this  value  must  be  representable.)
if  we  use  a[k,  t]  for  priority  in  this  program,  we  get  prim’s  algorithm for  finding  the  minimum  spanning  tree;  if  we  use  vaj[k]+a[k,   t]  for  priority we  get  dijkstra’s  algorithm  for  the  shortest  path  problem.  as  in  chapter 30,  if  we  include  the  code  to  maintain  now  as  the  number  of  vertices  so  far searched  and  use  v-now  for  priority,  we  get  depth-first  search;  if  we  use  now we  get  breadth-first  search.  this  program  differs  from  the  sparsepfs  program of  chapter  30  only  in  the  graph  representation  used  (adjacency  matrix  instead of  adjacency  list)  and  the  priority  queue  implementation  (unordered  array
so  (for  example,  it  might  be  inconvenient  to  have  a  large  contiguous  array). in  a  direct  linked  representation,  links  would  have  to  be  kept  in  each  node pointing  to  the  father  and  both  sons.
it  turns  out  that  the  heap  condition  itself  seems  to  be  too  strong  to  allow efficient  implementation  of  the  join  operation.  the  advanced  data  structures designed  to  solve  this  problem  all  weaken  either  the  heap  or  the  balance condition  in  order  to  gain  the  flexibility  needed  for  the  join.  these  structures allow  all  the  operations  be  completed  in  logarithmic  time.
encountered  is  connected  to  any  vertex  on  the  queue:  that  is,  we’re  entering a  new  connected  component.  this  is  automatically  handled  by  the  priority queue  mechanism,  so  there  is  no  need  for  a  separate  visit  procedure  inside  a main  procedure.  but  note  that  maintaining  the  proper  value  of  now  is  more complicated  than  for  the  recursive  depth-first  search  program  of  the  previous chapter.  the  convention  of  this  program  is  to  leave  the  val  entry  unseen  and zero  for  the  root  of  the  depth-first  search  tree  for  each  connected  component: it  might  be  more  convenient  to  set  it  to  zero  or  some  other  value  (for  example, now)  for  various  applications.
now,  recall  that  now  increases  from  1  to  v  during  the  execution  of  the algorithm  so  it  can  be  used  to  assign  unique  priorities  to  the  vertices.  if  we change the two occurrences of  priority  in  sparsepfs  to  v-now,  we  get  depthfirst  search,  because  newly  encountered  nodes  have  the  highest  priority.  if we  use now for  priority  we  get  breadth-first  search,  because  old  nodes  have the  highest  priority.  these  priority  assignments  make  the  priority  queues operate  like  stacks  and  queues  as  described  above.  (of  course,  if  we  were  only interested  in  using  depth-first  or  breadth-first  search,  we  would  use  a  direct implementation  for  stacks  or  queues,  not  priority  queues  as  in  sparsepfs.) in  the  next  chapter,  we’ll  see  that  other  priority  assignments  lead  to  other classical  graph  algorithms.
the  running  time  for  graph  traversal  when  implemented  in  this  way depends  on  the  method  of  implementing  the  priority  queue.  in  general,  we have to do a priority queue operation for each edge and for each vertex, so the worst  case  running  time  should  be  proportional  to  (e  +  v)  log  v  if  the  priority queue  is  implemented  as  a  heap  as  indicated.  however,  we’ve  already  noted that  for  both  depth-first  and  breadth-first  search  we  can  take  advantage  of the  fact  that  each  new  priority  is  the  highest  or  the  lowest  so  far  encountered to  get  a  running  time  proportional  to  e  +  v.  also,  other  priority  queue implementations  might  sometimes  be  appropriate:  for  example  if  the  graph  is dense  then  we  might  as  well  simply  keep  the  priority  queue  as  an  unordered array.  this  gives  a  worst  case  running  time  proportional  to  e  +  v2   (or  just v2),  since  each  edge  simply  requires  setting  or  resetting  a  priority,  but  each vertex  now  requires  searching  through  the  whole  queue  to  find  the  highest priority  vertex.  an  implementation  which  works  in  this  way  is  given  in  the next  chapter.
the  difference  between  depth-first  and  breadth-first  search  is  quite  evident  when  a  large  graph  is  considered.  the  diagram  at  left  below  shows  the edges  and  nodes  visited  when  depth-first  search  is  halfway  through  the  maze graph  of  the  previous  chapter  starting  at  the  upper  left  corner;  the  diagram at  right  is  the  corresponding  picture  for  breadth-first  search:
(calling  itself  recursively  to  place  2  through  n),   then  generating  the  (n  -  l)! permutations  with  the  1  in  the  second  position,  etc.
because  16! >  250.   still,  it  is  important  to  study  because  it  can  form  the  basis for  a  backtracking  program  to  solve  any  problem  involving  reordering  a  set of  elements.
for  example,  consider  the  euclidean  traveling  salesman  problem:  given a  set  of  n  points  in  the  plane,  find  the  shortest  tour  that  connects  them all.  since  each  ordering  of  the  points  corresponds  to  a  legal  tour,  the  above program  can  be  made  to  exhaustively  search  for  the  solution  to  this  problem simply  by  changing  it  to  keep  track  of  the  cost  of  each  tour  and  the  minimum of  the  costs  of  the  full  tours,  in  the  same  manner  as  above. then  the same  branch-and-bound  technique  as  above  can  be  applied,  as  well  as  various backtracking  heuristics  specific  to  the  euclidean  problem.  (for  example,  it  is easy to prove that the optimal tour cannot cross itself, so the search can be cut off  on  all  partial  paths  that  cross  themselves.)  different  search  heuristics  might correspond  to  different  ways  of  ordering  the  permutations.  such  techniques can  save  an  enormous  amount  of  work  but  always  leave  an  enormous  amount of  work  to  be  done.  it  is  not  at  all  a  simple  matter  to  find  an  exact  solution to  the  euclidean  traveling  salesman  problem,  even  for  n  as  low  as  16.
another  reason  that  permutation  generation  is  of  interest  is  that  there are  a  number  of  related  procedures  for  generating  other  combinatorial  objects. in  some  cases,  the  number  of  objects  generated  are  not  quite  so  numerous  are as  permutations,  and  such  procedures  can  be  useful  for  larger  n  in  practice. an example of this is a procedure to generate all ways of choosing a subset of size  k  out  of  a  set  of  n  items.  for  large  n  and  small  k,  the  number  of  ways of  doing  this  is  roughly  proportional  to n k. such  a  procedure  could  be  used as  the  basis  for  a  backtracking  program  to  solve  the  knapsack  problem.
since  finding  the  shortest  tour  seems  to  require  so  much  computation,  it  is reasonable  to  consider  whether  it  might  be  easier  to  find  a  tour  that  is  almost as  short  as  the  shortest.  if  we’re  willing  to  relax  the  restriction  that  we absolutely  must  have  the  shortest  possible  path,  then  it  turns  out  that  we  can deal  with  problems  much  larger  than  is  possible  with  the  techniques  above. for example, it’s relatively easy to find a tour which is longer by at most a factor of two than the optimal tour. the method is based on simply finding the minimum  spanning  tree:  this  not  only,  as  mentioned  above,  provides  a  lower  bound bound  on  the  length  of  the  tour  but  also  turns  out  to  provide  an  upper on  the  length  of  the  tour,  as  follows.  consider  the  tour  produced  by  visiting the  nodes  of  the  minimum  spanning  tree  using  the  following  procedure:  to
this  program  differs  from  the  description  above  in  two  important  ways.  first,, rather  than  simply  putting  two  subfiles   on  the  stack  in  some  arbitrary  order, their  sizes  are  checked  and  the  larger  of  the  two  is  put  on  the  stack  first. second, the smaller of the two subfiles  is not put on the stack at all; the values of  the  parameters  are  simply  reset,,  just  as  we  did  for  euclid’s  algorithm.  this technique,  called  “end-recursion  removal”  can  be  applied  to  any  procedure whose  last  action  is  a  recursive  call.  for  quicksort,  the  combination  of  endrecursion  removal  and  a  policy  of  processing  the  smaller  of  the  two  subfiles first  turns  out  to  ensure  that  the  stack  need  only  contain  room  for  about,  lg  n entries, since each entry on the stack after the top one must represent a  subfile less  than  half  the  size  of  the  previous  entry.
this  is  in  sharp  contrast  to  the  size  of  the  stack  in  the  worst  case  in  the recursive  implementation,  which  could  be  as  large  as  n  (for  example,  in  the case  that  the  file  is  already  sorted).  this  is  a  subtle  but  real  difficulty  with a  recursive  implementation  of  quicksort:  there’s  always  an  underlying  stack, and  a  degenerate  case  on  a  large  file  could  cause  the  program  to  terminate abnormally  because  of  lack  of  memory.  this  behavior  is  obviously  undesirable for  a  library  sorting  routine.  below  we’ll  see  ways  to  make  degenerate  cases extremely  unlikely,  but,  there’s  no  way  to  avoid  this  problem  completely  in a  recursive  implementation  (even  switching  the  order  in  which    are processed  doesn’t  help,  without  end-recursion  removal).
the  median-of-three  method  helps  quicksort  in  three  ways.  first,  it makes  the  worst  case  much  more  unlikely  to  occur  in  any  actual  sort.  in  order for  the  sort  to  take  n2  time,  two  out  of  the  three  elements  examined  must  be among  the  largest  or  among  the  smallest  elements  in  the  file,  and  this  must happen  consistently  through  most  of  the  partitions.  second,  it  eliminates  the need  for  a  sentinel  key  for  partitioning,  since  this  function  is  served  by  the three  elements  examined  before  partitioning.  third,  it  actually  reduces  the total  running  time  of  the  algorithm  by  about  5%.
the  combination  of  a  nonrecursive  implementation  of  the  median-ofthree  method  with  a  cutoff  for  small  subfiles   can  improve  the  running  time  of quicksort  from  the  naive  recursive  implementation  by  25%  to  30%.  further algorithmic  improvements  are  possible  (for  example  the  median  of  five  or  more elements  could  be  used),  but  the  amount  of  time  saved  will  be  marginal.  more significant  time  savings  can  be  realized  (with  less  effort)  by  coding  the  inner loops  (or  the  whole  program)  in  assembly  or  machine  language.  neither  path is  recommended  except  for  experts  with  serious  sorting  applications.
same  technique  as  used  in  patricia  can  be   used  in  binary  radix  trie  searching to  eliminate  one-way  branching,  but  this   only  exacerbates  the  multiple  node type  problem.
unlike  standard  binary  tree  search,  the  radix  methods  are  insensitive  to the  order  in  which  keys  are  inserted;  thtty   depend  only  upon  the  structure  of the  keys  themselves.  for  patricia  the  pl,icement   of  the  upwards  links  depend on  the  order  of  insertion,  but  the  tree  structure  depends  only  on  the  bits  in the  keys,  as  for  the  other  methods.  this,   even  patricia  would  have  trouble with  a  set  of  keys  like  001,  0001,  00001,  300001,  etc.,  but  for  normal  key  sets, the  tree  should  be  relatively  well-balanced  so  the  number  of  bit  inspections, even  for  very  long  keys,  will  be  roughly  proportional  to  1gn  when  there  are n  nodes  in  the  tree.
the  most  useful  feature  of  radix  trie  searching  is  that  it  can  be  done efficiently  with  keys  of  varying  length.  in  all  of  the  other  searching  methods we  have  seen  the  length  of  the  key  is  “built  into”  the  searching  procedure  in some  way,  so  that  the  running  time  is  dependent   on  the  length  of  the  keys as  well  as  the  number  of  keys.  the  spetific   savings  available  depends  on  the method  of  bit  access  used.  for  example,  suppose  we  have  a  computer  which can  efficiently  access  g-bit   “bytes”  of  tlata,  and  we  have  to  search  among hundreds  of  looo-bit   keys.  then  patricia  would  require  access  of  only  about thl:  search,  plus  one  125-byte  equality 9  or  10  bytes  of  the  search  key  for  comparison  while  hashing  requires  accest   of  all  125-bytes  of  the  search  key  for computing the hash function plus a few  elluality  comparisons, and comparisonbased  methods  require  several  long  comparisons.  this  effect  makes  patricia (or  radix  trie  searching  with  one-way  branching  removed)  the  search  method of choice when very long keys are involved.
the  worst  case  for  trees  built  with  digital  searching  will  be  much  better than  for  binary  search  trees.  the  length  of  the  longest  path  in  a  digital search  tree  is  the  length  of  the  longest  match  in  the  leading  bits  between any  two  keys  in  the  tree,  and  this  is  likely  to  be  relatively  short.  and  it  is obvious  that  no  path  will  ever  be  any  longer  than  the  number  of  bits  in  the keys:  for  example,  a  digital  search  tree  built  from  eight-character  keys  with, say,  six  bits  per  character  will  have  no  path  longer  than  48,  even  if  there are  hundreds  of  thousands  of  keys. for  random  keys,  digital  search  trees are  nearly  perfectly  balanced  (the  height  is  about  1gn).   thus,  they  provide an  attractive  alternative  to  standard  binary  search  trees,  provided  that  bit extraction  can  be  done  as  easily  as  key  comparison  (which  is  not  really  the case  in  pascal).
radix  search  tries it  is  quite  often  the  case  that  search  keys  are  very  long,  perhaps  consisting  of twenty  characters  or  more.  in  such  a  situation,  the  cost  of  comparing  a  search key  for  equality  with  a  key  from  the  data  structure  can  be  a  dominant  cost which  cannot  be  neglected.  digital  tree  searching  uses  such  a  comparison  at each  tree  node:  in  this  section  we’ll  see  that  it  is  possible  to  get  by  with  only one  comparison  per  search  in  most  cases.
the  idea  is  to  not  store  keys  in  tree  nodes  at  all,  but  rather  to  put  all the  keys  in  external  nodes  of  the  tree.  that  is,  instead  of  using  a  for  external nodes  of  the  structure,  we  put  nodes  which  contain  the  search  keys.  thus, we  have  two  types  of  nodes:  internal  nodes,  which  just  contain  links  to  other nodes,  and  external  nodes,  which  contain  keys  and  no  links.  (e.  fredkin
note  that  there  is  some  wasted  space  in  this  tree  because  of  the  large  number of  unused  external  links.  as  m  gets  larger,  this  effect  gets  worse:  it  turns  out that  the  number  of  links  used  is  about  mn/in   m  for  random  keys.  on  the other  hand  this  provides  a  very  efficient  searching  method:  the  running  time is  about  log,  n.  a  reasonable  compromise  can  be  struck  between  the  time efficiency of  multiway  tries and the space efficiency of other methods by using a  “hybrid”  method  with  a  large  value  of  m  at  the  top  (say  the  first  two  levels) and  a  small  value  of  m  (or  some  elementary  method)  at  the  bottom.  again, efficient  implementations  of  such  methods  can  be  quite  complicated  because of  multiple  node  types.
for  example,  a  two-level  32-way  tree  will  divide  the  keys  into  1024  categories,  each  accessible  in  two  steps  down  the  tree.  this  would  be  quite  useful for  files  of  thousands  of  keys,  because  there  are  likely  to  be  (only)  a  few  keys per  category.  on  the  other  hand,  a  smaller  m  would  be  appropriate  for  files of  hundreds  of  keys,  because  otherwise  most  categories  would  be  empty  and too  much  space  would  be  wasted,  and  a  larger  m  would  be  appropriate  for files  with  millions  of  keys,  because  otherwise  most  categories  would  have  too many  keys  and  too  much  time  would  be  wasted.
it  is  amusing  to  note  that  “hybrid”  searching  corresponds  quite  closely to  the  way  humans  search  for  things,  for  example,  names  in  a  telephone book.  the  first  step  is  a  multiway   decision  (“let’s  see,  it  starts  with  ‘a”‘), followed  perhaps  by  some  two  way  decisions  (“it’s  before  ‘andrews’,  but  after ‘aitken”‘)  followed  by  sequential  search  (“  ‘algonquin’  .  .  .  ‘algren’  .  .  .  no, ‘algorithms’  isn’t  listed!“).  of  course  computers  are  likely  to  be  somewhat better  than  humans  at  multiway   search,  so  two  levels  are  appropriate.  also, 26-way  branching  (with  even  more  levels)  is  a  quite  reasonable  alternative to  consider  for  keys  which  are  composed  simply  of  letters  (for  example,  in  a dictionary).
patricia the  radix  trie  searching  method  as  outlined  above  has  two  annoying  flaws: there  is  “one-way  branching”  which  leads  to  the  creation  of  extra  nodes  in  the tree,  and  there  are  two  different  types  of  nodes  in  the  tree,  which  complicates the  code  somewhat  (especially  the  insertion  code).  d.  r.  morrison  discovered a  way  to  avoid  both  of  these  problems  in  a  method  which  he  named  patricia (“practical  algorithm  to  retrieve  information  coded  in  alphanumeric”). the  algorithm  given  below  is  not  in  precisely  the  same  form  as  presented by  morrison,  because  he  was  interested  in  “string  searching”  applications  of the  type  that  we’ll  see  in  chapter  19.  in  the  present  context,  patricia  allows
leading  bit.  this  leads  to  an  immediate  correspondence  with  radix  sorting: binary  trie  searching  partitions  the  file  in  exactly  the  same  way  as  radix exchange  sorting.  (compare  the  trie  above  with  the  partitioning  diagram  we examined  for  radix  exchange  sorting,  after  noting  that  the  keys  are  slightly different.)  this  correspondence  is  analogous  to  that  between  binary  tree searching  and  quicksort.
an  annoying  feature  of  radix  tries  is  the  “one-way”  branching  required  for keys  with  a  large  number  of  bits  in  common,  for  example,  keys  which  differ only  in  the  last  bit  require  a  path  whose  length  is  equal  to  the  key  length,  no matter  how  many  keys  there  are  in  the  tree.  the  number  of  internal  nodes  can be  somewhat  larger  than  the  number  of  keys.  the  height  of  such  trees  is  still limited  by  the  number  of  bits  in  the  keys,  but  we  would  like  to  consider  the possibility  of  processing  records  with  very  long  keys  (say  1000  bits  or  more) which  perhaps  have  some  uniformity,  as  might  occur  in  character  encoded data.  one  way  to  shorten  the  paths  in  the  trees  is  to  use  many  more  than two  links  per  node  (though  this  exacerbates  the  “space”  problem  of  using  too many  nodes);  another  way  is  to  “collapse”  paths  containing  one-way  branches into  single  links.  we’ll  discuss  these  methods  in  the  next  two  sections.
multiway for  radix  sorting,  we  found  that  we  could  get  a  significant  improvement  in speed  by  considering  more  than  one  bit  at  a  time.  the  same  is  true  for  radix searching:  by  examining  m  bits  at  a  time,  we  can  speed  up  the  search  by  a factor of  2m. however,  there’s  a  catch  which  makes  it  necessary  to  be  more careful  applying  this  idea  than  was  necessary  for  radix  sorting.  the  problem is  that  considering  m  bits  at  a  time  corresponds  to  using  tree  nodes  with m  =  2m   links,  which  can  lead  to  a  considerable  amount  of  wasted  space  for unused  links.  for  example,  if  m  =  4  the  following  tree  is  formed  for  our sample  keys:
the  message:  this  means  that  we  need  to  save  the  tree  along  with  the  message in  order  to  decode  it.  fortunately,  this  does  not  present  any  real  difficulty. it  is  actually  necessary  only  to  store  the  code  array,  because  the  radix  search trie  which  results  from  inserting  the  entries  from  that  array  into  an  initially empty  tree  is  the  decoding  tree.
thus,  the  storage  savings  quoted  above  is  not  entirely  accurate,  because the  message  can’t  be  decoded  without  the  trie  and  we  must  take  into  account the  cost  of  storing  the  trie  (i.e., the  code  array)  along  with  the  message. huffman  encoding is therefore only effective for long files where the savings in the message is enough to offset the cost, or in situations where the coding trie can be  precomputed   and  used  for  a  large  number  of  messages.  for  example,  a trie  based  on  the  frequencies  of  occurrence  of  letters  in  the  english  language could  be  used  for  text  documents. for  that  matter,  a  trie  based  on  the frequency  of  occurrence  of  characters  in  pascal  programs  could  be  used  for encoding  programs  (for  example, “;”  is  likely  to  be  near  the  top  of  such  a trie).  a  huffman   encoding  algorithm  saves  about  23%  when  run  on  the  text for  this  chapter.
as  before,  for  truly  random  files,  even  this  clever  encoding  scheme  won’t work  because  each  character  will  occur  approximately  the  same  number  of times,  which  will  lead  to  a  fully  balanced  coding  tree  and  an  equal  number  of bits  per  letter  in  the  code.
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
contain  many  equal  keys.  radix  exchange  sort  is  actually  slightly  faster  than quicksort  if  the  keys  to  be  sorted  are  comprised  of  truly  random  bits,  but quicksort  can  adapt  better  to  less  randon   situations.
straight radix sort an  alternative  radix  sorting  method  is  tc  examine  the  bits  from  right  to  left. this  is  the  method  used  by  old  computer-card-sorting  machines:  a  deck  of cards was run through the machine 80  times, once for each column, proceeding from  right  to  left.  the  following  example  shows  how  a  right-to-left  bit-by-bit radix  sort  works  on  our  file  of  sample  ke:rs.
a 00001 r 10010 s 10011 t 10100 0 01111 n 01110 r 10010 x 11000 p 10000 t 10100 l 01100 i 01001 a 00001 n 01110 g 00111 s 10011 0 01111 e 00101 x 11000 i 01001 a 00001 g 00111 m 01101 e 00101 p 10000 a 00001 l 01100 m 01101 e 00101 e 00101
p 10000 a 00001 a 00001 a 00001 a 00001 e 00101 r 10010 e 00101 s 10011 g 00111 i 01001 t 10100 e 00101 l 01100 e 00101 m 01101 g 00111 n 01110 0 01111 x 11000 i 01001 p 10000 l 01100 r 10010 s 10011 m 01101 t 10100 n 01110 0 01111 x 11000
the  ith  column  in  this  table  is  sorted  on  the  trailing  i  bits  of  the  keys. the  ith  column  is  derived  from  the  (i  -  l$t  column  by  extracting  all  the  keys with  a  0  in  the  ith  bit,  then  all  the  keys  with  a  1  in  the  ith  bit.
it’s  not  easy  to  be  convinced  that  the  method  works;  in  fact  it  doesn’t work  at  all  unless  the  one-bit  partitioning  process  is  stable.  once  stability has  been  identified  as  being  important,  a  trivial  proof  that  the  method  works can  be  found:  after  putting  keys  with  ti,h  bit  0  before  those  with  ith  bit  1 (in  a  stable  manner)  we  know  that  any  l,wo   keys  appear  in  proper  order  (on the  basis  of  the  bits  so  far  examined)  in  the  file  either  because  their  ith  bits are  different,  in  which  case  partitioning  puts  them  in  the  proper  order,  or because  their  ith  bits  are  the  same,  in  which  case  they’re  in  proper  order because  of  stability.  the  requirement  01%  stability  means,  for  example,  that
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
not  particularly  random.  this  leads  to  a  common  and  serious  mistake  in  the use  of  linear  congruential  random  number  generators:  the  following  is  a  bad program  for  producing  random  numbers  in  the  range  [0,  r  -  11:
the  non-random  digits  on  the  right  are  the  only  digits  that  are  used, so  the  resulting  sequence  has  few  of  the  desired  properties.  this  problem  is easily  fixed  by  using  the  digits  on  the  left.  we  want  to  compute  a  number between  0  and  r-l  by  computing  a*r   mod  m,  but,  again,  overflow  must  be circumvented,  as  in  the  following  implementation:
another  common  technique  is  to  generate  random  real  numbers  between 0  and  1  by  treating  the  above  numbers  as  fractions  with  the  decimal  point to  the  left.  this  can  be  implemented  by  simply  returning  the  real  value  a/m rather  than  the  integer  a.  then  a  user  could  get  an  integer  in  the  range  [o,r) by  simply  multiplying  this  value  by  r  and  truncating  to  the  nearest  integer. or,  a  random  real  number  between  0  and  1  might  be  exactly  what  is  needed.
additive  congruential  method another  method  for  generating  random  numbers  is  based  on  linear  feedback shift registers  which  were  used  for  early  cryptographic  encryption  machines. the  idea  is  to  start  with  a  register  filled  with  some  arbitrary  pattern,  then shift  it  right  (say)  a  step  at  a  time,  filling  in  vacated  positions  from  the  left with  a  bit  determined  by  the  contents  of  the  register.  the  diagram  below shows  a  simple  4-bit  register,  with  the  new  bit  taken  as  the  “exclusive  or”  of the  two  rightmost  bits.
much  of  the  material  in  this  section  falls  within  the  domain  of  numerical  analysis,  and  several  excellent  textbooks  are  available.  one  which  pays particular  attention  to  computational  issues  is  the  1977  book  by  forsythe, malcomb  and  moler.  in  particular,  much  of  the  material  given  here  in  chapters 5,  6,  and  7  is  based  on  the  presentation  given  in  that  book.
the  second  major  reference  for  this  section  is  the  second  volume  of  d.  e. knuth’s  comprehensive  treatment  of  “the  art  of  computer  programming.” knuth  uses  the  term  “seminumerical”  to  describe  algorithms  which  lie  at the  interface  between  numerical  and  symbolic  computation,  such  as  random number  generation  and  polynomial  arithmetic.  among  many  other  topics, knuths  volume  2  covers  in  great  depth  the  material  given  here  in  chapters 1,  3,  and  4.  the  1975  book  by  borodin  and  munro  is  an  additional  reference for  strassen’s  matrix  multiplication  method  and  related  topics.  many  of the  algorithms  that  we’ve  considered  (and  many  others,  principally  symbolic methods  as  mentioned  in  chapter  7)  are  embodied  in  a  computer  system  called macsyma,  which  is  regularly  used  for  serious  mathematical  work.
certainly,  a  reader  seeking  more  information  on  mathematical  algorithms should  expect  to  find  the  topics  treated  at  a  much  more  advanced  mathematical  level  in  the  references  than  the  material  we’ve  considered  here.
chapter  2  is  concerned  with  elementary  data  structures,  as  well  as  polynomials.  beyond  the  references  mentioned  in  the  previous  part,  a  reader  interested  in  learning  more  about  this  subject  might  study  how  elementary  data structures  are  handled  in  modern  programming  languages  such  as  ada,  which have  facilities  for  building  abstract  data  structures.
a.  borodin  and  i.  munro,  the  computational  complexity  of  algebraic  and numerical  problems,  american  elsevier,  new  york,  1975. g.  e.  forsythe,  m.  a.  malcomb,  and  c.  b.  moler,  computer  methods  for mathematical  computations,  prentice-hall,  englewood  cliffs,  nj,  1977. d.  e.  knuth,  the  art  of  computer  programming.  volume  &:   seminumerical algorithms,  addison-wesley,  reading,  ma  (second  edition),  1981. mit  mathlab  group,  macsyma  reference  manual,  laboratory  for  computer  science,  massachusetts  institute  of  technology,  1977. p.  wegner,  programming  with  ada:  an  introduction  by  means  of  graduated examples,  prentice-hall,  englewood  cliffs,  nj,  1980.
our  next  set  of  algorithms  will  bie  methods  for  using  a  computer  to generate  random  numbers.  we  will  find  many  uses  for  random  numbers
often,  in  conversation,  people  use  the  term  random  when  they  really mean  arbitrary.  when  one  asks  for  an  trrbitrary   number,  one  is  saying  that one  doesn’t  really  care  what  number  one  gets:  almost  any  number  will  do. by  contrast,  a  random  number  is  a  precisely  defined  mathematical  concept: every  number  should  be  equally  likely  to  occur.  a  random  number  will  satisfy someone  who  needs  an  arbitrary  number,  but  not  the  other  way  around.
for  “every  number  to  be  equally  likely  to  occur”  to  make  sense,  we  must restrict  the  numbers  to  be  used  to  some  finite  domain.  you  can’t  have  a random integer, only a random integer in some range; you can’t have a random real  number,  only  a  random  fraction  in  some  range  to  some  fixed  precision. it  is  almost  always  the  case  that  not  just  one  random  number,  but  a sequence  of  random  numbers  is  needed  (otherwise  an  arbitrary  number  might do).  here’s  where  the  mathematics  comes  in:  it’s  possible  to  prove  many  facts about  properties  of  sequences  of  random  numbers.  for  example,  we  can  expect to  see  each  value  about  the  same  number  of  times  in  a  very  long  sequence of  random  numbers  from  a  small  domain.  random  sequences  model  many natural  situations,  and  a  great  deal  is  known  about  their  properties.  to  be consistent  with  current  usage,  we’ll  refer  to  numbers  from  random  sequences as  random  numbers.
there’s  no  way  to  produce  true  random  numbers  on  a  computer  (or  any deterministic  device).  once  the  program  is  written,  the  numbers  that  it  will produce can be deduced, so how could they be random? the best we can hope to do is to write programs which produce  isequences  of numbers having many of the  same  properties  as  random  numbers.  such  numbers  are  commonly  called pseudo-random  numbers:  they’re  not  really  random,  but  they  can  be  useful
linear  congruential  method  and  105.4  for  the  additive  congruential  method, both  certainly  well  within  20  of  100.  but  for  the  “bad”  generator  which  uses the  rightrhand   bits  from  the  linear  congruential  generator  the  statistic  is  0 (why?)  and  for  a  linear  congruential  method  with  a  bad  multiplier  (101011) the  statistic  is  77.8,  which  is  significantly  out  of  range.
implementation  notes there  are  a  number  of  facilities  commonly  added  to  make  a  random  number generator  useful  for  a  variety  of  applications.  usually,  it  is  desirable  to  set up  the  generator  as  a  function  that  is  initialized  and  then  called  repeatedly, returning  a  different  random  number  each  time.  another  possibility  is  to  call the  random  number  generator  once,  having  it  fill  up  an  array  with  all  the random  numbers  that  will  be  needed  for  a  particular  computation.  in  either case, it is desirable that the generator produce the same sequence on successive calls  (for  initial  debugging  or  comparison  of  programs  on  the  same  inputs)  and produce  an  arbitrary  sequence  (for  later  debugging).  these  facilities  all  involve manipulating  the  “state”  retained  by  the  random  number  generator  between calls.  this  can  be  very  inconvenient  in  some  programming  environments.  the additive  generator  has  the  disadvantage  that  it  has  a  relatively  large  state  (the array  of  recently  produced  words),  but  it  has  the  advantage  of  having  such  a long  cycle  that  it  is  probably  not  necessary  for  each  user  to  initialize  it.
a  conservative  way  to  protect  against  eccentricities  in  a  random  number generator  is  to  combine  two  generators.  (the  use  of  a  linear  congruential generator  to  initialize  the  table  for  an  additive  congruential  generator  is an  elementary  example  of  this.)  an  easy  way  to  implement  a  combination generator  is  to  have  the  first  generator  fill  a  table  and  the  second  choose random  table  positions  to  fetch  numbers  to  output  (and  store  new  numbers from  the  first  generator).
when  debugging  a  program  that  uses  a  random  number  generator,  it  is usually  a  good  idea  to  use  a  trivial  or  degenerate  generator  at  first,  such  as one  which  always  returns  0  or  one  which  returns  numbers  in  order.
as  a  rule,  random  number  generators  are  fragile  and  need  to  be  treated with  respect.  it’s  difficult  to  be  sure  that  a  particular  generator  is  good without  investing  an  enormous  amount  of  effort  in  doing  the  various  statistical tests  that  have  been  devised.  the  moral  is:  do  your  best  to  use  a  good generator,  based  on  the  mathematical  analysis  and  the  experience  of  others; just  to  be  sure,  examine  the  numbers  to  make  sure  that  they  “look”  random; if  anything  goes  wrong,  blame  the  random  number  generator!
our  next  set  of  algorithms  will  bie  methods  for  using  a  computer  to generate  random  numbers.  we  will  find  many  uses  for  random  numbers
often,  in  conversation,  people  use  the  term  random  when  they  really mean  arbitrary.  when  one  asks  for  an  trrbitrary   number,  one  is  saying  that one  doesn’t  really  care  what  number  one  gets:  almost  any  number  will  do. by  contrast,  a  random  number  is  a  precisely  defined  mathematical  concept: every  number  should  be  equally  likely  to  occur.  a  random  number  will  satisfy someone  who  needs  an  arbitrary  number,  but  not  the  other  way  around.
for  “every  number  to  be  equally  likely  to  occur”  to  make  sense,  we  must restrict  the  numbers  to  be  used  to  some  finite  domain.  you  can’t  have  a random integer, only a random integer in some range; you can’t have a random real  number,  only  a  random  fraction  in  some  range  to  some  fixed  precision. it  is  almost  always  the  case  that  not  just  one  random  number,  but  a sequence  of  random  numbers  is  needed  (otherwise  an  arbitrary  number  might do).  here’s  where  the  mathematics  comes  in:  it’s  possible  to  prove  many  facts about  properties  of  sequences  of  random  numbers.  for  example,  we  can  expect to  see  each  value  about  the  same  number  of  times  in  a  very  long  sequence of  random  numbers  from  a  small  domain.  random  sequences  model  many natural  situations,  and  a  great  deal  is  known  about  their  properties.  to  be consistent  with  current  usage,  we’ll  refer  to  numbers  from  random  sequences as  random  numbers.
there’s  no  way  to  produce  true  random  numbers  on  a  computer  (or  any deterministic  device).  once  the  program  is  written,  the  numbers  that  it  will produce can be deduced, so how could they be random? the best we can hope to do is to write programs which produce  isequences  of numbers having many of the  same  properties  as  random  numbers.  such  numbers  are  commonly  called pseudo-random  numbers:  they’re  not  really  random,  but  they  can  be  useful
the  running  time  of  this  program  is  proportional  to  the  number  of  grid  squares touched.  since  we  were  careful  to  arrange  things  so  that  each  grid  square contains  a  constant  number  of  points  on  the  average,  this  is  also  proportional, on  the  average,  to  the  number  of  points  examined.  if  the  number  of  points in  the  search  rectangle  is  r,  then  the  number  of  grid  squares  examined  is proportional  to  r.  the  number  of  grid  squares  examined  which  do  not  fall completely  inside  the  search  rectangle  is  certainly  less  than  a  small  constant times  r,  so  the  total  running  time  (on  the  average)  is  linear  in  r,  the  number of  points  sought.  for  large  r,  the  number  of  points  examined  which  don’t  fall in  the  search  rectangle  gets  quite  small:  all  such  points  fall  in  a  grid  square which  intersects  the  edge  of  the  search  rectangle,  and  the  number  of  such squares  is  proportional  to  fi  for  large  r.  note  that  this  argument  falls apart  if  the  grid  squares  are  too  small  (too  many  empty  grid  squares  inside the  search  rectangle)  or  too  large  (too  many  points  in  grid  squares  on  the perimeter  of  the  search  rectangle)  or  if  the  search  rectangle  is  thinner  than the  grid  squares  (it  could  intersect  many  grid  squares,  but  have  few  points inside  it).
the  grid  method  works  well  if  the  points  are  well  distributed  over  the assumed  range  but  badly  if  they  are  clustered  together.  (for  example,  all the  points  could  fall  in  one  grid  box,  which  would  mean  that  all  the  grid machinery  gained  nothing.)  the  next  method  that  we  will  examine  makes this  worst  case  very  unlikely  by  subdividing  the  space  in  a  nonuniform  way,
geometric  process.  in  three  dimensions,  branching  at  each  node  corresponds to  cutting  the  three-dimensional  region  of  interest  with  a  plane;  in  general  we cut  the  k-dimensional  region  of  interest  with  a  (k-  1)-dimensional  hyperplane. if  k  is  very  large,  there  is  likely  to  be  a  significant  amount  of  imbalance in  the  kd  trees,  again  because  practical  point  sets  can’t  be  large  enough  to take  notice  of  randomness  over  a  large  number  of  dimensions.  typically,  all points in a  subtree  will  have  the  same  value  across  several  dimensions,  which leads  to  several  one-way  branches  in  the  trees.  one  way  to  help  alleviate  this problem  is,  rather  than  simply  cycle  through  the  dimensions,  always  to  use  the dimension  that  will  divide  up  the  point  set  in  the  best  way.  this  technique can  also  be  applied  to  2d  trees.  it  requires  that  extra  information  (which dimension  should  be  discriminated  upon)  be  stored  in  each  node,  but  it  does relieve  imbalance,  especially  in  high-dimensional  trees.
in summary, though it is easy to see how to to generalize the programs for range  searching  that  we  have  developed  to  handle  multidimensional  problems, such a step should not be taken lightly for a large application. large databases with  many  attributes  per  record  can  be  quite  complicated  objects  indeed,  and it  is  often  necessary  to  have  a  good  understanding  of  the  characteristics  of the  database  in  order  to  develop  an  efficient  range-searching  method  for  a particular  application.  this  is  a  quite  important  problem  which  is  still  being activelv   studied.
geometric  process.  in  three  dimensions,  branching  at  each  node  corresponds to  cutting  the  three-dimensional  region  of  interest  with  a  plane;  in  general  we cut  the  k-dimensional  region  of  interest  with  a  (k-  1)-dimensional  hyperplane. if  k  is  very  large,  there  is  likely  to  be  a  significant  amount  of  imbalance in  the  kd  trees,  again  because  practical  point  sets  can’t  be  large  enough  to take  notice  of  randomness  over  a  large  number  of  dimensions.  typically,  all points in a  subtree  will  have  the  same  value  across  several  dimensions,  which leads  to  several  one-way  branches  in  the  trees.  one  way  to  help  alleviate  this problem  is,  rather  than  simply  cycle  through  the  dimensions,  always  to  use  the dimension  that  will  divide  up  the  point  set  in  the  best  way.  this  technique can  also  be  applied  to  2d  trees.  it  requires  that  extra  information  (which dimension  should  be  discriminated  upon)  be  stored  in  each  node,  but  it  does relieve  imbalance,  especially  in  high-dimensional  trees.
in summary, though it is easy to see how to to generalize the programs for range  searching  that  we  have  developed  to  handle  multidimensional  problems, such a step should not be taken lightly for a large application. large databases with  many  attributes  per  record  can  be  quite  complicated  objects  indeed,  and it  is  often  necessary  to  have  a  good  understanding  of  the  characteristics  of the  database  in  order  to  develop  an  efficient  range-searching  method  for  a particular  application.  this  is  a  quite  important  problem  which  is  still  being activelv   studied.
begin txl:=tt.key>=int.xl; tx2:=tt.key<=int.x2; if  txl  then  bstrange(tt   .l,  int); if  txl  and  tx2  then  write(name(tt.id), if  tx2  then  bstrange( tt.r,   int); end
(this  program  could  be  made  slightly  more  efficient  by  maintaining  the  interval  int  as  a  global  variable  rather  than  passing  its  unchanged  values  through the  recursive  calls.)  for  example,  when  called  on  the  interval  [5,9]   for  the  example  tree  above,  range  prints  out  e  c  h  f  i.  note  that  the  points  returned do  not  necessarily  need  to  be  connected  in  the  tree.
these  methods  require  time  proportional  to  about  n  log  n  for  preprocessing,  and  time  proportional  to  about  r+log  n for range, where  r  is the number of  points  actually  falling  in  the  range.  (the  reader  may  wish  to  check  that this  is  true.)  our  goal  in  this  chapter  will  be  to  achieve  these  same  running times  for  multidimensional  range  searching.
the parameter  r  can  be  quite  significant:  given  the  facility  to  make  range queries,  it  is  easy  for  a  user  to  formulate  queries  which  could  require  all  or nearly  all  of  the  points.  this  type  of  query  could  reasonably  occur  in  many applications,  but  sophisticated  algorithms  are  not  necessary  if  all  queries  are of  this  type.  the  algorithms  that  we  consider  are  designed  to  be  efficient  for queries  which  are  not  expected  to  return  a  large  number  of  points.
elementary  methods in  two  dimensions,  our  “range”  is  an  area  in  the  plane.  for  simplicity,  we’ll consider  the  problem  of  finding  all  points  whose  5  coordinates  fall  within  a given  x-interval  and  whose  y  coordinates  fall  within  a  given  y-interval:  that is,  we  seek  all  points  falling  within  a  given  rectangle.  thus,  we’ll  assume  a type  rectangle  which  is  a  record  of  four  integers,  the  horizontal  and  vertical interval  endpoints.  the  basic  operation  that  we’ll  use  is  to  test  whether  a point  falls  within  a  given  rectangle,  so  we’ll  assume  a  function  insiderect(p: point;  rect:  rectangle)  which checks this in the obvious way, returning true if
this  procedure  goes  down  both  subtrees  only  when  the  dividing  line  cuts  the rectangle,  which  should  happen  infrequently  for  relatively  small  rectangles. although  the  method  hasn’t  been  fully  analyzed,  its  running  time  seems  sure to  be  proportional  to  r + log n to retrieve  r  points  from  reasonable  ranges  in a  region  containing  n  points,  which  makes  it  very  competitive  with  the  grid method.
multidimensional  range  searching both  the  grid  method  and  2d  trees  generalize  directly  to  more  than  two  dimensions:  simple,  straightforward  extensions  to  the  above  algorithms  immediately yield  range-searching  methods  which  work  for  more  than  two  dimensions. however,  the  nature  of  multidimensional  space  dictates  that  some  caution  is called  for  and  that  the  performance  characteristics  of  the  algorithms  might be  difficult  to  predict  for  a  particular  application.
to  implement  the  grid  method  for  k-dimensional  searching,  we  simply make  grid  a  k-dimensional  array  and  use  one  index  per  dimension.  the  main problem  is  to  pick  a  reasonable  value  for  size.  this  problem  becomes  quite obvious when large  k  is  considered:  what  type  of  grid  should  we  use  for  lodimensional  search?  the  problem  is  that  even  if  we  use  only  three  divisions per  dimension,  we  need  31°   grid  squares,  most  of  which  will  be  empty,  for reasonable  values  of  n.
the  generalization  from  2d  to  kd  trees  is  also  straightforward:  simply cycle  through  the  dimensions  (as  we  did  for  two  dimensions  by  alternating between  x  and  y)  while  going  down  the  tree.  as  before,  in  a  random  situation, the  resulting  trees  have  the  same  characteristics  as  binary  search  trees.  also as  before,  there  is  a  natural  correspondence  between  the  trees  and  a  simple
the  procedure  to  write  out  what’s  on  a  list  is  the  simplest.  it  simply steps  through  the  list,  writing  out  the  value  of  the  coefficient  in  each  node encountered, until z is found:
building  a  list  involves  first  calling  new  to  create  a  node,  then  filling  in the coefficient, and then linking the node to the end of the partial list built so far.  the  following  function  reads  the  same  format as  before,  and  constructs  the  linked  list  which  represents  the  corresponding polynomial:
the dummy node z is used here to hold the link which points to the first node on  the  list  while  the  list  is  being  constructed.  after  this  list  is  built,  a  is  set to  link  to  itself.  this  ensures  that  once  we  reach  the  end  of  a  list,  we  stay there.  another  convention  which  leave  z pointing to the beginning, to provide a way to get from the back to the front. finally,  the  program  which  adds  two  polynomials  constructs  a  new  list in  a  manner  similar  to  readlist,  calculating  the  coefficients  for  the  result by  stepping  through  the  argument  lists  and  adding  together  corresponding coefficients:
to  introduce  the  general  approach  that  we’ll  be  taking  to  studying algorithms,  we’ll  examine  a  classic  elementary  problem:  “reduce  a  given fraction  to  lowest  terms.” we  want  to  write  213, not  416,  200/300,  or  178468/   to  finding  the  greatest  common 267702.  solving  this  problem  is  equival.ent divisor  (gcd)  of  the  numerator  and  the  denominator:  the  largest  integer  which divides  them  both.  a  fraction  is  reduced  to  lowest  terms  by  dividing  both numerator  and  denominator  by  their  greatest  common  divisor.
pascal a  concise  description  of  the  pascal  language  is  given  in  the  wirth  and  jensen pascal  user manual and  report that serves as the definition for the language. our  purpose  here  is  not  to  repeat  information  from  that  book  but  rather  to examine  the  implementation  of  a  few  simple  algorithms  which  illustrate  some of  the  basic  features  of  the  language  and.  the  style  that  we’ll  be  using.
pascal  has  a  rigorous  high-level  syntax  which  allows  easy  identification  of the  main  features  of  the  program.  the  variables  (var)  and  functions  (function) used  by  the  program  are  declared  first,  f~ollowed   by  the  body  of  the  program. (other  major  program  parts,  not  used  in  the  program  below  which  are  declared before  the  program  body  are  constants  and  types.)  functions  have  the  same format  as  the  main  program  except  that  they  return  a  value,  which  is  set  by assigning  something  to  the  function  name  within  the  body  of  the  function. (functions  that  return  no  value  are  called  procedures.)
the  built-in  function  readln  reads  a.  line  from  the  input  and  assigns  the values found to the variables given as arguments; writeln  is similar. a standard built-in  predicate,  eof,  is  set  to  true  when  there  is  no  more  input.  (input  and output  within  a  line  are  possible  with  read,  write,  and  eoln.)  the  declaration of  input  and  output  in  the  program  statement  indicates  that  the  program  is using  the  “standard”  input  and  output  &reams.
initialize  the  data  structure. search  for  a  record  (or  records)  having  a  given  key. insert  a  new  record. delete  a  specified  record. join  two  dictionaries  to  make  a  large  one. sort  the  dictionary;  output  all  the  records  in  sorted  order.
as  with  priority  queues,  it  is  sometimes  convenient  to  combine  some  of  these operations.  for  example,  a  search  and  insert  operation  is  often  included  for efficiency  in  situations  where  records  with  duplicate  keys  are  not  to  be  kept within  the  data  structure.  in  many  methods,  once  it  has  been  determined that  a  key  does  not  appear  in  the  data  structure,  then  the  internal  state  of the  search  procedure  contains  precisely  the  information  needed  to  insert  a  new record  with  the  given  key.
records  with  duplicate  keys  can  be  handled  in  one  of  several  ways, depending  on  the  application.  first,  we  could  insist  that  the  primary  searching data  structure  contain  only  records  with  distinct  keys.  then  each  “record”  in this  data  structure  might  contain,  for  example,  a  link  to  a  list  of  all  records having  that  key.  this  is  the  most  convenient  arrangement  from  the  point of  view  of  the  design  of  searching  algorithms,  and  it  is  convenient  in  some applications  since  all  records  with  a  given  search  key  are  returned  with  one search. the  second  possibility  is  to  leave  records  with  equal  keys  in  the primary  searching  data  structure  and  return  any  record  with  the  given  key for  a  search.  this  is  simpler  for  applications  that  process  one  record  at  a time,  where  the  order  in  which  records  with  duplicate  keys  are  processed  is not  important.  it  is  inconvenient  from  the  algorithm  design  point  of  view because  some  mechanism  for  retrieving  all  records  with  a  given  key  must  still be  provided.  a  third  possibility  is  to  assume  that  each  record  has  a  unique identifier  (apart  from  the  key),  and  require  that  a search  find  the  record  with a  given  identifier,  given  the  key.  or,  some  more  complicated  mechanism  could be  used  to  distinguish  among  records  with  equal  keys.
each  of  the  fundamental  operations  listed  above  has  important  applications,  and  quite  a  large  number  of  basic  organizations  have  been  suggested  to support efficient use of various combinations of the operations. in this and the next  few  chapters,  we’ll  concentrate  on  implementations  of  the  fundamental functions  search  and  insert  (and, of course,  initialize),  with  some  comment  on delete  and  sort  when  appropriate.  as  with  priority  queues,  the  join  operation normally  requires  advanced  techniques  which  we  won’t  be  able  to  consider here.
small  index  to  each  key  before  sorting  or  5y  lengthening  the  sort  key  in  some other  way.  it  is  easy  to  take  stability  for  granted:  people  often  react  to  the unpleasant  effects  of  instability  with  disbelief.  actually  there  are  few  methods which  achieve  stability  without  using  significant  extra  time  or  space.
the  following  program,  for  sorting  three  records,  is  intended  to  illustrate the  general  conventions  that  we’ll  be  using.  (in  particular,  the  main  program  is a  peculiar  way  to  exercise  a  program  that  is  known  to  work  only  for  n  =  3:  the point  is  that  most  of  the  sorting  programs  we’ll  consider  could  be  substituted for  sort3  in  this  “driver”  program.)
the  three  assignment  statements  following  each  if  actually  implement  an “exchange”  operation.  we’ll  write  out  the  code  for  such  exchanges  rather  than use  a  procedure  call  because  they’re  fundamental  to  many  sorting  programs and  often  fall  in  the  inner  loop.
in  order  to  concentrate  on  algorithmjc   issues,  we’ll  work  with  algorithms that  simply  sort  arrays  of  integers  into  numerical  order.  it  is  generally  straightforward  to  adapt  such  algorithms  for  use  in  a  practical  application  involving large  keys  or  records.  basically,  sorting  programs  access  records  in  one  of  two ways:  either  keys  are  accessed  for  comparison,  or  entire  records  are  accessed
to  be  moved.  most  of  the  algorithms  that  we  will  study  can  be  recast  in  terms of  performing  these  two  operations  on  arbitrary  records.  if  the  records  to  be sorted  are  large,  it  is  normally  wise  to  do  an  “indirect  sort”:  here  the  records themselves  are  not  necessarily  rearranged,  but  rather  an  array  of  pointers  (or indices)  is  rearranged  so  that  the  first  pointer  points  to  the  smallest  record, etc.  the  keys  can  be  kept  either  with  the  records  (if  they  are  large)  or  with the  pointers  (if  they  are  small).
by  using  programs  which  simply  operate  on  a  global  array,  we’re  ignoring “packaging  problems”  that  can  be  troublesome  in  some  programming  environments.  should  the  array  be  passed  to  the  sorting  routine  as  a  parameter? can  the  same  sorting  routine  be  used  to  sort  arrays  of  integers  and  arrays of  reals  (and  arrays  of  arbitrarily  complex  records)?  even  with  our  simple assumptions,  we  must  (as  usual)  circumvent  the  lack  of  dynamic  array  sizes in  pascal  by  predeclaring  a  maximum.  such  concerns  will  be  easier  to  deal with  in  programming  environments  of  the  future  than  in  those  of  the  past and  present.  for  example,  some  modern  languages  have  quite  well-developed facilities  for  packaging  together  programs  into  large  systems.  on  the  other hand,  such  mechanisms  are  not  truly  required  for  many  applications:  small programs  which  work  directly  on  global  arrays  have  many  uses;  and  some operating  systems  make  it  quite  easy  to  put  together  simple  programs  like the  one  above,  which  serve  as  “filters”  between  their  input  and  their  output. obviously,  these  comments  apply  to  many  of  the  other  algorithms  that  we’ll be  examining,  though  their  effects  are  perhaps  most  acutely  felt  for  sorting algorithms.
some  of  the  programs  use  a  few  other  global  variables.  declarations which  are  not  obvious  will  be  included  with  the  program  code.  also,  we’ll sometimes  assume  that  the  array  bounds  go  to  0  or  iv+1,  to  hold  special  keys used  by  some  of  the  algorithms.  we’ll  frequently  use  letters  from  the  alphabet rather  than  numbers  for  examples:  these  are  handled  in  the  obvious  way  using pascal’s  ord  and  chr   “transfer  functions”  between  integers  and  characters.
the sort3 program above uses an even more constrained access to the file: it  is  three  instructions  of  the  form  “compare  two  records  and  exchange  them if  necessary  to  put  the  one  with  the  smaller  key  first.”  programs  which  use only  this  type  of  instruction  are  interesting  because  they  are  well  suited  for hardware  implementation.  we’ll  study  this  issue  in  more  detail  in  chapter 35.
selection  sort one  of  the  simplest  sorting  algorithms  works  as  follows:  first  find  the  smallest element  in  the  array  and  exchange  it  with  the  element  in  the  first  position, then  find  the  second  smallest  element  and  exchange  it  with  the  element  in
exactly  the  same  as  the  remainder  left  after  dividing  u  by  v,  which  is  what the  mod  function  computes:  the  greatee:t   common  divisor  of  u  and  v  is  the same as the greatest common divisor of  1) and u mod v. if  u mod v is 0, then v divides  u  exactly  and  is  itself  their  greatest  common  divisor,  so  we  are  done. this  mathematical  description  explains  how  to  compute  the  greatest common  divisor  of  two  numbers  by  computing  the  greatest  common  divisor of  two  smaller  numbers.  we  can  implement  this  method  directly  in  pascal simply  by  having  the  gcd  function  call  itself  with  smaller  arguments:
(note  that  if  u  is  less  than  v,  then  u  m’od  v  is  just  u,  and  the  recursive  call just  exchanges  u  and  v  so  things  work  as  described  the  next  time  around.) if  the  two  inputs  are  461952  and  116298,  then  the  following  table  shows  the values  of  u  and  v  each  time  gcd  is  invoked:
recursion a  fundamental  technique  in  the  design  of  efficient  algorithms  is  recursion: solving  a  problem  by  solving  smaller  versions  of  the  same  problem,  as  in  the program  above.  we’ll  see  this  general  approach  used  throughout  this  book, and  we  will  encounter  recursion  many  tirnes.  it  is  important,  therefore,  for  us to  take  a  close  look  at  the  features  of  the  above  elementary  recursive  program. an  essential  feature  is  that  a  recursive  program  must  have  a  termination condition.  it  can’t  always  call  itself,  there  must  be  some  way  for  it  to  do
something  else.  this  seems  an  obvious  point  when  stated,  but  it’s  probably the  most  common  mistake  in  recursive  programming.  for  similar  reasons,  one shouldn’t  make  a  recursive  call  for  a  larger  problem,  since  that  might  lead  to a  loop  in  which  the  program  attempts  to  solve  larger  and  larger  problems.
not  all  programming  environments  support  a  general-purpose  recursion facility  because  of  intrinsic  difficulties  involved.  furthermore,  when  recursion is provided and used, it can be a source of unacceptable inefficiency. for these reasons,  we  often  consider  ways  of  removing  recursion.  this  is  quite  easy  to do when there is only one recursive call involved, as in the function above. we simply  replace  the  recursive  call  with  a  goto   to  the  beginning,  after  inserting some  assignment  statements  to  reset  the  values  of  the  parameters  as  directed by  the  recursive  call.  after  cleaning  up  the  program  left  by  these  mechanical transformations,  we  have  the  following  implementation  of  euclid’s  algorithm:
recursion  removal  is  much  more  complicated  when  there  is  more  than one  recursive  call.  the  algorithm  produced  is  sometimes  not  recognizable,  and indeed  is  very  often  useful  as  a  di.fferent   way  of  looking  at  a  fundamental  algorithm.  removing  recursion  almost  always  gives  a  more  efficient  implementation.  we’ll  see  many  examples  of  this  later  on  in  the  book.
analysis of algorithms in  this  short  chapter  we’ve  already  seen  three  different  algorithms  for  the  same problem;  for  most  problems  there  are  many  different  available  algorithms. how  is  one  to  choose  the  best  implementation  from  all  those  available?
this  is  actually  a  well  developed  area  of  study  in  computer  science. frequently,  we’ll  have  occasion  to  call  on  research  results  describing  the  performance  of  fundamental  algorithms.  however,  comparing  algorithms  can  be challenging  indeed,  and  certain  general  guidelines  will  be  useful.
usually  the  problems  that  we  solve  have  a  natural  “size”  (usually  the amount  of  data  to  be  processed;  in  the  above  example  the  magnitude  of the  numbers)  which  we’ll  normally  call  n.  we  would  like  to  know  the resources  used  (most  often  the  amount  of  time  taken)  as  a  function  of  n. we’re  interested  in  the  average  case,  the  amount  of  time  a  program  might  be
each  node  in  this  tree  represents  a  vertical  line  dividing  the  points  in  the  left and  right  subtree.  the  nodes  are  numbered  in  the  order  in  which  the  vertical lines  are  tried  in  the  algorithm.  thus,  first  the  line  between  g  and  0  is  tried and  the  pair  go  is  retained  as  the  closest  so  far.  then  the  line  between  a  and d is tried, but a and d are too far apart to change  min. then the line between 0 and a is tried and the pairs gd  g-4 and oa all are successively closer pairs. it  happens  for  this  example  that  no  closer  pairs  are  found  until  fk,  which  is the  last  pair  checked  for  the  last  dividing  line  tried.  this  diagram  reflects  the difference  between  top-down  and  bottom-up  mergesort.  a  bottom-up  version of the closest-pair problem can be developed in the same way as for mergesort, which  would  be  described  by  a  tree  like  the  one  above,  numbered  left  to  right and  bottom  to  top.
the  general  approach  that  we’ve  used  for  the  closest-pair  problem  can be  used  to  solve  other  geometric  problems.  for  example,  another  problem  of interest  is  the  all-nearest-neighbors  problem:  for  each  point  we  want  to  find the  point  nearest  to  it.  this  problem  can  be  solved  using  a  program  like  the one  above  with  extra  processing  along  the  dividing  line  to  find,  for  each  point, whether  there  is  a  point  on  the  other  side  closer  than  its  closest  point  on  its own  side.  again,  the  “free”  y  sort  is  helpful  for  this  computation.
voronoi diagrams the set of all points closer to a given point in a point set than to all other points in  the  set  is  an  interesting  geometric  structure  called  the  voronoi polygon  for the  point.  the  union  of  all  the  voronoi  polygons  for  a  point  set  is  called  its voronoi  diagram.  this  is  the  ultimate  in  closest-point  computations:  we’ll  see that  most  of  the  problems  involving  distances  between  points  that  we  face have  natural  and  interesting  solutions  based  on  the  voronoi  diagram.  the diagram  for  our  sample  point  set  is  comprised  of  the  thick  lines  in  the  diagram below:
(calling  itself  recursively  to  place  2  through  n),   then  generating  the  (n  -  l)! permutations  with  the  1  in  the  second  position,  etc.
because  16! >  250.   still,  it  is  important  to  study  because  it  can  form  the  basis for  a  backtracking  program  to  solve  any  problem  involving  reordering  a  set of  elements.
for  example,  consider  the  euclidean  traveling  salesman  problem:  given a  set  of  n  points  in  the  plane,  find  the  shortest  tour  that  connects  them all.  since  each  ordering  of  the  points  corresponds  to  a  legal  tour,  the  above program  can  be  made  to  exhaustively  search  for  the  solution  to  this  problem simply  by  changing  it  to  keep  track  of  the  cost  of  each  tour  and  the  minimum of  the  costs  of  the  full  tours,  in  the  same  manner  as  above. then  the same  branch-and-bound  technique  as  above  can  be  applied,  as  well  as  various backtracking  heuristics  specific  to  the  euclidean  problem.  (for  example,  it  is easy to prove that the optimal tour cannot cross itself, so the search can be cut off  on  all  partial  paths  that  cross  themselves.)  different  search  heuristics  might correspond  to  different  ways  of  ordering  the  permutations.  such  techniques can  save  an  enormous  amount  of  work  but  always  leave  an  enormous  amount of  work  to  be  done.  it  is  not  at  all  a  simple  matter  to  find  an  exact  solution to  the  euclidean  traveling  salesman  problem,  even  for  n  as  low  as  16.
another  reason  that  permutation  generation  is  of  interest  is  that  there are  a  number  of  related  procedures  for  generating  other  combinatorial  objects. in  some  cases,  the  number  of  objects  generated  are  not  quite  so  numerous  are as  permutations,  and  such  procedures  can  be  useful  for  larger  n  in  practice. an example of this is a procedure to generate all ways of choosing a subset of size  k  out  of  a  set  of  n  items.  for  large  n  and  small  k,  the  number  of  ways of  doing  this  is  roughly  proportional  to n k. such  a  procedure  could  be  used as  the  basis  for  a  backtracking  program  to  solve  the  knapsack  problem.
since  finding  the  shortest  tour  seems  to  require  so  much  computation,  it  is reasonable  to  consider  whether  it  might  be  easier  to  find  a  tour  that  is  almost as  short  as  the  shortest.  if  we’re  willing  to  relax  the  restriction  that  we absolutely  must  have  the  shortest  possible  path,  then  it  turns  out  that  we  can deal  with  problems  much  larger  than  is  possible  with  the  techniques  above. for example, it’s relatively easy to find a tour which is longer by at most a factor of two than the optimal tour. the method is based on simply finding the minimum  spanning  tree:  this  not  only,  as  mentioned  above,  provides  a  lower  bound bound  on  the  length  of  the  tour  but  also  turns  out  to  provide  an  upper on  the  length  of  the  tour,  as  follows.  consider  the  tour  produced  by  visiting the  nodes  of  the  minimum  spanning  tree  using  the  following  procedure:  to
the  simple  use  of  an  explicit  stack  above  leads  to  a  far  more  efficient program  than  the  direct  recursive  implementation,  but  there  is  still  overhead that  could  be  removed.  the  problem  is  that,  if  both  subfiles   have  only  one element,  entries  with  r-1  are  put  on  the  stack  only  to  be  immediately  taken off  and  discarded.  it  is  straightforward  to  change  the  program  to  simply  not put  any  such  files  on  the  stack.  this  change  is  more  important  when  the  next improvement  is  included,  which  involves  ignoring  small  subfiles   in  the  same way.
we  use  the  identical  partitioning  procedure  to  quicksort:  as  with  quicksort, this  could  be  changed  slightly  if  many  equal  keys  are  expected.  note  that  in this  non-recursive  program,  we’ve  eliminated  the  simple  calculations  involving k.
this  method  has  about  the  same  worst  case  as  quicksort:  using  it  to find  the  smallest  element  in  an  already  sorted  file  would  result  in  a  quadratic running  time.  it  is  probably  worthwhile  to  use  an  arbitrary  or  a  random partitioning  element  (but  not  the  median-of-three:  for  example,  if  the  smallest element  is  sought,  we  probably  don’t  want  the  file  split  near  the  middle).  the average  running  time  is  proportional  to  about  n  +  iclog(n/k),   which  is  linear for  any  allowed  value  of  k.
it is possible to modify this quicksort-based selection procedure so that its running  time  is  guaranteed  to  be  linear.  these  modifications,  while  important from  a  theoretical  standpoint,  are  extremely  complex  and  not  at  all  practical.
merging it  is  common  in  many  data  processing  environments  to  maintain  a  large (sorted)  data  file  to  which  new  entries  are  regularly  added.  typically,  a number  of  new  entries  are  “batched,”  appended  to  the  (much  larger)  main file,  and  the  whole  thing  resorted.  this  situation  is  tailor-made  for  merging:  a much  better  strategy  is  to  sort  the  (small)  batch  of  new  entries,  then  merge  it with  the  large  main  file.  merging  has  many  other  similar  applications  which
as  with  sequential  list  searching,  the  coding  in  this  program  is  simplified by  the  use  of  a  “tail”  node  z.  similarly,  the  insertion  code  given  below  is simplified  by  the  use  of  a  tree  header  node  head  whose  right  link  points  to  the root.  to  search  for  a  record  with  key  v  we  set  x:=  treesearch(v,  head).
if  a  node  has  no  left  (right)  subtree  then  its  left  (right)  link  is  set  to point  to  z.  as  in  sequential  search,  we  put  the  value  sought  in  a  to  stop an  unsuccessful  search.  thus,  the  “current  subtree”   pointed  to  by  x  never becomes  empty  and  all  searches  are  “successful”  :  the  calling  program  can check  whether  the  link  returned  points  to  a  to  determine  whether  the  search was successful. it is sometimes convenient to think of links which point to z as pointing  to  imaginary  external  nodes  with  all  unsuccessful  searches  ending  at external  nodes.  the  normal  nodes  which  cont,ain   our  keys  are  called  internal nodes;  by  introducing  external  nodes  we  can  say  that  every  internal  node points  to  two  other  nodes  in  the  tree,  even  though,  in  our  implementation,  all of  the  external  nodes  are  represented  by  the  single  node  z.
for  example,  if  d  is  sought  in  the  tree  above,  first  it  is  compared  against e,  the  key  at  the  root.  since  d  is  less,  it  is  next  compared  against  a,  the  key in the  left  son of the node containing e. continuing in this way, d is compared next  against  the  c  to  the  right  of  that  node.  the  links  in  the  node  containing c  are  pointers  to  z  so  the  search  terminates  with  d  being  compared  to  itself in  z  and  the  search  is  unsuccessful.
to  insert  a  node  into  the  tree,  we  just  do  an  unsuccessful  search  for  it, then  hook  it  on  in  place  of  z  at  the  point  where  the  search  terminated,  as  in the  following  code:
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
by a simple path in only one of two ways (corresponding to the truth or falsity of  the  variables).  these  small  components  are  attached  together  as  specified by  the  clauses,  using  more  complicated  subgraphs  which  can  be  traversed  by simple  paths  corresponding  to  the  truth  or  falsity  of  the  clauses.  it  is  quite a  large  step  from  this  brief  description  to  the  full  construction:  the  point is  to  illustrate  that  polynomial  reduction  can  be  applied  to  quite  dissimilar problems.
thus,  if  we  were  to  have  a  polynomial-time  algorithm  for  the  traveling salesman  problem,  then  we  would  have  a  polynomial-time  algorithm  for  the hamilton  cycle  problem,  which  would  also  give  us  a  polynomial-time  algorithm for  the  satisfiability  problem. each  problem  that  is  proven  np-complete provides  another  potential  basis  for  proving  yet  another  future  problem  npcomplete.  the  proof  might  be  as  simple  as  the  reduction  given  above  from  the hamilton  cycle  problem  to  the  traveling  salesman  problem,  or  as  complicated as  the  transformation  sketched  above  from  the  satisfiability  problem  to  the hamilton  cycle  problem,  or  somewhere  in  between.  literally  thousands  of problems  have  been  proven  to  be  np-complete  over  the  last  ten  years  by transforming  one  to  another  in  this  way.
cook’s  theorem reduction  uses  the  np-completeness  of  one  problem  to  imply  the  np-completeness  of  another.  there  is  one  case  where  it  doesn’t  apply:  how  was  the first   problem  proven  to  be  np-complete?  this  was  done  by  s.  a.  cook  in 1971.  cook  gave  a  direct  proof  that  satisfiability  is  np-complete:  that  if there  is  a  polynomial  time  algorithm  for  satisfiability,  then  all  problems  in np  can  be  solved  in  polynomial  time.
the  proof  is  extremely  complicated  but  the  general  method  can  be  explained.  first,  a  full  mathematical  definition  of  a  machine  capable  of  solving any  problem  in  np  is  developed.  this  is  a  simple  model  of  a  general-purpose computer  known  as  a  turing  machine  which  can  read  inputs,  perform  certain operations,  and  write  outputs.  a  turing  machine  can  perform  any  computation  that  any  other  general  purpose  computer  can,  using  the  same  amount  of time  (to  within  a  polynomial  factor),  and  it  has  the  additional  advantage  that it  can  be  concisely  described  mathematically.  endowed  with  the  additional power  of  nondeterminism,  a  turing  machine  can  solve  any  problem  in  np. the  next  step  in  the  proof  is  to  describe  each  feature  of  the  machine,  including  the  way  that  instructions  are  executed,  in  terms  of  logical  formulas  such as  appear  in  the  satisfiability  problem.  in  this  way  a  correspondence  is  established  between  every  problem  in  np  (which  can  be  expressed  as  a  program  on the  nondeterministic  turing  machine)  and  some  instance  of  satisfiability  (the translation  of  that  program  into  a  logical  formula).  now,  the  solution  to  the satisfiability  problem  essentially  corresponds  t,o  a  simulation  of  the  machine
it  can  be  shown  that,  if  the  keys  a-e  random,  the  runs  produced  using replacement selection are about twice the size of what could be produced using an  internal  method.  the  practical  effect  of  this  is  to  save  one  merging  pass: rather  than  starting  with  sorted  runs  about  the  size  of  the  internal  memory and  then  taking  a  merging  pass  to  produce  runs  about  twice  the  size  of  the internal  memory,  we  can  start  right  of’   with  runs  about  twice  the  size  of the  internal  memory,  by  using  replacement  selection  with  a  priority  queue  of size  m.  if  there  is  some  order  in  the  keys,  then  the  runs  will  be  much,  much longer.  for  example,  if  no  key  has  more  than  m  larger  keys  before  it  in  the file,  the  file  will  be  completely  sorted  by  the  replacement  selection  pass,  and no  merging  will  be  necessary!  this  is  the  most  important  practical  reason  to use  the  method.
in  summary,  the  replacement  seleclion  technique  can  be  used  for  both the  “sort”  and  the  “merge”  steps  of  a  balanced  multiway   merge.  to  sort  n l-word  records  using  an  internal  memo  y  of  size  m  and  p  +  1  tapes,  first use  replacement  selection  with  a  priority  queue  of  size  m  to  produce  initial runs  of  size  about  2m  (in  a  random  situation)  or  longer  (if  the  file  is  partially ordered)  then  use  replacement  selection  with  a  priority  queue  of  size  p  for about  log,(n/2m)   (or  fewer)  merge  passes.
practical considerations to  complete  an  implementation  of  the  llorting  method  outlined  above,  it  is necessary  to  implement  the  inputroutput   functions  which  actually  transfer data  between  the  processor  and  the  ex;ernal   devices.  these  functions  are obviously  the  key  to  good  performance  for  the  external  sort,  and  they  just as  obviously  require  careful  consideratisw  of  some  systems  (as  opposed  to algorithm)  issues.  (readers  unfamiliar  with  computers  at  the  “systems”  level may  wish  to  skim  the  next  few  paragraphs.)
a  major  goal  in  the  implementation  should  be  to  overlap  reading,  writing, and  computing  as  much  as  possible. most  large  computer  systems  have independent  processing  units  for  controlling  the  large-scale  input/output  (i/o) devices  which  make  this  overlapping  pos:#ible.   the  efficiency  achievable  by  an external  sorting  method  depends  on  the  number  of  such  devices  available.
for  each  file  being  read  or  written,  there  is  a  standard  systems  programming  technique  called  double-buffering  which  can  be  used  to  maximize  the overlap  of  i/o  with  computing.  the  idl:a   is  to  maintain  two  “buffers,”  one for  use  by  the  main  processor,  one  for  us:  by  the  i/o  device  (or  the  processor which  controls  the  i/o  device).  for  input,  the  processor  uses  one  buffer  while the  input  device  is  filling  the  other.  when  the  processor  has  finished  using its  buffer,  it  waits  until  the  input  device  has  filled  its  buffer,  then  the  buffers switch  roles:  the  processor  uses  the  near   data  in  the  just-filled  buffer  while
the  defining  property  of  a  tree  is  that  every  node  is  pointed  to  by  only one  other  node  called  its  father. (we  assume  the  existence  of  an  imaginary node  which  points  to  the  root.)  the  defining  property  of  a  binary  tree  is  that each  node  has  left  and  right  links.  for  s:arching,   each  node  also  has  a  record with  a  key  value;  in  a  binary  search tree  we  insist  that  all  records  with  smaller keys  are  in  the  left  subtree  and  that  i.11  records  in  the  right  subtree  have larger  (or  equal)  key  values.  we’ll  soon  see  that  it  is  quite  simple  to  ensure that  binary  search  trees  built  by  successively  inserting  new  nodes  satisfy  this defining  property.
structure.  to  find  a  record  with  a  give  root.  if  it  is  smaller,  go  to  the  left  si   btree;  if  it  is  equal,  stop;  and  if  it is  greater,  go  to  the  right  subtree. apljy   the  method  recursively.  at  each step,  we’re  guaranteed  that  no  parts  of  tlie   tree  other  than  the  current  subtree could  contain  records  with  key  v,  and,  just  as  the  size  of  the  interval  in  binary   gets  smaller.  the  procedure  stops search  shrinks,  the  “current  subtree” either  when  a  record  with  key  v  is  founcl   or,  if  there  is  no  such  record,  when the  “current  subtree”   becomes  empty.  (the  words  “binary,”  “search,”  and “tree”  are  admittedly  somewhat  overuse,1   at  this  point,  and  the  reader  should be  sure  to  understand  the  difference  betlveen   the  binarysearch  function  given above  and  the  binary  search  trees  described  here.  above,  we  used  a  binary tree  to  describe  the  sequence  of  comparisons  made  by  a  function  searching in  an  array;  here  we  actually  construct 2.  data  structure  of  records  connected with  links  which  is  used  for  the  search.)
values  (using  some  non-existent  weight  to  represent  false),  or  we  include  a field  for  the  edge  weight  in  adjacency  list  records  in  the  adjacency  structure.
it  is  often  necessary  to  associate  other  information  with  the  vertices or  nodes  of  a  graph  to  allow  it  to  model  more  complicated  objects  or  to save  bookkeeping  information  in  complicated  algorithms.  extra  information associated  with  each  vertex  can  be  accommodated  by  using  auxiliary  arrays indexed  by  vertex  number  (or  by  making  adj  an  array  of  records  in  the adjacency  structure  representation).  extra  information  associated  with  each edge  can  be  put  in  the  adjacency  list  nodes  (or  in  an  array  a  of  records  in the  adjacency  matrix  representation),  or  in  auxiliary  arrays  indexed  by  edge number  (this  requires  numbering  the  edges).
at  the  beginning  of  this  chapter,  we  saw  several  natural  questions  that  arise immediately  when  processing  a  graph.  is  the  graph  connected?  if  not,  what are  its  connected  components?  does  the  graph  have  a  cycle?  these  and  many other  problems  can  be  easily  solved  with  a  technique  called  depth-first search, which is a natural way to “visit” every node and check every edge in the graph systematically.  we’ll  see  in  the  chapters  that  follow  that  simple  variations on  a  generalization  of  this  method  can  be  used  to  solve  a  variety  of  graph problems.
for  now,  we’ll  concentrate  on  the  mechanics  of  examining  every  piece of  the  graph  in  an  organized  way.  below  is  an  implementation  of  depth-first search  which  fills  in  an  array  vaj   [l..vl  as  it  visits  every  vertex  of  the  graph. the  array  is  initially  set  to  all  zeros,  so  vaj[k]=o  indicates  that  vertex  k  has not  yet  been  visited.  the  goal  is  to  systematically  visit  all  the  vertices  of  the graph,  setting  the  vaj   entry  for  the  nowth  vertex  visited  to  now,  for  now= 1,2,..., v.  the  program  uses  a  recursive  procedure  visit  which  visits  all  the vertices  in  the  same  connected  component  as  the  vertex  given  in  the  argument. to  visit  a  vertex,  we  check  all  its  edges  to  see  if  they  lead  to  vertices  which haven’t  yet  been  visited  (as  indicated  by  0  vaj   entries);  if  so,  we  visit  them:
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
matrices  represented  in  this  way  is  similar  to  our  implementation  for  sparse polynomials,  but  is  complicated  by  the  fact  that  each  node  appears  on  two lists.
data  structures even  if  there  are  no  terms  with  zero  coefficients  in  a  polynomial  or  no  zero elements  in  a  matrix,  an  advantage  of  the  linked  list  representation  is  that  we don’t  need  to  know  in  advance  how  big  the  objects  that  we’ll  be  processing are.  this  is  a  significant  advantage  that  makes  linked  structures  preferable in  many  situations.  on  the  other  hand,  the  links  themselves  can  consume  a significant  part  of  the  available  space,  a  disadvantage  in  some  situations.  also, access  to  individual  elements  in  linked  structures  is  much  more  restricted  than in  arrays.
we’ll  see  examples  of  the  use  of  these  data  structures  in  various  algorithms,  and  we’ll  see  more  complicated  data  structures  that  involve  more constraints  on  the  elements  in  an  array  or  more  pointers  in  a  linked  representation.  for  example,  multidimensional  arrays  can  be  defined  which  use multiple  indices  to  access  individual  items. similarly,  we’ll  encounter  many “multidimensional”  linked  structures  with  more  than  one  pointer  per  node. the  tradeoffs  between  competing  structures  are  usually  complicated,  and different  structures  turn  out  to  be  appropriate  for  different  situations.
when  possible  it  is  wise  to  think  of  the  data  and  the  specific  operations to  be  performed  on  it  as  an  abstract data  structure  which  can  be  realized  in several  ways.  for  example,  the  abstract  data  structure  for  polynomials  in  the examples  above  is  the  set  of  coefficients:  a  user  providing  input  to  one  of  the programs  above  need  not  know  whether  a  linked  list  or  an  array  is  being  used. modern  programming  systems  have  sophisticated  mechanisms  which  make it  possible  to  change  representations  easily,  even  in  large,  tightly  integrated systems.
multiplication,  and  division  have  a.  very  long  history,  dating  back  to the  origins  of  algorithm  studies  in  the  work  of  the  arabic  mathematician al-khowdrizmi,  with  roots  going  even  further  back  to  the  greeks  and  the babylonians.
  of  many computer  systems  is  their  capability  for  doing  fast,  accurate  numerical  calculations.  computers  have  built-in  capabilities  to  perform  arithmetic  on  integers  and  floating-point  representations  of  real  numbers;  for  example,  pascal allows  numbers  to  be  of  type  integer  or  re;d,   with  all  of  the  normal  arithmetic operations  defined  on  both  types.  algorithms  come  into  play  when  the  operations  must  be  performed  on  more  complicated  mathematical  objects,  such  as polynomials  or  matrices.
in  this  section,  we’ll  look  at  pascal  implementations  of  some  simple algorithms  for  addition  and  multiplication  of  polynomials  and  matrices.  the algorithms  themselves  are  well-known  and  straightforward;  we’ll  be  examining sophisticated  algorithms  for  these  problems  in  chapter  4.  our  main  purpose in  this  section  is  to  get  used  to  treating  th’ese   mathematical  objects  as  objects for  manipulation  by  pascal  programs.  this  translation  from  abstract  data  to something  which  can  be  processed  by  a  computer  is  fundamental  in  algorithm design.  we’ll  see  many  examples  throughout  this  book  in  which  a  proper representation  can  lead  to  an  efficient  algorithm  and  vice  versa.  in  this chapter,  we’ll  use  two  fundamental  ways  of  structuring  data,  the  array  and the  linked list.  these  data  structures  are  used  by  many  of  the  algorithms  in this  book;  in  later  sections  we’ll  study  some  more  advanced  data  structures.
hash  functions the  first  problem  we  must  address  is  the  computation  of  the  hash  function which  transforms  keys  into  table  addresses.  this  is  an  arithmetic  computation with  properties  similar  to  the  random  number  generators  that  we  have  studied. what  is  needed  is  a  function  which  transforms  keys  (usually  integers  or  short character  strings)  into  integers  in  the  range  [o..m-11,   where  a4  is  the  amount of  memory  available.  an  ideal  hash  function  is  one  which  is  easy  to  compute and  approximates  a  “random”  function:  for  each  input,  every  output  should be  “equally  likely.”
since  the  methods  that  we  will  use  are  arithmetic,  the  first  step  is  to transform  keys  into  numbers  which  can  be  operated  on  (and  are  as  large  as possible).  for  example,  this  could  involve  removing  bits  from  character  strings and  packing  them  together  in  a  machine  word.  from  now  on,  we’ll  assume that  such  an  operation  has  been  performed  and  that  our  keys  are  integers which  fit  into  a  machine  word.
one  commonly  used  method  is  to  take  a4  to  be  prime  and,  for  any  key k,  compute  h(k)  =  k  mod  m.  this  is  a  straightforward  method  which  is  easy to  compute  in  many  environments  and  spreads  the  key  values  out  well.
a  second  commonly  used  method  is  similar  to  the  linear  congruential random  number  generator:  take  m  =  2m   and  h(k)  to  be  the  leading  m  bits  of (bkmod  w),  where  w  is  the  word  size  of  the  computer  and  b  is  chosen  as  for the  random  number  generator.  this  can  be  more  efficiently  computed  than the  method  above  on  some  computers,  and  it  has  the  advantage  that  it  can spread  out  key  values  which  are  close  to  one  another  (e.  g.,  templ,  temp$ temp3).  as  we’ve  noted  before,  languages  like  pascal  are  not  well-suited  to such operaiions.
separate  chaining the  hash  functions  above  will  convert  keys  into  table  addresses:  we  still  need to decide how to handle the case when two keys hash to the same address. the most  straightforward  method  is  to  simply  build  a  linked  list,  for  each  table address, of the records whose keys hash to that address. since the keys which hash  to  the  same  table  position  are  kept  in  a  linked  list,  they  might  as  well be  kept  in  order.  this  leads  directly  to  a  generalization  of  the  elementary  list searching  method  that  we  discussed  in  chapter  14.  rather  than  maintaining a single list with a single list header node head as discussed there, we maintain m  lists with  m  list  header  nodes,  initialized  as  follows:
a g :   100000a000000 a b :   2aooooa000000   3 a a o o o a o o o o o 0 a c : 0 o a 0 0  0 0 1 l 3 a a 0 lm: jm: 3 a a 0 0 o a 0 o l 0 2 l jl: 3aaoooaoolo2l* j k :   3aaoooaooll3l e d :   3aaeloaooll3l f d :   3aae2eaooll3l hi: f e :   3aae2ealhll3l*   gaaeaealhll   3 l a f : g e :   gaaeaealhll   3 l  g c :   gaaeaealhll   3 l   * g h :   8aaeaeaahll3l jg:12aaeaeaahllal lg:12aaeaeaahllal
for clarity in this table, each positive entry i is replaced by the ith letter of the alphabet  (the  name  of  the  father),  and  each  negative  entry  is  complemented to  give  a  positive  integer  (the  weight  of  the  tree).
several  other  techniques  have  been  developed  to  avoid  degenerate  structures.  for  example,  path  compression  has  the  disadvantage  that  it  requires another  pass  up  through  the  tree.  another  technique,  called  halving,  is  to make  each  node  point  to  its  granddad   on  the  way  up  the  tree.  still  another technique,  splitting,  is  like  halving,  but  is  applied  only  to  every  other  node on  the  search  path.  either  of  these  can  be  used  in  combination  with  weight balancing  or  with  height  balancing,  which  is  similar  but  uses  tree  height  instead  of  tree  size  to  decide  which  way  to  merge  trees.
how  is  one  to  choose  from  among  all  these  methods?  and  exactly  how “flat”  are  the  trees  produced?  analysis  for  this  problem  is  quite  difficult because  the  performance  depends  not  only  on  the  v  and  e  parameters,  but also on the number of find operations and, what’s worse, on the order in which the  union  and  find  operations  appear.  unlike  sorting,  where  the  actual  files that  appear  in  practice  are  quite  often  close  to  “random,”  it’s  hard  to  see how  to  model  graphs  and  request  patterns  that  might  appear  in  practice.  for this  reason,  algorithms  which  do  well  in  the  worst  case  are  normally  preferred for  union-find  (and  other  graph  algorithms),  though  this  may  be  an  overly ccnservative  approach.
note  that  an  extra  scaling  factor  of  n  arises.  this  is  the  “inversion  theorem” for  the  discrete  fourier  transform,  which  says  that  the  same  method  will convert  a  polynomial  both  ways:  between  its  representation  as  coefficients  and its  representation  as  values  at  the  complex  roots  of  unity.
while  the  mathematics  may  seem  complicated,  the  results  indicated  are quite  easy  to  apply:  to  interpolate  a  polynomial  on  the  nth  roots  of  unity, use  the  same  procedure  as  for  evaluation,  using  the  interpolation  values  as polynomial  coefficients,  then  rearrange  and  scale  the  answers.
implementation now  we  have  all  the  pieces  for  a  divide-and-conquer  algorithm  to  multiply two  polynomials  using  only  about  n  lg  n  operations.  the  general  scheme  is to:
evaluate  the  input  polynomials  at  the  (2n  -  1)st   roots  of  unity. multiply  the two values obtained at each point. interpolate  to  find  the  result  by  evaluating  the  polynomial  defined  by the  numbers  just  computed  at  the  (2n  -  1)st   roots  of  unity.
the  description  above  can  be  directly  translated  into  a  program  which  uses  a procedure  that  can  evaluate  a  polynomial  of  degree  n  -  1  at  the  nth  roots of  unity.  unfortunately,  all  the  arithmetic  in  this  algorithm  is  to  be  complex arithmetic,  and  pascal  has  no  built-in  type  complex.  while  it  is  possible
in  this  diagram,  ti  represents  the  tree  containing  all  the  records  with  keys less than a,  tz,   contains  all  the  records  with  keys  between  a  and  b,  and  so forth.  the  transformation  switches  the  orientation  of  the  s-node  containing a  and  b  without  disturbing  the  rest  of  the  tree.  thus  none  of  the  keys  in ti,   tz,   t3,   and  t,   are  touched.  in  this  case,  the  transformation  is  effected  by the  link  changes  st.l:=gsf.r;   gst.r:=s;   yt.l:=gs.   also,  note  carefully  that  the colors  of  a  and  b  are  switched.  there  are  three  analogous  cases:  the  3-node could  be  oriented  the  other  way,  or  it  could  be  on  the  right  side  of  y  (oriented either  way).
disregarding  the  colors,  this  single  rotation  operation  is  defined  on  any binary  search  tree  and  is  the  basis  for  several  balanced  tree  algorithms.  it  is important  to  note,  however,  that  doing  a  single  rotation  doesn’t  necessarily improve  the  balance  of  the  tree.  in  the  diagram  above,  the  rotation  brings all  the  nodes  in  tl   one  step  closer  to  the  root,  but  all  the  nodes  in  t3  are lowered  one  step.  if  t3  were  to  have  more  nodes  than  tl,   then  the  tree  after the  rotation  would  become  less  balanced,  not  more  balanced.  top-down  2-3-4 trees  may  be  viewed  as  simply  a  convenient  way  to  identify  single  rotations which  are  likely  to  improve  the  balance.
doing  a  single  rotation  involves  structurally  modifying  the  tree,  something  that  should  be  done  with  caution.  a  convenient  way  to  handle  the  four different  cases  outlined  above  is  to  use  the  search  key  v  to  “rediscover”  the relevant son (s) and grandson (gs) of the node y. (we know that we’ll only be reorienting  a  3-node  if  the  search  took  us  to  its  bottom  node.)  this  leads  to somewhat  simpler  code  that  the  alternative  of  remembering  during  the  search not  only  the  two  links  corresponding  to  s  and  gs  but  also  whether  they  are right  or  left  links.  we  have  the  following  function  for  reorienting  a  3-node along  the  search  path  for  v  whose  father  is  y:
if  s  is  the  left  link  of  y  and  gs  is  the  left  link  of  s,  this  makes  exactly  the  link transformations  for  the  diagram  above. the  reader  may  wish  to  check  the
the  combination  of  the  escape  character,  the  count,  and  the  one  copy of  the  repeated  character  is  called  an  escape  sequence.  note  that  it’s  not worthwhile  to  encode  runs  less  than  four  characters  long  since  at  least  three characters  are  required  to  encode  any  run.
but  what  if  the  escape  character  itself  happens  to  occur  in  the  input? we  can’t  afford  to  simply  ignore  this  possibility,  because  it  might  be  difficult to  ensure  that  any  particular  character  can’t  occur.  (for  example,  someone might  try  to  encode  a  string  that  has  already  been  encoded.)  one  solution  to this problem is to use an escape sequence with a count of zero to represent the escape  character.  thus,  in  our  example,  the  space  character  could  represent zero,  and  the  escape  sequence  “q(space)”  would  be  used  to  represent  any occurrence  of  q  in  the  input.  it  is  interesting  to  note  that  files  which  contain q  are  the  only  files  which  are  made  longer  by  this  compression  method.  if  a file  which  has  already  been  compressed  is  compressed  again,  it  grows  by  at least  the  number  of  characters  equal  to  the  number  of  escape  sequences  used. very  long  runs  can  be  encoded  with  multiple  escape  sequences.  for example, a run of 51 a’s would be encoded as qzaqya using the conventions above.  if  many  very  long  runs  are  expected,  it  would  be  worthwhile  to  reserve more  than  one  character  to  encode  the  counts.
in  practice,  it  is  advisable  to  make  both  the  compression  and  expansion programs  somewhat  sensitive  to  errors.  this  can  be  done  by  including  a  small amount  of  redundancy  in  the  compressed  file  so  that  the  expansion  program can  be  tolerant  of  an  accidental  minor  change  to  the  file  between  compression and  expansion.  for  example,  it  probably  is  worthwhile  to  put  “end-of-line” characters  in  the  compressed  version  of  the  letter  “q”  above,  so  that  the expansion  program  can  resynchronize   itself  in  case  of  an  error.
run-length  encoding  is  not  particularly  effective  for  text  files  because  the only  character  likely  to  be  repeated  is  the  blank,  and  there  are  simpler  ways  to encode  repeated  blanks.  (it  was  used  to  great  advantage  in  the  past  to  compress  text  files  created  by  reading  in  punched-card  decks,  which  necessarily contained  many  blanks.)  in  modern  systems,  repeated  strings  of  blanks  are never  entered,  never  stored:  repeated  strings  of  blanks  at  the  beginning  of lines  are  encoded  as  “tabs,”  blanks  at  the  ends  of  lines  are  obviated  by  the use  of  “end-of-line”  indicators.  a  run-length  encoding  implementation  like the  one  above  (but  modified  to  handle  all  representable  characters)  saves  only about  4%  when  used  on  the  text  file  for  this  chapter  (and  this  savings  all comes  from  the  letter  “q”  example!).
by a simple path in only one of two ways (corresponding to the truth or falsity of  the  variables).  these  small  components  are  attached  together  as  specified by  the  clauses,  using  more  complicated  subgraphs  which  can  be  traversed  by simple  paths  corresponding  to  the  truth  or  falsity  of  the  clauses.  it  is  quite a  large  step  from  this  brief  description  to  the  full  construction:  the  point is  to  illustrate  that  polynomial  reduction  can  be  applied  to  quite  dissimilar problems.
thus,  if  we  were  to  have  a  polynomial-time  algorithm  for  the  traveling salesman  problem,  then  we  would  have  a  polynomial-time  algorithm  for  the hamilton  cycle  problem,  which  would  also  give  us  a  polynomial-time  algorithm for  the  satisfiability  problem. each  problem  that  is  proven  np-complete provides  another  potential  basis  for  proving  yet  another  future  problem  npcomplete.  the  proof  might  be  as  simple  as  the  reduction  given  above  from  the hamilton  cycle  problem  to  the  traveling  salesman  problem,  or  as  complicated as  the  transformation  sketched  above  from  the  satisfiability  problem  to  the hamilton  cycle  problem,  or  somewhere  in  between.  literally  thousands  of problems  have  been  proven  to  be  np-complete  over  the  last  ten  years  by transforming  one  to  another  in  this  way.
cook’s  theorem reduction  uses  the  np-completeness  of  one  problem  to  imply  the  np-completeness  of  another.  there  is  one  case  where  it  doesn’t  apply:  how  was  the first   problem  proven  to  be  np-complete?  this  was  done  by  s.  a.  cook  in 1971.  cook  gave  a  direct  proof  that  satisfiability  is  np-complete:  that  if there  is  a  polynomial  time  algorithm  for  satisfiability,  then  all  problems  in np  can  be  solved  in  polynomial  time.
the  proof  is  extremely  complicated  but  the  general  method  can  be  explained.  first,  a  full  mathematical  definition  of  a  machine  capable  of  solving any  problem  in  np  is  developed.  this  is  a  simple  model  of  a  general-purpose computer  known  as  a  turing  machine  which  can  read  inputs,  perform  certain operations,  and  write  outputs.  a  turing  machine  can  perform  any  computation  that  any  other  general  purpose  computer  can,  using  the  same  amount  of time  (to  within  a  polynomial  factor),  and  it  has  the  additional  advantage  that it  can  be  concisely  described  mathematically.  endowed  with  the  additional power  of  nondeterminism,  a  turing  machine  can  solve  any  problem  in  np. the  next  step  in  the  proof  is  to  describe  each  feature  of  the  machine,  including  the  way  that  instructions  are  executed,  in  terms  of  logical  formulas  such as  appear  in  the  satisfiability  problem.  in  this  way  a  correspondence  is  established  between  every  problem  in  np  (which  can  be  expressed  as  a  program  on the  nondeterministic  turing  machine)  and  some  instance  of  satisfiability  (the translation  of  that  program  into  a  logical  formula).  now,  the  solution  to  the satisfiability  problem  essentially  corresponds  t,o  a  simulation  of  the  machine
algorithms  for  converting  geometric  objects  to  points  in  this  manner  are  called scan-conversion  algorithms.  this  example  illustrates  that  it  is  easy  to  draw nice-looking  diagonal  lines  like  eo  and  il,  but  that  it  is  somewhat  harder  to make  lines  of  arbitrary  slope  look  nice  using  a  coarse  matrix  of  characters.  the recursive  method  given  above  has  the  disadvantages  that  it  is  not  particularly efficient  (some  points  get  plotted  several  times)  and  that  it  doesn’t  draw certain  lines  very  well  (for  example  lines  which  are  nearly  horizontal  and nearly  vertical).  it  has  the  advantages  that  it  is  relatively  simple  and  that it  handles  all  the  cases  of  the  various  orientation  of  the  endpoints  of  the  line in  a  uniform  way.  many  sophisticated  scan-conversion  algorithms  have  been developed  which  are  more  efficient  and  more  accurate  than  this  recursive  one. if  the  array  has  a  very  large  number  of  dots,  then  the  ragged  edges  of the  lines  aren’t  discernible,  but  the  same  types  of  algorithms  are  appropriate. however,  the  very  high  resolution  necessary  to  make  high-quality  lines  can require  very  large  amounts  of  memory  (or  computer  time),  so  sophisticated algorithms  are  called  for,  or  other  technologies  might  be  appropriate.  for example,  the  text  of  this  book  was  printed  on  a  device  capable  of  printing millions of dots per square inch, but most of the lines in the figures were drawn
again,  the  primary  reference  for  this  section  is  knuth’s  volume  three. most  of  the  algorithms  that  we’ve  st  ldied  are  treated  in  great  detail  in that  book,  including  mathematical  analyses  and  suggestions  for  practical applications.
the  material  in  chapter  15  come:,   from  guibas  and  sedgewick’s  1978 paper,  which  shows  how  to  fit  many  classical  balanced  tree  algorithms  into the  “red-black”  framework,  and  which  gives  several  other  implementations. there  is  actually  quite  a  large  literature  on  balanced  trees.  comer’s  1979 survey  gives  many  references  on  the  subject  of  btrees.
the  extendible  hashing  algorithm  presented  in  chapter  18  comes  from fagin,   nievergelt,  pippenger  and  stron;‘s   1979  paper.  this  paper  is  a  must for  anyone  wishing  further  information  in  external  searching  methods:  it  ties together  material  from  our  chapters  16  and  17  to  bring  out  the  algorithm  in chapter  18.
trees  and  binary  trees  as  purely  mltthematical   objects  have  been  studied extensively,  quite  apart  from  computer  science.  a  great  deal  is  known  about the  combinatorial  properties  of  these  objects.  a  reader  interested  in  studying this  type  of  material  might  begin  with  icnuth’s  volume  1.
d. comer, “the ubquitous &tree,”  colrlputing surveys, 11 (1979). r.  fagin,  j.  nievergelt,  n.  pippenger  aild  h.  r.  strong,  “extendible  hashing -  a  fast  access  method  for  dynamic  fi.es,”   acm  transactions  on  database systems,  4,  3  (september,  1979). l.  guibas  and  r.  sedgewick,  “a  dichromatic  framework  for  balanced  trees,” in  19th  annual  sym.posium  on foundations  of computer science,  ieee,  1978. also  in a decade  of progress  1970-19811, d.  e.  knuth,  the art  of  computer  pr  ,gramming. algorithms,  addison-wesley, reading,  iaa,   1968. d.  e.  knuth,  the art  of  computer  pi.ogramming. searching,  addison-wesley,  reading,  ma,  second  printing,  1975. j. d. ullman,  principles  of database  sy:  terns,  computer science press,  rockville,  md,  1982.
some  care  must  be  exercised  to  pro.)erly   handle  records  with  equal  keys for  this  algorithm:  the  index  returned  cmluld  fall  in  the  middle  of  a  block  of records  with  key  v,  so  loops  which  scan  in  both  directions  from  that  index should  be  used  to  pick  up  all  the  records. of  course,  in  this  case  the  running time  for  the  search  is  proportional  to  lg)v  plus  the  number  of  records  found. the  sequence  of  comparisons  made  by  the  binary  search  algorithm  is predetermined:  the  specific  sequence  used  is  based  on  the  value  of  the  key being  sought  and  the  value  of  n.   the  comparison  structure  can  be  simply described  by  a  binary  tree  structure.  the  following  binary  tree  describes  the comparison  structure  for  our  example  se,  of  keys:
in  searching  for  the  key  s  for  instance,  it  is  first  compared  to  h.  since  it  is greater,  it  is  next  compared  to  n;  otheruise   it  would  have  been  compared  to c),  etc.  below  we  will  see  algorithms  that  use  an  explicitly  constructed  binary tree  structure  to  guide  the  search.
one  improvement  suggested  for  binary  search  is  to  try  to  guess  more precisely where the key being sought falls tvithin  the current interval of interest (rather  than  blindly  using  the  middle  element  at  each  step).  this  mimics  the way  one  looks  up  a  number  in  the  telephone  directory,  for  example:  if  the name  sought  begins  with  b,  one  looks  r(ear   the  beginning,  but  if  it  begins interpolation  search, with  y,  one  looks  near  the  end.  this  method,  called  requires  only  a  simple  modification  to  the  program  above.  in  the  program above,  the  new  place  to  search  (the  midpoint  of  the  interval)  is  computed with  the  statement  x:=(l+r)   div  2.  this   is  derived  from  the  computation z =  1+  $(r   -  1):  the  middle  of  the  interval  is  computed  by  adding  half  the  size of  the  interval  to  the  left  endpoint.  inte*polation  search  simply  amounts  to replacing  i  in  this  formula  by  an  estima;e   of  where  the  key  might  be  based on  the  values  available:  i  would  be  appropriate  if  v  were  in  the  middle  of  the interval  between  a[i].key   and  a[r].key,   but  we  might  have  better  luck  trying
way  of  implementing  binary  search  trees  to  aid  in  searching  large  arrays  of records  is  preferred  for  many  applications,  since  it  avoids  the  extra  expense  of copying  keys  as  in  the  previous  paragraph,  and  it  avoids  the  overhead  of  the storage  allocation  mechanism  implied  by  new.  the  disadvantage  is  that  space is  reserved  with  the  record  array  for  links  which  may  not  be  in  use,  which could  lead  to  problems  with  large  arrays  in  a  dynamic  situation.
the  worst  case  for  trees  built  with  digital  searching  will  be  much  better than  for  binary  search  trees.  the  length  of  the  longest  path  in  a  digital search  tree  is  the  length  of  the  longest  match  in  the  leading  bits  between any  two  keys  in  the  tree,  and  this  is  likely  to  be  relatively  short.  and  it  is obvious  that  no  path  will  ever  be  any  longer  than  the  number  of  bits  in  the keys:  for  example,  a  digital  search  tree  built  from  eight-character  keys  with, say,  six  bits  per  character  will  have  no  path  longer  than  48,  even  if  there are  hundreds  of  thousands  of  keys. for  random  keys,  digital  search  trees are  nearly  perfectly  balanced  (the  height  is  about  1gn).   thus,  they  provide an  attractive  alternative  to  standard  binary  search  trees,  provided  that  bit extraction  can  be  done  as  easily  as  key  comparison  (which  is  not  really  the case  in  pascal).
radix  search  tries it  is  quite  often  the  case  that  search  keys  are  very  long,  perhaps  consisting  of twenty  characters  or  more.  in  such  a  situation,  the  cost  of  comparing  a  search key  for  equality  with  a  key  from  the  data  structure  can  be  a  dominant  cost which  cannot  be  neglected.  digital  tree  searching  uses  such  a  comparison  at each  tree  node:  in  this  section  we’ll  see  that  it  is  possible  to  get  by  with  only one  comparison  per  search  in  most  cases.
the  idea  is  to  not  store  keys  in  tree  nodes  at  all,  but  rather  to  put  all the  keys  in  external  nodes  of  the  tree.  that  is,  instead  of  using  a  for  external nodes  of  the  structure,  we  put  nodes  which  contain  the  search  keys.  thus, we  have  two  types  of  nodes:  internal  nodes,  which  just  contain  links  to  other nodes,  and  external  nodes,  which  contain  keys  and  no  links.  (e.  fredkin
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
way  of  implementing  binary  search  trees  to  aid  in  searching  large  arrays  of records  is  preferred  for  many  applications,  since  it  avoids  the  extra  expense  of copying  keys  as  in  the  previous  paragraph,  and  it  avoids  the  overhead  of  the storage  allocation  mechanism  implied  by  new.  the  disadvantage  is  that  space is  reserved  with  the  record  array  for  links  which  may  not  be  in  use,  which could  lead  to  problems  with  large  arrays  in  a  dynamic  situation.
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
however,  direct  use  of  the  virtual  men  ory  is  not  recommended  as  an  easy searching  application.  as  mentioned  in  chapter  13,  virtual  memories  perform best  when  most  accesses  are  relatively  close  to  previous  accesses.  sorting algorithms  can  be  adapted  to  this,  but  the  very  nature  of  searching  is  that requests  are  for  information  from  arbitr  n-y   parts  of  the  database.
consider  two  examples  to  illustrate  the  nature  of  specially  adapted  hashing methods.  these  and  many  other  methods  are  fully  described  in  knuth’s  book. the  first,  called  ordered  hashing,  is  a  method  for  making  use  of  ordering within  an  open  addressing  table:  in  standard  linear  probing,  we  stop  the search  when  we  find  an  empty  table  position  or  a  record  with  a  key  equal to  the  search  key;  in  ordered  hashing,  we  stop  the  search  when  we  find  a record  with  a  key  greater  than  or  equal  to  the  search  key  (the  table  must  be cleverly  constructed  to  make  this  work).  this  method  turns  out  to  reduce the  time  for  unsuccessful  search  to  approximately  that  for  successful  search. (this  is  the  same  kind  of  improvement  that  comes  in  separate  chaining.)  this method  is  useful  for  applications  where  unsuccessful  searching  is  frequently used.  for  example,  a  text  processing  system  might  have  an  algorithm  for hyphenating  words  that  works  well  for  most  words,  but  not  for  bizarre  cases (such  as  “bizarre”).  the  situation  could  be  handled  by  looking  up  all  words in  a  relatively  small  exception  dictionary  of  words  which  must  be  handled  in a  special  way,  with  most  searches  likely  to  be  unsuccessful.
similarly,  there  are  methods  for  moving  some  records  around  during unsuccessful  search  to  make  successful  searching  more  efficient.  in  fact,  r.  p. brent  developed  a  method  for  which  the  average  time  for  a  successful  search can  be  bounded  by  a  constant,  giving  a  very  useful  method  for  applications with  frequent  successful  searching  in  very  large  tables  such  as  dictionaries.
these  are  only  two  examples  of  a  large  number  of  algorithmic  improvements  which  have  been  suggested  for  hashing.  many  of  these  improvements are  interesting  and  have  important  applications.  however,  our  usual  cautions must  be  raised  against  premature  use  of  advanced  methods  except  by  experts with  serious  searching  applications,  because  separate  chaining  and  double hashing  are  simple,  efficient,  and  quite  acceptable  for  most  applications.
hashing  is  preferred  to  the  binary  tree  structures  of  the  previous  two chapters  for  many  applications  because  it  is  somewhat  simpler  and  it  can provide  very  fast  (constant)  searching  times,  if  space  is  available  for  a  large enough  table.  binary  tree  structures  have  the  advantages  that  they  are dynamic  (no  advance  information  on  the  number  of  insertions  is  needed),  they can  provide  guaranteed  worst-case  performance  (everything  could  hash  to  the same  place  even  in  the  best  hashing  method),  and  they  support  a  wider  range of  operations  (most  important,  the  sort  function).  when  these  factors  are  not important,  hashing  is  certainly  the  searching  method  of  choice.
leading  bit.  this  leads  to  an  immediate  correspondence  with  radix  sorting: binary  trie  searching  partitions  the  file  in  exactly  the  same  way  as  radix exchange  sorting.  (compare  the  trie  above  with  the  partitioning  diagram  we examined  for  radix  exchange  sorting,  after  noting  that  the  keys  are  slightly different.)  this  correspondence  is  analogous  to  that  between  binary  tree searching  and  quicksort.
an  annoying  feature  of  radix  tries  is  the  “one-way”  branching  required  for keys  with  a  large  number  of  bits  in  common,  for  example,  keys  which  differ only  in  the  last  bit  require  a  path  whose  length  is  equal  to  the  key  length,  no matter  how  many  keys  there  are  in  the  tree.  the  number  of  internal  nodes  can be  somewhat  larger  than  the  number  of  keys.  the  height  of  such  trees  is  still limited  by  the  number  of  bits  in  the  keys,  but  we  would  like  to  consider  the possibility  of  processing  records  with  very  long  keys  (say  1000  bits  or  more) which  perhaps  have  some  uniformity,  as  might  occur  in  character  encoded data.  one  way  to  shorten  the  paths  in  the  trees  is  to  use  many  more  than two  links  per  node  (though  this  exacerbates  the  “space”  problem  of  using  too many  nodes);  another  way  is  to  “collapse”  paths  containing  one-way  branches into  single  links.  we’ll  discuss  these  methods  in  the  next  two  sections.
multiway for  radix  sorting,  we  found  that  we  could  get  a  significant  improvement  in speed  by  considering  more  than  one  bit  at  a  time.  the  same  is  true  for  radix searching:  by  examining  m  bits  at  a  time,  we  can  speed  up  the  search  by  a factor of  2m. however,  there’s  a  catch  which  makes  it  necessary  to  be  more careful  applying  this  idea  than  was  necessary  for  radix  sorting.  the  problem is  that  considering  m  bits  at  a  time  corresponds  to  using  tree  nodes  with m  =  2m   links,  which  can  lead  to  a  considerable  amount  of  wasted  space  for unused  links.  for  example,  if  m  =  4  the  following  tree  is  formed  for  our sample  keys:
same  technique  as  used  in  patricia  can  be   used  in  binary  radix  trie  searching to  eliminate  one-way  branching,  but  this   only  exacerbates  the  multiple  node type  problem.
unlike  standard  binary  tree  search,  the  radix  methods  are  insensitive  to the  order  in  which  keys  are  inserted;  thtty   depend  only  upon  the  structure  of the  keys  themselves.  for  patricia  the  pl,icement   of  the  upwards  links  depend on  the  order  of  insertion,  but  the  tree  structure  depends  only  on  the  bits  in the  keys,  as  for  the  other  methods.  this,   even  patricia  would  have  trouble with  a  set  of  keys  like  001,  0001,  00001,  300001,  etc.,  but  for  normal  key  sets, the  tree  should  be  relatively  well-balanced  so  the  number  of  bit  inspections, even  for  very  long  keys,  will  be  roughly  proportional  to  1gn  when  there  are n  nodes  in  the  tree.
the  most  useful  feature  of  radix  trie  searching  is  that  it  can  be  done efficiently  with  keys  of  varying  length.  in  all  of  the  other  searching  methods we  have  seen  the  length  of  the  key  is  “built  into”  the  searching  procedure  in some  way,  so  that  the  running  time  is  dependent   on  the  length  of  the  keys as  well  as  the  number  of  keys.  the  spetific   savings  available  depends  on  the method  of  bit  access  used.  for  example,  suppose  we  have  a  computer  which can  efficiently  access  g-bit   “bytes”  of  tlata,  and  we  have  to  search  among hundreds  of  looo-bit   keys.  then  patricia  would  require  access  of  only  about thl:  search,  plus  one  125-byte  equality 9  or  10  bytes  of  the  search  key  for  comparison  while  hashing  requires  accest   of  all  125-bytes  of  the  search  key  for computing the hash function plus a few  elluality  comparisons, and comparisonbased  methods  require  several  long  comparisons.  this  effect  makes  patricia (or  radix  trie  searching  with  one-way  branching  removed)  the  search  method of choice when very long keys are involved.
we  use  the  identical  partitioning  procedure  to  quicksort:  as  with  quicksort, this  could  be  changed  slightly  if  many  equal  keys  are  expected.  note  that  in this  non-recursive  program,  we’ve  eliminated  the  simple  calculations  involving k.
this  method  has  about  the  same  worst  case  as  quicksort:  using  it  to find  the  smallest  element  in  an  already  sorted  file  would  result  in  a  quadratic running  time.  it  is  probably  worthwhile  to  use  an  arbitrary  or  a  random partitioning  element  (but  not  the  median-of-three:  for  example,  if  the  smallest element  is  sought,  we  probably  don’t  want  the  file  split  near  the  middle).  the average  running  time  is  proportional  to  about  n  +  iclog(n/k),   which  is  linear for  any  allowed  value  of  k.
it is possible to modify this quicksort-based selection procedure so that its running  time  is  guaranteed  to  be  linear.  these  modifications,  while  important from  a  theoretical  standpoint,  are  extremely  complex  and  not  at  all  practical.
merging it  is  common  in  many  data  processing  environments  to  maintain  a  large (sorted)  data  file  to  which  new  entries  are  regularly  added.  typically,  a number  of  new  entries  are  “batched,”  appended  to  the  (much  larger)  main file,  and  the  whole  thing  resorted.  this  situation  is  tailor-made  for  merging:  a much  better  strategy  is  to  sort  the  (small)  batch  of  new  entries,  then  merge  it with  the  large  main  file.  merging  has  many  other  similar  applications  which
just  after  i  and  j  are  incremented,  it  has  been  determined  that  the  first  j-l characters  of  the  pattern  match  the  characters  in  positions  p  [i-j-  1.  .i-1  1,  the last  j-l  characters  in  the  first  i-l  characters  of  the  pattern.  and  this  is  the largest  j  with  this  property,  since  otherwise  a  “possible  match”  of  the  pattern with  itself  would  have  been  missed.  thus,  j  is  exactly  the  value  to  be  assigned to  next  [il.
an  interesting  way  to  view  this  algorithm  is  to  consider  the  pattern  as fixed,  so  that  the  next  table  can  be  “wired  in”  to  the  program.  for  example, the  following  program  is  exactly  equivalent  to  the  program  above  for  the pattern  that  we’ve  been  considering,  but  it’s  likely  to  be  much  more  efficient.
0: i:=i+l; 1:  if  a[i]<>‘l’then  goto  0;  i:=i+l; 2: if  a[i]<>‘o’then   goto  1;  i:=i+l; 3: if  a[i]<>‘l’then  goto  1;  i:=i+l; 4: if  a[i]<>‘o’then   goto 2;  i:=i+l; 5: if  a[i]<>‘o’then   goto  3;  i:=i+l; 6: if  a[i]<>‘l’then  goto  1;  i:=i+l; 7: if  a[i]<>‘l’then  goto 2;  i:=i+l; 8:  if  a[i]<>‘l’then  goto 2;  i:=i+l;
the  goto  labels  in  this  program  correspond  precisely  to  the  next  table.  in fact,  the  in&next   program  above  which  computes  the  next  table  could  easily be  modified  to  output  this  program! to  avoid  checking  whether  i>n  each time  i  is  incremented,  we  assume  that  the  pattern  itself  is  stored  at  the  end of  the  text  as  a  sentinel,  in  a[n+l  ..n+m].   (this  optimization  could  also be  applied  to  the  standard  implementation.)  this  is  a  simple  example  of  a “string-searching  compiler”  : given  a  pattern,  we  can  produce  a  very  efficient
multiple  searches the  algorithms  that  we’ve  been  discussing  are  all  oriented  towards  a  specific string  searching  problem:  find  an  occurrence  of  a  given  pattern  in  a  given text  string.  if  the  same  text  string  is  to  be  the  object  of  many  pattern searches,  then  it  will  be  worthwhile  to  do  some  processing  on  the  string  to make  subsequent  searches  efficient.
if  there  are  a  large  number  of  searches,  the  string  searching  problem  can be  viewed  as  a  special  case  of  the  general  searching  problem  that  we  studied in  the  previous  section.  we  simply  treat  the  text  string  as  n  overlapping “keys,”  the  ith  key  defined  to  be  a[l..n],  the  entire  text  string  starting  at position  i.  of  course,  we  don’t  manipulate  the  keys  themselves,  but  pointers to  them:  when  we  need  to  compare  keys  i  and  j  we  do  character-by-character compares starting at positions i  and j of the text string. (if we use a “sentinel” character  larger  than  all  other  characters  at  the  end,  then  one  of  the  keys will  always  be  greater  than  the  other.)  then  the  hashing,  binary  tree,  and other  algorithms  of  the  previous  section  can  be  used  directly.  first,  an  entire structure  is  built  up  from  the  text  string,  and  then  efficient  searches  can  be performed  for  particular  patterns.
there are many details which need to be worked out in applying searching algorithms  to  string  searching  in  this  way;  our  intent  is  to  point  this  out  as a  viable  option  for  some  string  searching  applications.  different  methods  will be  appropriate  in  different  situations.  for  example,  if  the  searches  will  always be  for  patterns  of  the  same  length,  a  hash  table  constructed  with  a  single  scan as  in  the  rabin-karp  method  will  yield  constant  search  times  on  the  average. on  the  other  hand,  if  the  patterns  are  to  be  of  varying  length,  then  one  of  the tree-based  methods  might  be  appropriate.  (patricia  is  especially  adaptable  to such  an  application.)
other  variations  in  the  problem  can  make  it  significantly  more  difficult and  lead  to  drastically  different  methods,  as  we’ll  discover  in  the  next  two chapters.
the  m  lists  is  equally  likely  to  be  searched  and,  as  in  sequential  list  searching, that  the  list  searched  is  only  traversed  halfway  (on  t,he   average).  the  average length  of  the  list  examined  (not  counting  z)  in  this  example  for  unsuccessful search  is  (0+4+2+2+0+4+0+2+2+1+0)/11  z  1.545.  this  would  be  the average  time  for  an  unsuccessful  search  were  the  lists  unordered;  by  keeping them  ordered  we  cut  the  time  in  half.  for  a  “successful  search”  (for  one  of  the records in the table), we assume that each record is equally likely to be sought: seven  of  the  keys  would  be  found  as  the  first  list  item  examined,  six  would  be found  as  the  second  item  examined,  etc.,  so  the  average  is  (7.1+   6.2 +  2.3  + 2.4)/17)  z  1.941.  (this  count  assumes  that  equal  keys  are  distinguished  with a  unique  identifier  or  some  other  mechanism,  and  the  search  routine  modified appropriately  to  be  able  to  search  for  each  individual  key.)
if  n,  the  number  of  keys  in  the  table,  is  much  larger  than  m  then  a  good approximation  to  the  average  length  of  the  lists  is  n/m.  as  in  chapter  14, unsuccessful  and  successful  searches  would  be  expected  to  go  about  halfway down  some  list.  thus,  hashing  provides  an  easy  way  to  cut  down  the  time required  for  sequential  search  by  a  factor  of  m,  on  the  average.
in  a  separate  chaining  implementation,  m  is  typically  chosen  to  be  relatively  small  so  as  not  to  use  up  a  large  area  of  contiguous  memory.  but  it’s probably  best  to  choose  m  sufficiently  large  that  the  lists  are  short  enough  to make  sequential  search  the  most  efficient  method  for  them:  “hybrid”  methods (such  as  using  binary  trees  instead  of  linked  lists)  are  probably  not  worth  the trouble.
the  implementation  given  above  uses  a  hash  table  of  links  to  headers of  the  lists  containing  the  actual  keys.  maintaining  m  list  header  nodes  is somewhat  wasteful  of  space;  it  is  probably  worthwhile  to  eliminate  them  and make  heads  be  a  table  of  links  to  the  first  keys  in  the  lists.  this  leads  to some  complication  in  the  algorithm.  for  example,  adding  a  new  record  to  the beginning  of  a  list  becomes  a  different  operation  than  adding  a  new  record anywhere  else  in  a  list,  because  it  involves  modifying  an  entry  in  the  table  of links,  not  a  field  of  a  record.  an  alternate  implementation  is  to  put  the  first key  within  the  table.  if  space  is  at  a  premium,  it  is  necessary  to  carefully analyze  the  tradeoff  between  wasting  space  for  a  table  of  links  and  wasting space for a key and a link for each empty list. if n is much bigger than m then the  alternate  method  is  probably  better,  though  m  is  usually  small  enough that  the  extra  convenience  of  using  list  header  nodes  is  probably  justified.
open addressing if  the  number  of  elements  to  be  put  in  the  hash  table  can  be  estimated  in advance,  then  it  is  probably  not  worthwhile  to  use  any  links  at  all  in  the  hash table.  several  methods  have  been  devised  which  store  n  records  in  a  table
this method takes about n steps for an unsuccessful search (every record must  be  examined  to  decide  that  a  record  with  any  particular  key  is  absent) and  about  n/2  steps,  on  the  average,  for  a  successful  search  (a  “random” search  for  a  record  in  the  table  will  require  examining  about  half  the  entries, on the average).
the  seqsearch  program  above  uses  purely  sequential  access  to  the  records, and  thus  can  be  naturally  adapted  to  use  a  linked  list  representation  for  the records.  one  advantage  of  doing  so  is  that  it  becomes  easy  to  keep  the  list sorted,  as  shown  in  the  following  implementation:
var  x:  link; begin zf.key:=v; while  tt.nextt.key<v   do  t:=tt.next; n e w ( x ) ;   xt.next:=tf.next;   tt.next:=x; xf.key:=v; jistinsert:=x; end ;
with  a  sorted  list,  a  search  can  be  terminated  unsuccessfully  when  a  record with  a  key  larger  than  the  search  key  is  found.  thus  only  about  half  the
even if only the worst case is being considered, the analysis of union-find algorithms  is  extremely  complex  and  intricate.  this  can  be  seen  even  from  the nature  of  the  results,  which  do  give  us  clear  indications  of  how  the  algorithms will  perform  in  a  practical  situation. if  either  weight  balancing  or  height balancing  is  used  in  combination  with  either  path  compression,  halving,  or splitting,  then  the  total  number  of  operations  required  to  build  up  a  structure with  e  edges  is  proportional  to  es(e),   where  a(e)  is  a  function  that  is  so slowly  growing  that  o(e)  <  4  unless  e  is  so  large  that  taking  lg  e,  then taking  lg  of  the  result,  then  taking  lg  of  that  result,  and  continuing  16  times still  gives  a  number  bigger  than  1.  this  is  a  stunningly  large  number;  for  all practical  purposes,  it  is  safe  to  assume  that  the  average  amount  of  time  to execute  each  union  and  find  operation  is  constant.  this  result  is  due  to  r.  e. tarjan,   who  further  showed  that  no  algorithm  for  this  problem  (from  a  certain general  class)  can  do  better  that  e&(e),  so  that  this  function  is  intrinsic  to the  problem.
an  important  practical  application  of  union-find  algorithms  is  that  they can  be  used  to  determine  whether  a  graph  with  v  vertices  and  e  edges  is connected  in  space  proportional  to  v  (and  almost  linear  time).  this  is  an advantage  over  depth-first  search  in  some  situations:  here  we  don’t  need  to ever  store  the  edges.  thus  connectivity  for  a  graph  with  thousands  of  vertices and  millions  of  edges  can  be  determined  with  one  quick  pass  through  the edges.
first  we  visit  the  closest  vertex  to  a,  which  is  b.  then  both  c  and  f  are distance  2  from  a,  so  we  visit  them  next  (in  whatever  order  the  priority  queue returns  them).  then  d  can  be  attached  at  f  or  at  b  to  get  a  path  of  distance 3  to  a.  (the  algorithm  attaches  it  to  b  because  it  was  put  on  the  tree  before f,  so  d  was  already  on  the  fringe  when  f  was  put  on  the  tree  and  f  didn’t provide  a  shorter  path  to  a.)  finally,  e  and  g  are  visited.  as  usual,  the  tree is  represented  by  the  dad  array  of  father  links.  the  following  table  shows  the array  computed  by  the  priority  graph  traversal  procedure  for  our  example:
thus  the  shortest  path  from  a  to  g  has  a  total  weight  of  5  (found  in  the  val entry  for  g)  and  goes  from  a  to  f  to  e  to  g  (found  by  tracing  backwards in  the  dad  array,  starting  at  g).  note  that  the  correct  operation  of  this program  depends  on  the  val  entry  for  the  root  being  zero,  the  convention  that we  adopted  for  sparsepfs.
as  before,  the  priority  graph  traversal  algorithm  has  a  worst-case  running time  proportional  to  (e  +  v)  log  v,  though  a  different  implementation  of  the priority  queue  can  give  a  v2   algorithm,  which  is  appropriate  for  dense  graphs. below,  we’ll  examine  this  implementation  of  priority  graph  traversal  for  dense graphs  in  full  detail.  for  the  shortest  path  problem,  this  reduces  to  a  method discovered  by  e.  dijkstra  in  1956. though  the  methods  are  the  same  in essence,  we’ll  refer  to  the  sparsepfs  program  of  the  previous  chapter  with priority replaced by val  [k] + tt . weight  as the “priority-first search solution” to the  shortest  paths  problem  and  the  adjacency  matrix  version  given  below  as “dijkstra’s  algorithm.”
dense  graphs as  we’ve  discussed,  when  a  graph  is  represented  with  a  adjacency  matrix,  it  is best  to  use  an  unordered  array  representation  for  the  priority  queue  in  order to  achieve  a  v2   running  time  for  any  priority  graph  traversal  algorithm.  that is,  this  provides  a  linear  algorithm  for  the  priority  first  search  (and  thus  the minimum  spanning  tree  and  shortest  path  problems)  for  dense  graphs.
specifically,  we  maintain  the  priority  queue  in  the  val   array  just  as  in sparsepfs  but  we  implement  the  priority  queue  operations  directly  rather  than using  heaps.  first,  we  adopt  the  convention  that  the  priority  values  in  the val  array  will  be  negated,  so  that  the  sign  of  a  val   entry  tells  whether  the corresponding  vertex  is  on  the  tree  or  the  priority  queue.  to  change  the
in  this  example,  b  is  used  as  the  anchor.  if  the  points  are  visited  in  the  order b  m  j  l  n  p  k  f  i  e  c  0  a  h  g  d  b  then  a  simple  closed  polygon  will  be traced  out.
if  dx and dy are the delta x and y distances from some point to the anchor point,  then  the  angle  needed  in  this  algorithm  is  tan-’   dyldx.  although the  arctangent  is  a  built-in  function  in  pascal  (and  some  other  programming environments), it is likely to be slow and it leads to at least two annoying extra conditions  to  compute:  whether  dx  is  zero,  and  which  quadrant  the  point  is in.  since  the  angle  is  used  only  for  the  sort  in  this  algorithm,  it  makes  sense to  use  a  function  that  is  much  easier  to  compute  but  has  the  same  ordering properties  as  the  arctangent  (so  that  when  we  sort,  we  get  the  same  result). a  good  candidate  for  such  a  function  is  simply  dy/(dy  +  dz).  testing  for exceptional  conditions  is  still  necessary,  but  simpler.  the  following  program returns  a  number  between  0  and  360  that  is  not  the  angle  made  by  pl  and p2  with  the  horizontal  but  which  has  the  same  order  properties  as  the  true angle.
in  some  programming  environments  it  may  not  be  worthwhile  to  use  such programs  instead  of  standard  trigonometric  functions;  in  others  it  might  lead to  significant  savings.  (in  some  cases  it  might  be  worthwhile  to  change  theta to  have  an  integer  value,  to  avoid  using  real  numbers  entirely.)
inclusion in a polygon the  next  problem  that  we’ll  consider  is  a  natural  one:  given  a  polygon  represented  as  an  array  of  points  and  another  point,  determine  whether  the  point is  inside  or  outside.  a  straightforward  solution  to  this  problem  immediately suggests  itself:  draw  a  long  line  segment  from  the  point  in  any  direction  (long enough so that its other endpoint is guaranteed to be outside the polygon) and
variables,  so  we  have  computed  a  feasible  basis  for  the  original  linear  program. in  degenerate  cases,  some  of  the  artificial  variables  may  remain  in  the  basis, so  it  is  necessary  to  do  further  pivoting  to  remove  them  (without  changing the cost).
to summarize, a two-phase process is normally used to solve general linear programs.  first,  we  solve  a  linear  program  involving  the  artificial  s  variables to  get  a  point  on  the  simplex  for  our  original  problem.  then,  we  dispose  of the  s  variables  and  reintroduce  our  original  objective  function  to  proceed  from this  point  to  the  solution.
the  analysis  of  the  running  time  of  the  simplex  method  is  an  extremely complicated  problem,  and  few  results  are  available.  no  one  knows  the  “best” pivot selection strategy, because there are no results to tell us how many pivot steps to expect, for any reasonable class of problems. it is possible to construct artificial  examples  for  which  the  running  time  of  the  simplex  could  be  very large  (an  exponential  function  of  the  number  of  variables).  however,  those who  have  used  the  algorithm  in  practical  settings  are  unanimous  in  testifying to  its  efficiency  in  solving  actual  problems.
the  simple  version  of  the  simplex  algorithm  that  we’ve  considered,  while quite  useful,  is  merely  part  of  a  general  and  beautiful  mathematical  framework providing  a  complete  set  of  tools  which  can  be  used  to  solve  a  variety  of  very important  practical  problems.
our  goal  is  to  compute  the  values  of  the  variables  which  simultaneously satisfy  the  equations.  depending  on  the  particular  equations  there  may  not always  be  a  solution  to  this  problem  (for  example,  if  two  of  the  equations  are contradictory,  such  as  2 +  y = 1,  z +  y  =  2)  or  there  may  be  many  solutions (for  example,  if  two  equations  are  the  same,  or  there  are  more  variables  than equations).  we’ll  assume  that  the  number  of  equations  and  variables  is  the same,  and  we’ll  look  at  an  algorithm  that  will  find  a  unique  solution  if  one exists.
to  avoid  writing  down  variables  repeatedly,  it  is  convenient  to  use  matrix notation  to  express  the  simultaneous  equations.  the  above  equations  are exactly  equivalent  to  the  matrix  equation
interchange  equations:  clearly,  the  order  in  which  the  equations  are written  down  doesn’t  affect  the  solution.  in  the  matrix  representation, this  operation  corresponds  to  interchanging  rows  in  the  matrix  (and the  vector  on  the  right  hand  side). rename  variables:  this  corresponds  to  interchanging  columns  in  the matrix  representation.  (if  columns  i  and  j  are  switched,  then  variables xi  and  xj  must  also  be  considered  switched.) multiply  equations  by  a  constant:  again,  in  the  matrix  representation, this  corresponds  to  multiplying  a  row  in  the  matrix  (and  the  corresponding element in the vector on the  righbhand  side) by a constant. add  two  equations  and  replace  one  of  them  by  the  sum.  (it  takes  a little  thought  to  convince  oneself  that  this  will  not  affect  the  solution.) for example, we can get a system of equations equivalent to the one above
this  method  is  easily  implemented  by  maintaining  a  two  dimensional array for the  f  vectors,  considering  y  as  the  (m  +  1)st   vector.  then  an  array a[l..m,   i..m+i]   can  be  filled  as  follows:
the  method  of  least  squares  can  be  extended  to  handle  nonlinear  functions  (for  example  a  function  such  as  f(x)  =  cle-c2zsincg~),   and  it  is  often
in  this  diagram,  ti  represents  the  tree  containing  all  the  records  with  keys less than a,  tz,   contains  all  the  records  with  keys  between  a  and  b,  and  so forth.  the  transformation  switches  the  orientation  of  the  s-node  containing a  and  b  without  disturbing  the  rest  of  the  tree.  thus  none  of  the  keys  in ti,   tz,   t3,   and  t,   are  touched.  in  this  case,  the  transformation  is  effected  by the  link  changes  st.l:=gsf.r;   gst.r:=s;   yt.l:=gs.   also,  note  carefully  that  the colors  of  a  and  b  are  switched.  there  are  three  analogous  cases:  the  3-node could  be  oriented  the  other  way,  or  it  could  be  on  the  right  side  of  y  (oriented either  way).
disregarding  the  colors,  this  single  rotation  operation  is  defined  on  any binary  search  tree  and  is  the  basis  for  several  balanced  tree  algorithms.  it  is important  to  note,  however,  that  doing  a  single  rotation  doesn’t  necessarily improve  the  balance  of  the  tree.  in  the  diagram  above,  the  rotation  brings all  the  nodes  in  tl   one  step  closer  to  the  root,  but  all  the  nodes  in  t3  are lowered  one  step.  if  t3  were  to  have  more  nodes  than  tl,   then  the  tree  after the  rotation  would  become  less  balanced,  not  more  balanced.  top-down  2-3-4 trees  may  be  viewed  as  simply  a  convenient  way  to  identify  single  rotations which  are  likely  to  improve  the  balance.
doing  a  single  rotation  involves  structurally  modifying  the  tree,  something  that  should  be  done  with  caution.  a  convenient  way  to  handle  the  four different  cases  outlined  above  is  to  use  the  search  key  v  to  “rediscover”  the relevant son (s) and grandson (gs) of the node y. (we know that we’ll only be reorienting  a  3-node  if  the  search  took  us  to  its  bottom  node.)  this  leads  to somewhat  simpler  code  that  the  alternative  of  remembering  during  the  search not  only  the  two  links  corresponding  to  s  and  gs  but  also  whether  they  are right  or  left  links.  we  have  the  following  function  for  reorienting  a  3-node along  the  search  path  for  v  whose  father  is  y:
if  s  is  the  left  link  of  y  and  gs  is  the  left  link  of  s,  this  makes  exactly  the  link transformations  for  the  diagram  above. the  reader  may  wish  to  check  the
small  index  to  each  key  before  sorting  or  5y  lengthening  the  sort  key  in  some other  way.  it  is  easy  to  take  stability  for  granted:  people  often  react  to  the unpleasant  effects  of  instability  with  disbelief.  actually  there  are  few  methods which  achieve  stability  without  using  significant  extra  time  or  space.
the  following  program,  for  sorting  three  records,  is  intended  to  illustrate the  general  conventions  that  we’ll  be  using.  (in  particular,  the  main  program  is a  peculiar  way  to  exercise  a  program  that  is  known  to  work  only  for  n  =  3:  the point  is  that  most  of  the  sorting  programs  we’ll  consider  could  be  substituted for  sort3  in  this  “driver”  program.)
the  three  assignment  statements  following  each  if  actually  implement  an “exchange”  operation.  we’ll  write  out  the  code  for  such  exchanges  rather  than use  a  procedure  call  because  they’re  fundamental  to  many  sorting  programs and  often  fall  in  the  inner  loop.
in  order  to  concentrate  on  algorithmjc   issues,  we’ll  work  with  algorithms that  simply  sort  arrays  of  integers  into  numerical  order.  it  is  generally  straightforward  to  adapt  such  algorithms  for  use  in  a  practical  application  involving large  keys  or  records.  basically,  sorting  programs  access  records  in  one  of  two ways:  either  keys  are  accessed  for  comparison,  or  entire  records  are  accessed
as  our  first  excursion  into  the  area  of  sorting  algorithms,  we’ll  study some  “elementary”  methods  which  are  appropriate  for  small  files  or files  with  some  special  structure.  there  are  several  reasons  for  studying  these simple  sorting  algorithms  in  some  detail. first,  they  provide  a  relatively painless  way  to  learn  terminology  and  basic  mechanisms  for  sorting  algorithms so  that  we  get  an  adequate  background  for  studying  the  more  sophisticated algorithms.  second,  there  are  a  great  ma.ny   applications  of  sorting  where  it’s better  to  use  these  simple  methods  than  the  more  powerful  general-purpose methods.  finally,  some  of  the  simple  methods  extend  to  better  generalpurpose  methods  or  can  be  used  to  improve  the  efficiency  of  more  powerful methods.  the  most  prominent  example  of  this  is  seen  in  recursive  sorts which  “divide  and  conquer”  big  files  into  many  small  ones.  obviously,  it  is advantageous  to  know  the  best  way  to  deal  with  small  files  in  such  situations. as  mentioned  above,  there  are  several  sorting  applications  in  which  a relatively  simple  algorithm  may  be  the  method  of  choice.  sorting  programs are  often  used  only  once  (or  only  a  few  times).  if  the  number  of  items  to  be sorted  is  not  too  large  (say,  less  than  five  hundred  elements),  it  may  well  be more  efficient  just  to  run  a  simple  method  than  to  implement  and  debug  a complicated  method.  elementary  metho’ds   are  always  suitable  for  small  files (say,  less  than  fifty  elements);  it  is  unlikely  that  a  sophisticated  algorithm would be justified for a small file, unless a very large number of such files are to be  sorted.  other  types  of  files  that  are  relatively  easy  to  sort  are  ones  that  are already  almost  sorted  (or  already  sorted!‘)  or  ones  that  contain  large  numbers of  equal  keys.  simple  methods  can  do  much  better  on  such  well-structured files  than  general-purpose  methods.
as  a  rule,  the  elementary  methods  that  we’ll  be  discussing  take  about n2  steps  to  sort  n  randomly  arranged  items.  if  n  is  small  enough,  this  may not  be  a  problem,  and  if  the  items  are  not  randomly  arranged,  some  of  the
the  primary  reference  for  this  section  is  volume  three  of  d.  e.  knuth’s series  on  sorting  and  searching.  further  information  on  virtually  every  topic that  we’ve  touched  upon  can  be  found  in  that  book.  in  particular,  the  results that  we’ve  quoted  on  performance  chal,acteristics   of  the  various  algorithms are  backed  up  by  complete  mathematic:tl   analyses  in  knuth’s  book.
there  is  a  vast  amount  of  literatllre   on  sorting.  knuth  and  rivest’s 1973  bibliography  contains  hundreds  of  entries,  and  this  doesn’t  include  the treatment  of  sorting  in  countless  books  ind  articles  on  other  subjects  (not  to mention  work  since  1973).
for  quicksort,  the  best  reference  is  hoare’s  original  1962  paper,  which suggests  all  the  important  variants,  including  the  use  for  selection  discussed in  chapter  12.  many  more  details  on  the  mathematical  analysis  and  the practical  effects  of  many  of  the  modifications  and  embellishments  which  have been suggested over the years may be  fat nd in this author’s 1975 ph.d. thesis. a  good  example  of  an  advanced  priority  queue  structure,  as  mentioned  in chapter  11,  is  j.  vuillemin’s  “binomial  cueues”   as  implemented  and  analyzed by  m.  r.  brown.  this  data  structure  supports  all  of  the  priority  queue operations  in  an  elegant  and  efficient  manner.
to  get  an  impression  of  the  myriall   details  of  reducing  algorithms  like those we have discussed to  general-purpoire  practical implementations, a reader would  be  advised  to  study  the  reference  material  for  his  particular  computer system’s  sort  utility.  such  material  necef   sarily  deals  primarily  with  formats  of keys,  records  and  files  as  well  as  many  other  details,  and  it  is  often  interesting to  identify  how  the  algorithms  themselv:s   are  brought  into  play.
m.  r.  brown,  “implementation  and  am.lysis   of  binomial  queue  algorithms,” siam  journal  of  computing,  7,  3,  (august,  1978). c.  a.  r.  hoare,  “quicksort,”  computer  journal,  5,  1  (1962). d.  e.  knuth,  the  art  of  computer  programming.   volume  s:  sorting  and searching,  addison-wesley,  reading,  m9,   second  printing,  1975.
r.  l.  rivest  and  d.  e.  knuth,  “bibliogiaphy   26:  computing  sorting,”  computing  reviews,  13,  6  (june,  1972). r.  sedgewick,  quicksort,  garland,  new  york,  1978.  (also  appeared  as  the author’s  ph.d.  dissertation,  stanford  university,  1975).
there  are  many  other  factors  to  be  t&ken   into  consideration  in  implementing  a  most  efficient  tape-sorting  method.  for  example,  a  major  factor  which we  have  not  considered  at  all  is  the  timt:  that  it  takes  to  rewind  a  tape.  this subject has been studied extensively,  ant  many  fascinating  methods  have  been defined.  however,  as  mentioned  above,  the  savings  achievable  over  the  simple multiway   balanced  merge  are  quite  limited.  even  polyphase  merging  is  only better  than  balanced  merging  for  small  p,  and  then  not  substantially.  for p  >  8,  balanced  merging  is  likely  to  run  j’aster   than  polyphase,  and  for  smaller p  the effect of polyphase is basically to sue  two tapes (a balanced merge with two  extra  tapes  will  run  faster).
an  easier  way many  modern  computer  systems  provide  a  large  virtual  memory  capability which  should  not  be  overlooked  in  imp  ementing  a  method  for  sorting  very large  files.  in  a  good  virtual  memory  syf#tem,   the  programmer  has  the  ability to address a very large amount of data, leaving to the system the responsibility of  making  sure  that  addressed  data  is  lransferred  from  external  to  internal storage when needed. this strategy relict on the fact that many programs have a  relatively  small  “locality  of  reference”  : each  reference  to  memory  is  likely  to be  to  an  area  of  memory  that  is  relatively   close  to  other  recently  referenced areas.  this  implies  that  transfers  from  e:rternal   to  internal  storage  are  needed infrequently.  an  int,ernal   sorting  method  with  a  small  locality  of  reference  can work  very  well  on  a  virtual  memory  system.  (for  example,  quicksort  has  two “localities”  : most  references  are  near  one  of  the  two  partitioning  pointers.) but  check  with  your  systems  programmclr   before  trying  it  on  a  very  large  file: a  method  such  as  radix  sorting,  which  he,s  no  locality  of  reference  whatsoever, would  be  disastrous  on  a  virtual  memory   system,  and  even  quicksort  could cause  problems,  depending  on  how  well  the  available  virtual  memory  system is  implemented.  on  the  other  hand,  th’:   strategy  of  using  a  simple  internal sorting  method  for  sorting  disk  files  desl:rves   serious  consideration  in  a  good virtual  memorv   environment.
working.  we’ll  see  methods  that  are  more  efficient  in  the  next  few  chapters, but  they’re  perhaps  only  twice  as  fast  (if  that  much)  except  for  large  n,  and they’re  significantly  more  complicated.  in  short,  if  you  have  a  sorting  problem, use  the  above  program,  then  determine  vvhether  the  extra  effort  required  to replace  it  with  a  sophisticated  method  will  be  worthwhile.  (on  the  other hand,  the  quicksort  algorithm  of  the  next  chapter  is  not  that  much  more difficult  to  implement.  .  .  )
digression: bubble  sort an  elementary  sorting  method  that  is  often  taught  in  introductory  classes  is bubble  sort:  keep  passing  through  the  file,  exchanging  adjacent  elements,  if necessary;  when  no  exchanges  are  required  on  some  pass,  the  file  is  sorted. an  implementation  of  this  method  is  given  below.
it  takes  a  moment’s  reflection  to  convince  oneself  first  that  this  works  at  all, it  is  not  clear  why  this  method second  that  the  running  time  is  quadratic. is  so  often  taught,  since  insertion  sort  seems  simpler  and  more  efficient  by almost  any  measure.  the  inner  loop  of  bubble  sort  has  about  twice  as  many instructions  as  either  insertion  sort  or  selection  sort.
distribution  counting a  very  special  situation  for  which  there  is  a  simple  sorting  algorithm  is  the following:  “sort  a  file  of  n  records  whose  keys  are  distinct  integers  between  1 and  n.”   the  algorithm  for  this  problem  is
for  example,  when  the  4  at  the  end  of  the  file  is  encountered,  it’s  put  into location  12,  since  count[4]  says  that  there  are  12  keys  less  than  or  equal  to 4.  then  count[4]  is  decremented,  since  there’s  now  one  less  key  less  than  or equal  to  4.  the  inner  loop  goes  from  n  down  to  1  so  that  the  sort  will  be stable.  (the  reader  may  wish  to  check  this.)
this  method  will  work  very  well  for  the  type  of  files  postulated.  furthermore,  it  can  be  extended  to  produce  a  much  more  powerful  method  that  we’ll examine  in  chapter  10.
non-random  files we  usually  think  of  sorting  files  that  are  in  some  arbitrarily  scrambled  order. however,  it  is  quite  often  the  case  that  we  have  a  lot  of  information  about  a file  to  be  sorted.  for  example,  one  often  wants  to  add  a  few  elements  to  a sorted file and thus produce a larger sorted file. one way to do so is to simply append  the  new  elements  to  the  end  of  t.he   file,  then  call  a  sorting  algorithm. general-purpose  sorts  are  commonly  mi’sused   for  such  applications;  actually, elementary  methods  can  take  advantage  of  the  order  present  in  the  file.
for  example,  consider  the  operation  of  insertion  sort  on  a  file  which  is already  sorted.  each  element  is  immediately  determined  to  be  in  its  proper  in the  file,  and  the  total  running  time  is  linear.  the  same  is  true  for  bubble  sort, but  selection  sort  is  still  quadratic.  (the  leading  term  in  the  running  time  of selection  sort  does  not  depend  on  the  order  in  the  file  to  be  sorted.)
even  if  a  file  is  not  completely  sorted,  insertion  sort  can  be  quite  useful because  the  running  time  of  insertion  sort  depends  quite  heavily  on  the  order present  in  the  file.  the  running  time  depends  on  the  number  of  inversions:  for each  element  count  up  the  number  of  e:iements   to  its  left  which  are  greater. this  is  the  distance  the  elements  have  to  move  when  inserted  into  the  file during  insertion  sort.  a  file  which  has  some  order  in  it  will  have  fewer inversions  in  it  than  one  which  is  arbitrarily  scrambled.
the  example  cited  above  of  a  file  formed  by  tacking  a  few  new  elenients onto  a  large  sorted  file  is  clearly  a  case  where  the  number  of  the  inversions is  low:  a  file  which  has  only  a  constant  number  of  elements  out  of  place  will have  only  a  linear  number  of  inversions.  another  example  is  a  file  where  each element  is  only  a  constant  distance  front  its  final  position.  files  like  this  can be  created  in  the  initial  stages  of  some  advanced  sorting  methods:  at  a  certain point  it  is  worthwhile  to  switch  over  to  jnsertion   sort.
in  short,  insertion  sort  is  the  method  of  choice  for  “almost  sorted”  files with  few  inversions:  for  such  files,  it  will  outperform  even  the  sophisticated methods  in  the  next  few  chapters.
as  our  first  excursion  into  the  area  of  sorting  algorithms,  we’ll  study some  “elementary”  methods  which  are  appropriate  for  small  files  or files  with  some  special  structure.  there  are  several  reasons  for  studying  these simple  sorting  algorithms  in  some  detail. first,  they  provide  a  relatively painless  way  to  learn  terminology  and  basic  mechanisms  for  sorting  algorithms so  that  we  get  an  adequate  background  for  studying  the  more  sophisticated algorithms.  second,  there  are  a  great  ma.ny   applications  of  sorting  where  it’s better  to  use  these  simple  methods  than  the  more  powerful  general-purpose methods.  finally,  some  of  the  simple  methods  extend  to  better  generalpurpose  methods  or  can  be  used  to  improve  the  efficiency  of  more  powerful methods.  the  most  prominent  example  of  this  is  seen  in  recursive  sorts which  “divide  and  conquer”  big  files  into  many  small  ones.  obviously,  it  is advantageous  to  know  the  best  way  to  deal  with  small  files  in  such  situations. as  mentioned  above,  there  are  several  sorting  applications  in  which  a relatively  simple  algorithm  may  be  the  method  of  choice.  sorting  programs are  often  used  only  once  (or  only  a  few  times).  if  the  number  of  items  to  be sorted  is  not  too  large  (say,  less  than  five  hundred  elements),  it  may  well  be more  efficient  just  to  run  a  simple  method  than  to  implement  and  debug  a complicated  method.  elementary  metho’ds   are  always  suitable  for  small  files (say,  less  than  fifty  elements);  it  is  unlikely  that  a  sophisticated  algorithm would be justified for a small file, unless a very large number of such files are to be  sorted.  other  types  of  files  that  are  relatively  easy  to  sort  are  ones  that  are already  almost  sorted  (or  already  sorted!‘)  or  ones  that  contain  large  numbers of  equal  keys.  simple  methods  can  do  much  better  on  such  well-structured files  than  general-purpose  methods.
as  a  rule,  the  elementary  methods  that  we’ll  be  discussing  take  about n2  steps  to  sort  n  randomly  arranged  items.  if  n  is  small  enough,  this  may not  be  a  problem,  and  if  the  items  are  not  randomly  arranged,  some  of  the
for  example,  when  the  4  at  the  end  of  the  file  is  encountered,  it’s  put  into location  12,  since  count[4]  says  that  there  are  12  keys  less  than  or  equal  to 4.  then  count[4]  is  decremented,  since  there’s  now  one  less  key  less  than  or equal  to  4.  the  inner  loop  goes  from  n  down  to  1  so  that  the  sort  will  be stable.  (the  reader  may  wish  to  check  this.)
this  method  will  work  very  well  for  the  type  of  files  postulated.  furthermore,  it  can  be  extended  to  produce  a  much  more  powerful  method  that  we’ll examine  in  chapter  10.
non-random  files we  usually  think  of  sorting  files  that  are  in  some  arbitrarily  scrambled  order. however,  it  is  quite  often  the  case  that  we  have  a  lot  of  information  about  a file  to  be  sorted.  for  example,  one  often  wants  to  add  a  few  elements  to  a sorted file and thus produce a larger sorted file. one way to do so is to simply append  the  new  elements  to  the  end  of  t.he   file,  then  call  a  sorting  algorithm. general-purpose  sorts  are  commonly  mi’sused   for  such  applications;  actually, elementary  methods  can  take  advantage  of  the  order  present  in  the  file.
for  example,  consider  the  operation  of  insertion  sort  on  a  file  which  is already  sorted.  each  element  is  immediately  determined  to  be  in  its  proper  in the  file,  and  the  total  running  time  is  linear.  the  same  is  true  for  bubble  sort, but  selection  sort  is  still  quadratic.  (the  leading  term  in  the  running  time  of selection  sort  does  not  depend  on  the  order  in  the  file  to  be  sorted.)
even  if  a  file  is  not  completely  sorted,  insertion  sort  can  be  quite  useful because  the  running  time  of  insertion  sort  depends  quite  heavily  on  the  order present  in  the  file.  the  running  time  depends  on  the  number  of  inversions:  for each  element  count  up  the  number  of  e:iements   to  its  left  which  are  greater. this  is  the  distance  the  elements  have  to  move  when  inserted  into  the  file during  insertion  sort.  a  file  which  has  some  order  in  it  will  have  fewer inversions  in  it  than  one  which  is  arbitrarily  scrambled.
the  example  cited  above  of  a  file  formed  by  tacking  a  few  new  elenients onto  a  large  sorted  file  is  clearly  a  case  where  the  number  of  the  inversions is  low:  a  file  which  has  only  a  constant  number  of  elements  out  of  place  will have  only  a  linear  number  of  inversions.  another  example  is  a  file  where  each element  is  only  a  constant  distance  front  its  final  position.  files  like  this  can be  created  in  the  initial  stages  of  some  advanced  sorting  methods:  at  a  certain point  it  is  worthwhile  to  switch  over  to  jnsertion   sort.
in  short,  insertion  sort  is  the  method  of  choice  for  “almost  sorted”  files with  few  inversions:  for  such  files,  it  will  outperform  even  the  sophisticated methods  in  the  next  few  chapters.
as  mentioned  above,  the  primary  reason  that  heapsort   is  of  practical interest is that the number of steps required to sort m  elements is  guaranteed to  be  proportional  to  m  log  m,  no  matter  what  the  input.  unlike  the  other methods  that  we’ve  seen,  there  is  no  “worst-case”  input  that  will  make  heapsort  run  slower.  the  proof  of  this  is  simple:  we  make  about  3m/2   calls  to downheap  (about  m/2   to  construct  the  heap  and  m  for  the  sort),  each  of which  examines  less  than  log  m  heap  elements,  since  the  heap  never  has  more than  m  elements.
actually,  the  above  proof  uses  an  overestimate.  in  fact,  it  can  be  proven that  the  construction  process  takes  linear  time  since  so  many  small  heaps  are processed.  this  is  not  of  particular  importance  to  heapsort,  since  this  time is  still  dominated  by  the  m  log  m  time  for  sorting,  but  it  is  important  for other  priority  queue  applications,  where  a  linear  time  construct  can  lead  to a  linear  time  algorithm.  note  that  constructing  a  heap  with  m  successive inserts  requires  m  log  m  steps  in  the  worst  case  (though  it  turns  out  to  be linear  on  the  average).
the  second  position,  continuing  in  this  way  until  the  entire  array  is  sorted. this  method  is  called  selection  sort  because  it  works  by  repeatedly  “selecting” the  smallest  remaining  element. the  following  program  sorts  a  [1..n]  into numerical  order:
this  is  among  the  simplest  of  sorting  methods,  and  it  will  work  very  well  for small  files.  its  running  time  is  proportional  to  n2:   the  number  of  comparisons between array elements is about  n2/2  since the outer loop (on i) is executed n times  and  the  inner  loop  (on  j)  is  executed  about  n/2  times  on  the  average.  it turns  out  that  the  statement  min:=j  is  executed  only  on  the  order  of  n  log  n times,  so  it  is  not  part  of  the  inner  loop
despite  its  simplicity,  selection  sort  has  a  quite  important  application: it  is  the  method  of  choice  for  sorting  files  with  very  large  records  and  small keys. if the records are  m words long (but the keys are only a few words long), then  the  exchange  takes  time  proportional  to  m,  so  the  total  running  time is  proportional  to  n2   (for  the  comparisons)  plus  nm  (for  the  exchanges).  if m  is  proportional  to  n  then  the  running  time  is  linear  in  the  amount  of  data input,  which  is  difficult  to  beat  even  with  an  advanced  method.  of  course  if it  is  not  absolutely  required  that  the  records  be  actually  rearranged,  then  an “indirect  sort”  can  be  used  to  avoid  the  nm  term  entirely,  so  a  method  which uses  less  comparisons  would  be  justified.  still  selection  sort  is  quite  attractive for  sorting  (say)  a  thousand  looo-word   records  on  one-word  keys.
insertion  sort an  algorithm  almost  as  simple  as  selection  sort  but  perhaps  more  flexible  is insertion sort.  this  is  the  method  often  used  by  people  to  sort  bridge  hands: consider  the  elements  one  at  a  time,  inserting  each  in  its  proper  place  among those  already  considered  (keeping  them  s.orted).   the  element  being  considered is  inserted  merely  by  moving  larger  elements  one  position  to  the  right,  then
as  is,  this  code  doesn’t  work,  because  the  while  will  run  past  the  left  end of  the  array  if  t  is  the  smallest  element  in  the  array.  one  way  to  fix  this  is to  put  a  “sentinel”  key  in  a[o], making  it  at  least  as  small  as  the  smallest element  in  the  array.  using  sentinels  in  situations  like  this  is  common  in sorting  programs  to  avoid  including  a  test  (in  this  case  j>l)  which  almost always  succeeds  within  the  inner  loop.  if  for  some  reason  it  is  inconvenient  to use  a  sentinel  and  the  array  really  must  have  the  bounds  [1..n],  then  standard pascal does not allow a clean alternative, since it does not have a “conditional” and  instruction:  the  test  while  (j>l) and  (a1j-l]>v)  won’t  work  because even  when  j=l,  the  second  part  of  the  and  will  be  evaluated  and  will  cause an  o&of-bounds   array  access.  a  goto  out  of  the  loop  seems  to  be  required. (some  programmers  prefer  to  goto   some  lengths  to  avoid  goto  instructions, for  example  by  performing  an  action  within  the  loop  to  ensure  that  the  loop terminates.  in  this  case,  such  a  solution  seems  hardly  justified,  since  it  makes the  program  no  clearer,  and  it  adds  extra  overhead  everytime  through  the loop  to  guard  against  a  rare  event.)
on  the  average,  the  inner  loop  of  insertion  sort  is  executed  about  n2/2 times:  the  “average”  insertion  goes  about  halfway  into  a  subfile  of  size  n/2. this  is  inherent  in  the  method.  the  point  of  insertion  can  be  found  more efficiently  using  the  searching  techniques  in  chapter  14,  but  n2/2   moves  (to make  room  for  each  element  being  inserted)  are  still  required;  or  the  number of  moves  can  be  lowered  by  using  a  linked  list  instead  of  an  array,  but  then the  methods  of  chapter  14  don’t  apply  and  n2/2   comparisons  are  required (to  find  each  insertion  point).
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
(it is convenient to pass the list length as  e. parameter to the recursive program: alternatively,  it  could  be  stored  with  the  list  or  the  program  could  scan  the list  to  find  its  length.)
this program sorts by splitting the list po: nted to by c into two halves, pointed to by a and b,  sorting the two halves recursively, then using merge to produce the  final  result.  again,  this  program  adheres  to  the  convention  that  all  lists end  with  z:  the  input  list  must  end  with  z  (and  therefore  so  does  the  b  list); and  the  explicit  instruction  cf.next:=z   puts  z  at  the  end  of  the  a  list.  this program  is  quite  simple  to  understand  in  a  recursive  formulation  even  though it  actually  is  a  rather  sophisticated  algorithm.
the  running  time  of  the  program  fits   the  standard  “divide-and-conquer” recurrence  m(n)  =  2m(n/2)   +  n.  the  program  is  thus  guaranteed  to  run in  time  proportional  to  nlogn.   (see  chapter  4).
our  file  of  sample  sorting  keys  is  processed  as  shown  in  the  following table.  each  line  in  the  table  shows  the  result  of  a  call  on  merge.  first  we merge  0  and  s  to  get  0  s,  then  we  merge  this  with  a  to  get  a  0  s.  eventually we  merge  r  t  with  i  n  to  get  i  n  r  t,  then  this  is  merged  with  a  0  s  to get  a  i  n  0  r  s  t,  etc.:
implement  a  recursive  quicksort  with  a  cutoff  to  insertion  sort  for  subfiles with  less  than  m  elements  and  empirically  determine  the  value  of  m  for which  it  runs  fastest  on  a  random  file  of  1000  elements.
use  a  least  squares  curvefitter  to  find  values  of  a  and  b  that  give  the best  formula  of  the  form  an   in  n  +  bn  for  describing  the  total  number of  instructions  executed  when  quicksort  is  run  on  a  random  file.
contain  many  equal  keys.  radix  exchange  sort  is  actually  slightly  faster  than quicksort  if  the  keys  to  be  sorted  are  comprised  of  truly  random  bits,  but quicksort  can  adapt  better  to  less  randon   situations.
straight radix sort an  alternative  radix  sorting  method  is  tc  examine  the  bits  from  right  to  left. this  is  the  method  used  by  old  computer-card-sorting  machines:  a  deck  of cards was run through the machine 80  times, once for each column, proceeding from  right  to  left.  the  following  example  shows  how  a  right-to-left  bit-by-bit radix  sort  works  on  our  file  of  sample  ke:rs.
a 00001 r 10010 s 10011 t 10100 0 01111 n 01110 r 10010 x 11000 p 10000 t 10100 l 01100 i 01001 a 00001 n 01110 g 00111 s 10011 0 01111 e 00101 x 11000 i 01001 a 00001 g 00111 m 01101 e 00101 p 10000 a 00001 l 01100 m 01101 e 00101 e 00101
p 10000 a 00001 a 00001 a 00001 a 00001 e 00101 r 10010 e 00101 s 10011 g 00111 i 01001 t 10100 e 00101 l 01100 e 00101 m 01101 g 00111 n 01110 0 01111 x 11000 i 01001 p 10000 l 01100 r 10010 s 10011 m 01101 t 10100 n 01110 0 01111 x 11000
the  ith  column  in  this  table  is  sorted  on  the  trailing  i  bits  of  the  keys. the  ith  column  is  derived  from  the  (i  -  l$t  column  by  extracting  all  the  keys with  a  0  in  the  ith  bit,  then  all  the  keys  with  a  1  in  the  ith  bit.
it’s  not  easy  to  be  convinced  that  the  method  works;  in  fact  it  doesn’t work  at  all  unless  the  one-bit  partitioning  process  is  stable.  once  stability has  been  identified  as  being  important,  a  trivial  proof  that  the  method  works can  be  found:  after  putting  keys  with  ti,h  bit  0  before  those  with  ith  bit  1 (in  a  stable  manner)  we  know  that  any  l,wo   keys  appear  in  proper  order  (on the  basis  of  the  bits  so  far  examined)  in  the  file  either  because  their  ith  bits are  different,  in  which  case  partitioning  puts  them  in  the  proper  order,  or because  their  ith  bits  are  the  same,  in  which  case  they’re  in  proper  order because  of  stability.  the  requirement  01%  stability  means,  for  example,  that
to  be  moved.  most  of  the  algorithms  that  we  will  study  can  be  recast  in  terms of  performing  these  two  operations  on  arbitrary  records.  if  the  records  to  be sorted  are  large,  it  is  normally  wise  to  do  an  “indirect  sort”:  here  the  records themselves  are  not  necessarily  rearranged,  but  rather  an  array  of  pointers  (or indices)  is  rearranged  so  that  the  first  pointer  points  to  the  smallest  record, etc.  the  keys  can  be  kept  either  with  the  records  (if  they  are  large)  or  with the  pointers  (if  they  are  small).
by  using  programs  which  simply  operate  on  a  global  array,  we’re  ignoring “packaging  problems”  that  can  be  troublesome  in  some  programming  environments.  should  the  array  be  passed  to  the  sorting  routine  as  a  parameter? can  the  same  sorting  routine  be  used  to  sort  arrays  of  integers  and  arrays of  reals  (and  arrays  of  arbitrarily  complex  records)?  even  with  our  simple assumptions,  we  must  (as  usual)  circumvent  the  lack  of  dynamic  array  sizes in  pascal  by  predeclaring  a  maximum.  such  concerns  will  be  easier  to  deal with  in  programming  environments  of  the  future  than  in  those  of  the  past and  present.  for  example,  some  modern  languages  have  quite  well-developed facilities  for  packaging  together  programs  into  large  systems.  on  the  other hand,  such  mechanisms  are  not  truly  required  for  many  applications:  small programs  which  work  directly  on  global  arrays  have  many  uses;  and  some operating  systems  make  it  quite  easy  to  put  together  simple  programs  like the  one  above,  which  serve  as  “filters”  between  their  input  and  their  output. obviously,  these  comments  apply  to  many  of  the  other  algorithms  that  we’ll be  examining,  though  their  effects  are  perhaps  most  acutely  felt  for  sorting algorithms.
some  of  the  programs  use  a  few  other  global  variables.  declarations which  are  not  obvious  will  be  included  with  the  program  code.  also,  we’ll sometimes  assume  that  the  array  bounds  go  to  0  or  iv+1,  to  hold  special  keys used  by  some  of  the  algorithms.  we’ll  frequently  use  letters  from  the  alphabet rather  than  numbers  for  examples:  these  are  handled  in  the  obvious  way  using pascal’s  ord  and  chr   “transfer  functions”  between  integers  and  characters.
the sort3 program above uses an even more constrained access to the file: it  is  three  instructions  of  the  form  “compare  two  records  and  exchange  them if  necessary  to  put  the  one  with  the  smaller  key  first.”  programs  which  use only  this  type  of  instruction  are  interesting  because  they  are  well  suited  for hardware  implementation.  we’ll  study  this  issue  in  more  detail  in  chapter 35.
selection  sort one  of  the  simplest  sorting  algorithms  works  as  follows:  first  find  the  smallest element  in  the  array  and  exchange  it  with  the  element  in  the  first  position, then  find  the  second  smallest  element  and  exchange  it  with  the  element  in
the  second  position,  continuing  in  this  way  until  the  entire  array  is  sorted. this  method  is  called  selection  sort  because  it  works  by  repeatedly  “selecting” the  smallest  remaining  element. the  following  program  sorts  a  [1..n]  into numerical  order:
this  is  among  the  simplest  of  sorting  methods,  and  it  will  work  very  well  for small  files.  its  running  time  is  proportional  to  n2:   the  number  of  comparisons between array elements is about  n2/2  since the outer loop (on i) is executed n times  and  the  inner  loop  (on  j)  is  executed  about  n/2  times  on  the  average.  it turns  out  that  the  statement  min:=j  is  executed  only  on  the  order  of  n  log  n times,  so  it  is  not  part  of  the  inner  loop
despite  its  simplicity,  selection  sort  has  a  quite  important  application: it  is  the  method  of  choice  for  sorting  files  with  very  large  records  and  small keys. if the records are  m words long (but the keys are only a few words long), then  the  exchange  takes  time  proportional  to  m,  so  the  total  running  time is  proportional  to  n2   (for  the  comparisons)  plus  nm  (for  the  exchanges).  if m  is  proportional  to  n  then  the  running  time  is  linear  in  the  amount  of  data input,  which  is  difficult  to  beat  even  with  an  advanced  method.  of  course  if it  is  not  absolutely  required  that  the  records  be  actually  rearranged,  then  an “indirect  sort”  can  be  used  to  avoid  the  nm  term  entirely,  so  a  method  which uses  less  comparisons  would  be  justified.  still  selection  sort  is  quite  attractive for  sorting  (say)  a  thousand  looo-word   records  on  one-word  keys.
insertion  sort an  algorithm  almost  as  simple  as  selection  sort  but  perhaps  more  flexible  is insertion sort.  this  is  the  method  often  used  by  people  to  sort  bridge  hands: consider  the  elements  one  at  a  time,  inserting  each  in  its  proper  place  among those  already  considered  (keeping  them  s.orted).   the  element  being  considered is  inserted  merely  by  moving  larger  elements  one  position  to  the  right,  then
shellsort insertion  sort  is  slow  because  it  exchang,es   only  adjacent  elements.  for  example,  if  the  smallest  element  happens  to  be  at  the  end  of  the  array,  it  takes n  steps  to  get  it  where  it  belongs.  shellsort  is  a  simple  extension  of  insertion sort  which  gets  around  this  problem  by  allowing  exchanges  of  elements  that are  far  apart.
if  we  replace  every  occurrence  of  “1”  by  “h”  (and  “2”  by  “h+l”)   in insertion  sort,  the  resulting  program  rearranges  a  file  to  give  it  the  property that  taking  every  hth  element  (starting  anywhere)  yields  a  sorted  file.  such  a file  is  said  to  be  h-sorted.  put  another  way,  an  h-sorted  file  is  h  independent sorted  files,  interleaved  together.  by  h-sorting  for  some  large  values  of  h,  we can move elements in the array long distances and thus make it easier to h-sort for  smaller  values  of  h.  using  such  a  procedure  for  any  sequence  of  values  of h  which  ends  in  1  will  produce  a  sorted  file:  this  is  shellsort.
in  the  first  pass,  the  a  in  position  1  is  compared  to  the  l  in  position  14,  then the  s  in  position  2  is  compared  (and  exchanged)  with  the  e  in  position  15.  in the  second  pass,  the  a  t  e  p  in  positions  1,  5,  9,  and  13  are  rearranged  to put  a  e  p  t  in  those  positions,  and  similarly  for  positions  2,  6,  10,  and  14, etc.  the  last  pass  is  just  insertion  sort,  but  no  element  has  to  move  very  far. the  above  description  of  how  shellsort  gains  efficiency  is  necessarily imprecise  because  no  one  has  been  able  to  analyze  the  algorithm.  some sequences  of  values  of  h  work  better  than  others,  but  no  explanation  for  this has been discovered. a sequence which has been shown empirically to do well is . . .  ,1093,364,121,40,13,4,1,
working.  we’ll  see  methods  that  are  more  efficient  in  the  next  few  chapters, but  they’re  perhaps  only  twice  as  fast  (if  that  much)  except  for  large  n,  and they’re  significantly  more  complicated.  in  short,  if  you  have  a  sorting  problem, use  the  above  program,  then  determine  vvhether  the  extra  effort  required  to replace  it  with  a  sophisticated  method  will  be  worthwhile.  (on  the  other hand,  the  quicksort  algorithm  of  the  next  chapter  is  not  that  much  more difficult  to  implement.  .  .  )
digression: bubble  sort an  elementary  sorting  method  that  is  often  taught  in  introductory  classes  is bubble  sort:  keep  passing  through  the  file,  exchanging  adjacent  elements,  if necessary;  when  no  exchanges  are  required  on  some  pass,  the  file  is  sorted. an  implementation  of  this  method  is  given  below.
it  takes  a  moment’s  reflection  to  convince  oneself  first  that  this  works  at  all, it  is  not  clear  why  this  method second  that  the  running  time  is  quadratic. is  so  often  taught,  since  insertion  sort  seems  simpler  and  more  efficient  by almost  any  measure.  the  inner  loop  of  bubble  sort  has  about  twice  as  many instructions  as  either  insertion  sort  or  selection  sort.
distribution  counting a  very  special  situation  for  which  there  is  a  simple  sorting  algorithm  is  the following:  “sort  a  file  of  n  records  whose  keys  are  distinct  integers  between  1 and  n.”   the  algorithm  for  this  problem  is
methods  might  run  much  faster  than  more  sophisticated  ones.  however,  it must  be  emphasized  that  these  methods  (with  one  notable  exception)  should not  be  used  for  large,  randomly  arranged  files.
rules  of  the  game before  considering  some  specific  algorithms,  it  will  be  useful  to  discuss  some general  terminology  and  basic  assumptions  for  sorting  algorithms.  we’ll  be considering  methods  of  sorting  files  of  records  containing  keys.  the  keys, which  are  only  part  of  the  records  (often  a  small  part),  are  used  to  control  the sort.  the  objective  of  the  sorting  method  is  to  rearrange  the  records  so  that their  keys  are  in  order  according  to  some  well-defined  ordering  rule  (usually numerical  or  alphabetical  order).
if  the  file  to  be  sorted  will  fit  into  memory  (or,  in  our  context,  if  it  will fit  into  a  pascal  array),  then  the  sorting  method  is  called  internal.  sorting files  from  tape  or  disk  is  called  external  sorting.  the  main  difference  between the  two  is  that  any  record  can  easily  be  accessed  in  an  internal  sort,  while an  external  sort  must  access  records  sequentially,  or  at  least  in  large  blocks. we’ll  look  at  a  few  external  sorts  in  chapter  13,  but  most  of  the  algorithms that  we’ll  consider  are  internal  sorts.
as  usual,  the  main  performance  parameter  that  we’ll  be  interested  in  is the  running  time  of  our  sorting  algorithms.  as  mentioned  above,  the  elementary  methods  that  we’ll  examine  in  this  chapter  require  time  proportional to  n2  to  sort  n  items,  while  more  advanced  methods  can  sort  n  items  in time  proportional  to  n  log  n.  it  can  be  shown  that  no  sorting  algorithm can  use  less  than  n  log  n  comparisons  between  keys,  but  we’ll  see  that  there are  methods  that  use  digital  properties  of  keys  to  get  a  total  running  time proportional  to  n.
the  amount  of  extra  memory  used  by  a  sorting  algorithm  is  the  second important  factor  we’ll  be  considering.  basically,  the  methods  divide  into  three types:  those  that  sort  in  place  and  use  no  extra  memory  except  perhaps  for a  small  stack  or  table;  those  that  use  a  linked-list  representation  and  so  use n  extra  words  of  memory  for  list  pointers;  and  those  that  need  enough  extra memory  to  hold  another  copy  of  the  array  to  be  sorted.
a  characteristic  of  sorting  methods  which  is  sometimes  important  in practice  is  stability: a  sorting  method  is  called  stable  if  it  preserves  the  relative order  of  equal  keys  in  the  file.  for  example,  if  an  alphabetized  class  list  is sorted  by  grade,  then  a  stable  method  will  produce  a  list  in  which  students with  the  same  grade  are  still  in  alphabetical  order,  but  a  non-stable  method  is likely  to  produce  a  list  with  no  evidence  of  the  original  alphabetic  order.  most of  the  simple  methods  are  stable,  but  most  of  the  well-known  sophisticated algorithms  are  not.  if  stability  is  vital,  it  can  be  forced  by  appending  a
small  index  to  each  key  before  sorting  or  5y  lengthening  the  sort  key  in  some other  way.  it  is  easy  to  take  stability  for  granted:  people  often  react  to  the unpleasant  effects  of  instability  with  disbelief.  actually  there  are  few  methods which  achieve  stability  without  using  significant  extra  time  or  space.
the  following  program,  for  sorting  three  records,  is  intended  to  illustrate the  general  conventions  that  we’ll  be  using.  (in  particular,  the  main  program  is a  peculiar  way  to  exercise  a  program  that  is  known  to  work  only  for  n  =  3:  the point  is  that  most  of  the  sorting  programs  we’ll  consider  could  be  substituted for  sort3  in  this  “driver”  program.)
the  three  assignment  statements  following  each  if  actually  implement  an “exchange”  operation.  we’ll  write  out  the  code  for  such  exchanges  rather  than use  a  procedure  call  because  they’re  fundamental  to  many  sorting  programs and  often  fall  in  the  inner  loop.
in  order  to  concentrate  on  algorithmjc   issues,  we’ll  work  with  algorithms that  simply  sort  arrays  of  integers  into  numerical  order.  it  is  generally  straightforward  to  adapt  such  algorithms  for  use  in  a  practical  application  involving large  keys  or  records.  basically,  sorting  programs  access  records  in  one  of  two ways:  either  keys  are  accessed  for  comparison,  or  entire  records  are  accessed
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
there  are  many  other  factors  to  be  t&ken   into  consideration  in  implementing  a  most  efficient  tape-sorting  method.  for  example,  a  major  factor  which we  have  not  considered  at  all  is  the  timt:  that  it  takes  to  rewind  a  tape.  this subject has been studied extensively,  ant  many  fascinating  methods  have  been defined.  however,  as  mentioned  above,  the  savings  achievable  over  the  simple multiway   balanced  merge  are  quite  limited.  even  polyphase  merging  is  only better  than  balanced  merging  for  small  p,  and  then  not  substantially.  for p  >  8,  balanced  merging  is  likely  to  run  j’aster   than  polyphase,  and  for  smaller p  the effect of polyphase is basically to sue  two tapes (a balanced merge with two  extra  tapes  will  run  faster).
an  easier  way many  modern  computer  systems  provide  a  large  virtual  memory  capability which  should  not  be  overlooked  in  imp  ementing  a  method  for  sorting  very large  files.  in  a  good  virtual  memory  syf#tem,   the  programmer  has  the  ability to address a very large amount of data, leaving to the system the responsibility of  making  sure  that  addressed  data  is  lransferred  from  external  to  internal storage when needed. this strategy relict on the fact that many programs have a  relatively  small  “locality  of  reference”  : each  reference  to  memory  is  likely  to be  to  an  area  of  memory  that  is  relatively   close  to  other  recently  referenced areas.  this  implies  that  transfers  from  e:rternal   to  internal  storage  are  needed infrequently.  an  int,ernal   sorting  method  with  a  small  locality  of  reference  can work  very  well  on  a  virtual  memory  system.  (for  example,  quicksort  has  two “localities”  : most  references  are  near  one  of  the  two  partitioning  pointers.) but  check  with  your  systems  programmclr   before  trying  it  on  a  very  large  file: a  method  such  as  radix  sorting,  which  he,s  no  locality  of  reference  whatsoever, would  be  disastrous  on  a  virtual  memory   system,  and  even  quicksort  could cause  problems,  depending  on  how  well  the  available  virtual  memory  system is  implemented.  on  the  other  hand,  th’:   strategy  of  using  a  simple  internal sorting  method  for  sorting  disk  files  desl:rves   serious  consideration  in  a  good virtual  memorv   environment.
priority-first  search  method  will  be  faster  for  some  graphs,  prim’s  for  some others,  kruskal’s  for  still  others.  as  mentioned  above,  the  worst  case  for  the priority-first  search  method  is  (e  +  v)logv   while  the  worst  case  for  prim’s is  v2  and  the  worst  case  for  kruskal’s  is  elog  e.  but  it  is  unwise  to  choose between  the  algorithms  on  the  basis  of  these  formulas  because  “worstrcase” graphs  are  unlikely  to  occur  in  practice. in  fact,  the  priority-first  search method  and  kruskal’s  method  are  both  likely  to  run  in  time  proportional  to e  for  graphs  that  arise  in  practice:  the  first  because  most  edges  do  not  really require  a  priority  queue  adjustment  that  takes  1ogv   steps  and  the  second because  the  longest  edge  in  the  minimum  spanning  tree  is  probably  sufficiently short  that  not  many  edges  are  taken  off  the  priority  queue.  of  course,  prim’s method  also  runs  in  time  proportional  to  about  e  for  dense  graphs  (but  it shouldn’t  be  used  for  sparse  graphs).
the  shortest path  problem  is  to  find  the  path  in  a  weighted  graph  connecting two  given  vertices  x  and  y  with  the  property  that  the  sum  of  the  weights  of all  the  edges  is  minimized  over  all  such  paths.
if  the  weights  are  all  1,  then  the  problem  is  still  interesting:  it  is  to  find the  path  containing  the  minimum  number  of  edges  which  connects  x  and  y. moreover,  we’ve  already  considered  an  algorithm  which  solves  the  problem: breadth-first  search.  it  is  easy  to  prove  by  induction  that  breadth-first  search starting  at  x  will  first  visit  all  vertices  which  can  be  reached  from  z  with  1 edge,  then  al:  vertices  which  can  be  reached  from  x  with  2  edges,  etc.,  visiting all  vertices  which  can  be  reached  with  k  edges  before  encountering  any  that require  k  +  1  edges.  thus,  when  y  is  first  encountered,  the  shortest  path  from x  has  been  found  (because  no  shorter  paths  reached  y).
consider  the  problem  of  finding  the  shortest  paths  connecting  a  given  vertex x  with  each  of  the  other  vertices  in  the  graph.  again,  it  turns  out  that  the problem  is  simple  to  solve  with  the  priority  graph  traversal  algorithm  of  the previous  chapter.
if  we  draw  the  shortest  path  from  x  to  each  other  vertex  in  the  graph, then  we  clearly  get  no  cycles,  and  we  have  a  spanning  tree.  each  vertex  leads to  a  different  spanning  tree;  for  example,  the  following  three  diagrams  show the  shortest  path  spanning  trees  for  vertices  a,  b,  and  e  in  the  example  graph that  we’ve  been  using.
instead  of  indirect  heap).  these  changes  yield  a  worst-case  running  time proportional  to  v2, as  opposed  to  (e  +  v)logv   for  sparsepfs.  that  is,  the running  time  is  linear  for  dense  graphs  (when  e  is  proportional  to  v2),  but sparsepfs  is  likely  to  be  much  faster  for  sparse  graphs.
geometric problems suppose  that  we  are  given  n  points  in  the  plane  and  we  want  to  find  the shortest  set  of  lines  connecting  all  the  points.  this  is  a  geometric  problem, called  the  euclidean  minimum  spanning  tree  problem.  it  can  be  solved  using  the  graph  algorithm  given  above,  but  it  seems  clear  that  the  geometry provides  enough  extra  structure  to  allow  much  more  efficient  algorithms  to  be developed.
the  way  to  solve  the  euclidean  problem  using  the  algorithm  given  above is  to  build  a  complete  graph  with  n  vertices  and  n(n  -  1)/2  edges,  one edge  connecting  each  pair  of  vertices  weighted  with  the  distance  between  the corresponding  points.  then  the  minimum  spanning  tree  can  be  found  with the  algorithm  above  for  dense  graphs  in  time  proportional  to  n2.
it  has  been  proven  that  it,   is  possible  to  do  better.  the  point  is  that  the geometric  structure  makes  most  of  the  edges  in  the  complete  graph  irrelevant to  the  problem,  and  we  can  eliminate  most  of  the  edges  before  even  starting to  construct  the  minimum  spanning  tree.  in  fact,  it  has  been  proven  that the  minimum  spanning  tree  is  a  subset  of  the  graph  derived  by  taking  only the  edges  from  the  dual  of  the  voronoi  diagram  (see  chapter  28).  we  know that  this  graph  has  a  number  of  edges  proportional  to  n,  and  both  kruskal’s algorithm  and  the  priority-first  search  method  work  efficiently  on  such  sparse graphs.  in  principle,  then,  we  could  compute  the  voronoi  dual  (which  takes time  proportional  to  nlog  n),  then  run  either  kruskal’s  algorithm  or  the priority-first  search  method  to  get  a  euclidean  minimum  spanning  tree  algorithm  which  runs  in  time  proportional  to  n  log  n.   but  writing  a  program to  compute  the  voronoi  dual  is  quite  a  challenge  even  for  an  experienced programmer.
another  approach  which  can  be  used  for  random  point  sets  is  to  take advantage  of  the  distribution  of  the  points  to  limit  the  number  of  edges included  in  the  graph,  as  in  the  grid  method  used  in  chapter  26  for  range searching.  if  we  divide  up  the  plane  into  squares  such  that  each  square is  likely  to  contain  about  5  points,  and  then  include  in  the  graph  only  the edges  connecting  each  point  to  the  points  in  the  neighboring  squares,  then  we are  very  likely  (though  not  guaranteed)  to  get  all  the  edges  in  the  minimum spanning  tree,  which  would  mean  that  kruskal’s  algorithm  or  the  priority-first search  method  would  efficiently  finish  the  job.
it  is  interesting  to  reflect  on  the  relationship  between  graph  and  geometric algorithms  brought  out  by  the  problem  posed  in  the  previous  paragraphs.  it
this system of equations is a simple “tridiagonal” form which is easily solved with a degenerate version of gaussian elimination as we saw in chapter 5. if we let  ui  =  zi+l  - zi, di =  2(zi+l -xi--i),  and  wi  =  zi   - zi.-1,  we have, for example, the following simultaneous equ.ations  for n = 7:
in  fact,  this  is  a  symmetric  tridiagonal  system,  with  the  diagonal  below  the main diagonal equal to the diagonal above the main diagonal. it turns out that pivoting  on  the  largest  available  element  is  not  necessary  to  get  an  accurate solution for this system of equations.
var  i:  integer; begin readln  (n)  ; for i:=l to n do readln(x[i],y[i]); for  i:=2  to  n-l  do  d[i]:=2*(x[i+l]-x[i-11); for  i:=l  to  n-l  do  u[i]:=x[i+l]-x[i]; for  i:=2  to  n-l  do
these  transformations  are  purely  “local”:  no  part  of  the  tree  need  be  examined or  modified  other  than  what  is  diagrammed.  each  of  the  transformations passes up one of the keys from a 4-node  to its father in the tree, restructuring links  accordingly.  note  that  we  don’t  have  to  worry  explicitly  about  the  father being  a  4-node  since  our  through  each node  in  the  tree,  we  come  out  on  a  node  that  is  not  a  4-node.  in  particular, when  we  come  out  the  bottom  of  the  tree,  we  are  not  on  a  4-node,  and  we can directly insert the new node either by transforming a 2-node to a 3-node or  a  3-node  to  a  4-node.  actually,  it  is  convenient  to  treat  the  insertion  as  a split of an imaginary 4-node at the bottom which passes up the new key to be inserted.  whenever  the  root  of  the  tree  becomes  a  4-node,  we’ll  split  it  into three  2-nodes,  as  we  did  for  our  first  node  split  in  the  example  above.  this (and only this) makes the tree grow one level “higher.”
the  algorithm  sketched  in  the  previous  paragraph  gives  a  way  to  do searches  and  insertions  in  2-3-4  trees;  since  the  4-nodes  are  split  up  on  the way  from  the  top  down,  the  trees  are  called  top-down  2-s-4 trees.  what’s interesting is that, even though we haven’t been worrying about balancing at all,  the  resulting  trees  are  perfectly  balanced!  the  distance  from  the  root  to every  external  node  is  the  same,  which  implies  that  the  time  required  by  a search  or  an  insertion  is  always  proportional  to  log  n.   the  proof  that  the  trees are always perfectly balanced is simple: the transformations that we perform have  no  effect  on  the  distance  from  any  node  to  the  root,  except  when  we  split the root, and in this case the distance from all nodes to the root is increased by  one.
the  description  given  above  is  sufficient  to  define  an  algorithm  for  searching  using  binary  trees  which  has  guaranteed  worst-case  performance.  however, we  are  only  halfway  towards  an  actual  implementation.  while  it  would  be possible  to  write  algorithms  which  actually  perform  transformations  on  distinct data types representing 2-, 3-, and  4-nodes,  most of the things that need to  be  done  are  very  inconvenient  in  this  direct  representation.  (one  can  become  convinced  of  this  by  trying  to  implement  even  the  simpler  of  the  two node  transformations.)  furthermore,  the  overhead  incurred  in  manipulating the  more  complex  node  structures  is  likely  to  make  the  algorithms  slower  than standard  binary  tree  search.  the  primary  purpose  of  balancing  is  to  provide “insurance”  against  a  bad  worst  case,  but  it  would  be  unfortunate  to  have to  pay  the  overhead  cost  for  that  insurance  on  every  run  of  the  algorithm. fortunately,  as  we’ll  see  below,  there  is  a  relatively  simple  representation  of 2-,  3-,  and  4-nodes  that  allows  the  transformations  to  be  done  in  a  uniform way  with  very  little  overhead  beyond  the  costs  incurred  by  standard  binary tree 
a  linear  sort  is  obviously  desirable  for  many  applications,  but  there  are reasons  why  it  is  not  the  panacea  that  it  might  seem.  first,  it  really  does depend  on  the  keys  being  random  bits,  randomly  ordered.  if  this  condition  is not  sati.sfied,   severely  degraded  performance  is  likely.  second,  it  requires  extra space  proportional  the  size  of  the  array  being  sorted.  third,  the  “inner  loop” of  the  program  actually  contains  quite  a  few  instructions,  so  even  though  it’s linear,  it  won’t  be  as  much  faster  than  quicksort  (say)  as  one  might  expect, except  for  quite  large  files  (at  which  point  the  extra  array  becomes  a  real liability).  the  choice  between  quicksort  and  radix  sort  is  a  difficult  one that  is  likely  to  depend  not  only  on  features  of  the  application  such  as  key, record,  and  file  size,  but  also  on  features  of  the  programming  and  machine environment  that  relate  to  the  efficiency  of  access  and  use  of  individual  bits. again,  such  tradeoffs  need  to  be  studied  by  an  expert  and  this  type  of  study is  likely  to  be  worthwhile  only  for  serious  sorting  applications.
such relationships can be challenging to solve precisely, they are often easy to solve  for  some  particular  values  of  n  to  get  solutions  which  give  reasonable estimates  for  all  values  of  n.  our  purpo,se   in  this  discussion  is  to  gain  some intuitive  feeling  for  how  divide-and-conquer  algorithms  achieve  efficiency,  not to  do  detailed  analysis  of  the  algorithms.  indeed,  the  particular  recurrences that  we’ve  just  solved  are  sufficient  to  describe  the  performance  of  most  of the  algorithms  that  we’ll  be  studying,  and  we’ll  simply  be  referring  back  to them.
matrix  multiplication the  most  famous  application  of  the  divide-and-conquer  technique  to  an  arithmetic  problem  is  strassen’s  method  for  matrix  multiplication.  we  won’t  go into  the  details  here,  but  we  can  sketch  the  method,  since  it  is  very  similar  to the  polynomial  multiplication  method  that  we  have  just  studied.
the  straightforward  method  for  multiplying  two  n-by-n  matrices  requires  n3   scalar  multiplications,  since  each  of  the  n2   elements  in  the  product matrix  is  obtained  by  n  multiplications.
strassen’s  method  is  to  divide  the  size  of  the  problem  in  half;  this  corresponds  to  dividing  each  of  the  matrice;s  into  quarters,  each  n/2  by  n/2. the  remaining  problem  is  equivalent  to  multiplying  2-by-2  matrices.  just  as we  were  able  to  reduce  the  number  of  multiplications  required  from  four  to three  by  combining  terms  in  the  polynomial  multiplication  problem,  strassen was  able  to  find  a  way  to  combine  terms  to  reduce  the  number  of  multiplications  required  for  the  2-by-2  matrix  multiplication  problem  from  8  to  7.  the rearrangement  and  the  terms  required  are  quite  complicated.
this  result  was  quite  surprising  when  it  first  appeared,  since  it  had  previously been  thought  that  n3  multiplications  were  absolutely  necessary  for  matrix multiplication.  the  problem  has  been  studied  very  intensively  in  recent  years, and  slightly  better  methods  than  strassen’s  have  been  found.  the  “best” algorithm  for  matrix  multiplication  has  still  not  been  found,  and  this  is  one of  the  most  famous  outstanding  problems  of  computer  science.
it  is  important  to  note  that  we  have  been  counting  multiplications  only. before  choosing  an  algorithm  for  a  practical  application,  the  costs  of  the extra  additions  and  subtractions  for  combining  terms  and  the  costs  of  the
recursive  calls  must  be  considered.  these  costs  may  depend  heavily  on  the particular  implementation  or  computer  used.  but  certainly,  this  overhead makes  strassen’s  method  less  efficient  than  the  standard  method  for  small matrices.  even  for  large  matrices,  in  terms  of  the  number  of  data  items  input, strassen’s  method  really  represents  an  improvement  only  from  n’.5   to  n1.41. this  improvement  is  hard  to  notice  except  for  very  large  n.  for  example,  n would  have  to  be  more  than  a  million  for  strassen’s  method  to  use  four  times as  few  multiplications  as  the  standard  method,  even  though  the  overhead  per multiplication  is  likely  to  be  four  times  as  large.  thus  the  algorithm  is  a theoretical,  not  practical,  contribution.
this  illustrates  a  general  tradeoff  which  appears  in  all  applications  (though the  effect,  is  not  always  so  dramatic):  simple  algorithms  work  best  for  small problems,  but  sophisticated  algorithms  can  reap  tremendous  savings  for  large problems.
or  the  accumulation  of  computational  errors.  for  some  types  of  tridiagonal matrices  which  arise  commonly,  it  can  be  proven  that  this  is  not  a  reason  for concern.
gauss-jordan  reduction  can  be  implemented  with  full  pivoting  to  replace a  matrix  by  its  inverse  in  one  sweep    it.  the  inverse  of  a  matrix a,  written  a-‘,   has  the  property  that  a  system  of  equations  ax  =  b  could be  solved  just  by  performing  the  matrix  multiplication  z  =  a-lb.   still,  n3 operations  are  required  to  compute  x  given  b.  however,  there  is  a  way  to preprocess  a  matrix  and  “decompose”  it  into  component  parts  which  make it  possible  to  solve  the  corresponding  system  of  equations  with  any  given rightchand  side  in  time  proportional  to  1v2,   a  savings  of  a  factor  of  n  over using  gaussian  elimination  each  time. roughly,  this  involves  remembering the  operations  that  are  performed  on  the  (n +  1)st   column  during  the  forward elimination  phase,  so  that  the  result  of  forward  elimination  on  a  new  (n  +  1)st column  can  be  computed  efficiently  and  then  back-substitution  performed  as usual.
solving systems of linear equations has been shown to be computationally equivalent  to  multiplying  matrices,  so  tlhere  exist  algorithms  (for  example, strassen’s  matrix  multiplication  algorithm)  which  can  solve  systems  of  n equations  in  n  variables  in  time  proportional  to  n2.*l....   as  with  matrix multiplication,  it  would  not  be  worthwhile  to  use  such  a  method  unless  very large  systems  of  equations  were  to  be  processed  routinely.  as  before,  the actual  running  time  of  gaussian  elimination  in  terms  of  the  number  of  inputs is  n312.   which  is  difficult  to  imnrove   uoon in  nractice.
much  of  the  material  in  this  section  falls  within  the  domain  of  numerical  analysis,  and  several  excellent  textbooks  are  available.  one  which  pays particular  attention  to  computational  issues  is  the  1977  book  by  forsythe, malcomb  and  moler.  in  particular,  much  of  the  material  given  here  in  chapters 5,  6,  and  7  is  based  on  the  presentation  given  in  that  book.
the  second  major  reference  for  this  section  is  the  second  volume  of  d.  e. knuth’s  comprehensive  treatment  of  “the  art  of  computer  programming.” knuth  uses  the  term  “seminumerical”  to  describe  algorithms  which  lie  at the  interface  between  numerical  and  symbolic  computation,  such  as  random number  generation  and  polynomial  arithmetic.  among  many  other  topics, knuths  volume  2  covers  in  great  depth  the  material  given  here  in  chapters 1,  3,  and  4.  the  1975  book  by  borodin  and  munro  is  an  additional  reference for  strassen’s  matrix  multiplication  method  and  related  topics.  many  of the  algorithms  that  we’ve  considered  (and  many  others,  principally  symbolic methods  as  mentioned  in  chapter  7)  are  embodied  in  a  computer  system  called macsyma,  which  is  regularly  used  for  serious  mathematical  work.
certainly,  a  reader  seeking  more  information  on  mathematical  algorithms should  expect  to  find  the  topics  treated  at  a  much  more  advanced  mathematical  level  in  the  references  than  the  material  we’ve  considered  here.
chapter  2  is  concerned  with  elementary  data  structures,  as  well  as  polynomials.  beyond  the  references  mentioned  in  the  previous  part,  a  reader  interested  in  learning  more  about  this  subject  might  study  how  elementary  data structures  are  handled  in  modern  programming  languages  such  as  ada,  which have  facilities  for  building  abstract  data  structures.
a.  borodin  and  i.  munro,  the  computational  complexity  of  algebraic  and numerical  problems,  american  elsevier,  new  york,  1975. g.  e.  forsythe,  m.  a.  malcomb,  and  c.  b.  moler,  computer  methods  for mathematical  computations,  prentice-hall,  englewood  cliffs,  nj,  1977. d.  e.  knuth,  the  art  of  computer  programming.  volume  &:   seminumerical algorithms,  addison-wesley,  reading,  ma  (second  edition),  1981. mit  mathlab  group,  macsyma  reference  manual,  laboratory  for  computer  science,  massachusetts  institute  of  technology,  1977. p.  wegner,  programming  with  ada:  an  introduction  by  means  of  graduated examples,  prentice-hall,  englewood  cliffs,  nj,  1980.
the  best  references  for  further  information  on  many  of  the  algorithms  in this  section  are  the  original  sources.  knuth,  morris,  and  pratt’s  1977  paper and  boyer  and  moore’s  1977  paper  form  the  basis  for  much  of  the  material from  chapter  19.  the  1968  paper  by  thompson  is  the  basis  for  the  regularexpression  pattern  matcher  of  chapters  20-21.  huffman’s  1952  paper,  though it  predates  many  of  the  algorithmic  considerations  here,  still  makes  interesting reading.  rivest,  shamir,  and  adleman  describe  fully  the  implementation  and applications  of  their  public-key  cryptosystem  in  their  1978  paper.
the  book  by  standish  is  a  good  general  reference  for  many  of  the  topics covered  in  these  chapters,  especially  chapters  19,  22,  and  23.  parsing  and compiling  are  viewed  by  many  to  be  the  heart  of  computer  science,  and  there are  a  large  number  of  standard  references  available,  for  example  the  book by  aho   and  ullman.  an  extensive  amount  of  background  information  on cryptography  may  be  found  in  the  book  by  kahn.
a. v.  aho   and  j.  d.  ullman,  principles  of  compiler  design,  addison-wesley, reading,  ma,  1977. r.  s.  boyer  and  j.  s.  moore,  “a  fast  string  searching  algorithm,”  communications  of the  acm,  20,  10  (october,  1977). d.  a.  huffman,  “a  method  for  the  construction  of  minimum-redundancy codes,” proceedings  of  d.  kahn,  the  codebreakers,  macmillan,  new  york,  1967. d.  e.  knuth,  j.  h.  morris,  and  v.  r.  pratt,  “fast  pattern  matching  in  strings,” siam  journal  on computing,  6,  2  (june,  1977). r.  l.  rivest,  a.  shamir  and  l.  adleman, “a  method  for  obtaining  digital signatures  and  public-key  cryptosystems,”  communications  of the  acm,  21, 2 (february, 1978). t.  a.  standish,  data structure  techniques,  addison-wesley,  reading,  ma, 1980. k.  thompson,  “regular  expression  search  algorithm,”  communications  of  the acm,  11,  6  (june,  1968).
multiple  searches the  algorithms  that  we’ve  been  discussing  are  all  oriented  towards  a  specific string  searching  problem:  find  an  occurrence  of  a  given  pattern  in  a  given text  string.  if  the  same  text  string  is  to  be  the  object  of  many  pattern searches,  then  it  will  be  worthwhile  to  do  some  processing  on  the  string  to make  subsequent  searches  efficient.
if  there  are  a  large  number  of  searches,  the  string  searching  problem  can be  viewed  as  a  special  case  of  the  general  searching  problem  that  we  studied in  the  previous  section.  we  simply  treat  the  text  string  as  n  overlapping “keys,”  the  ith  key  defined  to  be  a[l..n],  the  entire  text  string  starting  at position  i.  of  course,  we  don’t  manipulate  the  keys  themselves,  but  pointers to  them:  when  we  need  to  compare  keys  i  and  j  we  do  character-by-character compares starting at positions i  and j of the text string. (if we use a “sentinel” character  larger  than  all  other  characters  at  the  end,  then  one  of  the  keys will  always  be  greater  than  the  other.)  then  the  hashing,  binary  tree,  and other  algorithms  of  the  previous  section  can  be  used  directly.  first,  an  entire structure  is  built  up  from  the  text  string,  and  then  efficient  searches  can  be performed  for  particular  patterns.
there are many details which need to be worked out in applying searching algorithms  to  string  searching  in  this  way;  our  intent  is  to  point  this  out  as a  viable  option  for  some  string  searching  applications.  different  methods  will be  appropriate  in  different  situations.  for  example,  if  the  searches  will  always be  for  patterns  of  the  same  length,  a  hash  table  constructed  with  a  single  scan as  in  the  rabin-karp  method  will  yield  constant  search  times  on  the  average. on  the  other  hand,  if  the  patterns  are  to  be  of  varying  length,  then  one  of  the tree-based  methods  might  be  appropriate.  (patricia  is  especially  adaptable  to such  an  application.)
other  variations  in  the  problem  can  make  it  significantly  more  difficult and  lead  to  drastically  different  methods,  as  we’ll  discover  in  the  next  two chapters.
000000000000000000000000000011111111111111000000000 000000000000000000000000001111111111111111110000000 000000000000000000000001111111111111111111111110000 000000000000000000000011111111111111111111111111000 000000000000000000001111111111111111111111111111110 0000000000000000000111111100000000000000~0001111111 000000000000000000011111000000000000000000000011111 000000000000000000011100000000000000000000000000111 000000000000000000011100000000000000000000000000111 000000000000000000011100000000000000000000000000111 000000000000000000011100000000000000000000000000111 000000000000000000001111000000000000000000000001110 000000000000000000000011100000000000000000000111000 011111111111111111111111111111111111111111111111111 011111111111111111111111111111111111111111111111111 011111111111111111111111111111111111111111111111111 011111111111111111111111111111111111111111111111111 0!1111111111111111111111111111111111111111111111111 011000000000000000000000000000000000000000000000011
28  14  9 26  18  7 23  24  4 22  26  3 20  30  1 19 7 18 7 19 5 22 5 19 3 26 3 19  3263 19 3 26 3 19 3 26 3 20 4 23 3 1 22 3 20 3 3 1  50 1  50 1  50 1  50 1  50 1   2 4 6 2
that  is,  the  first  line  consists  of  28  o’s  followed  by  14  l’s  followed  by  9  more o’s, etc. the 63 counts in this table plus the number of bits per line (51) contain  sufficient  (in  particular,  note that  no  “end  of  line”  indicator  is  needed).  if  six  bits  are  used  to  represent  each count,  then  the  entire  file  is  represented  with  384  bits,  a  substantial  savings over  the  975  bits  required  to  store  it  explicitly.
file  to  be encoded  and  the  encoded  version  of  the  file,  so  that  it  can’t  work  for  all  files. this  can  be  quite  file  compression method  suggested  above  won’t  work  for  character  strings  that  contain  digits. if other characters are used to encode the counts, it won’t work for strings that  contain  those  characters.  to  from from  that  alphabet,  we’ll a  assume  that  we  only  have  the  26  letters  of  the  alphabet  (and  spaces)  to  work with.
how can we make some letters represent digits and others represent parts of the string to be encoded? one solution is to use some character which is likely to appear rarely in the text as a so-called escape character. each  appearance  of  that  character  signals  that  the  next  two  form  a (count,character) pair, with counts represented by having the ith letter of the alphabet represent the number i. thus our example string would be represented  as  follows  with  q  as  the  escape  character:
also,  the  declaration  of  r has to be  suita.bly   changed  to  accomodate   twice  as many  coefficients  for  the  product.  each  of  the  n  coefficients  of  p  is  multiplied by  each  of  the  n  coefficients  of  q,  so  this  is  clearly  a  quadratic  algorithm.
an  advantage  of  representing  a  polynomial  by  an  array  containing  its coefficients is that it’s easy to reference any coefficient directly; a disadvantage is  that  space  may  have  to  be  saved  for  more  numbers  than  necessary.  for example,  the  program  above  couldn’t  reasonably  be  used  to  multiply
 and the output only three.   is  to  use  a  linked  list.  this involves  storing  items  in  noncontiguous  memory  locations,  with  each  item containing  the  address  of  the  next.  the  pascal  mechanisms  for  linked  lists  are somewhat  more  complicated  than  for  arrays.  for  example,  the  following  program  computes  the  sum  of  two  polynomials  using  a  linked  list  representation (the  bodies  of  the  readlist   and  add  functions  and  the  writelist  procedure  are given  in  the  text  following):
the  polynomials  are  represented  by  linked  lists  which  are  built  by  the readlist  procedure.  the  format  of  these  is  described  in  the  type  statement: the  lists  are  made  up  of  nodes,  each  node  containing  a  coefficient  and  a  link to  the  next  node  on  the  list.  if  we  have  a  link  to  the  first  node  on  a  list,  then we  can  examine  the  coefficients  in  order,  by  following  links.  the  last  node on  each  list  contains  a  link  to  a  special  (dummy  node  called  a:  if  we  reach  z when  scanning  through  a  list,  we  know  we’re  at  the  end.  (it  is  possible  to  get by  without  such  dummy  nodes,  but  they  do  make  certain  manipulations  on the  lists  somewhat  simpler.)  the  type  statement  only  describes  the  formats of  the  nodes;  nodes  can  be  created  only  when  the  builtin   procedure  new  is called.  for  example,  the  call  new(z)  creates  a  new  node,  putting  a  pointer  to
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
records  (not  all)  need  to  be  examined  fo:*   an  unsuccessful  search.  the  sorted order  is  easy  to  maintain  because  a  new  record  can  simply  be  inserted  into  the list  at  the  point  at  which  the  unsuccessful  search  terminates.  as  usual  with linked  lists,  a  dummy  header  node  head  and  a  tail  node  a  allow  the  code  to be  substantially  simpler  than  without  th:m.   thus,  the  call  listinsert(v,   head) will put a new node with key v into the  lj st pointed to by the next field of the head,  and  listsearch  is  similar.  repeated  calls  on  listsearch  using  the  links returned  will  return  records  with  duplica,te   keys.  the  tail  node  z  is  used  as  a sentinel  in  the  same  way  as  above.  if  lis6search   returns  a,  then  the  search  was unsuccessful.
if  something  is  known  about  the  relative  frequency  of  access  for  various records,  then  substantial  savings  can  oftc:n   be  realized  simply  by  ordering  the records  intelligently.  the  “optimal”  arrangement  is  to  put  the  most  frequently accessed  record  at  the  beginning,  the  second  most  frequently  accessed  record in  the  second  position,  etc.  this  technique  can  be  very  effective,  especially  if only  a  small  set  of  records  is  frequently  accessed.
if  information  is  not  available  about  the  frequency  of  access,  then  an approximation  to  the  optimal  arrangerlent   can  be  achieved  with  a  “selforganizing”  search:  each  time  a  record  is  accessed,  move  it  to  the  beginning of  the  list.  this  method  is  more  conveniently  implemented  when  a  linked-list implementation  is  used.  of  course  the  running  time  for  the  method  depends on  the  record  access  distributions,  so  it  it;   difficult  to  predict  how  it  will  do  in general.  however,  it  is  well  suited  to  the   quite  common  situation  when  most of  the  accesses  to  each  record  tend  to  happen  close  together.
if  the  set  of  records  is  large,  then  the  total  search  time  can  be  significantly reduced  by  using  a  search  procedure  based  on  applying  the  “divide-andconquer”  paradigm:  divide  the  set  of  records  into  two  parts,  determine  which of  the  two  parts  the  key  being  sought  t’elongs   to,  then  concentrate  on  that part.  a  reasonable  way  to  divide  the  sets  of  records  into  parts  is  to  keep  the records  sorted,  then  use  indices  into  the  sorted  array  to  delimit  the  part  of  the array  being  worked  on.  to  find  if  a  given  key  v  is  in  the  table,  first  compare it  with  the  element  at  the  middle  position  of  the  table.  if  v  is  smaller,  then it  must  be  in  the  first  half  of  the  table;  if  v  is  greater,  then  it  must  be  in  the second  half  of  the  table.  then  apply  the  method  recursively.  (since  only  one recursive  call  is  involved,  it  is  simpler  to  express  the  method  iteratively.)  this brings  us  directly  to  the  following  implementation,   which  assumes  that  the array  a  is  sorted.
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
term, we  used lookahead to avoid such a loop; in this case the proper way to get  around  the  problem  is  to  switch  the  grammar  to  say  (term)+(expression). the  occurrence  of  a  nonterminal  as  the  first  thing  on  the  right  hand  side  of a  replacement  rule  for  itself  is  called  left  recursion.  actually,  the  problem is  more  subtle,  because  the  left  recursion  can  arise  indirectly:  for  example if  we  were  to  have  the  productions  (expression)  ::=  (term)  and  (term)  ::= v  1  (expression)  +  (term).  recursive  descent  parsers  won’t  work  for  such grammars:  they  have  to  be  transformed  to  equivalent  grammars  without  left recursion,  or  some  other  parsing  method  has  to  be  used.  in  general,  there is  an  intimate  and  very  widely  studied  connection  between  parsers  and  the grammars  they  recognize.  the  choice  of  a  parsing  technique  is  often  dictated by  the  characteristics  of  the  grammar  to  be  parsed.
bottom-  up  parsing though  there  are  several  recursive  calls  in  the  programs  above,  it  is  an  instructive  exercise  to  remove  the  recursion  systematically.  recall  from  chapter 9  (where  we  removed  the  recursion  from  quicksort)  that  each  procedure  call can  be  replaced  by  a  stack  push  and  each  procedure  return  by  a  stack  pop, mimicking  what  the  pascal  system  does  to  implement  recursion.  a  reason for  doing  this  is  that  many  of  the  calls  which  seem  recursive  are  not  truly recursive.  when  a  procedure  call  is  the  last  action  of  a  procedure,  then  a simple  goto   can  be  used.  this  turns  expression  and  term  into  simple  loops, which  can  be  incorporated  together  and  combined  with  factor  to  produce  a single  procedure  with  one  true  recursive  call  (the  call  to  expression  within factor).
this  view  leads  directly  to  a  quite  simple  way  to  check  whether  regular expressions  are  legal.  once  all  the  procedure  calls  are  removed,  we  see  that each  terminal  symbol  is  simply  scanned  as  it  is  encountered.  the  only  real processing  done  is  to  check  whether  there  is  a  right  parenthesis  to  match  each left  parenthesis  and  whether  each  ‘i+”  is  followed  by  either  a  letter  or  a  “(i’. that  is,  checking  whether  a  regular  expression  is  legal  is  essentially  equivalent to  checking  for  balanced  parentheses. this  can  be  simply  implemented  by keeping  a  counter,  initialized  to  0,  which  is  incremented  when  a  left  parenthesis  is  encountered,  decremented  when  a  right  parenthesis  is  encountered. if  the  counter  is  zero  when  the  end  of  the  expression  is  reached,  and  each  ‘i+” of  the  expression  is  followed  by  either  a  letter  or  a  “(“,  then  the  expression was legal.
of  course,  there  is  more  to  parsing  than  simply  checking  whether  the input  string  is  legal:  the  main  goal  is  to  build  the  parse  tree  (even  if  in  an implicit  way,  as  in  the  top-down  parser)  for  other  processing.  it  turns  out  to be  possible  to  do  this  with  programs  with  the  same  essential  structure  as  the parenthesis  checker  described  in  the  previous  paragraph.  one  type  of  parser
the  distinction  here  is  not  crucial:  performing  a  reverse  topological  sort  on  a graph  is  equivalent  to  performing  a  topological  sort  on  the  graph  obtained  by reversing all the edges.
but  we’ve  already  seen  an  algorithm  for  reverse  topological  sorting,  the standard  recursive  depth-first  search  procedure  of  chapter  29!   simply  changing  visit  to  print  out  the  vertex  visited  just  before  exiting,  for  example  by inserting  write(name[k]   )  right  at  the  end,  causes  dfs  to  print  out  the  vertices in  reverse  topological  order,  when  the  input  graph  is  a  dag.  a  simple  induction argument  proves  that  this  works:  we  print  out  the  name  of  each  vertex  after we’ve  printed  out  the  names  of  all  the  vertices  that  it  points  to.  when  visit is  changed  in  this  way  and  run  on  our  example,  it  prints  out  the  vertices  in the  reverse  topological  order  given  above. printing  out  the  vertex  name  on exit  from  this  recursive  procedure  is  exactly  equivalent  to  putting  the  vertex name  on  a  stack  on  entry,  then  popping  it  and  printing  it  on  exit.  it  would be  ridiculous  to  use  an  explicit  stack  in  this  case,  since  the  mechanism  for recursion  provides  it  automatically;  we  mention  this  because  we  do  need  a stack  for  the  more  difficult  problem  to  be  considered  next.
strongly  connected  components if  a  graph  contains  a directed cycle,  (if  we  can  get  from  a  node  back  to  itself by  following  edges  in  the  indicated  direction),  then  it  it  is  not  a  dag  and  it can’t  be  topologically  sorted:  whichever  vertex  on  the  cycle  is  printed  out  first will  have  another  vertex  which  points  to  it  which  hasn’t  yet  been  printed  out. the  nodes  on  the  cycle  are  mutually  accessible  in  the  sense  that  there  is  a way  to  get  from  every  node  on  the  cycle  to  another  node  on  the  cycle  and back.  on  the  other  hand,  even  though  a  graph  may  be  connected,  it  is  not likely  to  be  true  that  any  node  can  be  reached  from  any  other  via  a  directed path.  in  fact,  the  nodes  divide  themselves  into  sets  called  strongly  connected components  with  the  property  that  all  nodes  within  a  componenl  are  mutually accessible, but there is no way to get from a node in one component to a node in  another  component  and  back.  the  strongly  connected  components  of  the directed  graph  at  the  beginning  of  this  chapter  are  two  single  nodes  b  and  k, one pair of nodes h i, one triple of nodes d e f, and one large component with six  nodes  a  c  g  j  l  m.  for  example,  vertex  a  is  in  a  different  component from  vertex  f  because  though  there  is  a  path  from  a  to  f,  there  is  no  way  to get  from  f  to  a.
the  strongly  connected  components  of  a  directed  graph  can  be  found using  a  variant  of  depth-first  search,  as  the  reader  may  have  learned  to  expect. the  method  that  we’ll  examine  was  discovered  by  r.  e.  tarjan   in  1972.  since it  is  based  on  depth-first  search,  it  runs  in  time  proportional  to  v  + e,  but  it  is actually  quite  an  ingenious  method.  it  requires  only  a  few  simple  modifications to  our  basic  visit  procedure,  but  before  tarjan   presented  the  method,  no  linear
for  very  large  graphs,  this  computation  can  be  organized  so  that  the operations  on  bits  can  be  done  a  computer  word  at  a  time,  which  will  lead  to significant  savings  in  many  environments.  (as  we’ve  seen,  it  is  not  intended that  such  optimizations  be  tried  with  pascal.)
for  many  applications  involving  directed  graphs,  cyclic  graphs  do  arise.  if, however,  the  graph  above  modeled  a  manufacturing  line,  then  it  would  imply, say,  that  job  a  must  be  done  before  job  g,  which  must  be  done  before  job c,  which  must  be  done  before  job  a.  but  such  a  situation  is  inconsistent: for  this  and  many  other  applications,  directed  graphs  with  no  directed  cycles (cycles  with  all  edges  pointing  the  same  way)  are  called  for.  such  graphs  are called  directed  acyclic  graphs,  or  just  dags  for  short.  dags  may  have  many cycles  if  the  directions  on  the  edges  are  not  taken  into  account;  their  defining property  is  simply  that  one  should  never  get  in  a  cycle  by  following  edges  in the  indicated  direction.  a  dag  similar  to  the  directed  graph  above,  with  a few  edges  removed  or  directions  switched  in  order  to  remove  cycles,  is  given below.
the  edge  list  for  this  graph  is  the  same  as  for  the  connected  graph  of  chapter 30,  but  here,  again,  the  order  in  which  the  vertices  are  given  when  the  edge is  specified  makes  a  difference.
dags  really  are  quite  different  objects  from  general  directed  graphs:  in a  sense,  they  are  part  tree,  part  graph.  we  can  certainly  take  advantage  of their  special  structure  when  processing  them.  viewed  from  any  vertex,  a  dag looks  like  a  tree;  put  another  way,  the  depth-first  search  forest  for  a  dag  has no  up  edges.  for  example,  the  following  depth-first  search  forest  describes the  operation  of  dfs  on  the  example  dag  above.
by a simple path in only one of two ways (corresponding to the truth or falsity of  the  variables).  these  small  components  are  attached  together  as  specified by  the  clauses,  using  more  complicated  subgraphs  which  can  be  traversed  by simple  paths  corresponding  to  the  truth  or  falsity  of  the  clauses.  it  is  quite a  large  step  from  this  brief  description  to  the  full  construction:  the  point is  to  illustrate  that  polynomial  reduction  can  be  applied  to  quite  dissimilar problems.
thus,  if  we  were  to  have  a  polynomial-time  algorithm  for  the  traveling salesman  problem,  then  we  would  have  a  polynomial-time  algorithm  for  the hamilton  cycle  problem,  which  would  also  give  us  a  polynomial-time  algorithm for  the  satisfiability  problem. each  problem  that  is  proven  np-complete provides  another  potential  basis  for  proving  yet  another  future  problem  npcomplete.  the  proof  might  be  as  simple  as  the  reduction  given  above  from  the hamilton  cycle  problem  to  the  traveling  salesman  problem,  or  as  complicated as  the  transformation  sketched  above  from  the  satisfiability  problem  to  the hamilton  cycle  problem,  or  somewhere  in  between.  literally  thousands  of problems  have  been  proven  to  be  np-complete  over  the  last  ten  years  by transforming  one  to  another  in  this  way.
cook’s  theorem reduction  uses  the  np-completeness  of  one  problem  to  imply  the  np-completeness  of  another.  there  is  one  case  where  it  doesn’t  apply:  how  was  the first   problem  proven  to  be  np-complete?  this  was  done  by  s.  a.  cook  in 1971.  cook  gave  a  direct  proof  that  satisfiability  is  np-complete:  that  if there  is  a  polynomial  time  algorithm  for  satisfiability,  then  all  problems  in np  can  be  solved  in  polynomial  time.
the  proof  is  extremely  complicated  but  the  general  method  can  be  explained.  first,  a  full  mathematical  definition  of  a  machine  capable  of  solving any  problem  in  np  is  developed.  this  is  a  simple  model  of  a  general-purpose computer  known  as  a  turing  machine  which  can  read  inputs,  perform  certain operations,  and  write  outputs.  a  turing  machine  can  perform  any  computation  that  any  other  general  purpose  computer  can,  using  the  same  amount  of time  (to  within  a  polynomial  factor),  and  it  has  the  additional  advantage  that it  can  be  concisely  described  mathematically.  endowed  with  the  additional power  of  nondeterminism,  a  turing  machine  can  solve  any  problem  in  np. the  next  step  in  the  proof  is  to  describe  each  feature  of  the  machine,  including  the  way  that  instructions  are  executed,  in  terms  of  logical  formulas  such as  appear  in  the  satisfiability  problem.  in  this  way  a  correspondence  is  established  between  every  problem  in  np  (which  can  be  expressed  as  a  program  on the  nondeterministic  turing  machine)  and  some  instance  of  satisfiability  (the translation  of  that  program  into  a  logical  formula).  now,  the  solution  to  the satisfiability  problem  essentially  corresponds  t,o  a  simulation  of  the  machine
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
ing  to  those  edges  that  were  actually  used  to  visit  vertices  via  recursive  calls and  dotted  edges  corresponding  to  those  edges  pointing  to  vertices  that  had already  been  visited  at  the  time  the  edge  was  considered.  the  nodes  are visited  in  the  order  a  f  e  d  b  g  j  k  l  m  c  h  i.
note  that  the  directions  on  the  edges  make  this  depth-first  search  forest quite  different  from  the  depth-first  search  forests  that  we  saw  for  undirected graphs.  for  example,  even  though  the  original  graph  was  connected,  the depth-first  search  structure  defined  by  the  solid  edges  is  not  connected:  it  is a  forest,  not  a  tree.
for  undirected  graphs,  we  had  only  one  kind  of  dotted  edge,  one  that connected  a  vertex  with  some  ancestor  in  the  tree.  for  directed  graphs,  there are  three  kinds  of  dotted  edges:  up  edges,  which  point  from  a  vertex  to  some ancestor in the tree, down edges, which point from a vertex to some descendant in  the  tree,  and  cross  edges,  which  point  from  a  vertex  to  some  vertex  which is  neither  a  descendant  nor  an  ancestor  in  the  tree.
as  with  undirected  graphs,  we’re  interested  in  connectivity  properties  of directed  graphs.  we  would  like  to  be  able  to  answer  questions  like  “is  there a directed path  from  vertex  x  to  vertex  y  (a  path  which  only  follows  edges  in the  indicated  direction)?” and  “which  vertices  can  we  get  to  from  vertex  x with  a  directed  path?” and  “is  there  a  directed  path  from  vertex  x  to  vertex y  and  a  directed  path  from  y  to  x.7”  just  as  with  undirected  graphs,  we’ll  be able  to  answer  such  questions  by  appropriately  modifying  the  basic  depth-first search  algorithm,  though  the  various  different  types  of  dotted  edges  make  the modifications  somewhat  more  complicated.
in undirected graphs, simple connectivity gives the vertices that can be reached from  a  given  vertex  by  traversing  edges  from  the  graph:  they  are  all  those  in the  same  connected  component.  similarly,  for  directed  graphs,  we’re  often interested  in  the  set  of  vertices  which  can  be  reached  from  a  given  vertex  by traversing  edges  from  the  graph  in  the  indicated  direction.
it  is  easy  to  prove  that  the  recursive  visit  procedure  from  the  depth-first search  method  in  chapter  29  visits  all  the  nodes  that  can  be  reached  from  the start  node.  thus,  if  we  modify  that  procedure  to  print  out  the  nodes  that  it  is visiting  (say,  by  inserting  write(name(k))   just  upon  entering),  we  are  printing out  all  the  nodes  that  can  be  reached  from  the  start  node.  but  note  carefully that  it  is  not  necessarily  true  that  each  tree  in  the  depth-first  search  forest contains  all  the  nodes  that  can  be  reached  from  the  root  of  that  tree  (in  our example,  all  the  nodes  in  the  graph  can  be  reached  from  h,  not  just  i).  to get  all  the  nodes  that  can  be  visited  from  each  node,  we  simply  call  visit  v times,  once  for  each  node:
again  the  tree  is  drastically  smaller.  it  is  important  to  note  that  the  savings achieved  for  this  toy  problem  is  only  indicative  of  the  situation  for  larger problems.  a  cutoff  high  in  the  tree  can  lead  to  truly  significant  savings; missing  an  obvious  cutoff  can  lead  to  truly  significant  waste.
the  general  procedure  of  solving  a  problem  by  systematically  generating all  possible  solutions  as  described  above  is  called  backtracking.  whenever  we have  a  situation  where  partial  solutions  to  a  problem  can  be  successively  augmented  in  many  ways  to  produce  a  complete  solution,  a  recursive  implementation  like  the  program  above  may  be  appropriate.  as  above,  the  process can  be  described  by  an  exhaustive  search  tree  whose  nodes  correspond  to  the partial  solutions.  going  down  in  the  tree  corresponds  to  forward  progress towards  creating  a  more  complete  solution;  going  up  in  the  tree  corresponds to  “backtracking”  to  some  previously  generated  partial  solution,  from  which point  it  might  be  worthwhile  to  proceed  forwards  again.  the  general  technique of  calculating  bounds  on  partial  solutions  in  order  to  limit  the  number  of  full solutions  which  need  to  be  examined  is  sometimes  called  branch-and-bound. for  another  example,  consider  the  knapsack  problem  of  the  previous chapter,  where  the  values  are  not  necessarily  restricted  to  be  integers.  for this  problem,  the  partial  solutions  are  clearly  some  selection  of  items  for  the knapsack,  and  backtracking  corresponds  to  taking  an  item  out  to  try  some other  combination.  pruning  the  search  tree  by  removing  symmetries  is  quite effective  for  this  problem,  since  the  order  in  which  objects  are  put  into  the knapsack  doesn’t  affect  the  cost.
(the  functions  onpq  and  pqempty  are  priority  queue  utility  routines  which are  easily  implemented  additions  to  the  set  of  programs  given  in  chapter  11: pqempty  returns  true  if  the  priority  queue  is  empty;  onpq  returns  true  if  the given  vertex  is  currently  on  the  priority  queue.)  below  and  in  chapter  31,  we’ll see  how  the  substitution  of  various  expressions  for  priority  in  this  program yields  several  classical  graph  traversal  algorithms.  specifically,  the  program operates  as  follows:  first,  we  give  all  the  vertices  the  sentinel  value  unseen (which  could  be  maxi&)   and  initialize  the  dad  array,  which  is  used  to  store the  search  tree.  next  we  construct  an  indirect  priority  queue  containing  all the  vertices  (this  construction  is  trivial  because  the  values  are  initially  all  the same).  in  the  terminology  above,  tree  vertices  are  those  which  are  not  on the  priority  queue,  unseen  vertices  are  those  on  the  priority  queue  with  value unseen,  and  fringe  vertices  are  the  others  on  the  priority  queue.  with  these conventions  understood,  the  operation  of  the  program  is  straightforward:  it repeatedly  removes  the  highest  priority  vertex  from  the  queue  and  puts  it  on the  tree,  then  updates  the  priorities  of  all  fringe  or  unseen  vertices  connected to  that  vertex.
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
priority-first  search  method  will  be  faster  for  some  graphs,  prim’s  for  some others,  kruskal’s  for  still  others.  as  mentioned  above,  the  worst  case  for  the priority-first  search  method  is  (e  +  v)logv   while  the  worst  case  for  prim’s is  v2  and  the  worst  case  for  kruskal’s  is  elog  e.  but  it  is  unwise  to  choose between  the  algorithms  on  the  basis  of  these  formulas  because  “worstrcase” graphs  are  unlikely  to  occur  in  practice. in  fact,  the  priority-first  search method  and  kruskal’s  method  are  both  likely  to  run  in  time  proportional  to e  for  graphs  that  arise  in  practice:  the  first  because  most  edges  do  not  really require  a  priority  queue  adjustment  that  takes  1ogv   steps  and  the  second because  the  longest  edge  in  the  minimum  spanning  tree  is  probably  sufficiently short  that  not  many  edges  are  taken  off  the  priority  queue.  of  course,  prim’s method  also  runs  in  time  proportional  to  about  e  for  dense  graphs  (but  it shouldn’t  be  used  for  sparse  graphs).
the  shortest path  problem  is  to  find  the  path  in  a  weighted  graph  connecting two  given  vertices  x  and  y  with  the  property  that  the  sum  of  the  weights  of all  the  edges  is  minimized  over  all  such  paths.
if  the  weights  are  all  1,  then  the  problem  is  still  interesting:  it  is  to  find the  path  containing  the  minimum  number  of  edges  which  connects  x  and  y. moreover,  we’ve  already  considered  an  algorithm  which  solves  the  problem: breadth-first  search.  it  is  easy  to  prove  by  induction  that  breadth-first  search starting  at  x  will  first  visit  all  vertices  which  can  be  reached  from  z  with  1 edge,  then  al:  vertices  which  can  be  reached  from  x  with  2  edges,  etc.,  visiting all  vertices  which  can  be  reached  with  k  edges  before  encountering  any  that require  k  +  1  edges.  thus,  when  y  is  first  encountered,  the  shortest  path  from x  has  been  found  (because  no  shorter  paths  reached  y).
consider  the  problem  of  finding  the  shortest  paths  connecting  a  given  vertex x  with  each  of  the  other  vertices  in  the  graph.  again,  it  turns  out  that  the problem  is  simple  to  solve  with  the  priority  graph  traversal  algorithm  of  the previous  chapter.
if  we  draw  the  shortest  path  from  x  to  each  other  vertex  in  the  graph, then  we  clearly  get  no  cycles,  and  we  have  a  spanning  tree.  each  vertex  leads to  a  different  spanning  tree;  for  example,  the  following  three  diagrams  show the  shortest  path  spanning  trees  for  vertices  a,  b,  and  e  in  the  example  graph that  we’ve  been  using.
to  take  a  logarithmic  number  of  steps  for  all  searches  and  insertions.  this is  one  of  the  few  searching  algorithms  with  that  property,  and  its  use  is justified  whenever  bad  worst-case  performance  simply  cannot  be  tolerated. furthermore,  this  is  achieved  at  very  little  cost.  searching  is  done  just  as quickly as if the balanced tree were constructed by the elementary algorithm, and  insertion  involves  only  one  extra  bit  test  and  an  occasional  split.  for random keys the height of the tree seems to be quite close to  1gn (and only one or two splits are done for the average insertion) but no one has been able to  analyze  this  statistic  for  any  balanced  tree  algorithm.  thus  a  key  in  a  file of,  say,  half  a  million  records  can  be  found  by  comparing  it  against  only  about twenty  other  keys.
other algorithms the “top-down 2-3-4 tree” implementation using the “red-black” framework given  in  the  previous  section  is  one  of  several  similar  strategies  than  have been  proposed  for  implementing  balanced  binary  trees.  as  we  saw  above,  it is  actually  the  “rotate”  operations  that  balance  the  trees:  we’ve  been  looking at a particular view of the trees that makes it easy to decide when to rotate. other  views  of  the  trees  lead  to  other  algorithms,  a  few  of  which  we’ll  mention briefly in this section.
the oldest and most well-known data structure for balanced trees is the avl tree.  these trees have the property that the heights of the two  subtrees of each node differ by at most one. if  this  condition  is  violated  because  of an  insertion,  it  turns  out  that  it  can  be  reinstated  using  rotations.  but  this requires  an  extra  loop:  the  basic  algorithm  is  to  search  for  the  value  being inserted, then proceed  up  the  tree  along  the  path  just  travelled  adjusting  the heights of nodes using rotations. also, it is necessary to know whether each node  has  a  height  that  is  one  less  than,  the  same,  or  one  greater  than  t,he height  of  its  brother.  this  requires  two  bits  if  encoded  in  a  straightforward way, though there is a way to get by with just one bit per node.
a second well-known balanced tree structure is the  2-3 tree, where only 2-nodes and 3-nodes are allowed. it is possible to implement  insert using an “extra  loop”  involving  rotations  as  with  avl  trees,  but  there  is  not  quite enough flexibility to give a convenient top-down version.
in chapter 18, we’ll study the most important type of balanced tree, an extension of 2-3-4 trees called b-trees. these allow up to m keys per node for large  m, and  are  widely  used  for  searching  applications  involving  very  large files.
a g :   100000a000000 a b :   2aooooa000000   3 a a o o o a o o o o o 0 a c : 0 o a 0 0  0 0 1 l 3 a a 0 lm: jm: 3 a a 0 0 o a 0 o l 0 2 l jl: 3aaoooaoolo2l* j k :   3aaoooaooll3l e d :   3aaeloaooll3l f d :   3aae2eaooll3l hi: f e :   3aae2ealhll3l*   gaaeaealhll   3 l a f : g e :   gaaeaealhll   3 l  g c :   gaaeaealhll   3 l   * g h :   8aaeaeaahll3l jg:12aaeaeaahllal lg:12aaeaeaahllal
for clarity in this table, each positive entry i is replaced by the ith letter of the alphabet  (the  name  of  the  father),  and  each  negative  entry  is  complemented to  give  a  positive  integer  (the  weight  of  the  tree).
several  other  techniques  have  been  developed  to  avoid  degenerate  structures.  for  example,  path  compression  has  the  disadvantage  that  it  requires another  pass  up  through  the  tree.  another  technique,  called  halving,  is  to make  each  node  point  to  its  granddad   on  the  way  up  the  tree.  still  another technique,  splitting,  is  like  halving,  but  is  applied  only  to  every  other  node on  the  search  path.  either  of  these  can  be  used  in  combination  with  weight balancing  or  with  height  balancing,  which  is  similar  but  uses  tree  height  instead  of  tree  size  to  decide  which  way  to  merge  trees.
how  is  one  to  choose  from  among  all  these  methods?  and  exactly  how “flat”  are  the  trees  produced?  analysis  for  this  problem  is  quite  difficult because  the  performance  depends  not  only  on  the  v  and  e  parameters,  but also on the number of find operations and, what’s worse, on the order in which the  union  and  find  operations  appear.  unlike  sorting,  where  the  actual  files that  appear  in  practice  are  quite  often  close  to  “random,”  it’s  hard  to  see how  to  model  graphs  and  request  patterns  that  might  appear  in  practice.  for this  reason,  algorithms  which  do  well  in  the  worst  case  are  normally  preferred for  union-find  (and  other  graph  algorithms),  though  this  may  be  an  overly ccnservative  approach.
leading  bit.  this  leads  to  an  immediate  correspondence  with  radix  sorting: binary  trie  searching  partitions  the  file  in  exactly  the  same  way  as  radix exchange  sorting.  (compare  the  trie  above  with  the  partitioning  diagram  we examined  for  radix  exchange  sorting,  after  noting  that  the  keys  are  slightly different.)  this  correspondence  is  analogous  to  that  between  binary  tree searching  and  quicksort.
an  annoying  feature  of  radix  tries  is  the  “one-way”  branching  required  for keys  with  a  large  number  of  bits  in  common,  for  example,  keys  which  differ only  in  the  last  bit  require  a  path  whose  length  is  equal  to  the  key  length,  no matter  how  many  keys  there  are  in  the  tree.  the  number  of  internal  nodes  can be  somewhat  larger  than  the  number  of  keys.  the  height  of  such  trees  is  still limited  by  the  number  of  bits  in  the  keys,  but  we  would  like  to  consider  the possibility  of  processing  records  with  very  long  keys  (say  1000  bits  or  more) which  perhaps  have  some  uniformity,  as  might  occur  in  character  encoded data.  one  way  to  shorten  the  paths  in  the  trees  is  to  use  many  more  than two  links  per  node  (though  this  exacerbates  the  “space”  problem  of  using  too many  nodes);  another  way  is  to  “collapse”  paths  containing  one-way  branches into  single  links.  we’ll  discuss  these  methods  in  the  next  two  sections.
multiway for  radix  sorting,  we  found  that  we  could  get  a  significant  improvement  in speed  by  considering  more  than  one  bit  at  a  time.  the  same  is  true  for  radix searching:  by  examining  m  bits  at  a  time,  we  can  speed  up  the  search  by  a factor of  2m. however,  there’s  a  catch  which  makes  it  necessary  to  be  more careful  applying  this  idea  than  was  necessary  for  radix  sorting.  the  problem is  that  considering  m  bits  at  a  time  corresponds  to  using  tree  nodes  with m  =  2m   links,  which  can  lead  to  a  considerable  amount  of  wasted  space  for unused  links.  for  example,  if  m  =  4  the  following  tree  is  formed  for  our sample  keys:
the  message:  this  means  that  we  need  to  save  the  tree  along  with  the  message in  order  to  decode  it.  fortunately,  this  does  not  present  any  real  difficulty. it  is  actually  necessary  only  to  store  the  code  array,  because  the  radix  search trie  which  results  from  inserting  the  entries  from  that  array  into  an  initially empty  tree  is  the  decoding  tree.
thus,  the  storage  savings  quoted  above  is  not  entirely  accurate,  because the  message  can’t  be  decoded  without  the  trie  and  we  must  take  into  account the  cost  of  storing  the  trie  (i.e., the  code  array)  along  with  the  message. huffman  encoding is therefore only effective for long files where the savings in the message is enough to offset the cost, or in situations where the coding trie can be  precomputed   and  used  for  a  large  number  of  messages.  for  example,  a trie  based  on  the  frequencies  of  occurrence  of  letters  in  the  english  language could  be  used  for  text  documents. for  that  matter,  a  trie  based  on  the frequency  of  occurrence  of  characters  in  pascal  programs  could  be  used  for encoding  programs  (for  example, “;”  is  likely  to  be  near  the  top  of  such  a trie).  a  huffman   encoding  algorithm  saves  about  23%  when  run  on  the  text for  this  chapter.
as  before,  for  truly  random  files,  even  this  clever  encoding  scheme  won’t work  because  each  character  will  occur  approximately  the  same  number  of times,  which  will  lead  to  a  fully  balanced  coding  tree  and  an  equal  number  of bits  per  letter  in  the  code.
even if only the worst case is being considered, the analysis of union-find algorithms  is  extremely  complex  and  intricate.  this  can  be  seen  even  from  the nature  of  the  results,  which  do  give  us  clear  indications  of  how  the  algorithms will  perform  in  a  practical  situation. if  either  weight  balancing  or  height balancing  is  used  in  combination  with  either  path  compression,  halving,  or splitting,  then  the  total  number  of  operations  required  to  build  up  a  structure with  e  edges  is  proportional  to  es(e),   where  a(e)  is  a  function  that  is  so slowly  growing  that  o(e)  <  4  unless  e  is  so  large  that  taking  lg  e,  then taking  lg  of  the  result,  then  taking  lg  of  that  result,  and  continuing  16  times still  gives  a  number  bigger  than  1.  this  is  a  stunningly  large  number;  for  all practical  purposes,  it  is  safe  to  assume  that  the  average  amount  of  time  to execute  each  union  and  find  operation  is  constant.  this  result  is  due  to  r.  e. tarjan,   who  further  showed  that  no  algorithm  for  this  problem  (from  a  certain general  class)  can  do  better  that  e&(e),  so  that  this  function  is  intrinsic  to the  problem.
an  important  practical  application  of  union-find  algorithms  is  that  they can  be  used  to  determine  whether  a  graph  with  v  vertices  and  e  edges  is connected  in  space  proportional  to  v  (and  almost  linear  time).  this  is  an advantage  over  depth-first  search  in  some  situations:  here  we  don’t  need  to ever  store  the  edges.  thus  connectivity  for  a  graph  with  thousands  of  vertices and  millions  of  edges  can  be  determined  with  one  quick  pass  through  the edges.
another  convex  hull  algorithm.  we’ll  see  yet  another  example  in  chapter  31 of  a  problem  which  can  be  efficiently  solved  by  first  finding  the  voronoi  dual. the  defining  property  of  the  voronoi  diagram  means  that  it  can  be  used to  solve  the  nearest-neighbor  problem:  to  identify  the  nearest  neighbor  in  a point  set  to  a  given  point,  we  need  only  find  out  which  voronoi  polygon  the point  falls  in.  it  is  possible  to  organize  the  voronoi  polygons  in  a  structure like  a  2d  tree  to  allow  this  search  to  be  done  efficiently.
the  voronoi  diagram  can  be  computed  using  an  algorithm  with  the  same general  structure  as  the  closest-point  algorithm  above.  the  points  are  first sorted  on  their  x  coordinate.  then  that  ordering  is  used  to  split  the  points  in half,  leading  to  two  recursive  calls  to  find  the  voronoi  diagram  of  the  point set  for  each  half.  at  the  same  time,  the  points  are  sorted  on  y;  finally,  the two  voronoi  diagrams  for  the  two  halves  are  merged  together.  as  before,  the merging  together  (done  with  pass=2)   can  make  use  of  the  fact  that  the  points are sorted on x before the recursive calls and that they are sorted on y and the voronoi  diagrams  for  the  two  halves  have  been  built  after  the  recursive  calls. however,  even  with  these  aids,  it  is  quite  a  complicated  task,  and  presentation of  a  full  implementation  would  be  beyond  the  scope  of  this  book.
the  voronoi  diagram  is  certainly  the  natural  structure  for  closest-point problems,  and  understanding  the  characteristics  of  a  problem  in  terms  of the  voronoi  diagram  or  its  dual  is  certainly  a  worthwhile  exercise.  however, for  many  particular  problems,  a  direct  implementation  based  on  the  general schema  given  in  this  chapter  may  be  suitable.  this  is  powerful  enough  to compute  the  voronoi  diagram,  so  it  is  powerful  enough  for  algorithms  based on  the  voronoi  diagram,  and  it  may  admit  to  simpler,  more  efficient  code,  just as  we  saw  for  the  closest-nair  nroblem.
expected  to  take  on  “typical”  input  data,  and  in  the  worst case,  the  amount of  time  a  program  would  take  on  the  worst  possible  input  configuration.
many of the algorithms in this book are very well understood, to the point that  accurate  mathematical  formulas  are  known  for  the  average-  and  worstcase  running  time.  such  formulas  are  developed  first  by  carefully  studying the  program,  to  find  the  running  time  in  terms  of  fundamental  mathematical quantities  and  then  doing  a  mathematical  analysis  of  the  quantities  involved. for  some  algorithms,  it  is  easy  to  hgure  out  the  running  time.  for  example,  the  brute-force  algorithm  above  obviously  requires  min(u,   vu)-gcd(u,   v) iterations  of  the  while  loop,  and  this  quantity  dominates  the  running  time  if the  inputs  are  not  small,  since  all  the  other  statements  are  executed  either 0  or  1  times.  for  other  algorithms,  a  substantial  amount  of  analysis  is  involved.  for  example,  the  running  time  of  the  recursive  euclidean  algorithm obviously  depends  on  the  “overhead”  required  for  each  recursive  call  (which can  be  determined  only  through  detailed1  knowledge  of  the  programming  environment  being  used)  as  well  as  the  number  of  such  calls  made  (which  can be  determined  only  through  extremely  sophisticated  mathematical  analysis). several  important  factors  go  into  this  analysis  which  are  somewhat  outside  a  given  programmer’s  domain  of  influence.  first,  pascal  programs  are translated  into  machine  code  for  a  given  computer,  and  it  can  be  a  challenging task  to  figure  out  exactly  how  long  even  one  pascal  statement  might  take  to execute  (especially  in  an  environment  where  resources  are  being  shared,  so that  even  the  same  program  could  have  varying  performance  characteristics). second,  many  programs  are  extremely  sensitive  to  their  input  data,  and  performance  might  fluctuate  wildly  depending  on  the  input.  the  average  case might  be  a  mathematical  fiction  that  is  not  representative  of  the  actual  data on  which  the  program  is  being  used,  and  the  worst  case  might  be  a  bizarre construction  that  would  never  occur  in  practice.  third,  many  programs  of interest  are  not  well  understood,  and  specific  mathematical  results  may  not be  available.  finally,  it  is  often  the  case  that  programs  are  not  comparable  at all:  one  runs  much  more  efficiently  on  one  particular  kind  of  input,  the  other runs  efficiently  under  other  circumstances.
with  these  caveats  in  mind,  we’ll  use  rough  estimates  for  the  running time  of  our  programs  for  purposes  of  classification,  secure  in  the  knowledge that  a  fuller  analysis  can  be  done  for  important  programs  when  necessary. such  rough  estimates  are  quite  often  easy  to  obtain  via  the  old  programming saw  “90%  of  the  time  is  spent  in  10%  of  the  code.”  (this  has  been  quoted  in the  past  for  many  different  values  of  “go%.“)
the first step in getting a rough estimate of the running time of a program is  to  identify  the  inner  loop.  which  instructions  in  the  program  are  executed most  often?  generally,  it  is  only  a  few  instructions,  nested  deep  within  the
the  procedure  to  write  out  what’s  on  a  list  is  the  simplest.  it  simply steps  through  the  list,  writing  out  the  value  of  the  coefficient  in  each  node encountered, until z is found:
building  a  list  involves  first  calling  new  to  create  a  node,  then  filling  in the coefficient, and then linking the node to the end of the partial list built so far.  the  following  function  reads  the  same  format as  before,  and  constructs  the  linked  list  which  represents  the  corresponding polynomial:
the dummy node z is used here to hold the link which points to the first node on  the  list  while  the  list  is  being  constructed.  after  this  list  is  built,  a  is  set to  link  to  itself.  this  ensures  that  once  we  reach  the  end  of  a  list,  we  stay there.  another  convention  which  leave  z pointing to the beginning, to provide a way to get from the back to the front. finally,  the  program  which  adds  two  polynomials  constructs  a  new  list in  a  manner  similar  to  readlist,  calculating  the  coefficients  for  the  result by  stepping  through  the  argument  lists  and  adding  together  corresponding coefficients:
to  introduce  the  general  approach  that  we’ll  be  taking  to  studying algorithms,  we’ll  examine  a  classic  elementary  problem:  “reduce  a  given fraction  to  lowest  terms.” we  want  to  write  213, not  416,  200/300,  or  178468/   to  finding  the  greatest  common 267702.  solving  this  problem  is  equival.ent divisor  (gcd)  of  the  numerator  and  the  denominator:  the  largest  integer  which divides  them  both.  a  fraction  is  reduced  to  lowest  terms  by  dividing  both numerator  and  denominator  by  their  greatest  common  divisor.
pascal a  concise  description  of  the  pascal  language  is  given  in  the  wirth  and  jensen pascal  user manual and  report that serves as the definition for the language. our  purpose  here  is  not  to  repeat  information  from  that  book  but  rather  to examine  the  implementation  of  a  few  simple  algorithms  which  illustrate  some of  the  basic  features  of  the  language  and.  the  style  that  we’ll  be  using.
pascal  has  a  rigorous  high-level  syntax  which  allows  easy  identification  of the  main  features  of  the  program.  the  variables  (var)  and  functions  (function) used  by  the  program  are  declared  first,  f~ollowed   by  the  body  of  the  program. (other  major  program  parts,  not  used  in  the  program  below  which  are  declared before  the  program  body  are  constants  and  types.)  functions  have  the  same format  as  the  main  program  except  that  they  return  a  value,  which  is  set  by assigning  something  to  the  function  name  within  the  body  of  the  function. (functions  that  return  no  value  are  called  procedures.)
the  built-in  function  readln  reads  a.  line  from  the  input  and  assigns  the values found to the variables given as arguments; writeln  is similar. a standard built-in  predicate,  eof,  is  set  to  true  when  there  is  no  more  input.  (input  and output  within  a  line  are  possible  with  read,  write,  and  eoln.)  the  declaration of  input  and  output  in  the  program  statement  indicates  that  the  program  is using  the  “standard”  input  and  output  &reams.
also,  the  declaration  of  r has to be  suita.bly   changed  to  accomodate   twice  as many  coefficients  for  the  product.  each  of  the  n  coefficients  of  p  is  multiplied by  each  of  the  n  coefficients  of  q,  so  this  is  clearly  a  quadratic  algorithm.
an  advantage  of  representing  a  polynomial  by  an  array  containing  its coefficients is that it’s easy to reference any coefficient directly; a disadvantage is  that  space  may  have  to  be  saved  for  more  numbers  than  necessary.  for example,  the  program  above  couldn’t  reasonably  be  used  to  multiply
 and the output only three.   is  to  use  a  linked  list.  this involves  storing  items  in  noncontiguous  memory  locations,  with  each  item containing  the  address  of  the  next.  the  pascal  mechanisms  for  linked  lists  are somewhat  more  complicated  than  for  arrays.  for  example,  the  following  program  computes  the  sum  of  two  polynomials  using  a  linked  list  representation (the  bodies  of  the  readlist   and  add  functions  and  the  writelist  procedure  are given  in  the  text  following):
the  polynomials  are  represented  by  linked  lists  which  are  built  by  the readlist  procedure.  the  format  of  these  is  described  in  the  type  statement: the  lists  are  made  up  of  nodes,  each  node  containing  a  coefficient  and  a  link to  the  next  node  on  the  list.  if  we  have  a  link  to  the  first  node  on  a  list,  then we  can  examine  the  coefficients  in  order,  by  following  links.  the  last  node on  each  list  contains  a  link  to  a  special  (dummy  node  called  a:  if  we  reach  z when  scanning  through  a  list,  we  know  we’re  at  the  end.  (it  is  possible  to  get by  without  such  dummy  nodes,  but  they  do  make  certain  manipulations  on the  lists  somewhat  simpler.)  the  type  statement  only  describes  the  formats of  the  nodes;  nodes  can  be  created  only  when  the  builtin   procedure  new  is called.  for  example,  the  call  new(z)  creates  a  new  node,  putting  a  pointer  to
now  the  add  function  becomes  more  interesting,  since  it  has  to  perform an  addition  only  for  terms  whose  degrees  match,  and  then  make  sure  that  no term  with  coefficient  0  is  output:
these  complications  are  worthwhile  for  processing  “sparse”  polynomials with  many  zero  coefficients,  but  the  array  representation  is  better  if  there  are only  a  few  terms  with  zero  coefficients.  similar  savings  are  available  for  other operations  on  polynomials,  for  example  multiplication.
matrices we  can  proceed  in  a  similar  manner  to  implement  basic  operations  on  twodimensional  matrices,  though  the  programs  become  more  complicated.  suppose  that  we  want  to  compute  the  sum  of  the  two  matrices
this  is  term-by-term  addition,  just  as  for  polynomials,  so  the  addition  program  is  a  straightforward  extension  of  our  program  for  polynomials:
this method takes about n steps for an unsuccessful search (every record must  be  examined  to  decide  that  a  record  with  any  particular  key  is  absent) and  about  n/2  steps,  on  the  average,  for  a  successful  search  (a  “random” search  for  a  record  in  the  table  will  require  examining  about  half  the  entries, on the average).
the  seqsearch  program  above  uses  purely  sequential  access  to  the  records, and  thus  can  be  naturally  adapted  to  use  a  linked  list  representation  for  the records.  one  advantage  of  doing  so  is  that  it  becomes  easy  to  keep  the  list sorted,  as  shown  in  the  following  implementation:
var  x:  link; begin zf.key:=v; while  tt.nextt.key<v   do  t:=tt.next; n e w ( x ) ;   xt.next:=tf.next;   tt.next:=x; xf.key:=v; jistinsert:=x; end ;
with  a  sorted  list,  a  search  can  be  terminated  unsuccessfully  when  a  record with  a  key  larger  than  the  search  key  is  found.  thus  only  about  half  the
records  (not  all)  need  to  be  examined  fo:*   an  unsuccessful  search.  the  sorted order  is  easy  to  maintain  because  a  new  record  can  simply  be  inserted  into  the list  at  the  point  at  which  the  unsuccessful  search  terminates.  as  usual  with linked  lists,  a  dummy  header  node  head  and  a  tail  node  a  allow  the  code  to be  substantially  simpler  than  without  th:m.   thus,  the  call  listinsert(v,   head) will put a new node with key v into the  lj st pointed to by the next field of the head,  and  listsearch  is  similar.  repeated  calls  on  listsearch  using  the  links returned  will  return  records  with  duplica,te   keys.  the  tail  node  z  is  used  as  a sentinel  in  the  same  way  as  above.  if  lis6search   returns  a,  then  the  search  was unsuccessful.
if  something  is  known  about  the  relative  frequency  of  access  for  various records,  then  substantial  savings  can  oftc:n   be  realized  simply  by  ordering  the records  intelligently.  the  “optimal”  arrangement  is  to  put  the  most  frequently accessed  record  at  the  beginning,  the  second  most  frequently  accessed  record in  the  second  position,  etc.  this  technique  can  be  very  effective,  especially  if only  a  small  set  of  records  is  frequently  accessed.
if  information  is  not  available  about  the  frequency  of  access,  then  an approximation  to  the  optimal  arrangerlent   can  be  achieved  with  a  “selforganizing”  search:  each  time  a  record  is  accessed,  move  it  to  the  beginning of  the  list.  this  method  is  more  conveniently  implemented  when  a  linked-list implementation  is  used.  of  course  the  running  time  for  the  method  depends on  the  record  access  distributions,  so  it  it;   difficult  to  predict  how  it  will  do  in general.  however,  it  is  well  suited  to  the   quite  common  situation  when  most of  the  accesses  to  each  record  tend  to  happen  close  together.
if  the  set  of  records  is  large,  then  the  total  search  time  can  be  significantly reduced  by  using  a  search  procedure  based  on  applying  the  “divide-andconquer”  paradigm:  divide  the  set  of  records  into  two  parts,  determine  which of  the  two  parts  the  key  being  sought  t’elongs   to,  then  concentrate  on  that part.  a  reasonable  way  to  divide  the  sets  of  records  into  parts  is  to  keep  the records  sorted,  then  use  indices  into  the  sorted  array  to  delimit  the  part  of  the array  being  worked  on.  to  find  if  a  given  key  v  is  in  the  table,  first  compare it  with  the  element  at  the  middle  position  of  the  table.  if  v  is  smaller,  then it  must  be  in  the  first  half  of  the  table;  if  v  is  greater,  then  it  must  be  in  the second  half  of  the  table.  then  apply  the  method  recursively.  (since  only  one recursive  call  is  involved,  it  is  simpler  to  express  the  method  iteratively.)  this brings  us  directly  to  the  following  implementation,   which  assumes  that  the array  a  is  sorted.
as  with  sequential  list  searching,  the  coding  in  this  program  is  simplified by  the  use  of  a  “tail”  node  z.  similarly,  the  insertion  code  given  below  is simplified  by  the  use  of  a  tree  header  node  head  whose  right  link  points  to  the root.  to  search  for  a  record  with  key  v  we  set  x:=  treesearch(v,  head).
if  a  node  has  no  left  (right)  subtree  then  its  left  (right)  link  is  set  to point  to  z.  as  in  sequential  search,  we  put  the  value  sought  in  a  to  stop an  unsuccessful  search.  thus,  the  “current  subtree”   pointed  to  by  x  never becomes  empty  and  all  searches  are  “successful”  :  the  calling  program  can check  whether  the  link  returned  points  to  a  to  determine  whether  the  search was successful. it is sometimes convenient to think of links which point to z as pointing  to  imaginary  external  nodes  with  all  unsuccessful  searches  ending  at external  nodes.  the  normal  nodes  which  cont,ain   our  keys  are  called  internal nodes;  by  introducing  external  nodes  we  can  say  that  every  internal  node points  to  two  other  nodes  in  the  tree,  even  though,  in  our  implementation,  all of  the  external  nodes  are  represented  by  the  single  node  z.
for  example,  if  d  is  sought  in  the  tree  above,  first  it  is  compared  against e,  the  key  at  the  root.  since  d  is  less,  it  is  next  compared  against  a,  the  key in the  left  son of the node containing e. continuing in this way, d is compared next  against  the  c  to  the  right  of  that  node.  the  links  in  the  node  containing c  are  pointers  to  z  so  the  search  terminates  with  d  being  compared  to  itself in  z  and  the  search  is  unsuccessful.
to  insert  a  node  into  the  tree,  we  just  do  an  unsuccessful  search  for  it, then  hook  it  on  in  place  of  z  at  the  point  where  the  search  terminated,  as  in the  following  code:
to  insert  a  new  key  in  a  tree  with  a  tree  header  node  pointed  to  by  head,  we call  treeinsert(v,   head).  to  be  able  to  do  the  insertion,  we  must  keep  track  of the  father  f  of  x,  as  it  proceeds  down  the  tree.  when  the  bottom  of  the  tree (x=z)  is  reached,  f  points  to  the  node  whose  link  must  be  changed  to  point  to the  new  node  inserted.  the  function  returns  a  link  to  the  newly  created  node so  that  the  calling  routine  can  fill  in  the  info  field  as  appropriate.
when  a  new  node  whose  key  is  equal  to  some  key  already  in  the  tree is  inserted,  it  will  be  inserted  to  the  right  of  the  node  already  in  the  tree. all  records  with  key  equal  to  v  can  be  processed  by  successively  setting  t  to search(v,
whose  right  link  points  to  the  actual  root  node  of  the  tree,  and  whose  key  is smaller  than  all  other  key  values  (for  simplicity,  we  use  0  assuming  the  keys are  all  positive  integers).  the  left  link  of  head  is  not  used.  the  empty  tree  is represented  by  having  the  right  link  of  head  point  to  z,  as  constructed  by  the following code:
in  this  program,  x  moves  down  the  tree  as  before,  with  gg,  g,  and  f  kept pointing  to  x’s  great-grandfather,  grandfather,  and  father  in  the  tree.  to  see why  all  these  links  are  needed,  consider  the  addition  of  y  to  the  tree  above. when  the  external  node  at  the  right  of  the  3-node  containing  s  and  x  is reached,  gg  is  r,  g  is  s,  and  f  is  x.  now,  y  must  be  added  to  make  a  4-node containing  s,  x,  and  y,  resulting  in  the  following  tree:
we  need  a  pointer  to  r  (gg)  because  r’s  right  link  must  be  changed  to  point to  x,  not  s.  to  see  exactly  how  this  comes  about,  we  need  to  look  at  the operation  of  the  split  procedure.
to  understand  how  io  implement  the  split  operation,  let’s  consider  the red-black  representation  for  the  two  transformations  we  must  perform:  if  we have  a  2-node  connected  to  a  4-node,  then  we  should  convert  them  into  a
branching  in  the  tree  based  on  the  result  of  the  comparison  between  the  keys, we  branch  according  to  the  key’s  bits. at  the  first  level  the  leading  bit  is used,  at  the  second  level  the  second  leading  bit,  and  so  on  until  an  external node  is  encountered.  the  code  for  this  is  virtually  the  same  as  the  code for  binary  tree  search.  the  only  difference  is  that  the  key  comparisons  are replaced  by  calls  on  the  bits  function  that  we  used  in  radix  sorting.  (recall from  chapter  10  that  bits(x,   k,  j) is  the  j  bits  which  appear  k  from  the  right and  can  be  efficiently  implemented  in  machine  language  by  shifting  right  k bits  then  setting  to  0  all  but  the  rightmost  j  bits.)
the  data  structures  for  this  program  are  the  same  as  those  that  we  used  for elementary  binary  search  trees.  the  constant  maxb  is  the  number  of  bits  in the  keys  to  be  sorted.  the  program  assumes  that  the  first  bit  in  each  key (the  (maxb+l)st   from  the  right)  is  0  (perhaps  the  key  is  the  result  of  a  call  to bits  with  a  third  argument  of  maxb),  so  that  searching  is  done  by  setting  x:= digitalsearch(v,   head),  where  head  is  a  link  to  a  tree  header  node  with  0  key and  a  left  link  pointing  to  the  search  tree.  thus  the  initialization  procedure for  this  program  is  the  same  as  for  binary  tree  search,  except  that  we  begin with  headf.l:=z   instead  of  headt.r:=z.
we  saw  in  chapter  10  that  equal  keys  are  anathema  in  radix  sorting;  the same  is  true  in  radix  searching,  not  in  this  particular  algorithm,  but  in  the ones  that  we’ll  be  examining  later.  thus  we’ll  assume  in  this  chapter  that  all the  keys  to  appear  in  the  data  structure  are  distinct:  if  necessary,  a  linked  list could  be  maintained  for  each  key  value  of  the  records  whose  keys  have  that value.  as  in  previous  chapters,  we’ll  assume  that  the  ith  letter  of  the  alphabet is  represented  by  the  five-bit  binary  representation  of  i.  that  is,  we’ll  use  the following  sample  keys  in  this  chapter:
to  be  consistent  with  hits,  we  consider  the  bits  to  be  numbered  o-4,  from right  to  left.  thus  bit  0  is  a’s  only  nonzero   bit  and  bit  4  is  p’s  only  nonzero bit.
f:=x; if  bits(v, b:=b-f ; until  x=z; n e w ( x ) ;   xf.key:=v;  xf.j:=z;   xt.r:=z; if  bits(v,  b+l,  i)=0 then  q.‘.l:=x  else  ff.r:=x; digitalinsert:  =x end ;
to  see  how  the  algorithm  works,  consider  what  happens  when  a  new  key  z= 11010  is  added  to  the  tree  below.  we  go  right  twice  because  the  leading  two bits  of  z  are  1,  then  we  go  left,  where  we  hit  the  external  node  at  the  left  of x,  where  z  would  be  inserted.
definition)  decrease  as  we  travel  down  the  tree.  this  leads  to  the  following search  code  for  patricia,  which  is  as  simple  as  the  code  for  radix  tree  or  trie searching:
this  function  returns  a  link  to  the  unique  node  which  could  contain  the  record with key v. the calling routine then can t 3st  whether the search was successful or  not.  thus  to  search  for  z=llolo  in  tie  above  tree  we  go  right,  then  up  at the  right  link  of  x.  the  key  there  is  not  z  so  the  search  is  unsuccessful.
the search for  z=llolo   ends at the node  c:ontaining   x=11000.  by  the  defining property  of  the  tree,  x  is  the  only  key  i-1   the  tree  for  which  a  search  would terminate  at  that  node.  if  z  is  inserted,  there  would  be  two  such  nodes,  so the  upward  link  that  was  followed  into  the  node  containing  x  should  be  made to  point  to  a  new  node  containing  z,  with  a  bit  index  corresponding  to  the leftmost  point  where  x  and  z  differ,  and  with  two  upward  links:  one  pointing to  x  and  the  other  pointing  to  z.  this  corresponds  precisely  to  replacing  the
external  node  containing  x  with  a  new  internal  node  with  x  and  z  as  sons  in radix  trie  insertion,  with  one-way  branching  eliminated  by  including  the  bit index.
for  t  ends  at  p=loooo, pattern  10x0x.   now,  t  and  p  differ  at  bit  2,  a  position  that  was  skipped during  the  search.  the  requirement  that  the  bit  indices  decrease  as  we  go down  the  tree  dictates  that  t  be  inserted  between  x  and  p,  with  an  upward self  pointer  corresponding  to  its  own  bit  2.  note  carefully  that  the  fact  that bit  2  was  skipped  before  the  insertion  of  t  implies  that  p  and  r  have  the same  bit  2  value.
(this  code  assumes  that  head  is  initialized  with  key  field  of  0,  a  bit  index  of maxb  and  both  links  upward  self  pointers.)  first,  we  do  a  search  to  find  the key  which  must  be  distinguished  from  v,  then  we  determine  the  leftmost  bit position  at  which  they  differ,  travel  down  the  tree  to  that  point,  and  insert  a new  node  containing  v  at  that  point.
patricia  is  the  quintessential  radix  searching  method:  it  manages  to identify  the  bits  which  distinguish  the  search  keys  and  build  them  into  a data  structure  (with  no  surplus  nodes)  that  quickly  leads  from  any  search key  to  the  only  key  in  the  data  structure  that  could  be  equal.  clearly,  the
var  t:  link; begin new(t);  tf.p:=p; tf.next:=grid[p.x   div  size,p.y  div  size]; grid [p.x div size, p.y div size] := t ; end ; begin new(z); for  i:=o  to  gmax  do
this  program  uses  our  standard  linked  list  representations,  with  dummy  tail node  z.  the  point  type  is  extended  to  include  a  field  info  which  contains the  integer  k  for  the  jcth  point  read  in,  for  convenience  in  referencing  the points.  in  keeping  with  the  style  of  our  examples,  we’ll  assume  a  function name(k)  to  return  the  jcth  letter  of  the  alphabet:  clearly  a  more  general naming  mechanism  will  be  appropriate  for  actual  applications.
as  mentioned  above,  the  setting  of  the  variable  size  (which  is  omitted from  the  above  program)  depends  on  the  number  of  points,  the  amount  of memory  available,  and  the  range  of  coordinate  values.  roughly,  to  get  m points  per  grid  square,  size  should  be  chosen  to  be  the  nearest  integer  to max divided  by  ,/m.  this  leads  to  about  n/m   grid  squares.  these  estimates aren’t  accurate  for  small  values  of  the  parameters,  but  they  are  useful  for most  situations,  and  similar  estimates  can  easily  be  formulated  for  specialized applications.
first  b  is  inserted  into  an  empty  tree,  then  deleted.  then  d,  e,  and  f  are inserted.  at  this  point,  h  is  encountered,  and  a  range  search  for  the  interval defined  by  h  is  performed  on  the  rightmost  tree  in  the  above  diagram.  this search  discovers  the  intersection  between  h  and  f.  proceeding  down  the  list above  in  order,  we  add  j,  c,  then  g  to  get  the  following  sequence  of  trees:
next,  the  upper  endpoint  of  d  is  encountered,  so  it  is  deleted;  then  i  is  added and  c  deleted,  which  gives  the  following  sequence  of  trees:
by  a  is  performed  on  the  rightmost  tree  in  the  diagram  above.  this  search discovers  the  intersections  between  a  and  e,  f,  and  i.  (recall  that  although g  and  j  are  visited  during  this  search,  any  points  to  the  left  of  g  or  to  the right  of  j  would  not  be  touched.)  finally,  the  upper  endpoints  of  g,  j,  f,  e, and  i  are  encountered,  so  those  points  are  successively  deleted,  leading  back to  the  empty  tree.
the  first  step  in  the  implementation  is  to  sort  the  line  endpoints  on  their y  coordinate.  but  since  binary  trees  are  going  to  be  used  to  maintain  the status  of  vertical  lines  with  respect  to  the  horizontal  scan  line,  they  may  as well  be  used  for  the  initial  y  sort!  specifically,  we  will  use  two  “indirect” binary  trees  on  the  line  set,  one  with  header  node  hy  and  one  with  header node  hx.  the  y  tree  will  contain  all  the  line  endpoints,  to  be  processed  in order  one  at  a  time;  the  x  tree  will  contain  the  lines  that  intersect  the  current horizontal  scan  line.  we  begin  by  initializing  both  hx  and  hy  with  0  keys and  pointers  to  a  dummy  external  node  z,  as  in  treeinitialize  in  chapter  14. then  the  hy  tree  is  constructed  by  inserting  both  y  coordinates  from  vertical lines  and  the  y  coordinate  of  horizontal  lines  into  the  binary  search  tree  with header  node  hy,  as  follows:
this  program  reads  in  groups  of  four  numbers  which  specify  lines,  and  puts them  into  the  lines  array  and  the  binary  search  tree  on  the  y  coordinate.  the standard  b&insert   routine  from  chapter  14  is  used,  with  the  y  coordinates  as keys,  and  indices  into  the  array  of  lines  as  the  info  field.  for  our  example  set of  lines,  the  following  tree  is  constructed:
then  merging  to  complete  the  sort  on  y  and  applying  the  procedure  above  to complete  the  closest  pair  computation.  in  this  way,  we  avoid  the  cost  of  doing an  extra  y  sort  by  intermixing  the  data  movement  required  for  the  sort  with the  data  movement  required  for  the  closest  pair  computation.
for  the  y  sort,  the  split  in  half  could  be  done  in  any  way,  but  for the  closest  pair  computation,  it’s  required  that  the  points  in  one  half  all have  smaller  z  coordinates  than  the  points  in  the  other  half.  this  is  easily accomplished  by  sorting  on  x  before  doing  the  division.  in  fact,  we  may  as well  use  the  same  routine  to  sort  on  z!  once  this  general  plan  is  accepted, the  implementation  is  not  difficult  to  understand.
as  mentioned  above,  the  implementation  will  use  the  recursive  sort  and merge  procedures  of  chapter  12.  the  first  step  is  to  modify  the  list  structures to  hold  points  instead  of  keys,  and  to  modify  merge  to  check  a  global  variable pass  to  decide  how  to  do  its  comparison.  if  pass=l,   the  comparison  should be  done  using  the  x  coordinates  of  the  two  points;  if  pass=2  we  do  the  y coordinates  of  the  two  points.  the  dummy  node  z  which  appears  at  the end  of  all  lists  will  contain  a  “sentinel”  point  with  artificially  high  z  and  y coordinates.
the  next  step  is  to  modify  the  recursive  sort  of  chapter  12  also  to  do  the closest-point  computation  when  pass=2.   this  is  done  by  replacing  the  line containing  the  call  to  merge  and  the  recursive  calls  to  sort  in  that  program by the following code:
if  pass=1,   this  is  straight  mergesort:  it  returns  a  linked  list  containing  the points  sorted  on  their  z  coordinates  (because  of  the  change  to  merge).  the magic  of  this  implementation  comes  when  pass=2.   the  program  not  only  sorts on  y  but  also  completes  the  closest-point  computation,  as  described  in  detail below.  the  procedure  check  simply  checks  whether  the  distance  between  the two  points  given  as  arguments  is  less  than  the  global  variable  min.  if  so,  it resets  min  to  that  distance  and  saves  the  points  in  the  global  variables  cpl and  cp2.   thus,  the  global  min  always  contains  the  distance  between  cpl  and cp2,   the  closest  pair  found  so  far.
new(z);  zf.next:=z; zf.p.x:=maxint;   zt.p.y:=maxint; new(h);  ht.next:=readlist; min:=maxint; pass:=l;   hf.next:=sort(hf.next, pass:=2;   hf.next:=sort(hf.next,
after  these  calls,  the  closest  pair  of  points  is  found  in  the  global  variables  cpl and  cp2  which  are  managed  by  the  check  “find  the  minimum”  procedure.
the  crux  of  the  implementation  is  the  operation  of  sort  when  pass=2. before  the  recursive  calls  the  points  are  sorted  on  x:  this  ordering  is  used  to divide  the  points  in  half  and  to  find  the  x  coordinate  of  the  dividing  line.  ajter the  recursive  calls  the  points  are  sorted  on  y  and  the  distance  between  every pair  of  points  in  each  half  is  known  to  be  greater  than  min.  the  ordering  on y is used to scan the points near the dividing line; the value of min is used to limit  the  number  of  points  to  be  tested.  each  point  within  a  distance  of  min of  the  dividing  line  is  checked  against  each  of  the  previous  four  points  found within  a  distance  of  min  of  the  dividing  line.  this  is  guaranteed  to  find  any pair  of  points  closer  together  than  min  with  one  member  of  the  pair  on  either side  of  the  dividing  line.  this  is  an  amusing  geometric  fact  which  the  reader may  wish  to  check.  (we  know  that  points  which  fall  on  the  same  side  of  the dividing  line  are  spaced  by  at  least  min,  so  the  number  of  points  falling  in  any circle  of  radius  min  is  limited.)
it  is  interesting  to  examine  the  order  in  which  the  various  vertical  dividing lines  are  tried  in  this  algorithm.  this  can  be  described  with  the  aid  of  the following  binary  tree:
