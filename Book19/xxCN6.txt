higher latency than the average value. a high value of the standard deviation usually indicates that some messages are blocked for a long time in the network. the peak value of the latency can also help in identifying these situations.
latency is measured in time units. however, when comparing several design choices, the absolute value is not important. as many comparisons are performed by using network simulators, latency can be measured in simulator clock cycles. unless otherwise stated, the latency plots presented in this chapter for unicast messages measure the average value of the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. in most cases, the simulator clock cycle is the unit of measurement. however, in section 9.10, latency is measured in nanoseconds.
throughput is the maximum amount of information delivered per time unit. it can also be deﬁned as the maximum trafﬁc accepted by the network, where trafﬁc, or accepted trafﬁc is the amount of information delivered per time unit. throughput could be measured in messages per second or messages per clock cycle, depending on whether absolute or relative timing is used. however, throughput would depend on message and network size. so, throughput is usually normalized, dividing it by message size and network size. as a result, throughput can be measured in bits per node and microsecond, or in bits per node and clock cycle. again, when comparing different design choices by simulation, and assuming that channel width is equal to ﬂit size, throughput can be measured in ﬂits per node and clock cycle. alternatively, accepted trafﬁc and throughput can be measured as a fraction of network capacity. a uniformly loaded network is operating at capacity if the most heavily loaded channel is used 100% of the time [72]. again, network capacity depends on the communication pattern.
a standard way to measure accepted trafﬁc and throughput was proposed at the workshop on parallel computer routing and communication (pcrcw’94). it consists of representing them as a fraction of the network capacity for a uniform distribution of destinations, assuming that the most heavily loaded channels are located in the network bisection. this network capacity is referred to as normalized bandwidth. so, regardless of the communication pattern used, it is recommended to measure applied load, accepted trafﬁc, and throughput as a fraction of normalized bandwidth. normalized bandwidth can be easily derived by considering that 50% of uniform random trafﬁc crosses the bisection of the network. thus, if a network has bisection bandwidth b bits/s, each node in an n-node network can inject 2b/n bits/s at the maximum load. unless otherwise stated, accepted trafﬁc and throughput are measured as a fraction of normalized bandwidth. while this is acceptable when comparing different design choices in the same network, it should be taken into account that those choices may lead to different clock cycles. in this case, each set of design parameters may produce a different bisection bandwidth, therefore invalidating the normalized bandwidth as a trafﬁc unit. in that case, accepted trafﬁc and throughput can be measured in bits (ﬂits) per node and microsecond. we use this unit in section 9.10.
a common misconception consists of using throughput instead of trafﬁc. as mentioned above, throughput is the maximum accepted trafﬁc. another misconception consists of considering throughput or trafﬁc as input parameters instead of measurements,
in many environments, rather than minimizing message latency or maximizing network throughput, the overriding issue is the ability to tolerate the failure of network components such as routers and links. in wormhole switching, header ﬂits containing routing information establish a path through the network from source to destination. data ﬂits are pipelined through the path immediately following the header ﬂits. if the header cannot progress due to a faulty component, the message is blocked in place indeﬁnitely, holding buffer resources and blocking other messages. this situation can eventually result in a deadlocked conﬁguration of messages. while techniques such as adaptive routing can alleviate the problem, it cannot by itself solve the problem. this has motivated the development of different switching techniques.
pipelined circuit switching (pcs) combines aspects of circuit switching and wormhole switching. pcs sets up a path before starting data transmission as in circuit switching. basically, pcs differs from circuit switching in that paths are formed by virtual channels instead of physical channels. in pipelined circuit switching, data ﬂits do not immediately follow the header ﬂits into the network as in wormhole switching. consequently, increased ﬂexibility is available in routing the header ﬂit. for example, rather than blocking on a faulty output channel at an intermediate router, the header may backtrack to the preceding router and release the previously reserved channel. a new output channel may now be attempted at the preceding router in ﬁnding an alternative path to the destination. when the header ﬁnally reaches the destination node, an acknowledgment ﬂit is transmitted back to the source node. now data ﬂits can be pipelined over the path just as in wormhole switching. the resilience to component failures is obtained at the expense of larger path setup times. this approach is ﬂexible in that headers can perform a backtracking search of the network, reserving and releasing virtual channels in an attempt to establish a fault-free path to the destination. this technique combines message pipelining from wormhole switching with a more conservative path setup algorithm based on circuit-switching techniques. a timespace diagram of a pcs message transmission over three links in the absence of any trafﬁc or failures is shown in figure 2.21.
since headers do not block holding channel or buffer resources, routing restrictions are not necessary to avoid deadlock. this increases the probability of ﬁnding a path while still avoiding deadlocked conﬁgurations of messages. moreover, reservation of virtual channels by the header does not by itself lead to use of physical channel bandwidth. therefore, unlike circuit switching, path setup does not lead to excessive blocking of other messages. as a result, multipath networks in conjunction with the ﬂexibility of pcs are good candidates for providing low-latency, fault-tolerant performance. for purely performance-driven applications where fault tolerance is not a primary concern, the added overhead of pcs makes wormhole switching the mechanism of choice.
in pcs, we distinguish between ﬂits that carry control information (e.g., header ﬂits and acknowledgment ﬂits) and those that carry data. this distinction is supported in the virtual channel model that separates control ﬂit trafﬁc and data ﬂit trafﬁc. a unidirectional is composed of a data channel, a corresponding channel, and a virtual channel vi
and therefore considerable attention was paid to the optimization of the performance of the core data path and control ﬂow. however, when the network starts to become congested, messages may be moved from input frames to the multiqueue, and subsequently from the multiqueue to an output frame. when a packet is stalled in an input frame waiting for an output frame to become free, this packet is moved to the multiqueue to free channel resources to be used by other messages. to prevent deadlock, packets may also be moved from input frames to the multiqueue if the output frame on the same channel has a packet to be transmitted. this ensures that both receiving and transmitting buffers on both sides of a channel are not occupied, ensuring progress (reference the discussion of chaotic routing in chapter 6) and avoiding deadlocked conﬁgurations of messages. thus, messages may be transmitted to output frames either from an input frame or from the multiqueue, with the latter having priority. if the multiqueue is full and a message must be inserted, a randomly chosen message is misrouted.
although several messages may be arriving simultaneously, the routing decisions are serialized by traversing the output channels in order. the action dimension is the current dimension under consideration (both positive and negative directions are distinguished). packets in the multiqueue have priority in being routed to the output frame. if a packet in the multiqueue requires the active dimension output and the output frame is empty, the packet is moved to the output frame. if the queue is full and the output frame is empty, a randomly selected packet is misrouted to this output frame. misrouting is the longest operation and takes ﬁve cycles. if the multiqueue operations do not generate movement, then any packet in an input frame requesting this output dimension can be switched through the crossbar. this operation takes a single cycle. the routing delay experienced by a header is two cycles. finally, if the input frame of the active dimension is full, it is read into the multiqueue. this implements the policy of handling stalled packets and the deadlock avoidance protocol. the preceding steps encompass the process of routing the message. the no-load latency through the router is four cycles: one cycle for the header to arrive on the input, two cycles for the routing decision, and one cycle to be switched through the crossbar to the output frame [32]. if a message is misrouted, the routing decision takes ﬁve cycles.
logically, the multiqueue contains messages partially ordered according to the output channel that the messages are waiting to traverse. the above routing procedure requires complete knowledge of the contents of the multiqueue and the relative age of the messages. the current implementation realizes a ﬁve-packet multiqueue. this logically corresponds to ﬁve queues: one for each output including the output to the processor. since messages are adaptively routed, a message may wait on one of several output channels, and therefore must logically be entered in more than one queue. when a free output frame is available for transfers from the multiqueue, determination of the oldest message waiting on this output must be readily available. the state of the multiqueue is maintained in a destination scoreboard.
the logical structure of the destination scoreboard is illustrated in figure 7.32 for a multiqueue with six slots. this scoreboard keeps track of the relationship between the messages in the multiqueue, their fifo order, and the destination outputs. each row corresponds to an output frame in a particular dimension or to the local processor. each
the designers of the ﬁrst generation of streamlined messaging layers point out that this programming model is mismatched to the hardware capabilities, leading to high overheads due to repeated transitions across the user/operating system interface and excessive copying and buffer management [105]. a closer look reveals that messages are transmitted by handlers and received by handlers. if these handlers were integrated into the programs themselves and user-level access provided to the network interface, computation could be smoothly integrated with computation. buffering/copying can be minimized as well as the overheads due to system calls. this streamlined implementation can result in over an order of magnitude reduction in software overhead. active messages [104, 105] and fast messages [264] represent this approach to the implementation of the messaging layer.
such messaging layer implementations are necessarily at a lower level of abstraction, resembling interrupt handlers in invocation and use. therefore, these procedures tend to serve better as targets for compilers and library developers than as a user-levelapi. further, the use of industry-standard apis is very important for portability and development cost. nonetheless, these lean message layers are available for the implementation of ﬁnely tuned, efﬁcient parallel programs as well as serving as a target for portable message-passingapis. the following two sections describe examples of implementations of active messages and fast messages.
the basic principle underlying active messages is that the message itself contains the address of a user-level handler that is to be executed on message reception [105]. consider how this might work for the transmission of a one-word message. let us suppose that the network interface is mapped into the user address space so that user programs have access to the control registers and buffer memory within the interface. the network interface receives a message into the local buffer within the interface. the message interrupts the processor, and the handler speciﬁed within the message is invoked. this handler may read the message data in the buffer and assign it to a local program variable. alternatively, if the message data provide a request for a value, the handler can immediately transmit a reply. in either case, the message need not be extracted from the interface and buffered locally in memory. rather than using interrupts, the user program may be periodically polling the interface seeking messages. in this case, no interrupt is generated. there is no buffering and no invocation of the operating system. execution of the handler is extremely fast. compare this scenario with the one described in conjunction with figure 8.3.
given this basic model, the following presents a synopsis of the design of an active message layer for the hp workstations (hpam) using the medusa network interface [225]. the network interface is mapped into the address space of the processor and is comprised of 1 mbyte of vram partitioned into 8 kbyte blocks (one per message). four queues control the buffering and transmission of messages as shown in figure 8.4. queue entries are comprised of a memory block number and message length pair. transmission proceeds with the construction of a message in a block and insertion of the corresponding queue
request_for_data(src_vnn, dest_vnn) {   request_complete = 0;   am_request_4(src_vnn,request_handler, &arg0);   while (request_complete = 0)     am_poll(); }
figure 8.6 illustrates the logical ﬂow of information through handlers in an implementation of the active message paradigm. process 0 makes a request for remote data. the source handler initializes a completion ﬂag (request complete) and calls the active message procedure am request 4(). the source now polls waiting for completion of the remote read. the message contains the name of the handler to be invoked by the destination process on message reception. this handler responds to the request message with a read operation to return the data value. this handler runs to completion. note that no buffering is necessary at this point. similarly, reception of the message at the host invokes the reply handler, which assigns the message contents to a local memory location. buffering and copying of message contents are avoided. the request handler within the destination process is only permitted access to the network to reply to messages, while the reply handler at the source process is prevented from accessing the network. this helps prevent cyclic dependencies and certain deadlock situations.
a second example of a streamlined messaging layer implementation is the fast messages (fm) library from the university of illinois [264]. the fm procedures are similar to active messages in that a handler is speciﬁed within the message. however, there is no notion of request-reply message pairs. there are also no restrictions on the actions that a handler can take. thus, program properties such as deadlock freedom are the responsibility of,
header that contains information necessary to implement a novel sender-based protocol for low-latency messaging. a 4-byte crc trailer rounds out the message packet format. two types of messages are supported: normal messages and priority messages. while normal messages may be as large as 64k packets in length, priority messages are limited to a single packet. typically priority messages are used by the operating system, and normal messages are generated by the applications.
a block diagram of the r2 router is illustrated in figure 7.35. in the ﬁgure the ports are drawn as distinct inputs and outputs to emphasize the data path, although they are bidirectional. each fifo buffer is capable of storing one maximum-length packet. there are 12 fifo buffers for normal packets and 3 fifo buffers for storing priority packets. two 7 × 15 crossbars provide switching between the buffers and the bidirectional ports. each of the fifo buffers has a routing controller. routing decisions use the destination address in the packet header and the local node id. buffer management and ordered use of buffers provides deadlock freedom. the implementation is self-timed, and the time to receive a packet header and to compute the address of an output port is estimated to take between 60 and 120 ns.
routing is controlled at the sender by specifying a number of adaptive credits. each adaptive credit permits the message packet to traverse one link that takes the packet no further from the destination. at any node, there are always two no-farther directions. the adaptive credit ﬁeld in the header is decremented each time a message takes an adaptive step, and further adaptive routing is not permitted once this value is reduced to zero. shortest paths are given priority in packet routing. priority packets are only routed along shortest paths. when all outputs for a priority packet are blocked by a normal packet, the priority packet is embedded in the normal packet byte stream being transmitted along one of the shortest paths, analogous with the implementation of virtual channels.an interesting feature of the r2 router is the support for message throttling. packet headers are monitored,
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
6. the route taken by a message depends on its destination and the status of output channels (free or busy). at a given node, an adaptive routing function supplies a set of output channels based on the current and destination nodes. a selection from this set is made based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied. if all the output channels are busy, the message header will be routed again until it is able to reserve a channel, thus getting the ﬁrst channel that becomes free.
8. the routing function may allow messages to follow nonminimal paths. 9. for each source/destination pair, the routing function will supply at least one minimal path. this assumption is only required to prove the necessary condition for deadlock freedom.
an interconnection network i is a strongly connected directed multigraph, i = g(n, c). the vertices of the multigraph n represent the set of processing nodes. the arcs of the multigraph c represent the set of communication channels. more than a single channel is allowed to connect a given pair of nodes. each channel ci has an associated queue with capacity cap(ci ). the source and destination nodes of channel ci are denoted si and di, respectively. for the sake of simplicity, an enumeration of arbitrary channels is denoted c1, c2, . . . , ck instead of cn1 , cn2 , . . . , cnk . an enumeration does not imply any channel ordering. let f be the set of valid channel status, f = {free, busy}. an adaptive routing function r : n × n → p(c), where p(c) is the power set of c, supplies a set of alternative output channels to send a message from the current node nc to the destination node nd, r(nc, nd ) = {c1, c2, . . . , cp}. by deﬁnition, r(n, n) = ∅, ∀n ∈ n. a selection function s : p(c × f ) → c selects a free output channel (if any) from the set supplied by the routing function. from the deﬁnition, s takes into account the status of all the channels belonging to the set supplied by the routing function. the selection can be random or based on static or dynamic priorities.
in this chapter we study routing algorithms. routing algorithms establish the path followed by each message or packet. the list of routing algorithms proposed in the literature is almost endless. we clearly cannot do justice to all of these algorithms developed to meet many distinct requirements. we will focus on a representative set of approaches, being biased toward those being used or proposed in modern and future multiprocessor interconnects. thus, we hope to equip you with an understanding of the basic principles that can be used to study the spectrum of existing algorithms. routing algorithms for wormhole switching are also valid for other switching techniques. thus, unless explicitly stated, the routing algorithms presented in this chapter are valid for all the switching techniques. speciﬁc proposals for some switching techniques will also be presented. special emphasis is given to design methodologies because they provide a simple and structured way to design a wide variety of routing algorithms for different topologies.
deadlock and livelock freedom. ability to guarantee that packets will not block or wander across the network forever. this issue was discussed in depth in chapter 3.
fault tolerance. ability to route packets in the presence of faulty components. although it seems that fault tolerance implies adaptivity, this is not necessarily true. fault tolerance can be achieved without adaptivity by routing a packet in two or more phases, storing it in some intermediate nodes. fault tolerance also requires some additional hardware mechanisms, as will be detailed in chapter 6.
same message to a speciﬁc set of destinations without requiring assistance from any other processor. the approaches used to implement such functionality are highly dependent on the network topology and may affect the design of the switching strategy used in the network. before studying those approaches, some schemes to encode multiple destination addresses are described in the next section.
the header of multidestination messages must carry the addresses of the destination nodes. the header information is an overhead to the system, increasing message latency and reducing the effective network bandwidth. a good multiaddress encoding scheme should minimize the message header length, also reducing the header processing time.
in wormhole and vct switching, the routing algorithm is executed before the whole message arrives at the router. as the header may require several ﬂits to encode the destination addresses, it is desirable that the routing decision in each router could be made as soon as possible to reduce message latency. ideally, a message header should be processed on the ﬂy as header ﬂits arrive. when the number of destination addresses is variable, it is inefﬁcient to use a counter to indicate the number of destinations. such a counter should be placed at the beginning of a message header. since the value of the counter may be changed at a router if the destination set is split into several subsets, it would prevent the processing of message headers on the ﬂy. an alternative approach is to have an end-of-header (eoh) ﬂit to indicate the end of a header. another approach consists of using 1 bit in each ﬂit to distinguish between header and data ﬂits.
figure 5.9 shows ﬁve different encoding schemes, namely, all-destination encoding, bit string encoding, multiple-region broadcast encoding, multiple-region stride encoding, and multiple-region bit string encoding. these schemes were proposed in [54].
the all-destination encoding is a simple scheme in which all destination addresses are carried by the header. this encoding scheme has two important advantages. first, the same routing hardware used for unicast messages can be used for multidestination messages. second, the message header can be processed on the ﬂy as address ﬂits arrive. this scheme is good for a small number of addresses because the header length is proportional to the number of addresses. however, it produces a signiﬁcant overhead when the number of destinations is large.
one way to limit the size of the header is to encode destination addresses as a bit string, where each bit corresponds to a destination ranged between node b and node e, as shown in figure 5.9(b). since the length of the string in a system is predeﬁned, the eoh ﬁeld is not required. this encoding scheme is good when the average number of destinations is large. however, it is inefﬁcient when the system is large and the number of destinations is small. the main drawback of the bit string encoding scheme is that a router usually has to buffer the entire bit string in order to make the routing decision and to generate the output bit string(s). additionally, address decoding cannot be done with the same routing hardware as for unicast messages. finally, the length of the string usually depends on network size, limiting the scalability of the system.
and in the presence of congestion, rapid injection of packets from a source can cause negative acknowledgments to be propagated back to the source to throttle injection.
error handling was considered at the outset during the initial design, and basic mechanisms were provided to detect packets that could not be delivered. once detected, these packets were ejected at the local node to be handled in software by higher-level protocols. a variety of conditions may cause message exceptions. for example, the message packet headers contain a ﬁeld that keeps track of the maximum number of hops. this ﬁeld is decremented at each intermediate router, and when this value becomes negative, the message is ejected. an ejection ﬁeld in the header is used to mark packets that must be forwarded by the router to the local processor interface. the router also contains watchdog timers that monitor the ports/links. if detection protocols time out, a fault register marks the port/link as faulty. if the faulty port leads to a packet destination, this status is recorded in the header, and the packet is forwarded to another router adjacent to the destination. if the second port/link to the destination is also found to be faulty, the packet is ejected.
the alpha 21364 processor provides a high-performance, highly scalable, and highly reliable network architecture [247]. the alpha 21364 microprocessor integrates an alpha 21264 processor core, a 1.75 mbyte second-level cache, cache coherence hardware, two memory controllers, and a multiprocessor router on a single die. figure 7.36 shows the alpha 21364 ﬂoorplan. in the 0.18 µm bulk cmos process, the 21364 runs at 1.2 ghz and provides 12.8 gbytes/s of local memory bandwidth and 22.4 gbytes/s of router bandwidth. the alpha 21364 network connects up to 128 such processors in a 2-d torus network, as shown in figure 7.37. such a fully conﬁgured 128-processor shared-memory system can support up to four tbytes of rambus memory and hundreds of tbytes of disk storage. this section describes the alpha 21364 network and router architectures.
even representing latency as a function of trafﬁc. when running simulations with synthetic workloads, the applied load (also known as offered trafﬁc, generation rate, or injection rate) is an input parameter while latency and accepted trafﬁc are measurements. so, latency-trafﬁc graphs do not represent functions. it should be noted that the network may be unstable when accepted trafﬁc reaches its maximum value. in this case, increasing the applied load may reduce the accepted trafﬁc until a stable point is reached. as a consequence, for some values of the accepted trafﬁc there exist two values for the latency, clearly indicating that the graph does not represent a function.
in the presence of faults, both performance and reliability are important. when presenting performance plots, the chaos normal form (cnf) format (to be described below) should be preferred in order to analyze accepted trafﬁc as a function of applied load. plots can be represented for different values of the number of faults. in this case, accepted trafﬁc can be smaller than applied load because the network is saturated or because some messages cannot be delivered in the presence of faults. another interesting measure is the probability of message delivery as a function of the number of failures.
the next sections describe two standard formats to represent performance results. these formats were proposed at pcrcw’94. the cnf requires paired accepted trafﬁc versus applied load and latency versus applied load graphs. the burton normal form (bnf) uses a single latency versus accepted trafﬁc graph. use of only latency (including source queuing) versus applied load is discouraged because it is impossible to gain any data about performance above saturation using such graphs.
cnf graphs display accepted trafﬁc on one graph and network latency on a second graph. in both graphs, the x-axis corresponds to normalized applied load. by using two graphs, the latency is shown both below and above saturation, and the accepted trafﬁc above saturation is visible. while bnf graphs show the same data, cnf graphs are more clear in their presentation of the data.
switches. if several switches exist, they are connected between them by using point-topoint links. in this case, any communication between communicating devices requires transmitting the information through one or more switches. finally, hybrid approaches are possible. these network classes and the corresponding subclasses will be described in the following sections.
the least complex interconnect structure is one in which the transmission medium is shared by all communicating devices. in such shared-medium networks, only one device is allowed to use the network at a time. every device attached to the network has requester, driver, and receiver circuits to handle the passing of addresses and data. the network itself is usually passive, since the network itself does not generate messages.
an important issue here is the arbitration strategy that determines the mastership of the shared-medium network to resolve network access conﬂicts. a unique characteristic of a shared medium is its ability to support atomic broadcast, in which all devices on the medium can monitor network activities and receive the information transmitted on the shared medium. this property is important to efﬁciently support many applications requiring one-to-all or one-to-many communication services, such as barrier synchronization and snoopy cache coherence protocols. due to limited network bandwidth, a single shared medium can only support a limited number of devices before the medium becomes a bottleneck.
shared-medium networks constitute a well-established technology.additionally, their limited bandwidth restricts their use in multiprocessors. so, these networks will not be covered in this book, but we will present a short introduction in the following sections. there are two major classes of shared-medium networks: local area networks, mainly used to construct computer networks that span physical distances no longer than a few kilometers, and backplane buses, mainly used for internal communication in uniprocessors and multiprocessors.
high-speed lans can be used as a networking backbone to interconnect computers to provide an integrated parallel and distributed computing environment. physically, a sharedmedium lan uses copper wires or ﬁber optics in a bit-serial fashion as the transmission medium. the network topology is either a bus or a ring. depending on the arbitration mechanism used, different lans have been commercially available. for performance and implementation reasons, it is impractical to have a centralized control or to have some ﬁxed access assignment to determine the bus master who can access the bus. three major classes of lans based on distributed control are described below.
each output will be visited at least once every 20 cycles. thus, messages are 20 ﬂits to maintain maximum throughput. in reality, the bidirectional channels add another factor of 2; therefore, in the worst case we have 40 cycles between routing operations on a channel. finally, the router includes a number of features in support of fault-tolerant routing. timers on the channels are used to identify adjacent faulty channels/nodes. packet bodies are protected by checksums, and the header portion is protected by parity checks. a header that fails a parity check at an intermediate node is ejected and handled in software. a separate single-bit signal on each channel is used to propagate a fault detection signal throughout the network. when a router receives this signal, message injections are halted and the network is allowed to “drain” [35]. messages that do not successfully drain from the network are detected using a timer and ejected to the local processor. when all messages have been removed from the faulty network, the system enters a diagnostic and recovery phase. recovery is realized by masking out faulty links. this is achieved within the routing decision logic by performing the logical and of a functional channel mask with a vector of candidate output channels computed from the routing header. this will only permit consideration of nonfaulty output channels. if the result indicates that no candidate channels exist (this may occur due to faults), the packet is immediately misrouted.
at the time of this writing, a project is under way to implement a version of the chaos router as a lan switch. this switch will be constructed using a new version of the chaos router chip and will utilize 72-byte packets and full-duplex, bidirectional channels with 8-bit-wide links. as described in section 7.2.2, the arbitration between two sides of a half-duplex pipelined channel carries substantial performance penalties. with lan links being potentially quite long, half-duplex links represented a poor design choice. the projected speed of operation is 180 mhz using the byte-wide channels for a bidirectional channel bandwidth of 360 mbytes/s. the lan switch will be constructed using a network of chaos router chips. for example, a 16-port switch can be constructed using a 4 × 4 torus network of chaos routers. routing internal to the switch is expected to be normal chaos routing, while switch-to-switch routing is expected to use source routing and standard fifo channels with link pipelining if the links are long enough. packet-level acknowledgments will be used for ﬂow control across the lan links. with the design of a special-purpose interface, the goal is to obtain host-to-host latencies down in the 10–20 µs range. this new-generation chip design is being streamlined with clock cycle times approaching 5.6 ns (simulated). this would produce a minimal delay through the router of four cycles, or 22.4 ns. packet formats and channel control have also been smoothed in this next-generation chip to streamline the message pipeline implementation.
the arctic routing chip is a ﬂexible router architecture that is targeted for use in several t multiprocessor [270]. types of networks, including the fat tree network used in the the basic organization of the router is illustrated in figure 7.33. the current prototype is being used as part of a network with powerpc compute nodes. four input sections can be switched to any one of four output sections. the router is input buffered, and each input
data ﬂit trafﬁc.ariadne [7] is a router for pcs supporting the mb-m family of fault-tolerant routing protocols. the current implementation is for a k-ary 2-cube with 8-bit-wide, halfduplex physical data channels. there are two virtual data channels and one virtual control channel in each direction across each physical link. the router is fully asynchronous and is comprised of three major components—the control path, the physical link interface, and the data path.
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
advance toward its destination. such a pruning will release channels [9, 8] and [8, 4], which is blocking message b. then, message b will advance, eventually releasing channel [9, 5], which is requested by message a. as a result, deadlock has been recovered from. in the event that ﬂow control stopped the advancing of ﬂits at node 13, message b would also prune its branch destined for node 5. this pruning is redundant, but it shows that nodes performing a pruning do not need to synchronize.
note that this example of deadlock recovery only serves to illustrate the pruning mechanism. it would not produce pruning in a real network unless the network was larger and each message would be destined for several additional nodes. the reason is that pruning would only take place at node 9 if there were more destination address ﬂits in message a after the one destined for node 1. similarly, pruning could only take place at node 13 if message b contained enough destination address ﬂits to ﬁll the buffers at nodes 8 and 12.
the most natural way of implementing multicast/broadcast routing in multistage interconnection networks (mins) is by using tree-based routing. multicast can be implemented in a single pass through the network by simultaneously forwarding ﬂits to several outputs at some switches. broadcast can be implemented in a single pass by simultaneously forwarding ﬂits to all the outputs at all the switches traversed by a message.
the replication of messages at some switches can be synchronous or asynchronous. in synchronous replication, the branches of a multidestination message can only forward if all the requested output channels are available. hence, at a given time, all the message headers are at different switches of the same stage of the network. synchronous replication requires a complex hardware signaling mechanism, usually slowing down ﬂit propagation. in asynchronous replication, each branch can forward independently without coordinating with other branches. as a consequence, hardware design is much simpler. however, bubbles may arise when some requested output channels are not available. the reason is that branches ﬁnding a free output channel are able to advance while branches ﬁnding a busy output channel are blocked. a similar situation for direct networks was shown in figure 5.12.
the propagation of multidestination messages may easily lead to deadlock, regardless of whether synchronous or asynchronous message replication is performed, as shown in the following example.
figure 5.21(a) shows a deadlocked conﬁguration in a 16-node butterﬂy min using asynchronous message replication. figure 5.21(b) shows the same deadlocked conﬁguration when messages are replicated synchronously. in each part of the ﬁgure, two multidestination messages a and b, destined for nodes 1000 and 1111, are sent by nodes 1010 and 1110, respectively (see example 4.7 to see how these messages are routed). in figure 5.21(a), the upper branch of message a successfully reserved the required output channel at stage g2, proceeding
the most popular bus arbitration mechanism is to have all devices compete for the exclusive access right of the bus. due to the broadcast nature of the bus, all devices can monitor the state of the bus, such as idle, busy, and collision. here the term “collision” means that two or more devices are using the bus at the same time and their data collided. when the collision is detected, the competing devices will quit transmission and try later. the most well-known contention-based lan is ethernet, which adopts carrier-sense multiple access with collision detection (csma/cd) protocol. the bandwidth of ethernet is 10 mbits/s and the distance span is 250 meters (coaxial cable). as processors are getting faster, the number of devices that can be connected to ethernet is limited to avoid the network bottleneck. in order to break the 10 mbits/s bandwidth barrier, fast ethernet can provide 100 mbits/s bandwidth.
one drawback of the contention bus is its nondeterministic nature as there is no guarantee of how much waiting time is required to gain the bus access right. thus, the contention bus is not suitable to support real-time applications. to remove the nondeterministic behavior, an alternative approach involves passing a token among the network devices. the owner of the token has the right to access the bus. upon completion of the transmission, the token is passed to the next device based on some scheduling discipline. by restricting the maximum token holding time, the upper bound that a device has to wait for the token can be guaranteed. arcnet supports token bus with a bandwidth of 2.5 mbits/s.
the idea of token ring is a natural extension of token bus as the passing of the token forms a ring structure. ibm token ring supports bandwidths of both 4 and 16 mbits/s based on coaxial cable. fiber distributed data interface (fddi) provides a bandwidth of 100 mbits/s using ﬁber optics.
a backplane bus is the simplest interconnection structure for bus-based parallel computers. it is commonly used to interconnect processor(s) and memory modules to provide uma architecture. figure 1.3 shows a single-bus network. a typical backplane bus usually has 50–300 wires and is physically realized by printed lines on a circuit board or by discrete (backplane) wiring. additional costs are incurred by interface electronics, such as line drivers, receivers, and connectors.
there are three kinds of information in the backplane bus: data, address, and control signals. control signals include bus request signal and request grant signal, among many others. in addition to the width of data lines, the maximum bus bandwidth that can be
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
in addition to the topologies deﬁned above, many other topologies have been proposed in the literature. most of them were proposed with the goal of minimizing the network diameter for a given number of nodes and node degree. as will be seen in chapter 2, for pipelined switching techniques, network latency is almost insensitive to network diameter, especially when messages are long. so it is unlikely that those topologies are implemented. in the following paragraphs, we present an informal description of some relevant direct network topologies.
a popular topology is the tree. this topology has a root node connected to a certain number of descendant nodes. each of these nodes is in turn connected to a disjoint set (possibly empty) of descendants.a node with no descendants is a leaf node.a characteristic property of trees is that every node but the root has a single parent node. therefore, trees contain no cycles. a tree in which every node but the leaves has a ﬁxed number k of descendants is a k-ary tree. when the distance from every leaf node to the root is the same (i.e., all the branches of the tree have the same length), the tree is balanced. figure 1.6(a) and (b) shows an unbalanced and a balanced binary tree, respectively.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
one such performance measure is the bisection width of a network—the minimum number of wires that must be cut when the network is divided into two equal sets of nodes. since the primary goal is one of assessing bandwidth and resulting wiring demands, only wires carrying data are generally counted, excluding control, power, and ground signals. however, these effects are relatively easy to incorporate in the following analysis. the collective bandwidth over these wires is referred to as the bisection bandwidth. for example, consider a 2-d torus with n = k2 nodes. assume that each pair of adjacent √ nodes is connected with one bidirectional data channel of width w bits. if this network is n wires crossing this bisection. the factor of divided into two halves, there will be 2w 2 is due to the wraparound channels. imagine what happens when we bisect a 3-d torus with n = k3 nodes. each half comprises k3/2 nodes. a 2-d plane of k2 nodes has links crossing the bisection, leading to a bisection width of 2w k2 = 2w kn−1. these cases are illustrated in figure 7.1. for the sake of clarity the wraparound links in the 3-d topology are not shown. in general, a bisection of a k-ary n-cube will cut channels from a k-ary (n − 1)-cube leading to a bisection width of 2w kn−1.
using the above approach we can compute the bisection width of a number of popular topologies. these are summarized in table 7.1. the implicit assumption here is that all of these topologies are regular; that is, each dimension has the same radix. consider the case of networks with n = k1 × k2 × · · · × kn nodes. in this case the minimum bisection width is orthogonal to the dimension with the largest radix. for example, this is the case shown in figure 7.1. if the largest radix is km, then we have n/km nodes with channels across this bisection. thus, the bisection width can be expressed as 2w n /km bits.
the above computation assumes that each node devotes w pins to communication with a neighbor along a dimension. no assumptions are made about how these pins are used. in general, given a set of w pins between adjacent nodes, these may be utilized as two (opposite) w 2 -bit unidirectional channels, one w -bit bidirectional channel, or one w -bit unidirectional channel. in computing the bisection width, the manner in which these
one such performance measure is the bisection width of a network—the minimum number of wires that must be cut when the network is divided into two equal sets of nodes. since the primary goal is one of assessing bandwidth and resulting wiring demands, only wires carrying data are generally counted, excluding control, power, and ground signals. however, these effects are relatively easy to incorporate in the following analysis. the collective bandwidth over these wires is referred to as the bisection bandwidth. for example, consider a 2-d torus with n = k2 nodes. assume that each pair of adjacent √ nodes is connected with one bidirectional data channel of width w bits. if this network is n wires crossing this bisection. the factor of divided into two halves, there will be 2w 2 is due to the wraparound channels. imagine what happens when we bisect a 3-d torus with n = k3 nodes. each half comprises k3/2 nodes. a 2-d plane of k2 nodes has links crossing the bisection, leading to a bisection width of 2w k2 = 2w kn−1. these cases are illustrated in figure 7.1. for the sake of clarity the wraparound links in the 3-d topology are not shown. in general, a bisection of a k-ary n-cube will cut channels from a k-ary (n − 1)-cube leading to a bisection width of 2w kn−1.
using the above approach we can compute the bisection width of a number of popular topologies. these are summarized in table 7.1. the implicit assumption here is that all of these topologies are regular; that is, each dimension has the same radix. consider the case of networks with n = k1 × k2 × · · · × kn nodes. in this case the minimum bisection width is orthogonal to the dimension with the largest radix. for example, this is the case shown in figure 7.1. if the largest radix is km, then we have n/km nodes with channels across this bisection. thus, the bisection width can be expressed as 2w n /km bits.
the above computation assumes that each node devotes w pins to communication with a neighbor along a dimension. no assumptions are made about how these pins are used. in general, given a set of w pins between adjacent nodes, these may be utilized as two (opposite) w 2 -bit unidirectional channels, one w -bit bidirectional channel, or one w -bit unidirectional channel. in computing the bisection width, the manner in which these
that messages can travel larger distances. in particular, when l is equal to 1/e, we obtain an exponential distribution. as l approaches one, the distribution function  approaches the uniform distribution. conversely, as l approaches zero,  approaches a nearest-neighbor communication pattern. it should be noted that the decreasing probability distribution is adequate for the analysis of networks of different sizes. simply, decay(l, dmax) should be computed for each network.
the distributions described above exhibit different degrees of spatial locality but have no temporal locality. recently, several speciﬁc communication patterns between pairs of nodes have been used to evaluate the performance of interconnection networks: bit reversal, perfect shufﬂe, butterﬂy, matrix transpose, and complement. these communication patterns take into account the permutations that are usually performed in parallel numerical algorithms [175, 200, 239]. in these patterns, the destination node for the messages generated by a given node is always the same. therefore, the utilization factor of all the network links is not uniform. however, these distributions achieve the maximum degree of temporal locality. these communication patterns can be deﬁned as follows:
bit reversal. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, a1, . . . , an−2, an−1. perfect shufﬂe. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−2, an−3, . . . , a0, an−1 (rotate left 1 bit). butterﬂy. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, an−2, . . . , a1, an−1 (swap the most and least signiﬁcant bits). matrix transpose. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a n 2 complement. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−1, an−2, . . . , a1, a0.
finally, a distribution based on a least recently used stack model has been proposed in [288] to model temporal locality. in this model, each node has its own stack containing the m nodes that were most recently sent messages. for each position in the stack there is a probability of sending a message to the node in that position. the sum of probabilities for nodes in the stack is less than one. therefore, a node not currently in the stack may be chosen as the destination for the next transmission. in this case, after sending the message, its destination node will be included in the stack, replacing the least recently used destination.
for synthetic workloads, the injection rate is usually the same for all the nodes. in most cases, each node is chosen to generate messages according to an exponential distribution. the parameter λ of this distribution is referred to as the injection rate. other possible distributions include a uniform distribution within an interval, bursty trafﬁc, and traces from parallel applications. for the uniform distribution, the injection rate is the mean value of the interval. bursty trafﬁc can be generated either by injecting a burst of messages every
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
same message to a speciﬁc set of destinations without requiring assistance from any other processor. the approaches used to implement such functionality are highly dependent on the network topology and may affect the design of the switching strategy used in the network. before studying those approaches, some schemes to encode multiple destination addresses are described in the next section.
the header of multidestination messages must carry the addresses of the destination nodes. the header information is an overhead to the system, increasing message latency and reducing the effective network bandwidth. a good multiaddress encoding scheme should minimize the message header length, also reducing the header processing time.
in wormhole and vct switching, the routing algorithm is executed before the whole message arrives at the router. as the header may require several ﬂits to encode the destination addresses, it is desirable that the routing decision in each router could be made as soon as possible to reduce message latency. ideally, a message header should be processed on the ﬂy as header ﬂits arrive. when the number of destination addresses is variable, it is inefﬁcient to use a counter to indicate the number of destinations. such a counter should be placed at the beginning of a message header. since the value of the counter may be changed at a router if the destination set is split into several subsets, it would prevent the processing of message headers on the ﬂy. an alternative approach is to have an end-of-header (eoh) ﬂit to indicate the end of a header. another approach consists of using 1 bit in each ﬂit to distinguish between header and data ﬂits.
figure 5.9 shows ﬁve different encoding schemes, namely, all-destination encoding, bit string encoding, multiple-region broadcast encoding, multiple-region stride encoding, and multiple-region bit string encoding. these schemes were proposed in [54].
the all-destination encoding is a simple scheme in which all destination addresses are carried by the header. this encoding scheme has two important advantages. first, the same routing hardware used for unicast messages can be used for multidestination messages. second, the message header can be processed on the ﬂy as address ﬂits arrive. this scheme is good for a small number of addresses because the header length is proportional to the number of addresses. however, it produces a signiﬁcant overhead when the number of destinations is large.
one way to limit the size of the header is to encode destination addresses as a bit string, where each bit corresponds to a destination ranged between node b and node e, as shown in figure 5.9(b). since the length of the string in a system is predeﬁned, the eoh ﬁeld is not required. this encoding scheme is good when the average number of destinations is large. however, it is inefﬁcient when the system is large and the number of destinations is small. the main drawback of the bit string encoding scheme is that a router usually has to buffer the entire bit string in order to make the routing decision and to generate the output bit string(s). additionally, address decoding cannot be done with the same routing hardware as for unicast messages. finally, the length of the string usually depends on network size, limiting the scalability of the system.
theorem 6.1 is only applicable to physical channels if they are not split into virtual channels. if virtual channels are used, the theorem is only valid for virtual channels. however, it can be easily extended to support the failure of physical channels by considering that all the virtual channels belonging to a faulty physical channel will become faulty at the same time. theorem 6.1 is based on theorem 3.1. therefore, it is valid for the same switching techniques as theorem 3.1, as long as edge buffers are used.
the structure of fault-tolerant routing algorithms is a natural consequence of the types of faults that can occur and our ability to diagnose them. the patterns of component failures and expectations about the behavior of processors and routers in the presence of these failures determines the approaches to achieve deadlock and livelock freedom. this information is captured in the fault model. the fault-tolerant computing literature is extensive and thorough in the deﬁnition of fault models for the treatment of faulty digital systems. in this section we will focus on common fault models that have been employed in the design of fault-tolerant routing algorithms for reliable interprocessor networks.
one of the ﬁrst considerations is the level at which components are diagnosed as having failed. typically, detection mechanisms are assumed to have identiﬁed one of two classes of faults. either the entire processing element (pe) and its associated router can fail, or any communication channel may fail. the former is referred to as a node failure, and the latter as a link failure. on a node failure, all physical links incident on the failed node are also marked faulty at adjacent routers. when a physical link fails, all virtual channels on that particular physical link are marked faulty. note that many types of failures will simply manifest themselves as link or node failures. for example, the failure of the link controller, or the virtual channel buffers, appears as a link failure. on the other hand, the failure of the router control unit or the associated pe effectively appears as a node failure. even software errors in the messaging layer can lead to message handlers “locking up” the local interface and rendering the attached router inoperative, effectively resulting in a node fault. hence, this failure model is not as restrictive as it may ﬁrst appear.
this model of individual link and node failures leads to patterns of failed components. adjacent faulty links and faulty nodes are coalesced into fault regions. generally, it is assumed that fault regions do not disconnect the network, since each connected network component can be treated as a distinct network. constraints may now be placed on the structure of these fault regions. the most common constraint employed is that these regions be convex. as will become apparent in this chapter, concave regions present unique difﬁculties for fault-tolerant routing algorithms. some examples of fault regions are illustrated in figure 6.3. convex regions may be further constrained to be block fault regions—regions whose shape is rectangular. this distinction is meaningful only in some topologies, whereas in other topologies, convex faults imply a block structure. given a pattern of random faults in a multidimensional k-ary n-cube, rectangular fault regions can be constructed by marking some functioning nodes as faulty to ﬁll out the fault regions.
1. blocking. a connection between a free input/output pair is not always possible because of conﬂicts with the existing connections. typically, there is a unique
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
switches. if several switches exist, they are connected between them by using point-topoint links. in this case, any communication between communicating devices requires transmitting the information through one or more switches. finally, hybrid approaches are possible. these network classes and the corresponding subclasses will be described in the following sections.
the least complex interconnect structure is one in which the transmission medium is shared by all communicating devices. in such shared-medium networks, only one device is allowed to use the network at a time. every device attached to the network has requester, driver, and receiver circuits to handle the passing of addresses and data. the network itself is usually passive, since the network itself does not generate messages.
an important issue here is the arbitration strategy that determines the mastership of the shared-medium network to resolve network access conﬂicts. a unique characteristic of a shared medium is its ability to support atomic broadcast, in which all devices on the medium can monitor network activities and receive the information transmitted on the shared medium. this property is important to efﬁciently support many applications requiring one-to-all or one-to-many communication services, such as barrier synchronization and snoopy cache coherence protocols. due to limited network bandwidth, a single shared medium can only support a limited number of devices before the medium becomes a bottleneck.
shared-medium networks constitute a well-established technology.additionally, their limited bandwidth restricts their use in multiprocessors. so, these networks will not be covered in this book, but we will present a short introduction in the following sections. there are two major classes of shared-medium networks: local area networks, mainly used to construct computer networks that span physical distances no longer than a few kilometers, and backplane buses, mainly used for internal communication in uniprocessors and multiprocessors.
high-speed lans can be used as a networking backbone to interconnect computers to provide an integrated parallel and distributed computing environment. physically, a sharedmedium lan uses copper wires or ﬁber optics in a bit-serial fashion as the transmission medium. the network topology is either a bus or a ring. depending on the arbitration mechanism used, different lans have been commercially available. for performance and implementation reasons, it is impractical to have a centralized control or to have some ﬁxed access assignment to determine the bus master who can access the bus. three major classes of lans based on distributed control are described below.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
injected messages to use some predetermined virtual channel(s) or waiting until the number of free output virtual channels at a node is higher than a threshold are enough. injection limitation mechanisms are especially recommended when using routing algorithms that allow cyclic dependencies between channels, and to limit the frequency of deadlock when using deadlock recovery mechanisms.
buffer size. for wormhole switching and short messages, increasing buffer size above a certain threshold does not improve performance signiﬁcantly. for long messages (or packets), increasing buffer size increases performance because blocked messages occupy fewer channels. however, when messages are very long, increasing buffer size only helps if buffers are deep enough to allow blocked messages to leave the source node and release some channels. it should be noted that communication locality may prevent most messages from leaving the source node before reaching their destination even when using deep buffers. therefore, in most cases small buffers are enough to achieve good performance. however, this analysis assumes that ﬂits are individually acknowledged. indeed, buffer size in wormhole switching is mainly determined by the requirements of the ﬂow control mechanism. optimizations like block acknowledgments require a certain buffer capacity to perform efﬁciently. moreover, when channels are pipelined, buffers must be deep enough to store all the ﬂits in transit across the physical link plus the ﬂits injected into the link during the propagation of the ﬂow control signals. some additional capacity is required for the ﬂow control mechanism to operate without introducing bubbles in the message pipeline. thus, when channels are pipelined, buffer size mainly depends on the degree of channel pipelining. for vct switching, throughput increases considerably when moving from one to two packet buffers. adding more buffers yields diminishing returns.
reliability/performance trade-offs. an interconnection network should be reliable. depending on the intended applications and the relative value of mtbf and mttr, different trade-offs are possible. when mttr << mtbf, the probability of the second or the third fault occurring before the ﬁrst fault is repaired is very low. in such environments, software-based rerouting is a cost-effective and viable alternative. this technique supports many fault patterns and requires minimum hardware support. however, performance degrades signiﬁcantly when faults occur, increasing latency for messages that meet faults by a factor of 2–4. when faults are more frequent or performance degradation is not acceptable, a more complex hardware support is required. the fault tolerance properties of the routing algorithm are constrained by the underlying switching technique, as indicated in the next item.
switching technique. if performance is more important than reliability, fault tolerance should be achieved without modifying the switching technique. in this case, additional resources (usually virtual channels) are required to implement
reduces the data rate of individual messages, increasing the message latency. this increase in latency due to data multiplexing will eventually overshadow the reduction in latency due to blocking, leading to overall increasing average message latency. an analysis of this phenomenon can be found in chapter 9, which provides detailed performance data to quantify various aspects of network performance.
increasing the number of virtual channels has a direct impact on router performance through their effect on the achievable hardware cycle time of the router. the link controllers now become more complex since they must support arbitration between multiple virtual channels/lanes for the physical channel, and this arbitration function can be on the critical path for internode delay. the number of inputs and outputs that must be switched at each node is increased, substantially increasing the switch complexity. for a ﬁxed amount of buffer space in a node, how is this buffer space to be allocated among channels, and lanes within a channel? further, the ﬂow of messages through the router must be coordinated with the allocation of physical channel bandwidth. the increasing complexity of these functions can lead to net increases in internal and external ﬂow control latencies. this increase affects all messages through the routers. such trade-offs and related issues affecting the design of routers are discussed in detail in chapter 7 and evaluated in chapter 9.
the availability and ﬂexibility of virtual channels have led to the development of several hybrid switching techniques. these techniques have been motivated by a desire to combine the advantages of several basic approaches or have been motivated by the need to optimize performance metrics other than traditional latency and throughput, for example, fault tolerance and reliability. some common hybrid switching techniques are presented in this section.
a switching technique that combines aspects of wormhole switching and packet switching is buffered wormhole switching (bws), proposed and utilized in ibm’s power parallel sp systems. the switching technique and message formats have been motivated by the interconnection network utilized in the sp systems. this network is a multistage, generalized omega network using 4 × 4 crossbar switches with bidirectional links. the system building block is a 16-processor system conﬁgured around the two-stage switch with eight crossbar switches as shown in figure 2.19(a). this module is referred to as a frame. each 4 × 4 switch uses bidirectional full-duplex links, and therefore can be viewed as an 8 × 8 implementation of the router organization shown in figure 2.1 with the functionality described below.
the basic switching mechanism is wormhole switching. message packets can be of variable length and up to 255 ﬂits in length. each ﬂit is 1 byte and is equal to the physical
memory (copying and buffering) prior to injection into the network while the send() call returns to the main program. until recently, network interfaces were largely treated as i/o devices. traditionally, drivers that control such devices (interface control) were privileged and were available only through a system call (user/kernel transitions). more recently, the high overhead of such an approach has evolved into more efﬁcient schemes for transferring control to message handlers that execute in the user address space. once the network interface has the message packet, it is injected into the network, where the routers cooperate in delivering the message to the destination node interface. when the message is received at the node, there must be some way to invoke the messaging layer software (e.g., interrupts, polled access, etc.). similar device driver services may then be invoked to transfer the message from the network interface into temporary system buffers (copied later into the user buffers) or transmitted directly to user buffers. message transfer is now complete.
we can view each of the above functions as essential to the process of transmitting messages. collectively they determine the minimum latency of a message, and the slowest component (invariably a software component) determines the maximum bandwidth. consider the issues in each of these steps.
computation of the message headers, sequence numbers, parity, crc, checksums, and so on are overhead operations. when these operations are implemented in software they can exact signiﬁcant performance penalties. most interfaces now implement the majority, if not all, of packetization functions in hardware. for example, in the cray t3d, the interface hardware performs a table lookup to generate the routing tag [259], while it is possible to compute the crc while copying data between buffers or during a dma transfer [30]. packetization overheads have been largely minimized in modern machines.
as an i/o device, control of the network interface can take one of many forms. data may be transferred to/from the interface using direct memory access (dma). in this case the software must initialize the appropriate dma channel and initiate dma access. this can be as simple as requiring two instructions: one to load the starting address and one to load the counter ([105] for the ncube-2). if dma is only allowed into certain portions of memory, then interaction with the operating system may be required. pages used as targets of dma or used to hold network interface data structures such as queues should be pinned down to prevent them from being swapped out.
alternatively the interface may be memory mapped and accessible from user space. the messaging software may initialize the interface with stores to memory locations corresponding to control registers. message packets may be similarly transferred to interface memory. when a message is received, the messaging software may be invoked via
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
with input channel requests. conﬂicts for the same output must be arbitrated (in logarithmic time), and if relative addressing is used, a header must be selected. if the requested buffer(s) is (are) busy, the incoming message remains in the input buffer until a requested output becomes free. the ﬁgure shows a full crossbar that connects all input virtual channels to all output virtual channels. alternatively, the router may use a design where full connectivity is only provided between physical channels, and virtual channels arbitrate for crossbar input ports. fast arbitration policies are crucial to maintaining a low ﬂow control latency through the switch.
buffers. these are fifo buffers for storing messages in transit. in the above model, a buffer is associated with both the input physical channels and output physical channels. the buffer size is an integral number of ﬂow control units. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). in vct switching sufﬁcient buffer space is available for a complete message packet. for a ﬁxed buffer size, insertion and removal from the buffer is usually not on the router critical path.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
while the above router model is representative of routers constructed to date, router architectures are evolving with different implementations. the routing and arbitration unit may be replicated to reduce arbitration time, and channels may share ports on the crossbar. the basic functions appear largely unaltered but with distinct implementations to match the current generation of technology.
if the routers support adaptive routing, the presence of multiple choices makes the routing decision more complex. there is a need to generate information corresponding to these choices and to select among these choices. this naturally incurs some overhead in time as well as resources (e.g., chip area). similarly, the use of virtual channels, while reducing header blocking delays in the network, makes the link controllers more complex by requiring arbitration and more complex ﬂow control mechanisms.
the two main measures of intrarouter performance [57] are the routing latency or header latency and the ﬂow control latency. from figure 7.7 it is apparent that the latency experienced by the header ﬂit(s) through a router is comprised of several components. after the header has been received, it must be decoded and the connection request generated. since multiple headers may arrive simultaneously, the routing and arbitration unit arbitrates among multiple requests, and one of the headers is selected and routed. this involves computing an output and, if it is available, setting the crossbar. the updated header and subsequent data may now be driven through the switch and will experience some multiplexing delay through the link controllers as they multiplex multiple virtual channels across the physical channel. once a path has been set up, data ﬂits may now ﬂow
several different bus arbitration algorithms have been proposed, which can be classiﬁed into centralized or distributed. a centralized method has a central bus arbiter. when a processor wants to become the bus master, it sends out a bus request to the bus arbiter, which then sends out a request grant signal to the requesting processor. a bus arbiter can be an encoder-decoder pair in hardware design. in a distributed method, such as the daisy chain method, there is no central bus arbiter. the bus request signals form a daisy chain. the mastership is released to the next device when data transfer is done.
most bus transactions involve request and response. this is the case for memory read operations. after a request is issued, it is desirable to have a fast response. if a fast response time is expected, the bus mastership is not released after sending the request, and data can be received soon. however, due to memory latency, the bus bandwidth is wasted while waiting for a response. in order to minimize the waste of bus bandwidth, the split transaction protocol has been used in many bus networks.
in this protocol, the bus mastership is released immediately after the request, and the memory has to gain mastership before it can send the data. split transaction protocol has a better bus utilization, but its control unit is much more complicated. buffering is needed in order to save messages before the device can gain the bus mastership.
to support shared-variable communication, some atomic read/modify/write operations to memories are needed. with the split transaction protocol, those atomic operations can no longer be indivisible. one approach to solve this problem is to disallow bus release for those atomic operations.
gigaplane used in sun ultra enterprise x000 server (ca. 1996): 2.68 gbyte/s peak, 256 bits data, 42 bits address, split transaction protocol, 83.8 mhz clock. dec alphaserver8x00, that is, 8200 and 8400 (ca. 1995): 2.1 gbyte/s, 256 bits data, 40 bits address, split transaction protocol, 100 mhz clock (1 foot length). sgi powerpath-2 (ca. 1993): 1.2 gbyte/s, 256 bits data, 40 bits address, 6 bits control, split transaction protocol, 47.5 mhz clock (1 foot length). hp9000 multiprocessor memory bus (ca. 1993): 1 gbyte/s, 128 bits data, 64 bits address, 13 inches, pipelined bus, 60 mhz clock.
scalability is an important issue in designing multiprocessor systems. bus-based systems are not scalable because the bus becomes the bottleneck when more processors are added.
several different bus arbitration algorithms have been proposed, which can be classiﬁed into centralized or distributed. a centralized method has a central bus arbiter. when a processor wants to become the bus master, it sends out a bus request to the bus arbiter, which then sends out a request grant signal to the requesting processor. a bus arbiter can be an encoder-decoder pair in hardware design. in a distributed method, such as the daisy chain method, there is no central bus arbiter. the bus request signals form a daisy chain. the mastership is released to the next device when data transfer is done.
most bus transactions involve request and response. this is the case for memory read operations. after a request is issued, it is desirable to have a fast response. if a fast response time is expected, the bus mastership is not released after sending the request, and data can be received soon. however, due to memory latency, the bus bandwidth is wasted while waiting for a response. in order to minimize the waste of bus bandwidth, the split transaction protocol has been used in many bus networks.
in this protocol, the bus mastership is released immediately after the request, and the memory has to gain mastership before it can send the data. split transaction protocol has a better bus utilization, but its control unit is much more complicated. buffering is needed in order to save messages before the device can gain the bus mastership.
to support shared-variable communication, some atomic read/modify/write operations to memories are needed. with the split transaction protocol, those atomic operations can no longer be indivisible. one approach to solve this problem is to disallow bus release for those atomic operations.
gigaplane used in sun ultra enterprise x000 server (ca. 1996): 2.68 gbyte/s peak, 256 bits data, 42 bits address, split transaction protocol, 83.8 mhz clock. dec alphaserver8x00, that is, 8200 and 8400 (ca. 1995): 2.1 gbyte/s, 256 bits data, 40 bits address, split transaction protocol, 100 mhz clock (1 foot length). sgi powerpath-2 (ca. 1993): 1.2 gbyte/s, 256 bits data, 40 bits address, 6 bits control, split transaction protocol, 47.5 mhz clock (1 foot length). hp9000 multiprocessor memory bus (ca. 1993): 1 gbyte/s, 128 bits data, 64 bits address, 13 inches, pipelined bus, 60 mhz clock.
scalability is an important issue in designing multiprocessor systems. bus-based systems are not scalable because the bus becomes the bottleneck when more processors are added.
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
that messages can travel larger distances. in particular, when l is equal to 1/e, we obtain an exponential distribution. as l approaches one, the distribution function  approaches the uniform distribution. conversely, as l approaches zero,  approaches a nearest-neighbor communication pattern. it should be noted that the decreasing probability distribution is adequate for the analysis of networks of different sizes. simply, decay(l, dmax) should be computed for each network.
the distributions described above exhibit different degrees of spatial locality but have no temporal locality. recently, several speciﬁc communication patterns between pairs of nodes have been used to evaluate the performance of interconnection networks: bit reversal, perfect shufﬂe, butterﬂy, matrix transpose, and complement. these communication patterns take into account the permutations that are usually performed in parallel numerical algorithms [175, 200, 239]. in these patterns, the destination node for the messages generated by a given node is always the same. therefore, the utilization factor of all the network links is not uniform. however, these distributions achieve the maximum degree of temporal locality. these communication patterns can be deﬁned as follows:
bit reversal. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, a1, . . . , an−2, an−1. perfect shufﬂe. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−2, an−3, . . . , a0, an−1 (rotate left 1 bit). butterﬂy. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, an−2, . . . , a1, an−1 (swap the most and least signiﬁcant bits). matrix transpose. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a n 2 complement. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−1, an−2, . . . , a1, a0.
finally, a distribution based on a least recently used stack model has been proposed in [288] to model temporal locality. in this model, each node has its own stack containing the m nodes that were most recently sent messages. for each position in the stack there is a probability of sending a message to the node in that position. the sum of probabilities for nodes in the stack is less than one. therefore, a node not currently in the stack may be chosen as the destination for the next transmission. in this case, after sending the message, its destination node will be included in the stack, replacing the least recently used destination.
for synthetic workloads, the injection rate is usually the same for all the nodes. in most cases, each node is chosen to generate messages according to an exponential distribution. the parameter λ of this distribution is referred to as the injection rate. other possible distributions include a uniform distribution within an interval, bursty trafﬁc, and traces from parallel applications. for the uniform distribution, the injection rate is the mean value of the interval. bursty trafﬁc can be generated either by injecting a burst of messages every
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
in order to make the theoretical results as general as possible, we assume no restriction about packet generation rate, packet destinations, and packet length. also, we assume no restriction on the paths supplied by the routing algorithm. both minimal and nonminimal paths are allowed. however, for performance reasons, a routing algorithm should supply at least one channel belonging to a minimal path at each intermediate node. additionally, we are going to focus on deadlocks produced by the interconnection network. thus, we assume that packets will be consumed at their destination nodes in ﬁnite time.
several switching techniques can be used. each of them will be considered as a particular case of the general theory. however, a few speciﬁc assumptions are required for some switching techniques. for saf and vct switching, we assume that edge buffers are used. central queues will be considered in section 3.2.3. for wormhole switching, we assume that a queue cannot contain ﬂits belonging to different packets. after accepting a tail ﬂit, a queue must be emptied before accepting another header ﬂit. when a virtual channel has queues at both ends, both queues must be emptied before accepting another header ﬂit. thus, when a packet is blocked, its header ﬂit will always occupy the head of a queue. also, for every path p that can be established by a routing function r, all subpaths of p are also paths of r. the routing functions satisfying the latter property will be referred to as coherent. for mad postman switching, we assume the same restrictions as for wormhole switching. additionally, dead ﬂits are removed from the network as soon as they are blocked.
a conﬁguration is an assignment of a set of packets or ﬂits to each queue. before analyzing how to avoid deadlocks, we are going to present a deadlocked conﬁguration by using an example.
consider a 2-d mesh with bidirectional channels. the routing function r forwards packets following any minimal path. this routing function is not deadlock-free. figure 3.2 shows a deadlocked conﬁguration. dashed incomplete boxes represent nodes of a 3 × 3 mesh. dashed boxes represent switches. solid boxes represent packet buffers or ﬂit buffers, depending on the switching technique used. the number inside each buffer indicates the destination node. solid arrows indicate the channel requested by the packet or the header at the queue head. as packets are allowed to follow all the minimal paths, packets wait for each other in a cyclic way. additionally, there is no alternative path for the packets in the ﬁgure because packets are only allowed to follow minimal paths. as all the buffers are full, no packet can advance.
so, a deadlocked conﬁguration is a conﬁguration in which some packets are blocked forever, waiting for resources that will never be granted because they are held by other packets. the conﬁguration described in example 3.1 would also be deadlocked if there were some additional packets traveling across the network that are not blocked. a deadlocked conﬁguration in which all the packets are blocked is referred to as canonical. given a deadlocked conﬁguration, the corresponding canonical conﬁguration can be obtained by stopping packet injection at all the nodes, and waiting for the delivery of
the teraﬂops system is an architecture being designed by intel corporation in an effort to produce a machine capable of a sustained performance of 1012 ﬂoating-point operations per second and a peak performance of 1.8 × 1012 ﬂoating-point operations per second. cavallino is the name for the network interface chip and router that forms the communication fabric for this machine [48]. the network is a k-ary 3-cube, with a distinct radix in each dimension. up to 8 hops are permitted in the z dimension, 256 hops in the x dimension, and 64 hops in the y dimension. the router has six ports, and the interface design supports 2-d conﬁgurations where the two ports in the z dimension are connected to two processing nodes rather than other routers. this architecture provides a ﬂexible basis for constructing a variety of topologies.
unlike most other routers, cavallino uses simultaneous bidirectional signaling. physical channels are half duplex. a receiver interprets incoming data with respect to the reference levels received and the data being driven to determine the value being received. the physical channel is 16 bits wide (phits). message ﬂits are 64 bits and require four clock cycles to traverse the channel. three additional signals across the channel provide virtual channel and ﬂow control information. the buffer status transmitted back to the sending virtual channel is interpreted as almost full rather than an exact number of available locations. this scheme allows for some slack in the protocols to tolerate signaling errors. the physical channel operates at 200 mhz, providing an aggregate data rate of 400 mbytes/s in each direction across the link. four virtual channels are multiplexed in each direction across a physical link.
the internal structure of the router is shown in figure 7.15.a message enters along one virtual channel and is destined for one output virtual channel. the routing header contains offsets in each dimension. in fact, routing order is z − x − y − z. using offsets permits fast routing decisions within each node as a message either turns to a new dimension or continues along the same dimension. the router is input buffered. with six input links and four virtual channels/link, a 24-to-1 arbitration is required on each crossbar output. the speed of operation and the length of the signal paths led to the adoption of a two-stage arbitration scheme. the ﬁrst stage arbitrates between corresponding virtual channels from the six inputs (e.g., a 6-to-1 arbiter for virtual channel 0 on each link). the second stage is a 4-to-1 arbitration for all channels with valid data competing for the same physical output link. the internal data paths are 16 bits, matched to the output links. the ports were designed such that they could be reset while the network was operational. this would enable removal and replacement of routers and cpu boards while the network was active. a block diagram of the network interface is shown in figure 7.16. the router side of the interface utilizes portions of the router architecture conﬁgured as a three-port design. one port is used for message injection and ejection, while two other ports are used for connection to two routers. this ﬂexibility permits the construction of more complex topologies. this is often desired for the purposes of fault tolerance or simply increased bandwidth. each output virtual channel is 384 bytes deep, while each input virtual channel is 256 bytes deep. there are two additional interesting components of the interface. the ﬁrst is the remote memory access (rma) engine. the rma engine
several different bus arbitration algorithms have been proposed, which can be classiﬁed into centralized or distributed. a centralized method has a central bus arbiter. when a processor wants to become the bus master, it sends out a bus request to the bus arbiter, which then sends out a request grant signal to the requesting processor. a bus arbiter can be an encoder-decoder pair in hardware design. in a distributed method, such as the daisy chain method, there is no central bus arbiter. the bus request signals form a daisy chain. the mastership is released to the next device when data transfer is done.
most bus transactions involve request and response. this is the case for memory read operations. after a request is issued, it is desirable to have a fast response. if a fast response time is expected, the bus mastership is not released after sending the request, and data can be received soon. however, due to memory latency, the bus bandwidth is wasted while waiting for a response. in order to minimize the waste of bus bandwidth, the split transaction protocol has been used in many bus networks.
in this protocol, the bus mastership is released immediately after the request, and the memory has to gain mastership before it can send the data. split transaction protocol has a better bus utilization, but its control unit is much more complicated. buffering is needed in order to save messages before the device can gain the bus mastership.
to support shared-variable communication, some atomic read/modify/write operations to memories are needed. with the split transaction protocol, those atomic operations can no longer be indivisible. one approach to solve this problem is to disallow bus release for those atomic operations.
gigaplane used in sun ultra enterprise x000 server (ca. 1996): 2.68 gbyte/s peak, 256 bits data, 42 bits address, split transaction protocol, 83.8 mhz clock. dec alphaserver8x00, that is, 8200 and 8400 (ca. 1995): 2.1 gbyte/s, 256 bits data, 40 bits address, split transaction protocol, 100 mhz clock (1 foot length). sgi powerpath-2 (ca. 1993): 1.2 gbyte/s, 256 bits data, 40 bits address, 6 bits control, split transaction protocol, 47.5 mhz clock (1 foot length). hp9000 multiprocessor memory bus (ca. 1993): 1 gbyte/s, 128 bits data, 64 bits address, 13 inches, pipelined bus, 60 mhz clock.
scalability is an important issue in designing multiprocessor systems. bus-based systems are not scalable because the bus becomes the bottleneck when more processors are added.
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
the binary relation dimension order, denoted <d, is deﬁned between two nodes x and y as follows: x <d y if and only if either x = y or there exists an integer j such that σj (x) < σj (y) and σi (x) = σi (y) ∀ i, 0 ≤ i ≤ j − 1.
since <d is just lexicographic ordering, it is a total ordering on the nodes in an ndimensional mesh. therefore, it is reﬂexive, antisymmetric, and transitive. given a set of node addresses, they can be arranged in a unique, ordered sequence according to the <d relation.
a sequence of nodes x1, x2, . . . , xm is a dimension-ordered chain if and only if all the elements are distinct and either (1) xi <d xi+1 for 1 ≤ i < m or (2) xi <d xi−1 for 1 < i ≤ m.
lemmas 5.1 and 5.2 are critical to the development of efﬁcient multicast algorithms because they indicate how channel contention may be avoided. the chain algorithm is a distributed algorithm that can be used to multicast a message from a source node to one or more destinations. the algorithm applies to situations in which the address of the source node is either less than or greater than those of all the destinations, according to the <d relation. figure 5.52 gives the chain algorithm executed at each node. the source address and the destination addresses are arranged as a dimension-ordered chain in either increasing or decreasing order, with the source node occupying the position at the left end of the chain. the source node sends ﬁrst to the destination node halfway across the chain, then to the destination node one-quarter of the way across the chain, and so on. each destination receives a copy of the message from its parent in the tree and may be responsible for forwarding the message to other destinations. the message carries the addresses of those nodes to be in the subtree rooted at the receiving node. the chain algorithm is designed to produce minimum-time multicast implementations on top of dimension-ordered unicast routing. although some messages are passed through multiple routers before reaching their destinations, it turns out that channel contention will not occur among the messages, regardless of message length or start-up latency— referred to as depth-contention-free. the following theorem forms the basis for developing software-based multicast algorithms.
constant delay equal to 2.2 ns. the crossbar is usually implemented using a tree of selectors for each output. thus, its delay grows logarithmically with the number of ports. assuming that p is the number of ports of the crossbar, its delay is given by 0.4 + 0.6 log p ns. finally, the setup time of a latch is 0.8 ns. the operations and the associated delays are shown in figure 9.32. the switch delay is
channels. the time required to transfer a ﬂit across a physical channel includes the off-chip delay across the wires and the time required to latch it onto the destination. assuming that channel width and ﬂit size are identical, this time is the sum of the output buffer, input buffer, input latch, and synchronizer delays. typical values for the technology used are 2.5 (with 25 pf load), 0.6, 0.8, and 1.0 ns, respectively, yielding 4.9 ns per ﬂit. the output buffer delay includes the propagation time across the wires, assuming that they are short. this is the case for a 3-d torus when it is assembled in three dimensions. if virtual channels are used, the time required to arbitrate and select one of the ready ﬂits must be added. the virtual channel controller has a delay that grows logarithmically with the number of virtual channels per physical channel. notice that we do not include any additional delay to decode the virtual channel number at the input of the next node because virtual channels are usually identiﬁed using one signal for each one [76]. if v is the number of virtual channels per physical channel, virtual channel controller delay is 1.24 + 0.6 log v ns. the operations and the associated delays are shown in figure 9.33. the total channel delay yields
deterministic routing. this routing algorithm offers a single routing choice. the switch is usually made by cascading several low-size crossbars, one per dimension. each of these crossbars switches messages going in the positive or negative direction of the same dimension or crossing to the next dimension. as there are two virtual channels per physical channel and two directions per dimension, the ﬁrst crossbar in the cascade has eight ports, including four injection ports. taking
no packet has already arrived at its destination node. packets cannot advance because the queues for all the alternative output channels supplied by the routing function are full.
there is no packet whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (remember, we make the assumption that a queue cannot contain ﬂits belonging to different packets). data ﬂits cannot advance because the next channel reserved by their packet header has a full queue. note that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
in some cases, a conﬁguration cannot be reached by routing packets starting from an empty network. this situation arises when two or more packets require the use of the same channel at the same time to reach the conﬁguration. a conﬁguration that can be reached by routing packets starting from an empty network is reachable or routable [63]. it should be noted that by deﬁning the domain of the routing function as n × n, every legal conﬁguration is also reachable. effectively, as the routing function has no memory of the path followed by each packet, we can consider that, for any legal conﬁguration, a packet stored in a channel queue was generated by the source node of that channel. in wormhole switching, we can consider that the packet was generated by the source node of the channel containing the last ﬂit of the packet. this is important because when all the legal conﬁgurations are reachable, we do not need to consider the dynamic evolution of the network leading to those conﬁgurations. we can simply consider legal conﬁgurations, regardless of the packet injection sequence required to reach them. when all the legal conﬁgurations are reachable, a routing function is deadlock-free if and only if there is not any deadlocked conﬁguration for that routing function.
a routing function r is connected if it is able to establish a path between every pair of nodes x and y using channels belonging to the sets supplied by r. it is obvious that a routing function must be connected, and most authors implicitly assume this property. however, we mention it explicitly because we will use a restricted routing function to prove deadlock freedom, and restricting a routing function may disconnect it.
the theoretical model of deadlock avoidance we are going to present relies on the concept of channel dependency [77]. other approaches are possible. they will be brieﬂy described in section 3.3. when a packet is holding a channel, and then it requests the use of another channel, there is a dependency between those channels. both channels are in one of the
when we restrict our attention to escape channels, it is important to give an accurate deﬁnition of channel dependency because there are some subtle cases. there is a channel dependency from an escape channel ci to another escape channel ck if there exists a packet that is holding ci and it requests ck as an escape channel for itself. it does not matter whether ci is an escape channel for this packet, as far as it is an escape channel for some other packets. also, it does not matter whether ci and ck are adjacent or not. these cases will be analyzed in sections 3.1.4 and 3.1.5. channel dependencies can be grouped together to simplify the analysis of deadlocks. a convenient form is the channel dependency graph [77]. it is a directed graph, d = g(c, e). the vertices of d are the channels of the interconnection network i . the arcs of d are the pairs of channels (ci , cj ) such that there is a channel dependency from ci to cj . as indicated above, we can restrict our attention to a subset of channels c1 ⊂ c, thus deﬁning a channel dependency graph in which all the vertices belong to c1. that graph has been deﬁned as the extended channel dependency graph of r1 [92, 97]. the word “extended” means that although we are focusing on a channel subset, packets are allowed to use all the channels in the network.
the extended channel dependency graph is a powerful tool to analyze whether a routing function is deadlock-free or not. the following theorem formalizes the ideas presented in example 3.3 by proposing a necessary and sufﬁcient condition for a routing function to be deadlock-free [92, 97]. this theorem is only valid under the previously mentioned assumptions.
a connected routing function r for an interconnection network i is deadlockfree if and only if there exists a routing subfunction r1 that is connected and has no cycles in its extended channel dependency graph.
this theorem is mostly used for deadlock avoidance. however, it is valid even if packets wait for a ﬁnite period of time before using a channel supplied by r1, as shown in [88, 94]. thus, theorem 3.1 can also be applied to prove deadlock freedom when deadlock recovery techniques are used, as will be seen in section 3.6.
the theorem is valid for both deterministic and adaptive routing. however, for deterministic routing, restricting the routing function will disconnect it because a single path is supplied for each packet. the only connected routing subfunction is r1 = r. in this case, the channel dependency graph and the extended channel dependency graph of r1 are identical. the resulting condition for deadlock-free routing is stated in the following corollary. it was proposed as a theorem in [77].
a (deterministic) routing function r for an interconnection network i is deadlock-free if and only if there are no cycles in the channel dependency graph d.
for adaptive routing functions, the application of theorem 3.1 requires the deﬁnition of a suitable routing subfunction. this is not an easy task. intuition and experience help considerably. a rule of thumb that usually works consists of looking at deterministic
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
through the router with no further involvement with the routing and arbitration unit, but may compete for physical channel bandwidth with other virtual channels.
the ﬂow control latency determines the rate at which ﬂits can be transmitted along the path. in the terminology of pipelines, the ﬂow control latency is the message pipeline stage time. from the ﬁgure, we can see that the ﬂow control latency experienced by a data ﬂit is the sum of the delay through the channel ﬂow control, the time to drive the ﬂit through the crossbar, and the multiplexing delay experienced through the link controller, as well as the time to read and write the fifo buffers. the delay through the link controller is often referred to as the ﬂit multiplexing delay or channel multiplexing delay. a general model for these delays is quite difﬁcult to derive since the latencies are sensitive to the implementation. one approach to developing such a model for wormholeswitched networks is described in [57]. a canonical model of a wormhole-switched router is developed, containing units for address decoding, ﬂow control, and switching. implementations of the various components (e.g., tree of gates, or selectors for each output) are modeled and expressions for the implementation complexity of each component are derived, but parameterized by technology-dependent constants. the parameterized model is illustrated in table 7.6. by instantiating the constants with values based on a particular implementation technology, realistic delays can be computed for comparing alternative designs. in [57], the values are instantiated for a class of implementations based on gate arrays to derive estimates of ﬂow control latency through the router. the utility of the model stems from the application to many classes of routers. other router architectures using similar components for partially adaptive or fully adaptive routing can be constructed, and the parameterized model used to estimate and compare intrarouter latencies. in fact, comparisons are possible across technology implementations.
a major determinant of the intrarouter delays is the size and structure of the switch. the use of a crossbar switch to connect all input virtual channels to all output virtual channels is feasible for low-dimensional networks with a small number of virtual channels. a 2-d network with four virtual channels per link would require a 16 × 16 crossbar, and a 3-d network would require a 24 × 24 switch. considering that these channels may be byte or word wide, it is apparent that alternatives begin to become attractive. one alternative is to have the switch size determined by the number of physical channels. the data rate
6. the route taken by a message depends on its destination and the status of output channels (free or busy). at a given node, an adaptive routing function supplies a set of output channels based on the current and destination nodes. a selection from this set is made based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied. if all the output channels are busy, the message header will be routed again until it is able to reserve a channel, thus getting the ﬁrst channel that becomes free.
8. the routing function may allow messages to follow nonminimal paths. 9. for each source/destination pair, the routing function will supply at least one minimal path. this assumption is only required to prove the necessary condition for deadlock freedom.
an interconnection network i is a strongly connected directed multigraph, i = g(n, c). the vertices of the multigraph n represent the set of processing nodes. the arcs of the multigraph c represent the set of communication channels. more than a single channel is allowed to connect a given pair of nodes. each channel ci has an associated queue with capacity cap(ci ). the source and destination nodes of channel ci are denoted si and di, respectively. for the sake of simplicity, an enumeration of arbitrary channels is denoted c1, c2, . . . , ck instead of cn1 , cn2 , . . . , cnk . an enumeration does not imply any channel ordering. let f be the set of valid channel status, f = {free, busy}. an adaptive routing function r : n × n → p(c), where p(c) is the power set of c, supplies a set of alternative output channels to send a message from the current node nc to the destination node nd, r(nc, nd ) = {c1, c2, . . . , cp}. by deﬁnition, r(n, n) = ∅, ∀n ∈ n. a selection function s : p(c × f ) → c selects a free output channel (if any) from the set supplied by the routing function. from the deﬁnition, s takes into account the status of all the channels belonging to the set supplied by the routing function. the selection can be random or based on static or dynamic priorities.
deadlocks might be for certain network conﬁgurations and to understand the parameters that most inﬂuence deadlock formation. for example, wormhole switching is more prone to deadlock than are other switching techniques [347]. this is because each packet may hold several channel resources spanning multiple nodes in the network while being blocked. we therefore concentrate on wormhole-switched recovery schemes.
recent work by pinkston and warnakulasuriya on characterizing deadlocks in interconnection networks has shown that a number of interrelated factors inﬂuence the probability of deadlock formation [283, 347].among the more inﬂuential of these factors is the routing freedom, the number of blocked packets, and the number of resource dependency cycles. routing freedom corresponds to the number of routing options available to a packet being routed at a given node within the network and can be increased by adding physical channels, adding virtual channels, and/or increasing the adaptivity of the routing algorithm. it has been quantitatively shown that as the number of network resources (physical and virtual channels) and the routing options allowed on them by the routing function increase (routing freedom), the number of packets that tend to block signiﬁcantly decreases, the number of resource dependency cycles decreases, and the resulting probability of deadlock also decreases exponentially. this is due to the fact that deadlocks require highly correlated patterns of cycles, the complexity of which increases with routing freedom. a conclusion of pinkston and warnakulasuriya’s work is that deadlocks in interconnection networks can be highly improbable when sufﬁcient routing freedom is provided by the network and fully exploited by the routing function. in fact, it has been shown that as few as two virtual channels per physical channel are sufﬁcient to virtually eliminate all deadlocks up to and beyond saturation in 2-d toroidal networks when using unrestricted fully adaptive routing [283, 347]. this veriﬁes previous empirical results that estimated that deadlocks are infrequent [9, 179]. however, the frequency of deadlock increases considerably when no virtual channels are used.
the network state reﬂecting resource allocations and requests existing at a particular point in time can be depicted by the channel wait-for graph (cwg). the nodes of this graph are the virtual channels reserved and/or requested by some packet(s). the solid arcs point to the next virtual channel reserved by the corresponding packet. the dashed arcs in the cwg point to the alternative virtual channels a blocked packet may acquire in order to continue routing. the highly correlated resource dependency pattern required to form a deadlocked conﬁguration can be analyzed with the help of the cwg. a knot is a set of nodes in the graph such that from each node in the knot it is possible to reach every other node in the knot. the existence of a knot is a necessary and sufﬁcient condition for deadlock [283]. note that checking the existence of knots requires global information and cannot be efﬁciently done at run time.
deadlocks can be characterized by deadlock set, resource set, and knot cycle density attributes. the deadlock set is the set of packets that own the virtual channels involved in the knot. the resource set is the set of all virtual channels owned by members of
resources in a cyclic way. if we prevent packets from waiting on some resources when blocked, we can prevent deadlocks. however, the additional routing ﬂexibility offered by this model is usually small.
models based on waiting channels are more general than the ones based on channel dependencies. in fact, the latter are particular cases of the former when a packet is allowed to wait on all the channels supplied by the routing function. another particular case of interest arises when a blocked packet always waits on a single channel. in this case, this model matches the behavior of routers that buffer blocked packets in queues associated with output channels. note that those routers do not use wormhole switching because they buffer whole packets.
although models based on waiting channels are more general than the ones based on channel dependencies, note that adaptive routing is useful because it offers alternative paths when the network is congested. in order to maximize performance, a blocked packet should be able to reserve the ﬁrst valid channel that becomes available. thus, restricting the set of routing options when a packet is blocked does not seem to be an attractive choice from the performance point of view.
a model that increases routing ﬂexibility in the absence of contention is based on the wait-for graph [73]. this graph indicates the resources that blocked packets are waiting for. packets are even allowed to use nonminimal paths as long as they are not blocked. however, blocked packets are not allowed to wait for channels held by other packets in a cyclic way. thus, some blocked packets are obliged to use deterministic routing until delivered if they produce a cycle in the wait-for graph.
an interesting model based on waiting channels is the message ﬂow model [207, 209]. in this model, a routing function is deadlock-free if and only if all the channels are deadlock-immune. a channel is deadlock-immune if every packet that reserves that channel is guaranteed to be delivered. the model starts by analyzing the channels for which a packet reserving them is immediately delivered. those channels are deadlockimmune. then the model analyzes the remaining channels step by step. in each step, the channels adjacent to the ones considered in the previous step are analyzed. a channel is deadlock-immune if for all the alternative paths a packet can follow, the next channel to be reserved is also deadlock-immune. waiting channels play a role similar to routing subfunctions, serving as escape paths when a packet is blocked.
another model uses a channel waiting graph [309]. this graph captures the relationship between channels in the same way as the channel dependency graph. however, this model does not distinguish between routing and selection functions. thus, it considers the dynamic evolution of the network because the selection function takes into account channel status. two theorems are proposed. the ﬁrst one considers that every packet has a single waiting channel. it states that a routing algorithm is deadlock-free if it is waitconnected and there are no cycles in the channel waiting graph. the routing algorithm is wait-connected if it is connected by using only waiting channels.
but the most important result is a necessary and sufﬁcient condition for deadlock-free routing. this condition assumes that a packet can wait on any channel supplied by the routing algorithm. it uses the concept of true cycles. a cycle is a true cycle if it is reachable, starting from an empty network. the theorem states that a routing algorithm is deadlock-
latency through the chip is estimated to be 40 ns, while the latency from the input on a router to the input on the adjacent router is estimated to be 50 ns.
routers supporting vct switching share many of the attributes of wormhole routers. the principal distinguishing feature is the availability of sufﬁcient space for buffering complete message packets. the following examples discuss four router architectures implementing vct switching.
the chaos router chip is an example of a router designed for vct switching for operation in a 2-d mesh. to reduce pin count, each channel is a 16-bit, half-duplex bidirectional channel. the chip was designed for a cycle time of 15 ns. the latency for the common case of cut-through routing (no misrouting) is four cycles from input to output. this performance compares favorably to that found in more compact oblivious routers and has been achieved by keeping the misrouting and buffering logic off the critical path. a block diagram of the chaos router is shown in figure 7.31.
the router is comprised of the router core and the multiqueue [32]. the core connects input frames to output frames through a crossbar switch. each frame can store one 20-ﬂit message packet. each ﬂit is 16 bits wide and corresponds to the width of the physical channel. the architecture of the core is very similar to that of contemporary oblivious routers, with the additional ability to buffer complete 20-ﬂit packets in each input and output frame. under low-load conditions, the majority of the trafﬁc ﬂows through this core,
typical of livelocked messages are avoided with a high probability. the thesis is that this can be achieved without the higher overhead costs in header size and routing decision logic of deterministic approaches to livelock freedom.
the chaos router incorporates randomization to produce a nonminimal fault-tolerant routing algorithm using vct switching. the use of vct switching in conjunction with nonminimal routing and randomization is proposed as a means to produce very highspeed, reliable communication substrates [187, 188, 189]. in chaotic routing, messages are normally routed along any proﬁtable output channel. when a message resides in an input buffer for too long, blocked by busy output buffers, it is removed from the input buffer and stored in a local buffer pool. messages in the local buffer pool are given a higher priority when output buffers become free, and messages from this pool are periodically reinjected into the network. in principle, if we have inﬁnite storage at a node, removal of messages after a timeout breaks any cyclic dependencies and therefore prevents deadlock. in reality, the size of the buffer pool at a node is ﬁnite and is implemented as a queue. when this queue is full and a message must be stored in the queue, a random message is selected and misrouted. this is sufﬁcient to prevent deadlock. however, livelock is still a source of concern. the key idea here is that a message has a nonzero probability of avoiding misrouting. this is in contrast to deterministic approaches, where given a speciﬁc router state, one ﬁxed message is always selected for misrouting. the analysis of chaotic routing shows that the probability of inﬁnite length paths approaches 0. thus if a link is faulty, all messages using that link will be forwarded along other proﬁtable links or misrouted around the faulty region.
figure 6.7 provides a simpliﬁed illustration of the operation of a channel along a dimension using chaos routing. each channel supports one input frame and one output frame to an adjacent router. messages appearing in the input frames can be routed along any free proﬁtable output dimension if the corresponding output frame is free. if the message in the input frame cannot be forwarded, then it is stored in the local queue. if the queue is full, then a random message must be removed from the queue and misrouted. in this case, some output frame must be guaranteed to become free. the deroute buffers associated
channel width. the ﬁrst ﬂit of a message contains the length of the message, while the following ﬂits contain routing information. routing is source based where each routing ﬂit contains the address of the output ports in intermediate switches. there is 1 routing ﬂit for each frame, that is, for each group of 16 processors. the format of the routing ﬂit is shown in figure 2.19(b). note that these 4 × 4 crossbar switches have bidirectional links, and therefore eight input ports and eight output ports. bits 4–6 are used to select the output port of the ﬁrst switch in the frame. bits 0–2 are used to select the output port of the second switch in the frame. bit 7 is used to determine which ﬁeld is to be used. it is initially cleared and set by the ﬁrst switch in the frame. larger systems are built up as groups of frames. every frame requires 1 routing ﬂit.
as the message is routed through the switches, the corresponding routing ﬂit is discarded, shortening the message. this is similar to the mad postman switching strategy. as long as the path is conﬂict-free, the message progresses as in wormhole switching with interswitch ﬂow control operating at the ﬂit level. when output channels are available, data ﬂow through the switch is through byte-wide paths through the internal crossbar and to the output port. when messages block, ﬂow control within the switch is organized into 8-ﬂit units referred to as chunks. when messages block, chunks are constructed at the input port of a switch, transmitted through 64-bit-wide paths to the local memory. subsequently, buffered chunks are transferred to an output port where they are converted to a ﬂit stream for transmission across the physical channel.
when there is a conﬂict at the output of a routing node, ﬂits are buffered within the switch as chunks. these chunks are buffered in a dynamically allocated central storage. the storage is organized as a linked list for each output, where each element of the list is
a transmission in one clock cycle. therefore, the propagation delay across this channel is denoted by tw = 1 b . this assumption will be relaxed in section 7.1.5. once a path has been set up through the router, the intrarouter delay or switching delay is denoted by ts. the router internal data paths are assumed to be matched to the channel width of w bits. thus, in ts seconds a w -bit ﬂit can be transferred from the input of the router to the output. the source and destination processors are assumed to be d links apart. the relationship between these components as they are used to compute the no-load message latency is shown in figure 2.5.
in circuit switching, a physical path from the source to the destination is reserved prior to the transmission of the data. this is realized by injecting the routing header ﬂit into the network. this routing probe contains the destination address and some additional control information. the routing probe progresses toward the destination reserving physical links as it is transmitted through intermediate routers. when the probe reaches the destination, a complete path has been set up and an acknowledgment is transmitted back to the source. the message contents may now be transmitted at the full bandwidth of the hardware path. the circuit may be released by the destination or by the last few bits of the message. in the intel ipsc/2 routers [258], the acknowledgments are multiplexed in the reverse direction on the same physical line as the message. alternatively, implementations may provide separate signal lines to transmit acknowledgment signals. a time-space diagram of the transmission of a message over three links is shown in figure 2.6. the header probe is forwarded across three links, followed by the return of the acknowledgment. the shaded boxes represent the times during which a link is busy. the space between these boxes represents the time to process the routing header plus the intrarouter propagation delays. the clear box represents the duration that the links are busy transmitting data through the
consider two unrelated channels ci , cj that have some common predecessors and successors in the ordering. we can say that they are at the same level in the ordering or that they are equivalent. this equivalence relation groups equivalent channels so that they form an equivalence class [89]. note that there is no dependency between channels belonging to the same class. now, it is possible to deﬁne the concept of class dependency. there is a dependency from class ki to class kj if there exist two channels ci ∈ ki and cj ∈ kj such that there is a dependency from ci to cj . we can represent class dependencies by means of a graph. if the classes contain all the channels, we deﬁne the class dependency graph. if we restrict our attention to classes formed by the set of channels c1 supplied by a routing subfunction r1, we deﬁne the extended class dependency graph [89].
a connected routing function r for an interconnection network i is deadlockfree if there exist a connected routing subfunction r1 and an equivalence relation ∀x,y∈n r1(x, y), such that the extended class dependency
the existence of a dependency between two classes does not imply the existence of a dependency between every pair of channels in those classes. thus, the existence of cycles in the extended class dependency graph does not imply that the extended channel dependency graph has cycles. as a consequence, theorem 3.3 only supplies a sufﬁcient condition. however, this is enough to prove deadlock freedom.
it is not always possible to establish an equivalence relation between channels. however, when possible, it considerably simpliﬁes the analysis of deadlock freedom. the channels that can be grouped into the same class usually have some common topological properties, for instance, channels crossing the same dimension of a hypercube in the same direction. classes are not a property of the topology alone. they also depend on the routing function. the next example shows the deﬁnition of equivalence classes in a 2-d mesh.
consider the extended channel dependency graph drawn in figure 3.10. there is not any dependency between channels b01, b34, and b67. however, there are dependencies from b01, b34, and b67 to b12, b45, and b78. channels b01, b34, and b67 can be grouped into the same class. the same can be done with channels b12, b45, and b78. suppose that we deﬁne classes such that all the horizontal (vertical) channels in the same column (row) and direction are in the same class. it is easy to see that there is not any dependency between channels belonging to the same class. let us denote the classes containing east (x-positive) channels as x0+, x1+, starting from the left. for instance, x0+ = {b01, b34, b67}. similarly, classes containing west channels are denoted as x0−, x1−, starting from the right.also, classes containing north channels are denoted as y 0+, y 1+, starting from the bottom, and classes containing south
consider two unrelated channels ci , cj that have some common predecessors and successors in the ordering. we can say that they are at the same level in the ordering or that they are equivalent. this equivalence relation groups equivalent channels so that they form an equivalence class [89]. note that there is no dependency between channels belonging to the same class. now, it is possible to deﬁne the concept of class dependency. there is a dependency from class ki to class kj if there exist two channels ci ∈ ki and cj ∈ kj such that there is a dependency from ci to cj . we can represent class dependencies by means of a graph. if the classes contain all the channels, we deﬁne the class dependency graph. if we restrict our attention to classes formed by the set of channels c1 supplied by a routing subfunction r1, we deﬁne the extended class dependency graph [89].
a connected routing function r for an interconnection network i is deadlockfree if there exist a connected routing subfunction r1 and an equivalence relation ∀x,y∈n r1(x, y), such that the extended class dependency
the existence of a dependency between two classes does not imply the existence of a dependency between every pair of channels in those classes. thus, the existence of cycles in the extended class dependency graph does not imply that the extended channel dependency graph has cycles. as a consequence, theorem 3.3 only supplies a sufﬁcient condition. however, this is enough to prove deadlock freedom.
it is not always possible to establish an equivalence relation between channels. however, when possible, it considerably simpliﬁes the analysis of deadlock freedom. the channels that can be grouped into the same class usually have some common topological properties, for instance, channels crossing the same dimension of a hypercube in the same direction. classes are not a property of the topology alone. they also depend on the routing function. the next example shows the deﬁnition of equivalence classes in a 2-d mesh.
consider the extended channel dependency graph drawn in figure 3.10. there is not any dependency between channels b01, b34, and b67. however, there are dependencies from b01, b34, and b67 to b12, b45, and b78. channels b01, b34, and b67 can be grouped into the same class. the same can be done with channels b12, b45, and b78. suppose that we deﬁne classes such that all the horizontal (vertical) channels in the same column (row) and direction are in the same class. it is easy to see that there is not any dependency between channels belonging to the same class. let us denote the classes containing east (x-positive) channels as x0+, x1+, starting from the left. for instance, x0+ = {b01, b34, b67}. similarly, classes containing west channels are denoted as x0−, x1−, starting from the right.also, classes containing north channels are denoted as y 0+, y 1+, starting from the bottom, and classes containing south
in order to make the theoretical results as general as possible, we assume no restriction about packet generation rate, packet destinations, and packet length. also, we assume no restriction on the paths supplied by the routing algorithm. both minimal and nonminimal paths are allowed. however, for performance reasons, a routing algorithm should supply at least one channel belonging to a minimal path at each intermediate node. additionally, we are going to focus on deadlocks produced by the interconnection network. thus, we assume that packets will be consumed at their destination nodes in ﬁnite time.
several switching techniques can be used. each of them will be considered as a particular case of the general theory. however, a few speciﬁc assumptions are required for some switching techniques. for saf and vct switching, we assume that edge buffers are used. central queues will be considered in section 3.2.3. for wormhole switching, we assume that a queue cannot contain ﬂits belonging to different packets. after accepting a tail ﬂit, a queue must be emptied before accepting another header ﬂit. when a virtual channel has queues at both ends, both queues must be emptied before accepting another header ﬂit. thus, when a packet is blocked, its header ﬂit will always occupy the head of a queue. also, for every path p that can be established by a routing function r, all subpaths of p are also paths of r. the routing functions satisfying the latter property will be referred to as coherent. for mad postman switching, we assume the same restrictions as for wormhole switching. additionally, dead ﬂits are removed from the network as soon as they are blocked.
a conﬁguration is an assignment of a set of packets or ﬂits to each queue. before analyzing how to avoid deadlocks, we are going to present a deadlocked conﬁguration by using an example.
consider a 2-d mesh with bidirectional channels. the routing function r forwards packets following any minimal path. this routing function is not deadlock-free. figure 3.2 shows a deadlocked conﬁguration. dashed incomplete boxes represent nodes of a 3 × 3 mesh. dashed boxes represent switches. solid boxes represent packet buffers or ﬂit buffers, depending on the switching technique used. the number inside each buffer indicates the destination node. solid arrows indicate the channel requested by the packet or the header at the queue head. as packets are allowed to follow all the minimal paths, packets wait for each other in a cyclic way. additionally, there is no alternative path for the packets in the ﬁgure because packets are only allowed to follow minimal paths. as all the buffers are full, no packet can advance.
so, a deadlocked conﬁguration is a conﬁguration in which some packets are blocked forever, waiting for resources that will never be granted because they are held by other packets. the conﬁguration described in example 3.1 would also be deadlocked if there were some additional packets traveling across the network that are not blocked. a deadlocked conﬁguration in which all the packets are blocked is referred to as canonical. given a deadlocked conﬁguration, the corresponding canonical conﬁguration can be obtained by stopping packet injection at all the nodes, and waiting for the delivery of
an extended channel dependency graph de for a given interconnection network i and routing subfunction r1 of a routing function r is a directed graph, de = g(c1, ee). the vertices of de are the channels supplied by the routing subfunction r1 for some destinations. the arcs of de are the pairs of channels (ci , cj ) such that there is either a direct, indirect, direct cross-, or indirect crossdependency from ci to cj .
a routing function r for a given interconnection network i is coherent iff for every pair of nodes x, y ∈ n, x (cid:16)= y, and for every path p (x, y) = {c1, c2, . . . , ck} that can be established by r between them
that is, for every path p that can be established by r, every preﬁx of p is also a path of r. in other words, if a routing function r can establish a path p (x, y) between x and y, it can also establish a path between x and any intermediate node crossed by p (x, y) using a subset of the channels used by p (x, y). if the routing function r is coherent, there is not any loop (1-cycle) in the extended channel dependency graph of any routing subfunction. otherwise, there would be a destination node for which a message is allowed to cross a channel ci (and its source node si) twice. taking into account assumption 2, the subpath starting and ending at si is not allowed. thus, that routing function would not be coherent.
a connected and adaptive routing function r for an interconnection network i is deadlock-free if there exists a routing subfunction r1 that is connected and has no cycles in its extended channel dependency graph de.
a coherent, connected, and adaptive routing function r for an interconnection network i is deadlock-free iff there exists a routing subfunction r1 that is connected and has no cycles in its extended channel dependency graph de.
b is stored in a[1:n,n+1], and the vector x is stored in a[0,1:n]. the following code segment, based on a pseudo-high-level data parallel language, demonstrates one approach to solving the problem.
although the above code is by no means optimal parallel code, it serves to show how the data parallel operations mentioned above are fundamental to parallel programming. the function max in statement s2 is a reduction operation, which ﬁnds the location of the pivot element. the function exchange in s3 is a permutation operation, which exchanges the ith column and the column with the pivot element. s4 performs row-wise normalization with respect to the pivot element. since one operand has a higher dimension than the other operand, a replication operation of the lower-dimension data is implied. using data dependence analysis, both loops in s5 and s6 can be parallelized. furthermore, in s7 the replication of a[j,i] and a[i,k] across multiple processors is implied because one of the dimensions is not a function of the loop indices. the enddo statements in s8 and s9 imply barrier synchronization when the corresponding do is parallelized.
the above operations that involve global data movement and global control are known as collective communication as many processes are collectively involved in performing such operations. as indicated in [114], many scientiﬁc applications exhibit the need for such communication patterns, and providing collective communication services can simplify the programming of multicomputers. details of those frequently used collective communication operations will be described in the next section. this chapter will emphasize efﬁcient support, both hardware and software, for implementing multicast communication, which is essential to the efﬁcient implementation of collective communication operations.
collective communication involves a group of processes. in order to simplify the programming and allow for efﬁcient implementation, these communicating processes are usually
used to set up and retain the local parameters used for communication. the setup procedures (e.g., mpi send init() and mpi recv init()) return handles bound to arguments. these handles are used in subsequent mpi start() calls. the semantics of each pair of persistent communication procedure calls is identical to that of a nonblocking send or receive.
it is important to understand that while the send and receive call semantics are well deﬁned, this does not preclude the programmer from constructing erroneous programs. in particular, errant use of blocking receive calls can lead to deadlocked programs. there is also no notion of fairness in how receive calls match messages. these considerations should be taken into account when constructing programs.
a speciﬁc class of communication patterns that have received increasing attention in the recent past has been the class of collective communication operations. as the name suggests, collective communication involves the aggregation or dissemination of data from/to multiple processes. the importance of collective communications is derived from the fact that many frequently used parallel algorithms such as sorting, searching, and matrix manipulation share data among groups of processes. transmission of data to multiple destinations can be implemented with multiple calls for point-to-point transmission. however, these patterns of sharing data are very regular and are important enough to merit special procedures.
in general, collective communication involves one or more senders and one or more receivers. examples include broadcast of a single data item from one process to all other processes, broadcast of unique items from one process to all other processes, and the inverse operation: gathering data from a group of processes. there are also other operations that are collective in nature although no data are communicated (i.e., barrier synchronization). in general, we can identify the following common collective communication operations:
the mpi standard speciﬁes support for all of these collective communication operations as well as other global operations such as reduce and scan operations. as an example, consider the following mpi call:
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
tiprocessors implement latency-hiding mechanisms and application performance mainly depends on the throughput achievable by the interconnection network, then adaptive routing is expected to achieve higher performance than deterministic routing. in the case of using adaptive routing, the additional cost of implementing fully adaptive routing should be kept small. therefore, routing algorithms that require few resources to avoid deadlock or to recover from it, like the ones evaluated in this chapter, should be preferred. for these routing algorithms, the additional complexity of fully adaptive routing usually produces a small reduction in clock frequency. number of virtual channels. in wormhole switching, when no virtual channels are used, blocked messages do not allow other messages to use the bandwidth of the physical channels they are occupying. adding the ﬁrst additional virtual channel usually increases throughput considerably at the expense of a small increase in latency. on the other hand, adding more virtual channels produces a much smaller increment in throughput while increasing hardware delays considerably. for deterministic routing in meshes, two virtual channels provide a good trade-off. for tori, the partially adaptive algorithm evaluated in this chapter with two virtual channels also provides a good trade-off, achieving the advantages of channel multiplexing without increasing the number of virtual channels with respect to the deterministic algorithm. if fully adaptive routing is preferred, the minimum number of virtual channels should be used. fully adaptive routing requires a minimum of two (three) virtual channels to avoid deadlock in meshes (tori). again, for applications that require low latency and produce a relatively small amount of trafﬁc, adding virtual channels does not help. virtual channels only increase performance when applications beneﬁt from a higher network throughput. hardware support for collective communication. adding hardware support for multidestination message passing usually reduces the latency of collective communication operations with respect to software algorithms. however, this reduction is very small (if any) when the number of participating nodes is small. when many nodes participate and trafﬁc is only composed of multidestination messages, latency reduction ranges from 2 to 7, depending on several parameters. in real applications, trafﬁc for collective communication operations usually represents a much smaller fraction of network trafﬁc. also, the number of participating nodes may vary considerably from one application to another. in general, this number is small except for broadcast and barrier synchronization. in summary, whether adding hardware support for collective communication is worth its cost depends on the application requirements. injection limitation mechanism. when fully adaptive routing is used, network interfaces should include some mechanism to limit the injection of new messages when the network is heavily loaded. otherwise, increasing applied load above the saturation point may degrade performance severely. in some cases, the start-up latency is so high that it effectively limits the injection rate. when the start-up latency does not prevent network saturation, simple mechanisms like restricting
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
software messaging layer. a high percentage of the communication latency in multicomputers is produced by the overhead in the software messaging layer. reducing or hiding this overhead is likely to have a higher impact on performance than the remaining design parameters, especially when messages are relatively short. the use of vcc can eliminate or hide most of the software overhead by overlapping path setup with computation, and caching and retaining virtual circuits for use by multiple messages. vcc complements the use of techniques to reduce the software overhead, like active messages. it should be noted that software overhead is so high in some multicomputers that it makes no sense improving other design parameters. however, once this overhead has been removed or hidden, the remaining design parameters become much more important.
software support for collective communication. collective communication operations beneﬁt considerably from using speciﬁc algorithms. using separate addressing, latency increases linearly with the number of participating nodes. however, when algorithms for collective communication are implemented in software, latency is considerably reduced, increasing logarithmically with the number of participating nodes. this improvement is achieved with no additional hardware cost and no overhead for unicast messages.
number of ports. if the software overhead has been removed or hidden, the number of ports has a considerable inﬂuence on performance, especially when messages are sent locally. if the number of ports is too small, the network interface is likely to be a bottleneck for the network. the optimal number of ports heavily depends on the spatial locality of trafﬁc patterns.
switching technique. nowadays, most commercial and experimental parallel computers implement wormhole switching. although vct switching achieves a higher throughput, the performance improvement is small when virtual channels are used in wormhole switching. additionally, vct switching requires splitting messages into packets not exceeding buffer capacity. if messages are longer than buffer size, wormhole switching should be preferred. however, when messages are shorter than or equal to buffer size, vct switching performs slightly better than wormhole switching, also simplifying deadlock avoidance. this is the case for distributed, shared-memory multiprocessors. vct switching is also preferable when it is not easy to avoid deadlock in wormhole switching (multicast routing in multistage networks) or in some applications requiring real-time communication [294]. finally, a combination of wormhole switching and circuit switching with wave-pipelined switches and channels has the potential to increase performance, especially when messages are very long [101].
packet size. for pipelined switching techniques, ﬁlling the pipeline produces some overhead. also, routing a header usually takes longer than transmitting a data ﬂit
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
the speciﬁcation should permit support for a heterogeneous environment. the semantics should be language independent. the user should be relieved from the responsibility for reliable communication (i.e., checking or creating acknowledgments). usage should not require much deviation from well-understood current practice. the interface design should be supportive of thread safety.
the standard remains focused on the logical aspects of message passing and avoids specifying or addressing system-speciﬁc operations such as i/o operations, task management, and other operating-system-speciﬁc features. the standard also avoids explicitly addressing shared-memory operations, although mpi can be (and has been) implemented on shared-memory machines. some of these (non-mpi) features could conceivably be offered as extensions with a speciﬁc vendor implementation. there are several excellent texts and publications that describe in some detail the rationale and speciﬁcation of the mpi standard [138, 325], as well as tutorial descriptions on the use of mpi [138]. there is also a continually growing set of web pages that provides information on the standard, updates of evolving efforts, and links to a variety of tutorials, texts, and papers. an excellent place to start is at [243]. the purpose of this chapter is to merely highlight major features of the standard and, where possible, relate the interface features to the implementation of the messaging layer. as a result, we brieﬂy describe some of the novel features of mpi. this includes general concepts, the structure of point-to-point communication, support for collective communication, and the availability of virtual process topologies.
an mpi program consists of autonomous processes, executing their own code in either the mimd or spmd programming paradigms. each process usually executes in its own address space and communicates via mpi communication primitives. the basic communication mechanism within mpi is point-to-point communication between pairs of processes. the model assumes the use of a reliable user-level message transmission protocol. furthermore, messages are nonovertaking. the implication of these semantics is that if single-threaded programs are written with point-to-point message-passing calls with source and destination process addresses explicitly speciﬁed, program execution is deterministic.
the community experience with the development of many successful messagepassing library packages led to the precise deﬁnition and adoption of several logical concepts as an aid in thinking about and writing correct, efﬁcient, parallel programs. at the highest level mpi speciﬁes the notion of a communication domain. this domain is a set of n processes, which are implementation-dependent objects. from the programmer’s perspective, each process is assigned a rank in this group between 0 . . . (n − 1). messagepassing programs are written to send messages to processes identiﬁed by their rank. the group of processes is identiﬁed by an object called a communicator. each process
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
∗ complementary channel (vd i ) and is referred to as a virtual channel trio. the router header will traverse vc i . the complementary ∗ i is reserved for use by acknowledgment ﬂits and backtracking header ﬂits. the channel v complementary channel of a trio traverses the physical channel in the direction opposite to that of its associated data channel. the channel model is illustrated in figure 2.22. there are two virtual channels vi(vr) and vj (vs) from r1 (r2) to r2 (r1). only one message can be in progress over a given data channel. therefore, compared to existing channel models, this model requires exactly two extra ﬂit buffers for each data channel—one each for the corresponding channel and the complementary channel, respectively. since control ﬂit trafﬁc is a small percentage of the overall ﬂit trafﬁc, in practice all control channels across a physical link are multiplexed through a single virtual control channel as shown in figure 2.22(b). thus, compared to the more common use of virtual channels, this model requires one extra virtual channel in each direction between a pair of adjacent routers. ∗ for example, channel c1 in figure 2.22(b) corresponds to ﬂit buffers vc s in figure 2.22(a). the implementation of pcs in the ariadne router [7] utilized two data channels and one virtual control channel over each physical link.
this separation of control trafﬁc and data trafﬁc is useful in developing fault-tolerant routing and distributed fault recovery mechanisms. such mechanisms are discussed in greater detail in chapter 6. the ariadne router [7] is a single-chip pcs router with two virtual channel trios per physical channel. the prototype router had byte-wide physical channels and 8-bit ﬂits. the format of the header ﬂit is shown in figure 2.23. in this design a single bit distinguished a control ﬂit from a data ﬂit (this only left 7-bit data ﬂits!). a single bit distinguishes between backtracking ﬂits and ﬂits making forward progress. the misroute ﬁeld keeps track of the number of misrouting steps the header has taken. the maximum number of misroutes that the header can take in this design is three. finally, two ﬁelds provide x and y offsets for routing in a 2-d mesh.
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
consider the use of wormhole switching when the number of ﬂits in the message exceeds the number of links, d, between the source and destination nodes. if each router only buffers a single ﬂit, receipt of the header at the destination can be asserted at the source when ﬂit d + 1 is injected into the network. if messages are short, they can be padded with empty ﬂits so that the number of ﬂits in the message exceeds the distance between the source and destination (padding must also account for buffer space within each router). by keeping track of the number of ﬂits that have been injected into the network, the source router can determine if the header has been received at the destination. moreover, the source node can determine if the whole message has been received at the destination by injecting d + 1 padding ﬂits after the last data ﬂit of the message. such reliable switching techniques modify the basic wormhole-switching mechanisms to include additional ﬂits or control signals (e.g., acknowledgments or padding ﬂits). this particular technique was proposed as compressionless routing by its developers [179].
the need to support distinct trafﬁc types also leads to new optimizations of switching techniques [294]. real-time communication trafﬁc places distinct demands on the performance and behavior of network routers. such trafﬁc requires guaranteed bounds on latency and throughput. the manner in which messages are buffered and scheduled across physical channels must be able to provide such guarantees on a per-router basis. such guarantees would be used by higher-level, real-time scheduling algorithms. packet switching is attractive from this point of view since predictable demands are made on buffer requirements and channel bandwidth at each intermediate router. in contrast, the demands that will be placed on router resources by a message using vct will vary depending on the load and communication pattern (i.e., is the output link free). buffering of packets permits the application of priority-based scheduling algorithms and thus provides some control over packet latencies. wormhole-switched messages use demand-driven scheduling disciplines for accessing physical channel bandwidth and may be blocked across multiple nodes. demand-driven scheduling and very low buffer requirements work to provide low average latency but high variability and thus poor predictability. priority-based scheduling of virtual channels is infeasible since a channel may have messages of multiple priorities, and messages may be blocked over multiple links. these properties make it difﬁcult to utilize wormhole switching to support real-time trafﬁc. rexford and shin [294] observed that packet switching and wormhole switching made demands on distinct router resources while sharing physical channel bandwidth. thus, the authors proposed a scheme where virtual channels were partitioned to realize two distinct virtual networks: one packet switched and the other wormhole switched. the two networks share the physical link bandwidth in a controlled manner, thus enabling the network to provide latency and throughput guarantees for real-time trafﬁc, while standard trafﬁc realized low average latency. the switching technique experienced by a message is determined at the source node, based on the trafﬁc type.
we can envision other optimizations that deal with issues such as scheduling of virtual channels (equivalently messages) over the physical channel, allocation/deallocation of buffer space within routers, allocation/deallocation of virtual channels, and so on. some of these optimizations are examined in greater detail in the exercises at the end of this chapter.
of the packets to break the deadlock. however, because heuristic detection mechanisms operate locally and in parallel, several nodes may detect deadlock concurrently and release several buffers in the same deadlocked conﬁguration. in the worst case, it may happen that all the packets involved in a deadlock release the buffers they occupy. these overheads suggest that deadlock-recovery-based routing can beneﬁt from highly selective deadlock detection mechanisms. for instance, turn selection criteria could also be enforced to limit a packet’s eligibility to use recovery resources [279].
the most important limitation of heuristic deadlock detection mechanisms arises when packets have different lengths. the optimal value of the timeout for deadlock detection heavily depends on packet length unless some type of physical channel monitoring of neighboring nodes is implemented. when a packet is blocked waiting for channels occupied by long packets, the selected value for the timeout should be high in order to minimize false deadlock detection. as a consequence, deadlocked packets have to wait for a long time until deadlock is detected. in these situations, latency becomes much less predictable. the poor behavior of current deadlock detection mechanisms considerably limits the practical applicability of deadlock recovery techniques. some current research efforts aim at improving deadlock detection techniques [226].
once deadlocks are detected and packets made eligible to recover, there are several alternative actions that can be taken to release the buffer resources occupied by deadlocked packets. deadlock recovery techniques can be classiﬁed as progressive or regressive. progressive techniques deallocate resources from other (normal) packets and reassign them to deadlocked packets for quick delivery. regressive techniques deallocate resources from deadlocked packets, usually killing them (abort-and-retry). the set of possible actions taken depends on where deadlocks are detected.
if deadlocks are detected at the source node, regressive deadlock recovery is usually used.a packet can be killed by sending a control signal that releases buffers and propagates along the path reserved by the header. this is the solution proposed in compressionless routing [179]. after a random delay, the packet is injected again into the network. this reinjection requires a packet buffer associated with each injection port. note that a packet that is not really deadlocked may resume advancement and even start delivering ﬂits at the destination after the source node presumes it is deadlocked. thus, this situation also requires a packet buffer associated with each delivery port to store fragments of packets that should be killed if a kill signal reaches the destination node. if the entire packet is consumed without receiving a kill signal, it is delivered. obviously, this use of packet buffers associated with ports restricts packet size.
if deadlocks are detected at an intermediate node containing the header, then both regressive and progressive deadlock recovery are possible. in regressive recovery, a deadlocked packet can be killed by propagating a backward control signal that releases buffers from the node containing the header back to the source by following the path reserved by the packet in the opposite direction. instead of killing a deadlocked packet,
5. if a forward release ﬂit collides with a setup acknowledgment, the acknowledgment is removed from the network and the release ﬂit is allowed to continue along the path toward the destination.
release ﬂits are injected into the network only on the occurrence of faults. since faults are a dynamic phenomenon, they may occur at any time during path setup, message transmission, or message acknowledgment. the interaction of control ﬂits produces the actions deﬁned above to ensure proper recovery. in the fault-free case each virtual circuit contains at most one control ﬂit at a time. therefore, it is necessary to add additional buffer space for one reverse and one forward release ﬂit per virtual control channel. since nonheader control ﬂits are allowed to pass blocked headers, release ﬂits will not wait and therefore cannot create chains of blocked ﬂits as shown in figure 6.1. header ﬂits using backtracking pcs algorithms do not block on faults. thus, no ﬂit in the network blocks indeﬁnitely on a fault. as a result, release ﬂits cannot induce deadlock in the presence of faults, and the existing routing algorithm is deadlock-free.
as long as a path exists between the source and the destination, if a fault interrupts the message, both the source and the destination will be notiﬁed. however, it is possible that an inconsistent state can develop between a source and a destination where the destination has received the message intact, but the source believes the message was lost and must retransmit. this situation develops when a dynamic fault occurs in the virtual circuit after the last ﬂit is delivered, but before the ﬁnal message acknowledgment reaches the source. this situation can be remedied at the operating system level by assigning identiﬁcation tags to messages. if the source is prevented from sending the next message until the previous message has been successfully received, the destination can detect and discard duplicate messages relatively easily.
the above approach requires the use of virtual channels to support recovery trafﬁc. the use of virtual channels can complicate routing decisions and channel control, leading to an increase in the ﬂow control latency through the router and across the channel. fault-tolerant implementations of compressionless routing have been proposed [179], motivated by a desire to simplify router design to enable adaptive routing and fault recovery with speeds comparable to oblivious routers. compressionless routing exploits the small amount of buffering within the network routers to be able to indirectly determine when the header of a message has reached the destination. for example, assume that each router can only buffer two ﬂits at the input and output of a router. if a message is to traverse three routers to the destination, once the 12th ﬂit has been injected into the network the source node can assert that the header has been received into the destination node queue. figure 6.41 illustrates this case, with the exception that the message is actually comprised of 10 ﬂits. when the last data ﬂit is injected into the network, the header will not have reached the destination node. therefore, the message is padded with pad ﬂits. the number of pad ﬂits that must be added is a function of the distance to the destination and the message size.
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
as we might expect, solutions to these problems are dependent upon the type and pattern of the faulty components and the network topology. the next section shows how direct networks possess natural redundancy that can be exploited, by deﬁning the redundancy level of the routing function. the remaining sections will demonstrate how the issues mentioned above are addressed in the context of distinct switching techniques.
this section presents a theoretical basis to answer a fundamental question regarding faulttolerant routing in direct networks: what is the maximum number of simultaneous faulty channels tolerated by the routing algorithm? this question has been analyzed in [95] by deﬁning the redundancy level of the routing function and proposing a necessary and sufﬁcient condition to compute its value.
if a routing function tolerates f faulty channels, it must remain connected and deadlock-free for any number of faulty channels less than or equal to f . note that, in addition to connectivity, deadlock freedom is also required. otherwise, the network could reach a deadlocked conﬁguration when some channels fail.
the following deﬁnitions are required to support the discussion of the behavior of fault-tolerant routing algorithms. in conjunction with the fault model, the following terminology can be used to discuss and compare fault-tolerant routing algorithms.
a network is said to be connected with respect to a routing algorithm if the routing function can route a message between any pair of nonfaulty routing nodes.
this deﬁnition is useful since a network may be connected in the graph-theoretical sense, where a physical path exists between a pair of nodes. however, routing restrictions may result in a routing function that precludes selection of links along that path.
this deﬁnition indicates the conditions to support the failure of a given channel. fault-tolerant routing algorithms should be designed in such a way that all the channels are redundant, thus avoiding a single point of failure. however, even in this case, we cannot guarantee that the network will support two simultaneous faults. this issue is addressed by the following deﬁnitions.
a routing algorithm is said to be f fault tolerant if, for any f failed components in the network, the routing function is still connected and deadlock-free.
no packet has already arrived at its destination node. packets cannot advance because the queues for all the alternative output channels supplied by the routing function are full.
there is no packet whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (remember, we make the assumption that a queue cannot contain ﬂits belonging to different packets). data ﬂits cannot advance because the next channel reserved by their packet header has a full queue. note that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
in some cases, a conﬁguration cannot be reached by routing packets starting from an empty network. this situation arises when two or more packets require the use of the same channel at the same time to reach the conﬁguration. a conﬁguration that can be reached by routing packets starting from an empty network is reachable or routable [63]. it should be noted that by deﬁning the domain of the routing function as n × n, every legal conﬁguration is also reachable. effectively, as the routing function has no memory of the path followed by each packet, we can consider that, for any legal conﬁguration, a packet stored in a channel queue was generated by the source node of that channel. in wormhole switching, we can consider that the packet was generated by the source node of the channel containing the last ﬂit of the packet. this is important because when all the legal conﬁgurations are reachable, we do not need to consider the dynamic evolution of the network leading to those conﬁgurations. we can simply consider legal conﬁgurations, regardless of the packet injection sequence required to reach them. when all the legal conﬁgurations are reachable, a routing function is deadlock-free if and only if there is not any deadlocked conﬁguration for that routing function.
a routing function r is connected if it is able to establish a path between every pair of nodes x and y using channels belonging to the sets supplied by r. it is obvious that a routing function must be connected, and most authors implicitly assume this property. however, we mention it explicitly because we will use a restricted routing function to prove deadlock freedom, and restricting a routing function may disconnect it.
the theoretical model of deadlock avoidance we are going to present relies on the concept of channel dependency [77]. other approaches are possible. they will be brieﬂy described in section 3.3. when a packet is holding a channel, and then it requests the use of another channel, there is a dependency between those channels. both channels are in one of the
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
there are many ways to interconnect adjacent stages. figure 1.11 shows a generalized multistage interconnection network with n inputs and m outputs. it has g stages, g0 to gg−1. as shown in figure 1.12, each stage, say, gi, has wi switches of size ai,j × bi,j , where 1 ≤ j ≤ wi. thus, stage gi has pi inputs and qi outputs, where
the connection between two adjacent stages, gi−1 and gi, denoted ci, deﬁnes the connection pattern for pi = qi−1 links, where p0 = n and qg−1 = m. a min thus can be represented as
a connection pattern ci (pi ) deﬁnes how those pi links should be connected between the qi−1 = pi outputs from stage gi−1 and the pi inputs to stage gi. different connection patterns give different characteristics and topological properties of mins. the links are labeled from 0 to pi − 1 at ci.
in practice, all the switches will be identical, thus amortizing the design cost. banyan networks are a class of mins with the property that there is a unique path between any pair of source and destination [132]. an n-node (n = kn) delta network is a subclass of banyan networks, which is constructed from identical k × k switches in n stages, where each stage contains n k switches. many of the known mins, such as omega, ﬂip, cube, butterﬂy, and baseline, belong to the class of delta networks [273] and have been shown
in this chapter we study routing algorithms. routing algorithms establish the path followed by each message or packet. the list of routing algorithms proposed in the literature is almost endless. we clearly cannot do justice to all of these algorithms developed to meet many distinct requirements. we will focus on a representative set of approaches, being biased toward those being used or proposed in modern and future multiprocessor interconnects. thus, we hope to equip you with an understanding of the basic principles that can be used to study the spectrum of existing algorithms. routing algorithms for wormhole switching are also valid for other switching techniques. thus, unless explicitly stated, the routing algorithms presented in this chapter are valid for all the switching techniques. speciﬁc proposals for some switching techniques will also be presented. special emphasis is given to design methodologies because they provide a simple and structured way to design a wide variety of routing algorithms for different topologies.
deadlock and livelock freedom. ability to guarantee that packets will not block or wander across the network forever. this issue was discussed in depth in chapter 3.
fault tolerance. ability to route packets in the presence of faulty components. although it seems that fault tolerance implies adaptivity, this is not necessarily true. fault tolerance can be achieved without adaptivity by routing a packet in two or more phases, storing it in some intermediate nodes. fault tolerance also requires some additional hardware mechanisms, as will be detailed in chapter 6.
the evolution of switching techniques was naturally inﬂuenced by the need for better performance. vct switching introduced pipelined message transmission, and wormhole switching further contributed reduced buffer requirements in conjunction with ﬁne-grained pipelining. the mad postman switching technique carried pipelining to the bit level to maximize performance. in packet switching and vct, messages are completely buffered at a node. as a result, the messages consume network bandwidth proportional to the network load. on the other hand, wormhole-switched messages may block occupying buffers and channels across multiple routers, precluding access to the network bandwidth by other messages. thus, while average message latency can be low, the network saturates at a fraction of the maximum available bandwidth, and the variance of message latency can be high. the use of virtual channels decouples the physical channel from blocked messages, thus reducing the blocking delays experienced by messages and enabling a larger fraction of the available bandwidth to be utilized. however, the increasing multiplexing of multiple messages increases the delay experienced by data ﬂits. furthermore, multiple virtual channels can increase the ﬂow control latency through the router and across the physical channel, producing upward pressure on average message latency.
the effects of wormhole switching on individual messages can be highly unpredictable. since buffer requirements are low, contention in the network can substantially increase the latency of a message in parts of the network. packet switching tends to have more predictable latency characteristics, particularly at low loads since messages are buffered at each node. vct operates like wormhole switching at low loads and approximates packet switching at high loads where link contention forces packets to be buffered at each node. thus, at low loads we expect to see wormhole-switching techniques providing superior latency/throughput relative to packet-switched networks, while at high loads we expect to see packet-switched schemes perform better. as expected, the performance of vct approaches that of wormhole switching at low loads and that of packet switching at high loads. more detailed performance comparisons can be found in chapter 9.
these switching techniques can be characterized as optimistic in the sense that buffer resources and links are allocated as soon as they become available, regardless of the state of progress of the remainder of the message. in contrast, pipelined circuit switching and scouting switching may be characterized as conservative. data ﬂits are transmitted only after it is clear that ﬂits can make forward progress. these ﬂow control protocols are motivated by fault tolerance concerns. bws seeks to improve the fraction of available bandwidth that can be exploited by wormhole switching by buffering groups of ﬂits.
in packet switching, error detection and retransmission can be performed on a linkby-link basis. packets may be adaptively routed around faulty regions of the network. when messages are pipelined over several links, error recovery and control becomes complicated. error detection and retransmission (if feasible) must be performed by higherlevel protocols operating between the source and destination, rather than at the level of the physical link. if network routers or links have failed, message progress can be indeﬁnitely halted, with messages occupying buffer and channel resources. this can lead to deadlocked conﬁgurations of messages and eventually failure of the network.
some node architectures (e.g., intel paragon) make use of a coprocessor to execute all message-passing functions. the interaction of the coprocessor with the compute processor may be interrupt driven or polled. this permits signiﬁcant overlap between the message processing and computation; however, it may do little for latency. since the message processor controls the network interface, similar considerations about protected communications arise when seeking to run message handlers in user context rather than the kernel on the coprocessor.
buffering policies are extremely important to the design of the message layer. they are crucial to both correctness as well as performance. network routing protocols remain deadlock-free under the consumption assumption: all messages destined for a node are eventually consumed. deadlock freedom proofs that rely on this assumption (as do virtually all such proofs) are based on memory of inﬁnite extent. in reality, memory is necessarily limited, and therefore some ﬂow control between senders and receivers is necessary to ensure that a ﬁxed amount of storage can be allocated, deallocated, and reused over large numbers of messages while avoiding the loss of messages. credit-based ﬂow control, windowing schemes, and so on are examples of techniques for managing a limited amount of memory.
to ensure the availability of buffer space, the intel ipsc/2 machines employed a three-trip protocol for sending long messages (> 100 bytes). an initial request message is transmitted to the receiver to allocate buffer space. on receiving a reply message, the message can be transmitted. various token protocols can be employed where each node has a number of tokens corresponding to buffers. to send a message a processor must have a token from the destination. tokens can be returned by piggybacking on other messages. the fast message library [264] uses a return-to-sender optimistic ﬂow control protocol for buffer management. packets are optimistically transmitted after allocating buffer space at the source for the packet. if the packet cannot be received due to the lack of buffer space, it is returned to the sender where it can be retransmitted, and buffer space is guaranteed to be available to receive the rejected packet. if the packet is successfully delivered, acknowledgments are used to free buffer space at the source. this scheme has the advantage of requiring buffer space proportional to the number of outstanding message packets rather than having to preallocate space proportional to the number of nodes.
the overhead of buffer management can be measured as the time spent in the message handlers to acquire and release buffers, performing status updates such as marking buffers as allocated and free, and updating data structures that track available buffers. generally memory is statically allocated to keep these management costs down.
in addition to the above functions, interprocessor communication is often expected to preserve other properties that may require additional functionality within the message layer. a detailed study of the source of software overheads in the message layer by karamcheti and chien [171] identiﬁed several functions implemented in the messaging layer to provide services that are not provided by the network, but are expected by the user-
memory (copying and buffering) prior to injection into the network while the send() call returns to the main program. until recently, network interfaces were largely treated as i/o devices. traditionally, drivers that control such devices (interface control) were privileged and were available only through a system call (user/kernel transitions). more recently, the high overhead of such an approach has evolved into more efﬁcient schemes for transferring control to message handlers that execute in the user address space. once the network interface has the message packet, it is injected into the network, where the routers cooperate in delivering the message to the destination node interface. when the message is received at the node, there must be some way to invoke the messaging layer software (e.g., interrupts, polled access, etc.). similar device driver services may then be invoked to transfer the message from the network interface into temporary system buffers (copied later into the user buffers) or transmitted directly to user buffers. message transfer is now complete.
we can view each of the above functions as essential to the process of transmitting messages. collectively they determine the minimum latency of a message, and the slowest component (invariably a software component) determines the maximum bandwidth. consider the issues in each of these steps.
computation of the message headers, sequence numbers, parity, crc, checksums, and so on are overhead operations. when these operations are implemented in software they can exact signiﬁcant performance penalties. most interfaces now implement the majority, if not all, of packetization functions in hardware. for example, in the cray t3d, the interface hardware performs a table lookup to generate the routing tag [259], while it is possible to compute the crc while copying data between buffers or during a dma transfer [30]. packetization overheads have been largely minimized in modern machines.
as an i/o device, control of the network interface can take one of many forms. data may be transferred to/from the interface using direct memory access (dma). in this case the software must initialize the appropriate dma channel and initiate dma access. this can be as simple as requiring two instructions: one to load the starting address and one to load the counter ([105] for the ncube-2). if dma is only allowed into certain portions of memory, then interaction with the operating system may be required. pages used as targets of dma or used to hold network interface data structures such as queues should be pinned down to prevent them from being swapped out.
alternatively the interface may be memory mapped and accessible from user space. the messaging software may initialize the interface with stores to memory locations corresponding to control registers. message packets may be similarly transferred to interface memory. when a message is received, the messaging software may be invoked via
in many environments, rather than minimizing message latency or maximizing network throughput, the overriding issue is the ability to tolerate the failure of network components such as routers and links. in wormhole switching, header ﬂits containing routing information establish a path through the network from source to destination. data ﬂits are pipelined through the path immediately following the header ﬂits. if the header cannot progress due to a faulty component, the message is blocked in place indeﬁnitely, holding buffer resources and blocking other messages. this situation can eventually result in a deadlocked conﬁguration of messages. while techniques such as adaptive routing can alleviate the problem, it cannot by itself solve the problem. this has motivated the development of different switching techniques.
pipelined circuit switching (pcs) combines aspects of circuit switching and wormhole switching. pcs sets up a path before starting data transmission as in circuit switching. basically, pcs differs from circuit switching in that paths are formed by virtual channels instead of physical channels. in pipelined circuit switching, data ﬂits do not immediately follow the header ﬂits into the network as in wormhole switching. consequently, increased ﬂexibility is available in routing the header ﬂit. for example, rather than blocking on a faulty output channel at an intermediate router, the header may backtrack to the preceding router and release the previously reserved channel. a new output channel may now be attempted at the preceding router in ﬁnding an alternative path to the destination. when the header ﬁnally reaches the destination node, an acknowledgment ﬂit is transmitted back to the source node. now data ﬂits can be pipelined over the path just as in wormhole switching. the resilience to component failures is obtained at the expense of larger path setup times. this approach is ﬂexible in that headers can perform a backtracking search of the network, reserving and releasing virtual channels in an attempt to establish a fault-free path to the destination. this technique combines message pipelining from wormhole switching with a more conservative path setup algorithm based on circuit-switching techniques. a timespace diagram of a pcs message transmission over three links in the absence of any trafﬁc or failures is shown in figure 2.21.
since headers do not block holding channel or buffer resources, routing restrictions are not necessary to avoid deadlock. this increases the probability of ﬁnding a path while still avoiding deadlocked conﬁgurations of messages. moreover, reservation of virtual channels by the header does not by itself lead to use of physical channel bandwidth. therefore, unlike circuit switching, path setup does not lead to excessive blocking of other messages. as a result, multipath networks in conjunction with the ﬂexibility of pcs are good candidates for providing low-latency, fault-tolerant performance. for purely performance-driven applications where fault tolerance is not a primary concern, the added overhead of pcs makes wormhole switching the mechanism of choice.
in pcs, we distinguish between ﬂits that carry control information (e.g., header ﬂits and acknowledgment ﬂits) and those that carry data. this distinction is supported in the virtual channel model that separates control ﬂit trafﬁc and data ﬂit trafﬁc. a unidirectional is composed of a data channel, a corresponding channel, and a virtual channel vi
the cray t3d router the cray t3d utilizes a k-ary 3-cube interconnect. an example of a 4 × 4 × 2 conﬁguration of processing element (pe) nodes is illustrated in figure 7.17. each pe node consists of two dec alpha 150 mhz processors sharing a single network interface to the local router. the network has a maximum radix of 8 in the x and z dimensions and 16 in the y dimension. therefore, the maximum conﬁguration is 1,024 pe nodes or 2,048 processors. the system is packaged with two pe nodes per board. this choice of radix is not arbitrary. rather it is based on the observation that under random trafﬁc, messages will travel 1 4 the way around each dimension in the presence of bidirectional links. if we consider only the packets that travel in, say, the x+ direction, then the average distance traveled by these packets will be 1 8 the distance in that direction. if the injection and ejection ports of the
the t3e represents a second-generation multiprocessor system [310, 313]. the network and router architecture of the t3e retains many of the operational characteristics of the t3d but is very different in other ways. the system design evolved to substantially improve latency tolerance, resulting in a shift in emphasis in the design of the network from reducing latency to providing sustained bandwidth. this resulted in a change in the balance between processors and network bandwidth—only one processor/network node in the t3e—and the use of standard-cell cmos (single-chip) rather than ecl (three-chip) technology. along with a doubling of the link bandwidth, this produces a factor of 4 increase in the per-processor bandwidth. the topology is still that of a 3-d torus with a maximum radix of 8, 32, and 8 in the x, y , and z dimensions, respectively. a block diagram of the t3e router is shown in figure 7.22.
a t3e packet is 1–10 ﬂits, and each ﬂit is 5 phits. the network physical channel is full duplex with 14-bit data, producing a ﬂit size of 70 bits. a ﬂit can carry one 64-bit word with some additional control information. the router operates on a 75 mhz clock. during
node. therefore, the distance between two nodes is the distance between the switches directly connected to those nodes plus two units. similarly, the diameter is the maximum distance between two switches connected to some node plus two units. it may be argued that it is not necessary to add two units because direct networks also have internal links between routers and processing nodes. however, those links are external in the case of indirect networks. this gives a consistent view of the diameter as the maximum number of external links between two processing nodes. in particular, the distance between two nodes connected through a single switch is two instead of zero.
similar to direct networks, an indirect network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the switches are interconnected by channels and can be modeled by a graph as indicated above. for indirect networks with n nodes, the ideal topology would connect those nodes through a single n × n switch. such a switch is known as a crossbar. although using a single n × n crossbar is much cheaper than using a fully connected direct network topology (requiring n routers, each one having an internal n × n crossbar), the cost is still prohibitive for large networks. similar to direct networks, the number of physical connections of a switch is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of crossbar networks for large network sizes. as a consequence, many alternative topologies have been proposed. in these topologies, messages may have to traverse several switches before reaching the destination node. in regular networks, these switches are usually identical and have been traditionally organized as a set of stages. each stage (but the input/output stages) is only connected to the previous and next stages using regular connection patterns. input/output stages are connected to the nodes as well as to another stage in the network. these networks are referred to as multistage interconnection networks and have different properties depending on the number of stages and how those stages are arranged.
the remaining issues discussed in section 1.6.1 (routing, switching, ﬂow control, buffer allocation, and their impact on performance) are also applicable to indirect networks.
crossbar networks allow any processor in the system to connect to any other processor or memory unit so that many processors can communicate simultaneously without contention. a new connection can be established at any time as long as the requested input and output ports are free. crossbar networks are used in the design of high-performance small-scale multiprocessors, in the design of routers for direct networks, and as basic components in the design of large-scale indirect networks. a crossbar can be deﬁned as a switching network with n inputs and m outputs, which allows up to min{n, m} one-toone interconnections without contention. figure 1.9 shows an n × m crossbar network. usually, m = n except for crossbars connecting processors and memory modules.
the cost of such a network is o(n m), which is prohibitively high with large n and m. crossbar networks have been traditionally used in small-scale shared-memory multiprocessors, where all processors are allowed to access memories simultaneously as long as each processor reads from, or writes to, a different memory. when two or more processors contend for the same memory module, arbitration lets one processor proceed while the others wait. the arbiter in a crossbar is distributed among all the switch points connected to the same output. however, the arbitration scheme can be less complex than the one for a bus because conﬂicts in crossbar are the exception rather than the rule, and therefore easier to resolve.
for a crossbar network with distributed control, each switch point may have four states, as shown in figure 1.10. in figure 1.10(a), the input from the row containing the switch point has been granted access to the corresponding output, while inputs from upper rows requesting the same output are blocked. in figure 1.10(b), an input from an upper row has been granted access to the output. the input from the row containing the switch point does not request that output and can be propagated to other switches. in figure 1.10(c), an input from an upper row has also been granted access to the output. however, the input from the row containing the switch point also requests that output and is blocked. the
architectures and emerging switched networks for workstation clusters utilize some form of cut-through switching or some variant of it (e.g., vct switching, wormhole switching, buffered wormhole switching, etc.). therefore, this chapter largely focuses on a discussion of issues and designs of such routers. while the router implementations for wormhole and vct switching differ in many of the architectural trade-offs, they share many common features derived from the use of some form of cut-through switching. the common issues and features facing the design of routers can be categorized as intrarouter or interrouter, and are discussed below. this discussion is followed by descriptions of recent router designs, emphasizing their unique features and reinforcing the commonly held trade-offs.
in an effort to capture the commonalities and enable quantiﬁable comparisons, chien [57] developed an abstract model for the architecture of routers in wormhole-switched k-ary n-cubes. this model is largely concentrated on the intrarouter architecture, while the performance of interrouter link operation is very sensitive to packaging implementations. the basic wormhole router functions can be captured in an abstract router architecture as shown in figure 7.7. we are interested in the implementation complexity of each of the components in the ﬁgure.
crossbar switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
the most important drawback of trees as general-purpose interconnection networks is that the root node and the nodes close to it become a bottleneck. additionally, there are no alternative paths between any pair of nodes. the bottleneck can be removed by allocating a higher channel bandwidth to channels located close to the root node. the shorter the distance to the root node, the higher the channel bandwidth. however, using channels with different bandwidths is not practical, especially when message transmission is pipelined. a practical way to implement trees with higher channel bandwidth in the vicinity of the root node (fat trees) will be described in section 1.7.5.
one of the most interesting properties of trees is that, for any connected graph, it is possible to deﬁne a tree that spans the complete graph.as a consequence, for any connected network, it is possible to build an acyclic network connecting all the nodes by removing some links. this property can be used to deﬁne a routing algorithm for any irregular topology. however, that routing algorithm may be inefﬁcient due to the concentration of trafﬁc across the root node. a possible way to circumvent that limitation will be presented in section 4.9.
some topologies have been proposed with the purpose of reducing node degree while keeping the diameter small. most of these topologies can be viewed as a hierarchy of topologies. this is the case for the cube-connected cycles [284]. this topology can be considered as an n-dimensional hypercube of virtual nodes, where each virtual node is a ring with n nodes, for a total of n2n nodes. each node in the ring is connected to a single dimension of the hypercube. therefore, node degree is ﬁxed and equal to three: two links connecting to neighbors in the ring, and one link connecting to a node in another ring through one of the dimensions of the hypercube. however, the diameter is of the same order of magnitude as that of a hypercube of similar size. figure 1.7(a) shows a 24-node cube-connected cycles network. it is worth noting that cube-connected cycles are weakly orthogonal because the ring is a one-dimensional network, and displacement inside the ring does not change the position in the other dimensions. similarly, a displacement along a hypercube dimension does not affect the position in the ring. however, it is not possible to cross every dimension from each node.
many topologies have been proposed with the purpose of minimizing the network diameter for a given number of nodes and node degree. two well-known topologies proposed with this purpose are the de bruijn network and the star graphs. in the de bruijn network [301] there are d n nodes, and each node is represented by a set of n digits in base d. a node (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ d − 1 for 0 ≤ i ≤ n − 1, is connected to nodes (xn−2, . . . , x1, x0, p) and (p, xn−1, xn−2, . . . , x1), for all p such that 0 ≤ p ≤ d − 1. in other words, two nodes are connected if the representation of one node is a right or left shift of the representation of the other. figure 1.7(b) shows an eight-node de bruijn network. when networks are very large, this network topology achieves a very low diameter for a given number of nodes and node degree. however, routing is complex. additionally, the average distance between nodes is high, close to the network diameter. finally, some nodes have links connecting to themselves. all of these issues make the practical application of these networks very difﬁcult.
a star graph [6] can be informally described as follows. the vertices of the graph are labeled by permutations of n different symbols, usually denoted as 1 to n. a permutation
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
in many environments, rather than minimizing message latency or maximizing network throughput, the overriding issue is the ability to tolerate the failure of network components such as routers and links. in wormhole switching, header ﬂits containing routing information establish a path through the network from source to destination. data ﬂits are pipelined through the path immediately following the header ﬂits. if the header cannot progress due to a faulty component, the message is blocked in place indeﬁnitely, holding buffer resources and blocking other messages. this situation can eventually result in a deadlocked conﬁguration of messages. while techniques such as adaptive routing can alleviate the problem, it cannot by itself solve the problem. this has motivated the development of different switching techniques.
pipelined circuit switching (pcs) combines aspects of circuit switching and wormhole switching. pcs sets up a path before starting data transmission as in circuit switching. basically, pcs differs from circuit switching in that paths are formed by virtual channels instead of physical channels. in pipelined circuit switching, data ﬂits do not immediately follow the header ﬂits into the network as in wormhole switching. consequently, increased ﬂexibility is available in routing the header ﬂit. for example, rather than blocking on a faulty output channel at an intermediate router, the header may backtrack to the preceding router and release the previously reserved channel. a new output channel may now be attempted at the preceding router in ﬁnding an alternative path to the destination. when the header ﬁnally reaches the destination node, an acknowledgment ﬂit is transmitted back to the source node. now data ﬂits can be pipelined over the path just as in wormhole switching. the resilience to component failures is obtained at the expense of larger path setup times. this approach is ﬂexible in that headers can perform a backtracking search of the network, reserving and releasing virtual channels in an attempt to establish a fault-free path to the destination. this technique combines message pipelining from wormhole switching with a more conservative path setup algorithm based on circuit-switching techniques. a timespace diagram of a pcs message transmission over three links in the absence of any trafﬁc or failures is shown in figure 2.21.
since headers do not block holding channel or buffer resources, routing restrictions are not necessary to avoid deadlock. this increases the probability of ﬁnding a path while still avoiding deadlocked conﬁgurations of messages. moreover, reservation of virtual channels by the header does not by itself lead to use of physical channel bandwidth. therefore, unlike circuit switching, path setup does not lead to excessive blocking of other messages. as a result, multipath networks in conjunction with the ﬂexibility of pcs are good candidates for providing low-latency, fault-tolerant performance. for purely performance-driven applications where fault tolerance is not a primary concern, the added overhead of pcs makes wormhole switching the mechanism of choice.
in pcs, we distinguish between ﬂits that carry control information (e.g., header ﬂits and acknowledgment ﬂits) and those that carry data. this distinction is supported in the virtual channel model that separates control ﬂit trafﬁc and data ﬂit trafﬁc. a unidirectional is composed of a data channel, a corresponding channel, and a virtual channel vi
example, 16 nodes can be allocated as an 8 × 2 × 1 topology or as a 4 × 2 × 2 topology. in the latter case the ﬁrst 2 bits of the virtual address are allocated to the x offset and the remaining 2 bits to the y and z offsets. when a message is transmitted, the operating system or hardware translates the virtual pe address to a logical pe address. this logical pe address is used as an index into a table that provides the offsets in each dimension to forward the message to the correct node. in this manner if a spare pe is mapped in to replace a faulty pe, the routing tables at each node simply need to be updated.
dimension-order routing prevents cyclic dependencies between dimensions. deadlock within a dimension is avoided by preventing cyclic channel dependencies as shown in figure 7.20. one channel is identiﬁed by the software as the dateline communication link. consider a request message that must traverse this dimension. if the message will traverse the dateline communication link, it will be forwarded along virtual channel 1; otherwise it will be forwarded along virtual channel 0. the message does not switch between virtual channels within a dimension. the dateline effectively identiﬁes the wraparound channel, and the routing restrictions implement the dally and seitz [77] restrictions to guarantee
the most important drawback of trees as general-purpose interconnection networks is that the root node and the nodes close to it become a bottleneck. additionally, there are no alternative paths between any pair of nodes. the bottleneck can be removed by allocating a higher channel bandwidth to channels located close to the root node. the shorter the distance to the root node, the higher the channel bandwidth. however, using channels with different bandwidths is not practical, especially when message transmission is pipelined. a practical way to implement trees with higher channel bandwidth in the vicinity of the root node (fat trees) will be described in section 1.7.5.
one of the most interesting properties of trees is that, for any connected graph, it is possible to deﬁne a tree that spans the complete graph.as a consequence, for any connected network, it is possible to build an acyclic network connecting all the nodes by removing some links. this property can be used to deﬁne a routing algorithm for any irregular topology. however, that routing algorithm may be inefﬁcient due to the concentration of trafﬁc across the root node. a possible way to circumvent that limitation will be presented in section 4.9.
some topologies have been proposed with the purpose of reducing node degree while keeping the diameter small. most of these topologies can be viewed as a hierarchy of topologies. this is the case for the cube-connected cycles [284]. this topology can be considered as an n-dimensional hypercube of virtual nodes, where each virtual node is a ring with n nodes, for a total of n2n nodes. each node in the ring is connected to a single dimension of the hypercube. therefore, node degree is ﬁxed and equal to three: two links connecting to neighbors in the ring, and one link connecting to a node in another ring through one of the dimensions of the hypercube. however, the diameter is of the same order of magnitude as that of a hypercube of similar size. figure 1.7(a) shows a 24-node cube-connected cycles network. it is worth noting that cube-connected cycles are weakly orthogonal because the ring is a one-dimensional network, and displacement inside the ring does not change the position in the other dimensions. similarly, a displacement along a hypercube dimension does not affect the position in the ring. however, it is not possible to cross every dimension from each node.
many topologies have been proposed with the purpose of minimizing the network diameter for a given number of nodes and node degree. two well-known topologies proposed with this purpose are the de bruijn network and the star graphs. in the de bruijn network [301] there are d n nodes, and each node is represented by a set of n digits in base d. a node (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ d − 1 for 0 ≤ i ≤ n − 1, is connected to nodes (xn−2, . . . , x1, x0, p) and (p, xn−1, xn−2, . . . , x1), for all p such that 0 ≤ p ≤ d − 1. in other words, two nodes are connected if the representation of one node is a right or left shift of the representation of the other. figure 1.7(b) shows an eight-node de bruijn network. when networks are very large, this network topology achieves a very low diameter for a given number of nodes and node degree. however, routing is complex. additionally, the average distance between nodes is high, close to the network diameter. finally, some nodes have links connecting to themselves. all of these issues make the practical application of these networks very difﬁcult.
a star graph [6] can be informally described as follows. the vertices of the graph are labeled by permutations of n different symbols, usually denoted as 1 to n. a permutation
however, if fault rings in tori overlap, then messages that use the wraparound links share virtual channels with messages that do not use the wraparound links. this sharing occurs over the physical channels corresponding to the overlapping region of the fault rings. the virtual networks are no longer independent, and cyclic dependencies can be created between the virtual networks and, as a result, among messages. it follows from the preceding discussion that four more virtual channels [37] can be introduced across each physical link, creating four additional networks to further separate the message trafﬁc over shared links. this is a rather expensive solution. the alternative is to have the network functioning as a mesh. in this case the beneﬁts of a toroidal connection are lost.
a different labeling procedure is used in [43] to permit misrouting around rectangular fault regions in mesh networks. initially fault regions are grown in a manner similar to the preceding schemes, and all nonfaulty nodes that are marked as faulty are labeled as deactivated. nonfaulty nodes on the boundary of the fault region are now labeled as unsafe. thus, all unsafe nodes are adjacent to at least one nonfaulty node. an example of faulty, unsafe, and deactivated nodes is shown in figure 6.23. there are three virtual channels traversing each physical channel. these virtual channels are partitioned into classes. nodes adjacent to only nonfaulty nodes have the virtual channels labeled as two class 1 channels and one class 2 channel. nodes adjacent to any other node type have the channels partitioned into class 2, class 3, and class 4 channels. in nonfaulty regions of the network, a message may traverse a class 1 channel along any shortest path to the destination. if no class 1 channel is available, class 2 channels are traversed in two phases. the ﬁrst phase permits dimension-order traversal of positive direction channels, and in the second phase negative direction class 2 channels can be traversed in any order (i.e., not in dimension order). the only time the fully adaptive variant of this algorithm must consider a fault region is when the last dimension to be traversed is blocked by a fault region necessitating nonminimal routing. dimension (i + 1) and dimension i channels are used to route the message around the fault region. messages ﬁrst attempt routing along a path using class 3 channels and class 2 channels in the positive direction. subsequently messages utilize class 4 channels and class 2 channels in the negative direction. note that class 3 and class 4 channels only exist across physical channels in the vicinity of faults. the routing restrictions prevent the occurrence of cyclic dependencies between the channels, avoiding deadlock.
is referred to as a dead address ﬂit. in a multidimensional network, each time a message changes to a new dimension, a dead ﬂit is generated and the message becomes smaller. at any point if a dead ﬂit is buffered (i.e., blocked by another message packet), it can be detected in the local router and removed. let us consider an example of routing in a 4 × 4, 2-d mesh. in this example the routing header is comprised of 2 ﬂits. each ﬂit is 3 bits long: a special start bit and 2 bits to identify the destination node in each dimension. the message is pipelined through the network at the bit level. each input and output buffer is 2 bits deep. consider the case where a message is being transmitted from node 20 to node 32. figure 2.16 illustrates the progress and location of the header ﬂits. the message is transmitted along dimension 0 to node 22, where it is transmitted along dimension 1 to node 32. at node 22, the ﬁrst ﬂit is pipelined through to the output as it is received. after receiving the third bit, it is determined the message must continue along dimension 1. the ﬁrst bit of the second header ﬂit is forwarded to the output in dimension 1 as shown in the ﬁgure. note that
the nodes of an interconnection network send and receive messages or packets through the network interface. both messages and packets carry information about the destination node. thus, the techniques described in this chapter can be applied to both of them indistinctly. without loss of generality, in what follows we will only refer to packets.
in direct networks, packets usually travel across several intermediate nodes before reaching the destination. in switch-based networks, packets usually traverse several switches before reaching the destination. however, it may happen that some packets are not able to reach their destinations, even if there exist fault-free paths connecting the source and destination nodes for every packet. assuming that the routing algorithm is able to use those paths, there are several situations that may prevent packet delivery. this chapter studies those situations and proposes techniques to guarantee packet delivery.
as seen in chapter 2, some buffers are required to store fragments of each packet, or even the whole packet, at each intermediate node or switch. however, buffer storage is not free. thus, buffer capacity is ﬁnite. as each packet whose header has not already arrived at its destination requests some buffers while keeping the buffers currently storing the packet, a deadlock may arise. a deadlock occurs when some packets cannot advance toward their destination because the buffers requested by them are full. all the packets involved in a deadlocked conﬁguration are blocked forever. note that a packet may be permanently blocked in the network because the destination node does not consume it. this kind of deadlock is produced by the application, and it is beyond the scope of this book. in this chapter, we will assume that packets are always consumed by the destination node in ﬁnite time. therefore, in a deadlocked conﬁguration, a set of packets is blocked forever. every packet is requesting resources held by other packet(s) while holding resources requested by other packet(s).
a different situation arises when some packets are not able to reach their destination, even if they never block permanently. a packet may be traveling around its destination node, never reaching it because the channels required to do so are occupied by other packets. this situation is known as livelock. it can only occur when packets are allowed to follow nonminimal paths.
finally, a packet may be permanently stopped if trafﬁc is intense and the resources requested by it are always granted to other packets also requesting them. this situation is
deadlock avoidance when deﬂection routing is used. in this case, routing is probabilistically livelock-free. this issue will be analyzed in section 3.7.
deadlock is by far the most difﬁcult problem to solve. this chapter is almost completely dedicated to this subject. there are three strategies for deadlock handling: deadlock prevention, deadlock avoidance, and deadlock recovery1 [321]. in deadlock prevention, resources (channels or buffers) are granted to a packet in such a way that a request never leads to a deadlock. it can be achieved by reserving all the required resources before starting packet transmission. this is the case for all the variants of circuit switching when backtracking is allowed. in deadlock avoidance, resources are requested as a packet advances through the network. however, a resource is granted to a packet only if the resulting global state is safe. this strategy should avoid sending additional packets to update the global state because these packets consume network bandwidth and they may contribute to produce deadlock. achieving this in a distributed manner is not an easy task. a common technique consists of establishing an ordering between resources and granting resources to each packet in decreasing order. in deadlock recovery strategies, resources are granted to a packet without any check. therefore, deadlock is possible and some detection mechanism must be provided. if a deadlock is detected, some resources are deallocated and granted to other packets. in order to deallocate resources, packets holding those resources are usually aborted.
deadlock prevention strategies are very conservative. however, reserving all the required resources before starting packet transmission may lead to a low resource utilization. deadlock avoidance strategies are less conservative, requesting resources when they are really needed to forward a packet. finally, deadlock recovery strategies are optimistic. they can only be used if deadlocks are rare and the result of a deadlock can be tolerated. deadlock avoidance and recovery techniques considerably evolved during the last few years, making obsolete most of the previous proposals. in this chapter we present a uniﬁed approach to deadlock avoidance for the most important ﬂow control techniques proposed up to now. with a simple trick, this technique is also valid for deadlock recovery. also, we will survey the most interesting deadlock-handling strategies proposed up to now. the techniques studied in this chapter are restricted to unicast routing in fault-free networks. deadlock handling in multicast routing and fault-tolerant routing will be studied in chapters 5 and 6, respectively.
this chapter is organized as follows. section 3.1 proposes a necessary and sufﬁcient condition for deadlock-free routing in direct networks, giving application examples for saf, vct, and wormhole switching. this theory is extended in section 3.2 by grouping channels into classes, extending the domain of the routing function, and considering central queues instead of edge buffers. alternative approaches for deadlock avoidance are considered in section 3.3. switch-based networks are considered in section 3.4. deadlock prevention and recovery are covered in sections 3.5 and 3.6, respectively. finally, livelock avoidance is studied in section 3.7. the chapter ends with a discussion of some engineering issues and commented references.
deadlock avoidance when deﬂection routing is used. in this case, routing is probabilistically livelock-free. this issue will be analyzed in section 3.7.
deadlock is by far the most difﬁcult problem to solve. this chapter is almost completely dedicated to this subject. there are three strategies for deadlock handling: deadlock prevention, deadlock avoidance, and deadlock recovery1 [321]. in deadlock prevention, resources (channels or buffers) are granted to a packet in such a way that a request never leads to a deadlock. it can be achieved by reserving all the required resources before starting packet transmission. this is the case for all the variants of circuit switching when backtracking is allowed. in deadlock avoidance, resources are requested as a packet advances through the network. however, a resource is granted to a packet only if the resulting global state is safe. this strategy should avoid sending additional packets to update the global state because these packets consume network bandwidth and they may contribute to produce deadlock. achieving this in a distributed manner is not an easy task. a common technique consists of establishing an ordering between resources and granting resources to each packet in decreasing order. in deadlock recovery strategies, resources are granted to a packet without any check. therefore, deadlock is possible and some detection mechanism must be provided. if a deadlock is detected, some resources are deallocated and granted to other packets. in order to deallocate resources, packets holding those resources are usually aborted.
deadlock prevention strategies are very conservative. however, reserving all the required resources before starting packet transmission may lead to a low resource utilization. deadlock avoidance strategies are less conservative, requesting resources when they are really needed to forward a packet. finally, deadlock recovery strategies are optimistic. they can only be used if deadlocks are rare and the result of a deadlock can be tolerated. deadlock avoidance and recovery techniques considerably evolved during the last few years, making obsolete most of the previous proposals. in this chapter we present a uniﬁed approach to deadlock avoidance for the most important ﬂow control techniques proposed up to now. with a simple trick, this technique is also valid for deadlock recovery. also, we will survey the most interesting deadlock-handling strategies proposed up to now. the techniques studied in this chapter are restricted to unicast routing in fault-free networks. deadlock handling in multicast routing and fault-tolerant routing will be studied in chapters 5 and 6, respectively.
this chapter is organized as follows. section 3.1 proposes a necessary and sufﬁcient condition for deadlock-free routing in direct networks, giving application examples for saf, vct, and wormhole switching. this theory is extended in section 3.2 by grouping channels into classes, extending the domain of the routing function, and considering central queues instead of edge buffers. alternative approaches for deadlock avoidance are considered in section 3.3. switch-based networks are considered in section 3.4. deadlock prevention and recovery are covered in sections 3.5 and 3.6, respectively. finally, livelock avoidance is studied in section 3.7. the chapter ends with a discussion of some engineering issues and commented references.
deadlock avoidance when deﬂection routing is used. in this case, routing is probabilistically livelock-free. this issue will be analyzed in section 3.7.
deadlock is by far the most difﬁcult problem to solve. this chapter is almost completely dedicated to this subject. there are three strategies for deadlock handling: deadlock prevention, deadlock avoidance, and deadlock recovery1 [321]. in deadlock prevention, resources (channels or buffers) are granted to a packet in such a way that a request never leads to a deadlock. it can be achieved by reserving all the required resources before starting packet transmission. this is the case for all the variants of circuit switching when backtracking is allowed. in deadlock avoidance, resources are requested as a packet advances through the network. however, a resource is granted to a packet only if the resulting global state is safe. this strategy should avoid sending additional packets to update the global state because these packets consume network bandwidth and they may contribute to produce deadlock. achieving this in a distributed manner is not an easy task. a common technique consists of establishing an ordering between resources and granting resources to each packet in decreasing order. in deadlock recovery strategies, resources are granted to a packet without any check. therefore, deadlock is possible and some detection mechanism must be provided. if a deadlock is detected, some resources are deallocated and granted to other packets. in order to deallocate resources, packets holding those resources are usually aborted.
deadlock prevention strategies are very conservative. however, reserving all the required resources before starting packet transmission may lead to a low resource utilization. deadlock avoidance strategies are less conservative, requesting resources when they are really needed to forward a packet. finally, deadlock recovery strategies are optimistic. they can only be used if deadlocks are rare and the result of a deadlock can be tolerated. deadlock avoidance and recovery techniques considerably evolved during the last few years, making obsolete most of the previous proposals. in this chapter we present a uniﬁed approach to deadlock avoidance for the most important ﬂow control techniques proposed up to now. with a simple trick, this technique is also valid for deadlock recovery. also, we will survey the most interesting deadlock-handling strategies proposed up to now. the techniques studied in this chapter are restricted to unicast routing in fault-free networks. deadlock handling in multicast routing and fault-tolerant routing will be studied in chapters 5 and 6, respectively.
this chapter is organized as follows. section 3.1 proposes a necessary and sufﬁcient condition for deadlock-free routing in direct networks, giving application examples for saf, vct, and wormhole switching. this theory is extended in section 3.2 by grouping channels into classes, extending the domain of the routing function, and considering central queues instead of edge buffers. alternative approaches for deadlock avoidance are considered in section 3.3. switch-based networks are considered in section 3.4. deadlock prevention and recovery are covered in sections 3.5 and 3.6, respectively. finally, livelock avoidance is studied in section 3.7. the chapter ends with a discussion of some engineering issues and commented references.
progressive recovery allows resources to be temporarily deallocated from normal packets and assigned to a deadlocked packet so that it can reach its destination. once the deadlocked packet is delivered, resources are reallocated to the preempted packets. this progressive deadlock recovery technique was ﬁrst proposed in [8, 9]. this technique is clearly more efﬁcient than regressive deadlock recovery. hence, we will study it in more detail.
figure 3.16 shows the router organization for the ﬁrst progressive recovery technique proposed, referred to as disha [8, 9, 10]. it is identical to the router model proposed in section 2.1 except that each router is equipped with an additional central buffer, or deadlock buffer. this resource can be accessed from all neighboring nodes by asserting a control signal (arbitration for it will be discussed below) and is used only in the case of suspected deadlock. systemwide, these buffers form what is collectively a deadlock-free recovery lane that can be visualized as a ﬂoating virtual channel shared by all physical dimensions of a router. on the event of a suspected deadlock, a packet is switched to the deadlock-free lane and routed adaptively along a path leading to its destination, where it is consumed to avert the deadlock. physical bandwidth is deallocated from normal packets and assigned to the packet being routed over the deadlock-free lane to allow swift recovery. the aforementioned control signal is asserted every time a deadlocked packet ﬂit uses physical channel bandwidth so that the ﬂit is assigned to the deadlock buffer at the next router. the mechanism that makes the recovery lane deadlock-free is speciﬁc to how recovery resources are allocated to suspected deadlocked packets.
output: selected output channel procedure: xoffset := xdest − xcurrent; yoffset := ydest − ycurrent; if xoffset < 0 and yoffset < 0 then channel := select(xa−, y a−, xb−, y b−); endif if xoffset < 0 and yoffset > 0 then channel := select(xa−, y a+, xb−, y b+); endif if xoffset < 0 and yoffset = 0 then channel := select(xa−, xb−); endif if xoffset > 0 and yoffset < 0 then channel := select(xa+, y a−, xb+, y b−); endif if xoffset > 0 and yoffset > 0 then channel := select(xa+, y a+, xb+, y b+); endif if xoffset > 0 and yoffset = 0 then channel := select(xa+, xb+); endif if xoffset = 0 and yoffset < 0 then channel := select(y a−, y b−); endif if xoffset = 0 and yoffset > 0 then channel := select(y a+, y b+); endif if xoffset = 0 and yoffset = 0 then channel := internal; endif
packet is eventually suspected of being deadlocked. once this determination is made, its eligibility to progressively recover using the central deadlock buffer recovery path is checked. as only one of the packets involved in a deadlock needs to be eliminated from the dependency cycle to break the deadlock, a packet either uses the recovery path (is eligible to recover) or will eventually use one of the normal edge virtual channel buffers for routing
in this chapter we study routing algorithms. routing algorithms establish the path followed by each message or packet. the list of routing algorithms proposed in the literature is almost endless. we clearly cannot do justice to all of these algorithms developed to meet many distinct requirements. we will focus on a representative set of approaches, being biased toward those being used or proposed in modern and future multiprocessor interconnects. thus, we hope to equip you with an understanding of the basic principles that can be used to study the spectrum of existing algorithms. routing algorithms for wormhole switching are also valid for other switching techniques. thus, unless explicitly stated, the routing algorithms presented in this chapter are valid for all the switching techniques. speciﬁc proposals for some switching techniques will also be presented. special emphasis is given to design methodologies because they provide a simple and structured way to design a wide variety of routing algorithms for different topologies.
deadlock and livelock freedom. ability to guarantee that packets will not block or wander across the network forever. this issue was discussed in depth in chapter 3.
fault tolerance. ability to route packets in the presence of faulty components. although it seems that fault tolerance implies adaptivity, this is not necessarily true. fault tolerance can be achieved without adaptivity by routing a packet in two or more phases, storing it in some intermediate nodes. fault tolerance also requires some additional hardware mechanisms, as will be detailed in chapter 6.
across a switch. finally, splitting messages into packets and reassembling them at the destination node also produces some overhead. these overheads can be amortized if packets are long enough. however, once packets are long enough to amortize the overhead, it is not convenient to increase packet size even more because blocking time for some packets will be high. on the other hand, switching techniques like vct may limit packet size, especially when packet buffers are implemented in hardware. finally, it should be noted that this parameter only makes sense when messages are long. if they are shorter than the optimal packet size, messages should not be split into packets. this is the case for dsms.
deadlock-handling technique. current deadlock avoidance techniques allow fully adaptive routing across physical channels. however, some buffer resources (usually some virtual channels) must be dedicated to avoid deadlock by providing escape paths to messages blocking cyclically. on the other hand, progressive deadlock recovery techniques require a minimum amount of dedicated hardware to deliver deadlocked packets. deadlock recovery techniques do not restrict routing at all and therefore allow the use of all the virtual channels to increase routing freedom, achieving the highest performance when packets are short. however, when packets are long or have very different lengths and the network approaches the saturation point, the small bandwidth offered by the recovery hardware may saturate. in this case, some deadlocked packets may have to wait for a long time, thus degrading performance and making latency less predictable. also, recovery techniques require efﬁcient deadlock detection mechanisms. currently available detection techniques only work efﬁciently when all the packets are short and have a similar length. otherwise, many false deadlocks are detected, quickly saturating the bandwidth of the recovery hardware. the poor behavior of current deadlock detection mechanisms considerably limits the practical applicability of deadlock recovery techniques unless all the packets are short. this may change when more accurate distributed deadlock detection mechanisms are developed.
routing algorithm. for regular topologies and uniform trafﬁc, the difference between deterministic and fully adaptive routing algorithms is small. however, for switch-based networks with irregular topology and uniform trafﬁc, adaptive routing algorithms considerably improve performance over deterministic or partially adaptive ones because the latter usually route many messages across nonminimal paths. moreover, for nonuniform trafﬁc patterns, adaptive routing considerably increases throughput over deterministic routing algorithms, regardless of network topology. on the other hand, adaptive routing does not reduce latency when trafﬁc is low to moderate because contention is small and base latency is the same for deterministic and fully adaptive routing, provided that both algorithms only use minimal paths. so, for real applications, the best choice depends on the application requirements. if most applications exhibit a high degree of communication locality, fully adaptive routing does not help. if the trafﬁc produced by the applications does not saturate the network (regardless of the routing algorithm) and latency is critical, then adaptive routing will not increase performance. however, when mul-
deadlocks might be for certain network conﬁgurations and to understand the parameters that most inﬂuence deadlock formation. for example, wormhole switching is more prone to deadlock than are other switching techniques [347]. this is because each packet may hold several channel resources spanning multiple nodes in the network while being blocked. we therefore concentrate on wormhole-switched recovery schemes.
recent work by pinkston and warnakulasuriya on characterizing deadlocks in interconnection networks has shown that a number of interrelated factors inﬂuence the probability of deadlock formation [283, 347].among the more inﬂuential of these factors is the routing freedom, the number of blocked packets, and the number of resource dependency cycles. routing freedom corresponds to the number of routing options available to a packet being routed at a given node within the network and can be increased by adding physical channels, adding virtual channels, and/or increasing the adaptivity of the routing algorithm. it has been quantitatively shown that as the number of network resources (physical and virtual channels) and the routing options allowed on them by the routing function increase (routing freedom), the number of packets that tend to block signiﬁcantly decreases, the number of resource dependency cycles decreases, and the resulting probability of deadlock also decreases exponentially. this is due to the fact that deadlocks require highly correlated patterns of cycles, the complexity of which increases with routing freedom. a conclusion of pinkston and warnakulasuriya’s work is that deadlocks in interconnection networks can be highly improbable when sufﬁcient routing freedom is provided by the network and fully exploited by the routing function. in fact, it has been shown that as few as two virtual channels per physical channel are sufﬁcient to virtually eliminate all deadlocks up to and beyond saturation in 2-d toroidal networks when using unrestricted fully adaptive routing [283, 347]. this veriﬁes previous empirical results that estimated that deadlocks are infrequent [9, 179]. however, the frequency of deadlock increases considerably when no virtual channels are used.
the network state reﬂecting resource allocations and requests existing at a particular point in time can be depicted by the channel wait-for graph (cwg). the nodes of this graph are the virtual channels reserved and/or requested by some packet(s). the solid arcs point to the next virtual channel reserved by the corresponding packet. the dashed arcs in the cwg point to the alternative virtual channels a blocked packet may acquire in order to continue routing. the highly correlated resource dependency pattern required to form a deadlocked conﬁguration can be analyzed with the help of the cwg. a knot is a set of nodes in the graph such that from each node in the knot it is possible to reach every other node in the knot. the existence of a knot is a necessary and sufﬁcient condition for deadlock [283]. note that checking the existence of knots requires global information and cannot be efﬁciently done at run time.
deadlocks can be characterized by deadlock set, resource set, and knot cycle density attributes. the deadlock set is the set of packets that own the virtual channels involved in the knot. the resource set is the set of all virtual channels owned by members of
for each channel, the queue capacity is not exceeded and all the ﬂits stored in the queue (if any) can reach the channel from the previous node using the routing function.
a deadlocked conﬁguration for a given interconnection network i and routing function r is a nonempty legal conﬁguration verifying the following conditions:
in a deadlocked conﬁguration there is no message whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (see assumption 5). data and tail ﬂits cannot advance because the next channel reserved by their message header has a full queue. no condition is imposed on empty channels. it must be noticed that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
a routing function r for an interconnection network i is deadlock-free iff there is not any deadlocked conﬁguration for that routing function on that network.
a routing subfunction r1 for a given routing function r is a routing function deﬁned on the same domain as r that supplies a subset of the channels supplied by r:
thus, r1 is a restriction of r. it should be noted that this deﬁnition allows the restriction of channel routing capability instead of simply removing channels. in other words, it is possible to restrict the use of a channel ci when r1 routes messages to some destinations while still allowing the use of ci when routing messages to other destinations.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect dependency from ci to cj iff
in order to make the theoretical results as general as possible, we assume no restriction about packet generation rate, packet destinations, and packet length. also, we assume no restriction on the paths supplied by the routing algorithm. both minimal and nonminimal paths are allowed. however, for performance reasons, a routing algorithm should supply at least one channel belonging to a minimal path at each intermediate node. additionally, we are going to focus on deadlocks produced by the interconnection network. thus, we assume that packets will be consumed at their destination nodes in ﬁnite time.
several switching techniques can be used. each of them will be considered as a particular case of the general theory. however, a few speciﬁc assumptions are required for some switching techniques. for saf and vct switching, we assume that edge buffers are used. central queues will be considered in section 3.2.3. for wormhole switching, we assume that a queue cannot contain ﬂits belonging to different packets. after accepting a tail ﬂit, a queue must be emptied before accepting another header ﬂit. when a virtual channel has queues at both ends, both queues must be emptied before accepting another header ﬂit. thus, when a packet is blocked, its header ﬂit will always occupy the head of a queue. also, for every path p that can be established by a routing function r, all subpaths of p are also paths of r. the routing functions satisfying the latter property will be referred to as coherent. for mad postman switching, we assume the same restrictions as for wormhole switching. additionally, dead ﬂits are removed from the network as soon as they are blocked.
a conﬁguration is an assignment of a set of packets or ﬂits to each queue. before analyzing how to avoid deadlocks, we are going to present a deadlocked conﬁguration by using an example.
consider a 2-d mesh with bidirectional channels. the routing function r forwards packets following any minimal path. this routing function is not deadlock-free. figure 3.2 shows a deadlocked conﬁguration. dashed incomplete boxes represent nodes of a 3 × 3 mesh. dashed boxes represent switches. solid boxes represent packet buffers or ﬂit buffers, depending on the switching technique used. the number inside each buffer indicates the destination node. solid arrows indicate the channel requested by the packet or the header at the queue head. as packets are allowed to follow all the minimal paths, packets wait for each other in a cyclic way. additionally, there is no alternative path for the packets in the ﬁgure because packets are only allowed to follow minimal paths. as all the buffers are full, no packet can advance.
so, a deadlocked conﬁguration is a conﬁguration in which some packets are blocked forever, waiting for resources that will never be granted because they are held by other packets. the conﬁguration described in example 3.1 would also be deadlocked if there were some additional packets traveling across the network that are not blocked. a deadlocked conﬁguration in which all the packets are blocked is referred to as canonical. given a deadlocked conﬁguration, the corresponding canonical conﬁguration can be obtained by stopping packet injection at all the nodes, and waiting for the delivery of
for each channel, the queue capacity is not exceeded and all the ﬂits stored in the queue (if any) can reach the channel from the previous node using the routing function.
a deadlocked conﬁguration for a given interconnection network i and routing function r is a nonempty legal conﬁguration verifying the following conditions:
in a deadlocked conﬁguration there is no message whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (see assumption 5). data and tail ﬂits cannot advance because the next channel reserved by their message header has a full queue. no condition is imposed on empty channels. it must be noticed that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
a routing function r for an interconnection network i is deadlock-free iff there is not any deadlocked conﬁguration for that routing function on that network.
a routing subfunction r1 for a given routing function r is a routing function deﬁned on the same domain as r that supplies a subset of the channels supplied by r:
thus, r1 is a restriction of r. it should be noted that this deﬁnition allows the restriction of channel routing capability instead of simply removing channels. in other words, it is possible to restrict the use of a channel ci when r1 routes messages to some destinations while still allowing the use of ci when routing messages to other destinations.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect dependency from ci to cj iff
figure 4.7 allowed paths in (a) fully adaptive routing in three-dimensional networks, (b) planar-adaptive routing in three-dimensional networks, and (c) planar-adaptive routing in four-dimensional networks.
until the offset in dimension di is reduced to zero. thus, in order to offer alternative routing choices for as long as possible, a higher priority is given to channels in dimension di while routing in plane ai.
as deﬁned, planar-adaptive routing requires three virtual channels per physical channel to avoid deadlocks in meshes and six virtual channels to avoid deadlocks in tori. in what follows, we analyze meshes in more detail. channels in the ﬁrst and last dimension need only one and two virtual channels, respectively. let di,j be the set of virtual channels j crossing dimension i of the network. this set can be decomposed into two subsets, one in the positive direction and one in the negative direction. let di,j+ and di,j− denote the positive and negative direction channels, respectively.
in order to avoid deadlock, the set of virtual channels in ai is divided into two classes: increasing and decreasing networks (figure 4.8). the increasing network is formed by
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
free if and only if there exists a restricted channel waiting graph that is wait-connected and has no true cycles [309]. this condition is valid for incoherent routing functions and for routing functions deﬁned on c × n. however, it proposes a dynamic condition for deadlock avoidance, thus requiring the analysis of all the packet injection sequences to determine whether a cycle is reachable (true cycle). true cycles can be identiﬁed by using the algorithm proposed in [309]. this algorithm has nonpolynomial complexity. when all the cycles are true cycles, this theorem is equivalent to theorem 3.1. the theory proposed in [309] has been generalized in [307], supporting saf, vct, and wormhole switching. basically, the theory proposed in [307] replaces the channel waiting graph by a buffer waiting graph.
up to now, nobody has proposed static necessary and sufﬁcient conditions for deadlock-free routing for incoherent routing functions and for routing functions deﬁned on c × n. this is a theoretical open problem. however, as mentioned in previous sections, it is of very little practical interest because the cases where theorem 3.1 cannot be applied are very rare. remember that this theorem can be used to prove deadlock freedom for incoherent routing functions and for routing functions deﬁned on c × n. in these cases it becomes a sufﬁcient condition.
unlike wormhole switching, saf and vct switching provide more buffer resources when packets are blocked. a single central or edge buffer is enough to store a whole packet. as a consequence, it is much simpler to avoid deadlock.
a simple technique, known as deﬂection routing [137] or hot potato routing, is based on the following idea: the number of input channels is equal to the number of output channels. thus, an incoming packet will always ﬁnd a free output channel.
the set of input and output channels includes memory ports. if a node is not injecting any packet into the network, then every incoming packet will ﬁnd a free output channel. if several options are available, a channel belonging to a minimal path is selected. otherwise, the packet is misrouted. if a node is injecting a packet into the network, it may happen that all the output channels connecting to other nodes are busy. the only free output channel is the memory port. in this case, if another packet arrives at the node, it is buffered. buffered packets are reinjected into the network before injecting any new packet at that node.
deﬂection routing has two limitations. first, it requires storing the packet into the current node when all the output channels connecting to other nodes are busy. thus, it cannot be applied to wormhole switching. second, when all the output channels belonging to minimal paths are busy, the packet is misrouted. this increases packet latency and bandwidth consumption, and may produce livelock. the main advantages are its simplicity and ﬂexibility. deﬂection routing can be used in any topology, provided that the number of input and output channels per node is the same.
deﬂection routing was initially proposed for communication networks. it has been shown to be a viable alternative for networks using vct switching. misrouting has a small impact on performance [188]. livelock will be analyzed in section 3.7.
only select an escape channel if the packet header is waiting for longer than the timeout. the motivation for this kind of selection function is that there is a high probability of an adaptive channel becoming available before the timeout expires. this kind of selection function is referred to as a time-dependent selection function [88, 94]. in particular, the behavior of progressive deadlock recovery mechanisms (see section 3.6) can be modeled by using a time-dependent selection function.
the selection function also plays a major role when real-time communication is required. in this case, best-effort packets and guaranteed packets compete for network resources. if the number of priority classes for guaranteed packets is small, each physical channel may be split into as many virtual channels as priority classes. in this case, the selection function will select the appropriate virtual channel for each packet according to its priority class. when the number of priority classes is high, the set of virtual channels may be split into two separate virtual networks, assigning best-effort packets and guaranteed packets to different virtual networks. in this case, scheduling of guaranteed packets corresponding to different priority classes can be achieved by using packet switching [294]. in this switching technique, packets are completely buffered before being routed in intermediate nodes, therefore allowing the scheduling of packets with the earliest deadline ﬁrst. wormhole switching is used for best-effort packets. latency of guaranteed packets can be made even more predictable by using an appropriate policy for channel bandwidth allocation, as indicated in the next section.
there exist some situations where several packets contend for the use of resources, therefore requiring some arbitration and allocation policy. these situations are not related to the routing algorithm but are described here for completeness. the most interesting cases of conﬂict in the use of resources arise when several virtual channels belonging to the same physical channel are ready to transfer a ﬂit, and when several packet headers arrive at a node and need to be routed. in the former case, a virtual channel allocation policy is required, while the second case requires a routing control unit allocation policy. flit-level ﬂow control across a physical channel involves allocating channel bandwidth among virtual channels that have a ﬂit ready to transmit and have space for this ﬂit at the receiving end. any arbitration algorithm can be used to allocate channel bandwidth including random, round-robin, or priority schemes. for random selection, an arbitrary virtual channel satisfying the above-mentioned conditions is selected. for round-robin selection, virtual channels are arranged in a circular list. when a virtual channel transfers a ﬂit, the next virtual channel in the list satisfying the above-mentioned conditions is selected for the next ﬂit transmission. this policy is usually referred to as demand-slotted round-robin and is the most frequently used allocation policy for virtual channels.
priority schemes require some information to be carried in the packet header. this information should be stored in a status register associated with each virtual channel reserved by the packet. deadline scheduling can be implemented by allocating channel bandwidth based on a packet’s deadline or age (earliest deadline or oldest age
the binary relation dimension order, denoted <d, is deﬁned between two nodes x and y as follows: x <d y if and only if either x = y or there exists an integer j such that σj (x) < σj (y) and σi (x) = σi (y) ∀ i, 0 ≤ i ≤ j − 1.
since <d is just lexicographic ordering, it is a total ordering on the nodes in an ndimensional mesh. therefore, it is reﬂexive, antisymmetric, and transitive. given a set of node addresses, they can be arranged in a unique, ordered sequence according to the <d relation.
a sequence of nodes x1, x2, . . . , xm is a dimension-ordered chain if and only if all the elements are distinct and either (1) xi <d xi+1 for 1 ≤ i < m or (2) xi <d xi−1 for 1 < i ≤ m.
lemmas 5.1 and 5.2 are critical to the development of efﬁcient multicast algorithms because they indicate how channel contention may be avoided. the chain algorithm is a distributed algorithm that can be used to multicast a message from a source node to one or more destinations. the algorithm applies to situations in which the address of the source node is either less than or greater than those of all the destinations, according to the <d relation. figure 5.52 gives the chain algorithm executed at each node. the source address and the destination addresses are arranged as a dimension-ordered chain in either increasing or decreasing order, with the source node occupying the position at the left end of the chain. the source node sends ﬁrst to the destination node halfway across the chain, then to the destination node one-quarter of the way across the chain, and so on. each destination receives a copy of the message from its parent in the tree and may be responsible for forwarding the message to other destinations. the message carries the addresses of those nodes to be in the subtree rooted at the receiving node. the chain algorithm is designed to produce minimum-time multicast implementations on top of dimension-ordered unicast routing. although some messages are passed through multiple routers before reaching their destinations, it turns out that channel contention will not occur among the messages, regardless of message length or start-up latency— referred to as depth-contention-free. the following theorem forms the basis for developing software-based multicast algorithms.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
the binary relation dimension order, denoted <d, is deﬁned between two nodes x and y as follows: x <d y if and only if either x = y or there exists an integer j such that σj (x) < σj (y) and σi (x) = σi (y) ∀ i, 0 ≤ i ≤ j − 1.
since <d is just lexicographic ordering, it is a total ordering on the nodes in an ndimensional mesh. therefore, it is reﬂexive, antisymmetric, and transitive. given a set of node addresses, they can be arranged in a unique, ordered sequence according to the <d relation.
a sequence of nodes x1, x2, . . . , xm is a dimension-ordered chain if and only if all the elements are distinct and either (1) xi <d xi+1 for 1 ≤ i < m or (2) xi <d xi−1 for 1 < i ≤ m.
lemmas 5.1 and 5.2 are critical to the development of efﬁcient multicast algorithms because they indicate how channel contention may be avoided. the chain algorithm is a distributed algorithm that can be used to multicast a message from a source node to one or more destinations. the algorithm applies to situations in which the address of the source node is either less than or greater than those of all the destinations, according to the <d relation. figure 5.52 gives the chain algorithm executed at each node. the source address and the destination addresses are arranged as a dimension-ordered chain in either increasing or decreasing order, with the source node occupying the position at the left end of the chain. the source node sends ﬁrst to the destination node halfway across the chain, then to the destination node one-quarter of the way across the chain, and so on. each destination receives a copy of the message from its parent in the tree and may be responsible for forwarding the message to other destinations. the message carries the addresses of those nodes to be in the subtree rooted at the receiving node. the chain algorithm is designed to produce minimum-time multicast implementations on top of dimension-ordered unicast routing. although some messages are passed through multiple routers before reaching their destinations, it turns out that channel contention will not occur among the messages, regardless of message length or start-up latency— referred to as depth-contention-free. the following theorem forms the basis for developing software-based multicast algorithms.
some recent proposals allow cyclic dependencies between channels, relying on some version of theorem 3.1 to guarantee deadlock freedom [95]. in general, virtual networks are deﬁned in such a way that the corresponding routing functions are deadlock-free. packet transfers between virtual networks are restricted in such a way that deadlocks are avoided. by allowing cyclic dependencies between channels and cyclic transfers between virtual networks, guaranteeing deadlock freedom is much more difﬁcult [95, 217]. however, virtual networks still have proven to be useful to deﬁne deadlock-free routing algorithms. see exercise 3.3 for an example.
dally and aoki proposed two adaptive routing algorithms based on the concept of dimension reversal [73]. the most interesting one is the dynamic algorithm. this routing algorithm allows the existence of cyclic dependencies between channels, as long as packets do not wait for channels in a cyclic way. before describing the dynamic algorithm, let us deﬁne the concept of dimension reversal. the dimension reversal (dr) number of a packet is the count of the number of times a packet has been routed from a channel in one dimension, p, to a channel in a lower dimension, q < p.
the dynamic algorithm divides the virtual channels of each physical channel into two nonempty classes: adaptive and deterministic. packets injected into the network are ﬁrst routed using adaptive channels. while in these channels, packets may be routed in any direction without a maximum limit on the number of dimension reversals a packet may make. whenever a packet acquires a channel, it labels the channel with its current dr number. to avoid deadlock, a packet with a dr of p cannot wait on a channel labeled with a dr of q if p ≥ q. a packet that reaches a node where all output channels are occupied by packets with equal or lower drs must switch to the deterministic class of virtual channels. once on the deterministic channels, the packet must be routed in dimension order using only the deterministic channels and cannot reenter the adaptive channels.
the dynamic algorithm represents a ﬁrst step toward relaxing the restrictions for deadlock avoidance. instead of requiring the absence of cyclic dependencies between channels as the algorithms presented in section 4.4.3, it allows those cyclic dependencies as long as packets do not wait for channels in a cyclic way. when a packet may produce a cyclic waiting, it is transferred to the deterministic class of virtual channels, where it is routed in dimension order. the dynamic algorithm provides the maximum routing ﬂexibility when packets are routed using adaptive channels. however, that ﬂexibility is lost when packets are transferred to the deterministic channels.
more routing ﬂexibility can be obtained by using theorem 3.1. fully adaptive routing algorithms described in sections 4.4.1, 4.4.2, and 4.4.3 do not allow cyclic dependencies between resources to avoid deadlocks. however, those algorithms require a large set of buffer resources. using theorem 3.1, it is possible to avoid deadlocks even when cyclic
1. phase 1: the message is adaptively routed in the negative directions, avoiding the negative edges of the mesh as far as possible. if the message encounters a faulty node on the negative edge, route one hop perpendicular to the edge.
2. phase 2: the message is adaptively routed in the positive directions toward the destination. if a faulty node on a negative edge of the mesh is encountered, the message is routed along a minimal distance path around that node.
of faults that can be tolerated is one less than the number of dimensions. this has been generalized to avoid up to (n − 1) faults in n-dimensional meshes [131]. the version of this algorithm for 2-d meshes is provided in figure 6.32.
the advantage of this approach is that no virtual channels are required. from a practical point of view, in environments where mttr << mtbf, it is likely that no more than a single fault will occur within any repair interval. in this case the turn model approach can provide gracefully degraded communication performance in 2-d meshes until the faulty component can be replaced. meanwhile the hardware architectures of the routers have been minimally impacted and remain compact and fast. for higher-fault-rate environments, more robust techniques that can provide delivery guarantees are desirable. the dimension reversal (dr) approach deﬁned by dally and aoki [73] produces gracefully degradable network performance in the presence of faults by dividing messages into classes. each message is permitted to be routed in any direction. however, if a message is routed from a channel in dimension di to a channel in dimension dj < di, a dr counter maintained in the message header is incremented. this dr counter is used to prevent the occurrence of cycles in the channel dependency graph. a static approach to doing so in multidimensional meshes creates r virtual networks by using r virtual channels across each physical channel. all messages are initialized with their dr values equal to zero. messages are injected into virtual network 0 and permitted to be routed in any direction toward the destination. misrouting is also permitted, although it should be controlled to ensure livelock freedom. when a dimension reversal takes place, the message moves into the next virtual network. messages that experience greater than (r − 1) dimension reversals are routed in dimension order (i.e., deterministic, nonadaptive routing) in the last virtual network. this approach can require a large number of virtual channels.
an alternative dynamic version of this approach has only two virtual networks: the adaptive network and the deterministic network. messages are now injected into the adaptive network. these messages can be routed in any direction using only adaptive channels and with no constraints on the number of misroutes other than those required to ensure livelock freedom. however, if a message must block waiting for a free channel at any node, the message may only block on channels being used by messages with a
to be identical, in the latter the routing decision is independent of (i.e., oblivious to) the state of the network. however, the choice is not necessarily deterministic. for example, a routing table may include several options for an output channel based on the destination address. a speciﬁc option may be selected randomly, cyclically, or in some other manner that is independent of the state of the network. a deterministic routing algorithm will always provide the same output channel for the same destination. while deterministic algorithms are oblivious, the converse is not necessarily true.
deterministic routing became very popular when wormhole switching was invented [77]. wormhole switching requires very small buffers. wormhole routers are compact and fast. however, pipelining does not work efﬁciently if one of the stages is much slower than the remaining stages. thus, wormhole routers have the routing algorithm implemented in hardware. it is not surprising that designers chose the simplest routing algorithms in order to keep routing hardware as compact and fast as possible. most commercial multicomputers (intel paragon [164], cray t3d [174], ncube-2/3 [248]) and experimental multiprocessors (stanford dash [203], mit j-machine [256]) use deterministic routing.
in this section, we present the most popular deterministic routing algorithms as well as a design methodology. obviously, the most popular routing algorithms are the simplest ones. some topologies can be decomposed into several orthogonal dimensions. this is the case for hypercubes, meshes, and tori. in these topologies, it is easy to compute the distance between current and destination nodes as the sum of the offsets in all the dimensions. progressive routing algorithms will reduce one of those offsets in each routing step. the simplest progressive routing algorithm consists of reducing an offset to zero before considering the offset in the next dimension. this routing algorithm is known as dimension-order routing. this routing algorithm routes packets by crossing dimensions in strictly increasing (or decreasing) order, reducing to zero the offset in one dimension before routing in the next one.
for n-dimensional meshes and hypercubes, dimension-order routing produces deadlock-free routing algorithms. these algorithms are very popular and receive several names, like xy routing (for 2-d mesh) or e-cube (for hypercubes) [334]. these algorithms are described in figures 4.3 and 4.4, respectively, where firstone() is a function that returns the position of the ﬁrst bit set to one, and internal is the channel connecting to the local node. although these algorithms assume that the packet header carries the absolute address of the destination node, the ﬁrst few sentences in each algorithm compute the offset from the current node to the destination node. this offset is the value carried by the header when relative addressing is used. so, the remaining sentences in each algorithm describe the operations for routing using relative addressing. note that relative addressing would also require updating the header at each intermediate node. exercises 3.1 and 4.3 show that the channel dependency graphs for dimension-order routing in n-dimensional meshes and hypercubes are acyclic. however, the channel dependency graph for tori has cycles. this topology was analyzed by dally and seitz [77], who proposed a design methodology for deadlock-free deterministic routing algorithms.
the methodology starts by considering some connected routing function and its channel dependency graph d. if it is not acyclic, routing is restricted by removing arcs
the binary relation dimension order, denoted <d, is deﬁned between two nodes x and y as follows: x <d y if and only if either x = y or there exists an integer j such that σj (x) < σj (y) and σi (x) = σi (y) ∀ i, 0 ≤ i ≤ j − 1.
since <d is just lexicographic ordering, it is a total ordering on the nodes in an ndimensional mesh. therefore, it is reﬂexive, antisymmetric, and transitive. given a set of node addresses, they can be arranged in a unique, ordered sequence according to the <d relation.
a sequence of nodes x1, x2, . . . , xm is a dimension-ordered chain if and only if all the elements are distinct and either (1) xi <d xi+1 for 1 ≤ i < m or (2) xi <d xi−1 for 1 < i ≤ m.
lemmas 5.1 and 5.2 are critical to the development of efﬁcient multicast algorithms because they indicate how channel contention may be avoided. the chain algorithm is a distributed algorithm that can be used to multicast a message from a source node to one or more destinations. the algorithm applies to situations in which the address of the source node is either less than or greater than those of all the destinations, according to the <d relation. figure 5.52 gives the chain algorithm executed at each node. the source address and the destination addresses are arranged as a dimension-ordered chain in either increasing or decreasing order, with the source node occupying the position at the left end of the chain. the source node sends ﬁrst to the destination node halfway across the chain, then to the destination node one-quarter of the way across the chain, and so on. each destination receives a copy of the message from its parent in the tree and may be responsible for forwarding the message to other destinations. the message carries the addresses of those nodes to be in the subtree rooted at the receiving node. the chain algorithm is designed to produce minimum-time multicast implementations on top of dimension-ordered unicast routing. although some messages are passed through multiple routers before reaching their destinations, it turns out that channel contention will not occur among the messages, regardless of message length or start-up latency— referred to as depth-contention-free. the following theorem forms the basis for developing software-based multicast algorithms.
with the packet along the outgoing links. such clock forwarding provides rapid transport of bits between connected 21364 chips and minimizes synchronization time between them.
circuit switching along with packet switching represent the ﬁrst switching techniques used in multiprocessor architectures. this section presents one such example and some more recent ideas toward improving the performance of the next generation of circuit-switched networks.
the second-generation network from intel included in the ipsc/2 series machines utilized a router that implements circuit switching. although most machines currently use some form of cut-through routing, circuit-switched networks do present some advantages. for example, once a path is set up, data can be transmitted at the full physical bandwidth without delays due to sharing of links. the system can also operate completely synchronously without the need for ﬂow control buffering of data between adjacent routers. while the previous generation of circuit-switched networks may have been limited by the path lengths, the use of link pipelining offers new opportunities.
the network topology used in the ipsc series machines is that of a binary hypercube. the router is implemented in a module referred to as the direct connect module (dcm). the dcm implements dimension-order routing and supports up to a maximum of eight dimensions. one router port is reserved for direct connections to external devices. the remaining router ports support topologies of up to 128 nodes. the node architecture and physical channel structure are shown in figure 7.40. individual links are bit serial, full duplex, and provide a link bandwidth of 2.8 mbytes/s. data, control, and status information are multiplexed across the serial data link. the strobe lines supply the clock from the source. the synchronous nature of the channel precludes the need for any handshaking signals between routers.
when a message is ready to be transmitted, a 32-bit word is transferred to the dcm. the least signiﬁcant byte contains the routing tag—the exclusive-or of the source and destination addresses. these bits are examined in ascending order by each routing element. a bit value of 1 signiﬁes that the message must traverse the corresponding dimension. the input serializer generates a request for the channel in the dimension corresponding to the least signiﬁcant asserted bit in the routing tag. subsequent routing elements at intermediate nodes receive the header and make similar requests to output channels. multiple requests for the same output channel are arbitrated and resolved in a round-robin fashion. note the connectivity at the outputs in figure 7.40. an outgoing channel can expect to receive a message only from lower-numbered channels. this signiﬁcantly reduces the design complexity of the routing elements. channel status information is continually multiplexed across the data lines in the reverse direction. the status information corresponds to rdy (ready) or eom (end of message) signals. these signals achieve end-to-end ﬂow control.
deadlock-free routing functions previously proposed for the same topology. if one of those routing functions is a restriction of the routing function we are analyzing, we can try it. as we will see in chapter 4, a simple way to propose adaptive routing algorithms follows this rule in the opposite way. it starts from a deterministic deadlock-free routing function, adding channels in a regular way. the additional channels can be used for fully adaptive routing.
theorem 3.1 is valid for saf, vct, and wormhole switching under the previously mentioned assumptions. the application to each switching technique will be presented in sections 3.1.4 and 3.1.5. for wormhole switching, the condition proposed by theorem 3.1 becomes a sufﬁcient one if the routing function is not coherent. thus, it is still possible to use this theorem to prove deadlock freedom on incoherent routing functions, as can be seen in exercise 3.3.
finally, when a packet uses an escape channel at a given node, it can freely use any of the available channels supplied by the routing function at the next node. it is not necessary to restrict that packet to use only channels belonging to the escape paths. also, when a packet is blocked because all the alternative output channels are busy, the header is not required to wait on any predetermined channel. instead, it waits in the input channel queue. it is repeatedly routed until any of the channels supplied by the routing function becomes free. both issues are important because they considerably increase routing ﬂexibility, especially when the network is heavily loaded.
in this section, we restrict our attention to saf and vct switching. note that dependencies arise because a packet is holding a resource while requesting another one. although vct pipelines packet transmission, it behaves in the same way as saf regarding deadlock because packets are buffered when they are blocked. a blocked packet is buffered into a single channel. thus, dependencies only exist between adjacent channels.
according to expression (3.1), it is possible to restrict a routing function in such a way that a channel ci supplied by r for destination nodes x and y is only supplied by the routing subfunction r1 for destination node x. in this case, ci is an escape channel for packets destined for x but not for packets destined for y.
as indicated in section 3.1.3, there is a channel dependency from an escape channel ci to another escape channel ck if there exists a packet that is holding ci and it requests ck as an escape channel for itself. if ci is also an escape channel for the packet destination, we refer to this kind of channel dependency as direct dependency [86, 91]. if ci is not an escape channel for the packet destination, the dependency is referred to as direct crossdependency [92, 97]. direct cross-dependencies must be considered because a packet may be holding a channel needed by another packet to escape from cycles. the following example shows both kinds of dependency.
that is, it is possible to establish a path from si to dj for messages destined for some node x, and ci and cj are the ﬁrst and last channels in that path and the only ones in that path supplied by r1 for the destination of the message. therefore, cj can be requested after using ci by some messages. as ci and cj are not adjacent, some other channels not supplied by r1 for the destination of the message are reserved while establishing the path between them. those channels are supplied by r.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of adjacent channels ci , cj ∈ c1, there is a direct cross-dependency from ci to cj iff
that is, cj can be requested immediately after using ci by messages destined for some node y, cj is supplied by r1 for the destination of the message, and ci cannot be supplied by r1 for that destination. however, ci is supplied by r1 for some other destination(s).
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect crossdependency from ci to cj iff
that is, it is possible to establish a path from si to dj for messages destined for some node y, ci and cj are the ﬁrst and last channels in that path, cj is the only channel in that path supplied by r1 for the destination of the message, and ci cannot be supplied by r1 for that destination. however, ci is supplied by r1 for some other destination(s). therefore, cj can be requested after using ci by some messages. as ci and cj are not adjacent, some other channels not supplied by r1 for the destination of the message are reserved while establishing the path between them.
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
deadlock-free routing functions previously proposed for the same topology. if one of those routing functions is a restriction of the routing function we are analyzing, we can try it. as we will see in chapter 4, a simple way to propose adaptive routing algorithms follows this rule in the opposite way. it starts from a deterministic deadlock-free routing function, adding channels in a regular way. the additional channels can be used for fully adaptive routing.
theorem 3.1 is valid for saf, vct, and wormhole switching under the previously mentioned assumptions. the application to each switching technique will be presented in sections 3.1.4 and 3.1.5. for wormhole switching, the condition proposed by theorem 3.1 becomes a sufﬁcient one if the routing function is not coherent. thus, it is still possible to use this theorem to prove deadlock freedom on incoherent routing functions, as can be seen in exercise 3.3.
finally, when a packet uses an escape channel at a given node, it can freely use any of the available channels supplied by the routing function at the next node. it is not necessary to restrict that packet to use only channels belonging to the escape paths. also, when a packet is blocked because all the alternative output channels are busy, the header is not required to wait on any predetermined channel. instead, it waits in the input channel queue. it is repeatedly routed until any of the channels supplied by the routing function becomes free. both issues are important because they considerably increase routing ﬂexibility, especially when the network is heavily loaded.
in this section, we restrict our attention to saf and vct switching. note that dependencies arise because a packet is holding a resource while requesting another one. although vct pipelines packet transmission, it behaves in the same way as saf regarding deadlock because packets are buffered when they are blocked. a blocked packet is buffered into a single channel. thus, dependencies only exist between adjacent channels.
according to expression (3.1), it is possible to restrict a routing function in such a way that a channel ci supplied by r for destination nodes x and y is only supplied by the routing subfunction r1 for destination node x. in this case, ci is an escape channel for packets destined for x but not for packets destined for y.
as indicated in section 3.1.3, there is a channel dependency from an escape channel ci to another escape channel ck if there exists a packet that is holding ci and it requests ck as an escape channel for itself. if ci is also an escape channel for the packet destination, we refer to this kind of channel dependency as direct dependency [86, 91]. if ci is not an escape channel for the packet destination, the dependency is referred to as direct crossdependency [92, 97]. direct cross-dependencies must be considered because a packet may be holding a channel needed by another packet to escape from cycles. the following example shows both kinds of dependency.
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
to reliability, interconnection networks should have a modular design, allowing hot upgrades and repairs. nodes can also fail or be removed from the network. in particular, a node can be powered off in a network of workstations. thus, nows usually require some reconﬁguration algorithm for the automatic reconﬁguration of the network when a node is powered on or off.
9. expected workloads. users of a general-purpose machine may have very different requirements. if the kind of applications that will be executed in the parallel computer are known in advance, it may be possible to extract some information on usual communication patterns, message sizes, network load, and so on. that information can be used for the optimization of some design parameters. when it is not possible to get information on expected workloads, network design should be robust; that is, design parameters should be selected in such a way that performance is good over a wide range of trafﬁc conditions.
10. cost constraints. finally, it is obvious that the “best” network may be too expensive. design decisions often are trade-offs between cost and other design factors. fortunately, cost is not always directly proportional to performance. using commodity components whenever possible may considerably reduce the overall cost.
among other criteria, interconnection networks have been traditionally classiﬁed according to the operating mode (synchronous or asynchronous) and network control (centralized, decentralized, or distributed). nowadays, multicomputers, multiprocessors, and nows dominate the parallel computing market. all of these architectures implement asynchronous networks with distributed control. therefore, we will focus on other criteria that are currently more signiﬁcant.
a classiﬁcation scheme is shown in figure 1.2, which categorizes the known interconnection networks into four major classes based primarily on network topology: shared-medium networks, direct networks, indirect networks, and hybrid networks. for each class, the ﬁgure shows a hierarchy of subclasses, also indicating some real implementations for most of them. this classiﬁcation scheme is based on the classiﬁcation proposed in [253], and it mainly focuses on networks that have been implemented. it is by no means complete, as other new and innovative interconnection networks may emerge as technology further advances, such as mobile communication and optical interconnections. in shared-medium networks, the transmission medium is shared by all communicating devices. an alternative to this approach consists of having point-to-point links directly connecting each communicating device to a (usually small) subset of other communicating devices in the network. in this case, any communication between nonneighboring devices requires transmitting the information through several intermediate devices. these networks are known as direct networks. instead of directly connecting the communicating devices between them, indirect networks connect those devices by means of one or more
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
five virtual channels are multiplexed in each direction over each physical channel. four of the channels essentially function as they do in the t3d (with differences identiﬁed below) while the ﬁfth channel is used for fully adaptive routing. each adaptive channel can buffer up to 22 ﬂits, while each of the regular channels can buffer up to 12 ﬂits. the channels utilize credit-based ﬂow control for buffer management with acknowledgments in the reverse direction being piggybacked on other messages or transmitted as idle ﬂits. roundrobin scheduling across active virtual channels within a group determines which channel will compete for crossbar outputs. the winning channel can request a virtual channel in the deterministic direction or an adaptive channel in the highest-ordered direction yet to be satisﬁed. if both are available, the adaptive channel is used. once an output virtual channel is allocated, it must remain allocated, while other virtual channel inputs may still request and use the same physical output channel. finally, output virtual channels must arbitrate for the physical channel. this process is also round-robin, although the adaptive channel has the lowest priority to start transmitting.
routing is fully adaptive. the deterministic paths use four virtual channels for the request/reply network as in the t3d. however, the path is determined by ordering dimensions and the direction of traversal within each dimension. this style of routing can be regarded as direction order rather than dimension order. the ordering used in the t3e is x+, y+, z+, x−, y−, and z−. in general, any other ordering would work just as well. by applying the concept of channel classes from chapter 3, it is apparent that each direction corresponds to a class of channels, and the order in which channel classes are used by a message is acyclic. thus, routing is deadlock-free. in addition, the network supports an initial hop in any positive direction and permits one ﬁnal hop in the z− direction at the end of the route. these initial and ﬁnal hops add routing ﬂexibility and are only necessary if a normal direction-order route does not exist. each message has a ﬁxed path that is determined at the source router. a message can be routed along the statically conﬁgured path, or along the adaptive virtual channel over any physical link along a minimal path to the destination. however, the implementation does not require the extended channel dependency graph to be acyclic. the adaptive virtual channels are large enough to buffer two maximal messages. therefore, indirect dependencies between deterministic channels do not exist. this permits optimizations in the manner in which virtual channels are assigned to improve virtual channel utilization and minimize imbalance due to routing restrictions. finally, adaptive routing can be turned off via control ﬁelds within the routing tables and on an individual packet basis through a single bit recorded in the message. since adaptively routed messages may arrive out of order at the destination, the latter capability enables the processor to issue sequences of messages that are delivered in order.
memory management and the network interface operation have been made more ﬂexible than the ﬁrst-generation support found in the t3d.an arbitrary number of message queues are supported in both user memory and system memory. message queues can be set up to be interrupt driven, polled, or interrupt driven based on some threshold on the number of messages. in general, message passing is more tightly coupled with operations for the support of shared-memory abstractions.
progressive recovery allows resources to be temporarily deallocated from normal packets and assigned to a deadlocked packet so that it can reach its destination. once the deadlocked packet is delivered, resources are reallocated to the preempted packets. this progressive deadlock recovery technique was ﬁrst proposed in [8, 9]. this technique is clearly more efﬁcient than regressive deadlock recovery. hence, we will study it in more detail.
figure 3.16 shows the router organization for the ﬁrst progressive recovery technique proposed, referred to as disha [8, 9, 10]. it is identical to the router model proposed in section 2.1 except that each router is equipped with an additional central buffer, or deadlock buffer. this resource can be accessed from all neighboring nodes by asserting a control signal (arbitration for it will be discussed below) and is used only in the case of suspected deadlock. systemwide, these buffers form what is collectively a deadlock-free recovery lane that can be visualized as a ﬂoating virtual channel shared by all physical dimensions of a router. on the event of a suspected deadlock, a packet is switched to the deadlock-free lane and routed adaptively along a path leading to its destination, where it is consumed to avert the deadlock. physical bandwidth is deallocated from normal packets and assigned to the packet being routed over the deadlock-free lane to allow swift recovery. the aforementioned control signal is asserted every time a deadlocked packet ﬂit uses physical channel bandwidth so that the ﬂit is assigned to the deadlock buffer at the next router. the mechanism that makes the recovery lane deadlock-free is speciﬁc to how recovery resources are allocated to suspected deadlocked packets.
it could happen that a recovering packet in the deadlock buffer requires the same output physical channel at a router as normal unblocked (active) packets currently being routed using edge buffer resources. this case, which is illustrated in figure 3.19 at router rb, poses no problem. here, packet p8 may be active. however, p1 takes precedence and preempts the physical channel bandwidth, suspending normal packet transmission until deadlock has cleared.although this might lead to additional discontinuities in ﬂit reception beyond that suffered from normal virtual channel multiplexing onto physical channels, all ﬂits of packet p8 are guaranteed to reach their destination safely. given that suspected deadlock occurrences are infrequent anyway, the occasional commandeering of output channels should not signiﬁcantly degrade performance. however, router organizations that place the deadlock buffer on the input side of the crossbar would require crossbar reconﬁguration and logic to remember the state of the crossbar before it was reconﬁgured so that the suspended input buffer can be reconnected to the preempted output buffer once deadlock has cleared [9]. this situation can be avoided simply by connecting the deadlock buffer to the output side of the crossbar as shown in figure 3.16. this positioning of the deadlock buffer also makes recovering from self-deadlocking situations less complicated (i.e., those situations arising from nonminimal routing where packets are allowed to visit the same node twice).
although deadlock buffers are dedicated resources, no dedicated channels are required to handle deadlocked packets. instead, network bandwidth is allocated for deadlock freedom only in the rare instances when probable deadlocks are detected; otherwise, all network bandwidth is devoted to fully adaptive routing of normal packets. thus, most of the time, all edge virtual channels are used for fully adaptive routing to optimize performance. this is as opposed to deadlock avoidance routing schemes that constantly devote some number of edge virtual channels for avoiding improbable deadlock situations. the number of deadlock buffers per node should be kept minimal. otherwise, crossbar size would increase, requiring more gates and resulting in increased propagation delay. this is the reason why central buffers are used in disha. some alternative approaches use edge buffers instead of central buffers, therefore providing a higher bandwidth to drain blocked packets [94, 274]. however, in these cases more buffers are required, increasing crossbar size and propagation delay.
deadlock freedom on the deadlock buffers is essential to recovery. this can be achieved by restricting access with a circulating token that enforces mutual exclusion on the deadlock buffers. in this scheme, referred to as disha sequential [8], only one packet at a time is allowed to use the deadlock buffers. an asynchronous token scheme was proposed in [8] and implemented in [282] to drastically reduce propagation time and hide token capture latency. the major drawback of this scheme, however, is the token logic, which presents a single point of failure. another potential drawback is the fact that recovery from deadlocks is sequential. this could affect performance if deadlocks become frequent or get clustered in time. simultaneous deadlock recovery avoids these drawbacks, which will be discussed shortly.
an attractive and unique feature of disha recovery is that it reassigns resources to deadlocked packets instead of killing packets and releasing the resources occupied by them. based on the extension of theorem 3.1 as mentioned in section 3.2.3, disha ensures
using normalized delays will represent performance relative to this improved base 2-d case. let the switch delay and routing delay be represented as integer multiples of the wire delay in a 2-d network: s and r, respectively. from the earlier discussion, we know the value of tw relative to a 2-d network. thus, the latency expression can be written as follows (for networks where k is even):
the above expression is based on a linear delay model for wires. for short wires, delay is logarithmic [70] and, for comparison purposes, it is useful (though perhaps unrealistic) to have a model based on constant wire delay. the above expression is normalized to the wire delay in a 2-d mesh. therefore, in the constant delay model, wire delay simply remains at 1. in the logarithmic delay model, delay is a logarithmic function of wire length
the latency expressions can be viewed as the sum of two components: a distance component, which is the delay experienced by the header ﬂit, and a message component, which is the delay due to the length of the message. the expressions also include terms representing physical constraints (wire length, channel widths), network topology (dimensions, average distance), applications (message lengths), and router architecture (routing and switching delays). although in the above expression routing and switching delays are constants, we could model them as an increasing function of network dimension. this appeals to intuition since a larger number of dimensions may make routing decisions more complex and increase the delay through internal networks that must switch between a larger number of router inputs and outputs. we revisit this issue in the exercises at the end of this chapter.
the above expression provides us with insight into selecting the appropriate number of dimensions for a ﬁxed number of nodes. the minimum latency is realized when the component due to distance is equal to the component due to message length. the choice of parameters such as the switching and routing delays, channel widths, and message lengths determine the dimension at which this minimum is realized. the parameters themselves are not independent and are related by implementation constraints. examples of such analysis are presented in the following sections.
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
several different bus arbitration algorithms have been proposed, which can be classiﬁed into centralized or distributed. a centralized method has a central bus arbiter. when a processor wants to become the bus master, it sends out a bus request to the bus arbiter, which then sends out a request grant signal to the requesting processor. a bus arbiter can be an encoder-decoder pair in hardware design. in a distributed method, such as the daisy chain method, there is no central bus arbiter. the bus request signals form a daisy chain. the mastership is released to the next device when data transfer is done.
most bus transactions involve request and response. this is the case for memory read operations. after a request is issued, it is desirable to have a fast response. if a fast response time is expected, the bus mastership is not released after sending the request, and data can be received soon. however, due to memory latency, the bus bandwidth is wasted while waiting for a response. in order to minimize the waste of bus bandwidth, the split transaction protocol has been used in many bus networks.
in this protocol, the bus mastership is released immediately after the request, and the memory has to gain mastership before it can send the data. split transaction protocol has a better bus utilization, but its control unit is much more complicated. buffering is needed in order to save messages before the device can gain the bus mastership.
to support shared-variable communication, some atomic read/modify/write operations to memories are needed. with the split transaction protocol, those atomic operations can no longer be indivisible. one approach to solve this problem is to disallow bus release for those atomic operations.
gigaplane used in sun ultra enterprise x000 server (ca. 1996): 2.68 gbyte/s peak, 256 bits data, 42 bits address, split transaction protocol, 83.8 mhz clock. dec alphaserver8x00, that is, 8200 and 8400 (ca. 1995): 2.1 gbyte/s, 256 bits data, 40 bits address, split transaction protocol, 100 mhz clock (1 foot length). sgi powerpath-2 (ca. 1993): 1.2 gbyte/s, 256 bits data, 40 bits address, 6 bits control, split transaction protocol, 47.5 mhz clock (1 foot length). hp9000 multiprocessor memory bus (ca. 1993): 1 gbyte/s, 128 bits data, 64 bits address, 13 inches, pipelined bus, 60 mhz clock.
scalability is an important issue in designing multiprocessor systems. bus-based systems are not scalable because the bus becomes the bottleneck when more processors are added.
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
programming multicomputers is not an easy task. the programmer has to take care of distributing code and data among the processors in an efﬁcient way, invoking messagepassing calls whenever some data are needed by other processors. on the other hand, shared-memory multiprocessors provide a single memory space to all the processors, simplifying the task of exchanging data among processors. access to shared memory has been traditionally implemented by using an interconnection network between processors and memory (figure 1.1(b)). this architecture is referred to as uniform memory access (uma) architecture. it is not scalable because memory access time includes the latency of the interconnection network, and this latency increases with system size.
more recently, shared-memory multiprocessors followed some trends previously established for multicomputers. in particular, memory has been physically distributed among processors, therefore reducing the memory access time for local accesses and increasing scalability. these parallel computers are referred to as distributed sharedmemory multiprocessors (dsms). accesses to remote memory are performed through an interconnection network, very much like in multicomputers. the main difference between dsms and multicomputers is that messages are initiated by memory accesses rather than by calling a system function. in order to reduce memory latency, each processor has several levels of cache memory, thus matching the speed of processors and memories. this architecture provides nonuniform memory access (numa) time. indeed, most of the nonuniformity is due to the different access time between caches and main memories, rather than the different access time between local and remote memories. the main problem arising in dsms is cache coherence. several hardware and software cache coherence protocols have been proposed. these protocols produce additional trafﬁc through the interconnection network.
the use of custom interconnects makes multicomputers and dsms quite expensive. so, networks of workstations (nows) have been proposed as an inexpensive approach to build parallel computers. nows take advantage of recent developments in lans. in particular, the use of atm switches has been proposed to implement nows. however, atm switches are still expensive, which has motivated the development of high-performance
however, a parallel computer requires some kind of communication subsystem to interconnect processors, memories, disks, and other peripherals. the speciﬁc requirements of these communication subsystems depend on the architecture of the parallel computer. the simplest solution consists of connecting processors to memories and disks as if there were a single processor, using system buses and i/o buses. then, processors can be interconnected using the interfaces to local area networks. unfortunately, commodity communication subsystems have been designed to meet a different set of requirements, that is, those arising in computer networks. although networks of workstations have been proposed as an inexpensive approach to build parallel computers, the communication subsystem becomes the bottleneck in most applications.
therefore, designing high-performance interconnection networks becomes a critical issue to exploit the performance of parallel computers. moreover, as the interconnection network is the only subsystem that cannot be efﬁciently implemented by using commodity components, its design becomes very critical. this issue motivated the writing of this book. up to now, most manufacturers designed custom interconnection networks (ncube2, ncube-3, intel paragon, cray t3d, cray t3e, thinking machines corp. cm-5, nec cenju-3, ibm sp2). more recently, several high-performance switches have been developed (autonet, myrinet, servernet) and are being marketed. these switches are targeted to workstations and personal computers, offering the customer the possibility of building an inexpensive parallel computer by connecting cost-effective computers through high-performance switches. the main issues arising in the design of networks for both approaches are covered in this book.
in this section, we brieﬂy introduce the most popular parallel computer architectures. this description will focus on the role of the interconnection network. a more detailed description is beyond the scope of this book.
the idea of using commodity components for the design of parallel computers led to the development of distributed-memory multiprocessors, or multicomputers, in the early 1980s. these parallel computers consist of a set of processors, each one connected to its own local memory. processors communicate between themselves by passing messages through an interconnection network. figure 1.1(a) shows a simple scheme for this architecture. the ﬁrst commercial multicomputers utilized commodity components, including ethernet controllers to implement communication between processors. unfortunately, commodity communication subsystems were too slow, and the interconnection network became the bottleneck of those parallel computers. several research efforts led to the development of interconnection networks that are several orders of magnitude faster than ethernet networks. most of the performance gain is due to architectural rather than technological improvements.
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
output: selected output channel procedure: xoffset := xdest − xcurrent; yoffset := ydest − ycurrent; if xoffset < 0 and yoffset < 0 then channel := select(xa−, y a−, xb−); endif if xoffset < 0 and yoffset > 0 then channel := select(xa−, y a+, xb−); endif if xoffset < 0 and yoffset = 0 then channel := select(xa−, xb−); endif if xoffset > 0 and yoffset < 0 then channel := select(xa+, y a−, xb+); endif if xoffset > 0 and yoffset > 0 then channel := select(xa+, y a+, xb+); endif if xoffset > 0 and yoffset = 0 then channel := select(xa+, xb+); endif if xoffset = 0 and yoffset < 0 then channel := select(y a−, y b−); endif if xoffset = 0 and yoffset > 0 then channel := select(y a+, y b+); endif if xoffset = 0 and yoffset = 0 then channel := internal; endif
double-y. the double-y routing algorithm uses one set of y channels, namely, y 1, for packets traveling x−, and the second set of y channels, namely, y 2, for packets traveling x+.
based on the turn model, glass and ni analyzed the double-y routing algorithm, eliminating the unnecessary restrictions [130]. the resulting algorithm is called maximally
free routing algorithm, including oblivious e-cube routing and fully adaptive duato’s protocol (dp). the resulting fault-tolerant routing protocols are referred to as e-sft and dp-sft, respectively.
the performance of e-sft and dp-sft was evaluated with ﬂit-level simulation studies of message passing in a 16-ary 2-cube with 16-ﬂit messages, a single-ﬂit routing header, and eight virtual channels per physical channel. message destination trafﬁc was uniformly distributed. simulation runs were made repeatedly until the 95% conﬁdence intervals for the sample means were acceptable (less than 5% of the mean values). the simulation model was validated using deterministic communication patterns. we use a congestion control mechanism (similar to [36]) by placing a limit on the size of the buffer on the injection channels. if the input buffers are ﬁlled, messages cannot be injected into the network until a message in the buffer has been routed. injection rates are normalized.
the header format for e-sft and dp-sft was presented in figure 6.29. a 32-bit header enables routing within a 64 × 64 torus. the ﬂags require 6 bits, and the offsets require 6 bits each.
we assume the use of half-duplex channels. with half-duplex channels, 32-bit channels are comparable to the number of data pins devoted to interrouter communication in modern machines (e.g., there are two 16-bit full-duplex channels in the intel paragon router). a ﬂit crosses a channel in a single cycle and traverses a router from input to output in a single cycle. use of 16-bit full-duplex channels would double the number of cycles to transmit the routing header. routing decisions are assumed to take a single cycle with the network operating with a 50 mhz clock and 20 ns cycle time. the software cost for absorbing and reinjecting a message is derived from measured times on an intel paragon and reported work with active message implementations [105]. based on these studies, we assess this cost nominally at 25 µs per absorption/injection or 50 µs each time a message must be processed by the messaging software at an intermediate node. if the message encounters busy injection buffers when it is being reinjected, it is requeued for reinjection at a later time. absorbed messages have priority over new messages to prevent starvation.
the router hardware can retain the same basic architecture as the canonical form for dimension-order routers or adaptive routers presented in section 7.2.1, which captures the basic organization of several commercial and research designs. the only additional functionality required is of the routing and arbitration unit. when a message is routed to a faulty output channel, the f bit must be set and the x-dest and y -dest ﬁelds reset to 0 to cause the message to be absorbed. one side effect of the increased header size is a possible increase in virtual channel buffer size and the width of the internal data paths, although 32-bit data paths appear to be reasonable for the next generation of routers. finally, as with any approach for fault-tolerant routing, support for testing of channels and nodes must be available. the remaining required functionality of e-sft and dp-sft is implemented in the messaging layer software.
deﬁning routing algorithms and checking if they are deadlock-free is a tedious task. in order to simplify this task, duato proposed some design methodologies [86, 91, 98]. in general, design methodologies do not supply optimal routing algorithms. however, they usually supply near-optimal routing algorithms with much less effort. in this section, we present a general methodology for the design of deadlock-free fully adaptive routing algorithms that combines the methodologies previously proposed by duato. for vct and saf switching, it automatically supplies deadlock-free routing algorithms. for wormhole switching, a veriﬁcation step is required.
the design methodology presented in this section is based on the use of edge buffers. for saf switching, a similar methodology can be deﬁned for networks using central queues. this methodology describes a way to add channels to an existing network, also deriving the new routing function from the old one. channels are added following a regular pattern, using the same number of virtual channels for all the physical channels. this is important from the implementation point of view because bandwidth sharing and propagation delay remain identical for all the output channels.
this methodology supplies fully adaptive minimal and nonminimal routing algorithms, starting from a deterministic or partially adaptive routing algorithm. when restricted to minimal paths, the resulting routing algorithms have been referred to as duato’s protocol (dp) [125]. the steps are the following:
1. given an interconnection network i1, select one of the existing routing functions for it. let r1 be this routing function. it must be deadlock-free and connected. it can be deterministic or adaptive. for wormhole switching, it is recommended that r1 is minimal. let c1 be the set of channels at this point. split each physical channel into a set of additional virtual channels. let c be the set of all the (virtual) channels in the network. let cxy be the set of output channels from node x belonging to a path (minimal or not) from node x to node y. deﬁne the new routing function r as follows:
that is, the new routing function can use any of the new channels or, alternatively, the channels supplied by r1. the selection function can be deﬁned in any way. however, it is recommended to give a higher priority to the new channels belonging to minimal paths. for wormhole switching, it is recommended that r is restricted to use only minimal paths. for wormhole switching, verify that the extended channel dependency graph for r1 is acyclic. if it is, the routing algorithm is valid. otherwise, it must be discarded, returning to step 1.
this can be achieved in a fully distributed manner. the nodes are generally assumed to possess some ability for self-test as well as the ability to test neighboring nodes. we can envision an approach where, in one step, each node performs a self-test and interrogates the status of its neighbors. if neighbors or links in two or more dimensions are faulty, the node transitions to a faulty state, even though it may be nonfaulty. this diagnosis step is repeated. after a ﬁnite number of steps bounded by the diameter of the network, fault regions will have been created and will be rectangular in shape. in multidimensional meshes and tori under the block fault model, fault-free nodes are adjacent to at most one faulty node (i.e., along only one dimension). note that single component faults correspond to the block fault model with block sizes of 1.
the block fault model is particularly well suited to evolving trends in packaging technology and the use of low-dimensional networks. we will continue to see subnetworks implemented in chips, multichip modules (mcms), and boards. failures within these components will produce block faults at the chip, board, and mcm level. construction of block fault regions often naturally falls along chip, mcm, and board boundaries. for example, if submeshes are implemented on a chip, failure of two or more processors on a chip may result in marking the chip as faulty, leading to a block fault region comprised of all of the processors on the chip. the advantages in doing so include simpler solutions to deadlock- and livelock-free routing.
failures may be either static or dynamic. static failures are present in the network when the system is powered on. dynamic failures appear at random during the operation of the system. both types of faults are generally considered to be permanent; that is, they remain in the system until it is repaired.alternatively, faults may be transient.as integrated circuit feature sizes continue to decrease and speeds continue to increase, problems arise with soft errors that are transient or dynamic in nature. the difﬁculty of designing for such faults is that they often cannot be reproduced. for example, soft errors that occur in ﬂight tests of avionics hardware often cannot be reproduced on the ground. in addition,
limited misrouting and route messages in the presence of faults. if reliability is more important than performance, pcs has the potential to increase fault tolerance considerably at the cost of some overhead in path setup. pcs can be combined with misrouting backtracking protocols, achieving an excellent level of tolerance to static faults. finally, dynamically conﬁgurable ﬂow control techniques, like scouting, offer an excellent performance/reliability trade-off at the expense of a more complex hardware support. this switching technique achieves performance similar to that of wormhole switching in the absence of faults, and reliability halfway between wormhole switching and pcs.
support for dynamic faults. faults may occur at any time. when a fault interrupts a message in transit, the message cannot be delivered, and a fragment of the message may remain in the network forever, therefore producing deadlock. so, hardware support for dynamic faults has to perform two different actions: removing fragments of interrupted messages and notifying the source node so that the message is transmitted again. adding support to remove fragments of interrupted messages is very convenient because it does not slow down normal operation signiﬁcantly, and deadlocks produced by interrupted messages are avoided. however, notifying the source node produces an extra use of resources for every transmitted message, therefore degrading performance signiﬁcantly even in the absence of faults. so, this additional support for reliable transmission should only be included when reliability is more important than performance.
many researchers evaluated the performance of interconnection networks with different combinations of design parameters. in this section, we brieﬂy comment on some of those results. these references may be useful as pointers for further reading.
several evaluation studies compared the relative performance of different routing algorithms or analyzed the performance of some proposed routing algorithm. most recent studies focused on wormhole switching [24, 36, 73, 91, 175, 344], but results also exist for packet switching [278], vct switching [33, 189, 249, 251], hybrid switching [317], pcs [125, 126], scouting [79, 99], and wave pipelining [101]. although most results were obtained for direct networks, multistage networks [254] and switch-based interconnects with irregular topologies [30, 285, 319, 320] were also considered.
some studies analyzed the impact of the network topology on performance, considering implementation constraints [1, 3, 70, 102, 250, 311]. hierarchical topologies were also studied [18, 160]. also, some researchers compared the performance of different switching techniques [125, 293, 294].
some performance evaluations analyzed the impact of using virtual channels [72, 88, 312]. several selection functions were compared in [73]. the effect of the number of delivery ports was studied in [17, 189]. injection limitation techniques to prevent
some recent proposals allow cyclic dependencies between channels, relying on some version of theorem 3.1 to guarantee deadlock freedom [95]. in general, virtual networks are deﬁned in such a way that the corresponding routing functions are deadlock-free. packet transfers between virtual networks are restricted in such a way that deadlocks are avoided. by allowing cyclic dependencies between channels and cyclic transfers between virtual networks, guaranteeing deadlock freedom is much more difﬁcult [95, 217]. however, virtual networks still have proven to be useful to deﬁne deadlock-free routing algorithms. see exercise 3.3 for an example.
dally and aoki proposed two adaptive routing algorithms based on the concept of dimension reversal [73]. the most interesting one is the dynamic algorithm. this routing algorithm allows the existence of cyclic dependencies between channels, as long as packets do not wait for channels in a cyclic way. before describing the dynamic algorithm, let us deﬁne the concept of dimension reversal. the dimension reversal (dr) number of a packet is the count of the number of times a packet has been routed from a channel in one dimension, p, to a channel in a lower dimension, q < p.
the dynamic algorithm divides the virtual channels of each physical channel into two nonempty classes: adaptive and deterministic. packets injected into the network are ﬁrst routed using adaptive channels. while in these channels, packets may be routed in any direction without a maximum limit on the number of dimension reversals a packet may make. whenever a packet acquires a channel, it labels the channel with its current dr number. to avoid deadlock, a packet with a dr of p cannot wait on a channel labeled with a dr of q if p ≥ q. a packet that reaches a node where all output channels are occupied by packets with equal or lower drs must switch to the deterministic class of virtual channels. once on the deterministic channels, the packet must be routed in dimension order using only the deterministic channels and cannot reenter the adaptive channels.
the dynamic algorithm represents a ﬁrst step toward relaxing the restrictions for deadlock avoidance. instead of requiring the absence of cyclic dependencies between channels as the algorithms presented in section 4.4.3, it allows those cyclic dependencies as long as packets do not wait for channels in a cyclic way. when a packet may produce a cyclic waiting, it is transferred to the deterministic class of virtual channels, where it is routed in dimension order. the dynamic algorithm provides the maximum routing ﬂexibility when packets are routed using adaptive channels. however, that ﬂexibility is lost when packets are transferred to the deterministic channels.
more routing ﬂexibility can be obtained by using theorem 3.1. fully adaptive routing algorithms described in sections 4.4.1, 4.4.2, and 4.4.3 do not allow cyclic dependencies between resources to avoid deadlocks. however, those algorithms require a large set of buffer resources. using theorem 3.1, it is possible to avoid deadlocks even when cyclic
mttr << mtbf, the number of faulty components in a repair interval is small. in fact, the probability of the second or the third fault occurring before the ﬁrst fault is repaired is very low. in such environments, software-based rerouting can be a cost-effective and viable alternative [333].
the software-based approach is based on the observation that the majority of messages do not encounter faults and should be minimally impacted, while the relatively few messages that do encounter faults may experience substantially increased latency, although the network throughput may not be signiﬁcantly affected. the basic idea is simple. when a message encounters a faulty link, it is removed from the network or absorbed by the local router and delivered to the messaging layer of the local node’s operating system. the message-passing software either (1) modiﬁes the header so that the message may follow an alternative path or (2) computes an intermediate node address. in either case, the message is reinjected into the network. in the case that the message is transmitted to an intermediate node, it will be forwarded upon receipt to the ﬁnal destination. a message may encounter multiple faults and pass through multiple intermediate nodes. the issues are distinct from adaptive packet routing in networks using packet switching or vct switching. since messages are routed using wormhole switching, rerouting algorithms and the selection of intermediate nodes must consider dependencies across multiple routers caused by small buffers (< message size) and pipelined data ﬂow. since messages are reinjected into the network, dependencies are introduced between delivery channels at a router and the injection channels. these dependencies are introduced via local storage for absorbed packets. memory allocation must ensure that sufﬁcient buffer space can be allocated within the nodes or interfaces to avoid creating deadlock due to the introduction of these dependencies.
since messages are removed from the network and reinjected, the problems of routing in the presence of concave fault regions are simpliﬁed. consider the case of messages being routed obliviously using e-cube routing in a 2-d torus. figure 6.28 illustrates an example of software-based rerouting in the presence of concave fault regions. the resulting routing algorithm has been referred to as e-sft [333]. in step 1 a message from the source is routed through an e-cube path to the destination and encounters the fault region at node a. the message is absorbed at the node, and the header is modiﬁed by the messaging layer to reﬂect the reverse path in the same dimension, using the wraparound channels. the message again encounters the fault region at node b and is routed to an intermediate node in the vertical direction in an attempt to ﬁnd a path around the fault region. this procedure is repeated at intermediate nodes e and f before the message is successfully routed along the wraparound channels in the vertical dimension to the destination node d. while the path and the sequence of routing decisions to ﬁnd the path in the preceding example are easy to convey intuitively, the messaging layer must implement routing algorithms that can implement this decision process in a fully distributed manner with only local information about faults. these rerouting decisions must avoid deadlock and livelock. this is realized through the use of routing tables and by incorporating additional information in the routing header to capture the history of encounters with fault regions by this message. the important characteristic of this approach is that messages are still routed in dimension order between any pair of intermediate nodes.
free routing algorithm, including oblivious e-cube routing and fully adaptive duato’s protocol (dp). the resulting fault-tolerant routing protocols are referred to as e-sft and dp-sft, respectively.
the performance of e-sft and dp-sft was evaluated with ﬂit-level simulation studies of message passing in a 16-ary 2-cube with 16-ﬂit messages, a single-ﬂit routing header, and eight virtual channels per physical channel. message destination trafﬁc was uniformly distributed. simulation runs were made repeatedly until the 95% conﬁdence intervals for the sample means were acceptable (less than 5% of the mean values). the simulation model was validated using deterministic communication patterns. we use a congestion control mechanism (similar to [36]) by placing a limit on the size of the buffer on the injection channels. if the input buffers are ﬁlled, messages cannot be injected into the network until a message in the buffer has been routed. injection rates are normalized.
the header format for e-sft and dp-sft was presented in figure 6.29. a 32-bit header enables routing within a 64 × 64 torus. the ﬂags require 6 bits, and the offsets require 6 bits each.
we assume the use of half-duplex channels. with half-duplex channels, 32-bit channels are comparable to the number of data pins devoted to interrouter communication in modern machines (e.g., there are two 16-bit full-duplex channels in the intel paragon router). a ﬂit crosses a channel in a single cycle and traverses a router from input to output in a single cycle. use of 16-bit full-duplex channels would double the number of cycles to transmit the routing header. routing decisions are assumed to take a single cycle with the network operating with a 50 mhz clock and 20 ns cycle time. the software cost for absorbing and reinjecting a message is derived from measured times on an intel paragon and reported work with active message implementations [105]. based on these studies, we assess this cost nominally at 25 µs per absorption/injection or 50 µs each time a message must be processed by the messaging software at an intermediate node. if the message encounters busy injection buffers when it is being reinjected, it is requeued for reinjection at a later time. absorbed messages have priority over new messages to prevent starvation.
the router hardware can retain the same basic architecture as the canonical form for dimension-order routers or adaptive routers presented in section 7.2.1, which captures the basic organization of several commercial and research designs. the only additional functionality required is of the routing and arbitration unit. when a message is routed to a faulty output channel, the f bit must be set and the x-dest and y -dest ﬁelds reset to 0 to cause the message to be absorbed. one side effect of the increased header size is a possible increase in virtual channel buffer size and the width of the internal data paths, although 32-bit data paths appear to be reasonable for the next generation of routers. finally, as with any approach for fault-tolerant routing, support for testing of channels and nodes must be available. the remaining required functionality of e-sft and dp-sft is implemented in the messaging layer software.
any of the edges; (b) destination is in the upper edge and the message encounters one fault; (c) destination is in the upper edge and the message encounters two faults; (d) destination is in the upper-left corner; (e) destination is in the right edge; and (f) destination is in the upper-right corner.
as the eastward virtual network. the second virtual network also consists of one virtual channel in each direction. it is used to send packets toward the west (x-negative), and it will be referred to as the westward virtual network. when the x coordinates of the source and destination nodes are equal, packets can be introduced in either virtual network. the routing algorithms for eastward and westward virtual networks are based on nonminimal west-last and east-last, respectively. once a packet has used a west (east) channel in the eastward (westward) virtual network, it cannot turn again. however, 180-degree turns are allowed in y channels except in the east (west) edge of the mesh. this routing algorithm is fully adaptive nonminimal. it is deadlock-free, as shown in exercise 3.3 for 2-d meshes. while routing in the eastward (westward) virtual network, it tolerates faults except when routing on the east (west) edge. in order to tolerate faults in the edges, two extra virtual networks are used. the extra eastward (westward) virtual network is used when a fault is met while routing on the west (east) edge of the mesh. the resulting routing algorithm is very resilient to faults under both the node failure and the link failure models [93, 95]. figure 6.33 shows the paths followed when some channels fail. figure 6.34 shows the paths followed when some nodes fail. this approach also supports rectangular fault regions.
same message to a speciﬁc set of destinations without requiring assistance from any other processor. the approaches used to implement such functionality are highly dependent on the network topology and may affect the design of the switching strategy used in the network. before studying those approaches, some schemes to encode multiple destination addresses are described in the next section.
the header of multidestination messages must carry the addresses of the destination nodes. the header information is an overhead to the system, increasing message latency and reducing the effective network bandwidth. a good multiaddress encoding scheme should minimize the message header length, also reducing the header processing time.
in wormhole and vct switching, the routing algorithm is executed before the whole message arrives at the router. as the header may require several ﬂits to encode the destination addresses, it is desirable that the routing decision in each router could be made as soon as possible to reduce message latency. ideally, a message header should be processed on the ﬂy as header ﬂits arrive. when the number of destination addresses is variable, it is inefﬁcient to use a counter to indicate the number of destinations. such a counter should be placed at the beginning of a message header. since the value of the counter may be changed at a router if the destination set is split into several subsets, it would prevent the processing of message headers on the ﬂy. an alternative approach is to have an end-of-header (eoh) ﬂit to indicate the end of a header. another approach consists of using 1 bit in each ﬂit to distinguish between header and data ﬂits.
figure 5.9 shows ﬁve different encoding schemes, namely, all-destination encoding, bit string encoding, multiple-region broadcast encoding, multiple-region stride encoding, and multiple-region bit string encoding. these schemes were proposed in [54].
the all-destination encoding is a simple scheme in which all destination addresses are carried by the header. this encoding scheme has two important advantages. first, the same routing hardware used for unicast messages can be used for multidestination messages. second, the message header can be processed on the ﬂy as address ﬂits arrive. this scheme is good for a small number of addresses because the header length is proportional to the number of addresses. however, it produces a signiﬁcant overhead when the number of destinations is large.
one way to limit the size of the header is to encode destination addresses as a bit string, where each bit corresponds to a destination ranged between node b and node e, as shown in figure 5.9(b). since the length of the string in a system is predeﬁned, the eoh ﬁeld is not required. this encoding scheme is good when the average number of destinations is large. however, it is inefﬁcient when the system is large and the number of destinations is small. the main drawback of the bit string encoding scheme is that a router usually has to buffer the entire bit string in order to make the routing decision and to generate the output bit string(s). additionally, address decoding cannot be done with the same routing hardware as for unicast messages. finally, the length of the string usually depends on network size, limiting the scalability of the system.
consider two unrelated channels ci , cj that have some common predecessors and successors in the ordering. we can say that they are at the same level in the ordering or that they are equivalent. this equivalence relation groups equivalent channels so that they form an equivalence class [89]. note that there is no dependency between channels belonging to the same class. now, it is possible to deﬁne the concept of class dependency. there is a dependency from class ki to class kj if there exist two channels ci ∈ ki and cj ∈ kj such that there is a dependency from ci to cj . we can represent class dependencies by means of a graph. if the classes contain all the channels, we deﬁne the class dependency graph. if we restrict our attention to classes formed by the set of channels c1 supplied by a routing subfunction r1, we deﬁne the extended class dependency graph [89].
a connected routing function r for an interconnection network i is deadlockfree if there exist a connected routing subfunction r1 and an equivalence relation ∀x,y∈n r1(x, y), such that the extended class dependency
the existence of a dependency between two classes does not imply the existence of a dependency between every pair of channels in those classes. thus, the existence of cycles in the extended class dependency graph does not imply that the extended channel dependency graph has cycles. as a consequence, theorem 3.3 only supplies a sufﬁcient condition. however, this is enough to prove deadlock freedom.
it is not always possible to establish an equivalence relation between channels. however, when possible, it considerably simpliﬁes the analysis of deadlock freedom. the channels that can be grouped into the same class usually have some common topological properties, for instance, channels crossing the same dimension of a hypercube in the same direction. classes are not a property of the topology alone. they also depend on the routing function. the next example shows the deﬁnition of equivalence classes in a 2-d mesh.
consider the extended channel dependency graph drawn in figure 3.10. there is not any dependency between channels b01, b34, and b67. however, there are dependencies from b01, b34, and b67 to b12, b45, and b78. channels b01, b34, and b67 can be grouped into the same class. the same can be done with channels b12, b45, and b78. suppose that we deﬁne classes such that all the horizontal (vertical) channels in the same column (row) and direction are in the same class. it is easy to see that there is not any dependency between channels belonging to the same class. let us denote the classes containing east (x-positive) channels as x0+, x1+, starting from the left. for instance, x0+ = {b01, b34, b67}. similarly, classes containing west channels are denoted as x0−, x1−, starting from the right.also, classes containing north channels are denoted as y 0+, y 1+, starting from the bottom, and classes containing south
consider two unrelated channels ci , cj that have some common predecessors and successors in the ordering. we can say that they are at the same level in the ordering or that they are equivalent. this equivalence relation groups equivalent channels so that they form an equivalence class [89]. note that there is no dependency between channels belonging to the same class. now, it is possible to deﬁne the concept of class dependency. there is a dependency from class ki to class kj if there exist two channels ci ∈ ki and cj ∈ kj such that there is a dependency from ci to cj . we can represent class dependencies by means of a graph. if the classes contain all the channels, we deﬁne the class dependency graph. if we restrict our attention to classes formed by the set of channels c1 supplied by a routing subfunction r1, we deﬁne the extended class dependency graph [89].
a connected routing function r for an interconnection network i is deadlockfree if there exist a connected routing subfunction r1 and an equivalence relation ∀x,y∈n r1(x, y), such that the extended class dependency
the existence of a dependency between two classes does not imply the existence of a dependency between every pair of channels in those classes. thus, the existence of cycles in the extended class dependency graph does not imply that the extended channel dependency graph has cycles. as a consequence, theorem 3.3 only supplies a sufﬁcient condition. however, this is enough to prove deadlock freedom.
it is not always possible to establish an equivalence relation between channels. however, when possible, it considerably simpliﬁes the analysis of deadlock freedom. the channels that can be grouped into the same class usually have some common topological properties, for instance, channels crossing the same dimension of a hypercube in the same direction. classes are not a property of the topology alone. they also depend on the routing function. the next example shows the deﬁnition of equivalence classes in a 2-d mesh.
consider the extended channel dependency graph drawn in figure 3.10. there is not any dependency between channels b01, b34, and b67. however, there are dependencies from b01, b34, and b67 to b12, b45, and b78. channels b01, b34, and b67 can be grouped into the same class. the same can be done with channels b12, b45, and b78. suppose that we deﬁne classes such that all the horizontal (vertical) channels in the same column (row) and direction are in the same class. it is easy to see that there is not any dependency between channels belonging to the same class. let us denote the classes containing east (x-positive) channels as x0+, x1+, starting from the left. for instance, x0+ = {b01, b34, b67}. similarly, classes containing west channels are denoted as x0−, x1−, starting from the right.also, classes containing north channels are denoted as y 0+, y 1+, starting from the bottom, and classes containing south
at node n2 destined for n0 can reserve ca2 and then request ca3. finally, a packet at node n3 destined for n1 can reserve ca3 and then request ca0 and ch 0.
however, the routing function is deadlock-free. although we focus on wormhole switching, the following analysis is also valid for other switching techniques. let us show that there is not any deadlocked conﬁguration by trying to build one. if there were a packet stored in the queue of channel ch 2, it would be destined for n3 and ﬂits could advance. so, ch 2 must be empty. also, if there were a packet stored in the queue of ch 1, it would be destined for n2 or n3. as ch 2 is empty, ﬂits could also advance and ch 1 must be empty. if there were ﬂits stored in the queue of ch 0, they would be destined for n1, n2, or n3. even if their header were stored in ca1 or ca2, as ch 1 and ch 2 are empty, ﬂits could advance and ch 0 must be empty.
thus, any deadlocked conﬁguration can only use channels cai. although there is a cyclic dependency between them, ca0 cannot contain ﬂits destined for n0. that conﬁguration would not be legal because n0 cannot forward packets for itself through the network. for any other destination, those ﬂits can advance because ch 1 and ch 2 are empty. again, ca0 can be emptied, thus breaking the cyclic dependency. thus, the routing function is deadlock-free.
example 3.3 shows that deadlocks can be avoided even if there are cyclic dependencies between some channels. obviously, if there were cyclic dependencies between all the channels in the network, there would be no path to escape from cycles. thus, the key idea consists of providing a path free of cyclic dependencies to escape from cycles. that path can be considered as an escape path. note that at least one packet from each cycle should be able to select the escape path at the current node, whichever its destination is. in example 3.3, for every legal conﬁguration, a packet whose header ﬂit is stored in channel ca0 must be destined for either n1, n2, or n3. in the ﬁrst case, it can be immediately delivered. in the other cases, it can use channel ch 1.
it seems that we could focus only on the escape paths and forget about the other channels to prove deadlock freedom. in order to do so, we can restrict a routing function in such a way that it only supplies channels belonging to the escape paths as routing choices. in other words, if a routing function supplies a given set of channels to route a packet from the current node toward its destination, the restricted routing function will supply a subset of those channels. the restricted routing function will be referred to as a routing subfunction. formally, if r is a routing function and r1 is a routing subfunction of r, we have
channels supplied by r1 for a given packet destination will be referred to as escape channels for that packet. note that the routing subfunction is only a mathematical tool to prove deadlock freedom. packets can be routed by using all the channels supplied by the routing function r. simply, the concept of a routing subfunction will allow us to focus on
trafﬁc decreases considerably. further increments of the applied load do not modify accepted trafﬁc. simply, injection buffers at source nodes grow continuously.also, average message latency increases by an order of magnitude when the saturation point is reached, remaining constant as applied load increases further. note that latency does not include source queuing time.
this behavior typically arises when the routing algorithm allows cyclic dependencies between resources. as indicated in chapter 3, deadlocks are avoided by using a subset of channels without cyclic dependencies between them to escape from cyclic waiting chains (escape channels). the bandwidth provided by the escape channels should be high enough
spread across multiple routers when the header ﬂit blocks. thus, link-level monitoring and retransmission are difﬁcult to realize on a message packet basis. the reliable router developed at the massachussets institute of technology [74, 75] has implemented a novel approach utilizing a ﬂit-level copying and forwarding algorithm to solve this problem and realize exactly-once injection and subsequent delivery of all packets in wormhole-switched networks.
networks of reliable routers utilize three virtual networks. five virtual channels are provided across each physical channel. two of these virtual channels are used to implement an adaptive network where messages may utilize any outgoing channel from a router. two more virtual channels are utilized to implement a dimension-ordered network where messages traverse links in dimension order. the two sets of dimension-order channels are used by packets at two priority levels. the two priorities are used with distinct resources to prevent software deadlocks. the remaining virtual channel is used as part of a network that implements the turn model for fault-tolerant routing. the adaptive and dimensionordered networks together implement dp (see section 4.4.4) for fully adaptive routing. when a message cannot acquire an adaptive channel, it attempts the dimension-order channel. if this channel is also busy, the message blocks for one cycle, and then ﬁrst attempts to route along the adaptive channel. when a message can no longer progress due to a fault on the outgoing dimension-ordered link, the message employs nonminimal turn model-style routing in the fault-handling network to route around the fault (except for 180-degree turns). at the adjacent router, the message may transition back to the adaptive and dimension-ordered networks, or may remain in the fault-handling network until it is delivered to the destination. this choice depends on the dimension across which the failed channel was encountered. if the packet traversed a nonminimal link in the y direction, this dimension traversal can be subsequently corrected without violating the x-y ordering in the dimension-order virtual network. however, nonminimal traversals in the x dimension result in the packet remaining in the fault-handling network until it has reached its destination. this restriction is necessary to ensure deadlock freedom. based on this overall routing strategy, dynamic faults are handled as described below.
link-level monitoring and retransmission are used to guarantee the successful progress of data ﬂits across links and between routers. this implies that a copy of each data ﬂit is maintained by the sending router until it can be asserted that the ﬂit was received error-free. the ﬂit-level ﬂow control ensures that there always exist exactly two copies of every ﬂit in the network at all times. thus, when a data ﬂit successfully advances, ﬂow control information must travel in the reverse direction to free the oldest copy of the ﬂit. if a link fails, the message can now be partitioned into two messages. data ﬂits on the header side of the fault continue toward the destination. data ﬂits on the other side of the failure must construct a new header and reroute this new message along an alternative path. this requires that copies of the header ﬂit exist in each router containing a data ﬂit of the message. finally a special token deﬁnes the end of the message. the ﬂit-level ﬂow control and copying is illustrated in figure 6.40.
each message is comprised of four components: a header, tail, data, and the token signifying the end of the message. figure 6.40(a) shows the state of a message just prior to failure of the link. now assume that a link failure has been detected. figure 6.40(b)
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
an extended channel dependency graph de for a given interconnection network i and routing subfunction r1 of a routing function r is a directed graph, de = g(c1, ee). the vertices of de are the channels supplied by the routing subfunction r1 for some destinations. the arcs of de are the pairs of channels (ci , cj ) such that there is either a direct, indirect, direct cross-, or indirect crossdependency from ci to cj .
a routing function r for a given interconnection network i is coherent iff for every pair of nodes x, y ∈ n, x (cid:16)= y, and for every path p (x, y) = {c1, c2, . . . , ck} that can be established by r between them
that is, for every path p that can be established by r, every preﬁx of p is also a path of r. in other words, if a routing function r can establish a path p (x, y) between x and y, it can also establish a path between x and any intermediate node crossed by p (x, y) using a subset of the channels used by p (x, y). if the routing function r is coherent, there is not any loop (1-cycle) in the extended channel dependency graph of any routing subfunction. otherwise, there would be a destination node for which a message is allowed to cross a channel ci (and its source node si) twice. taking into account assumption 2, the subpath starting and ending at si is not allowed. thus, that routing function would not be coherent.
a connected and adaptive routing function r for an interconnection network i is deadlock-free if there exists a routing subfunction r1 that is connected and has no cycles in its extended channel dependency graph de.
a coherent, connected, and adaptive routing function r for an interconnection network i is deadlock-free iff there exists a routing subfunction r1 that is connected and has no cycles in its extended channel dependency graph de.
consider two unrelated channels ci , cj that have some common predecessors and successors in the ordering. we can say that they are at the same level in the ordering or that they are equivalent. this equivalence relation groups equivalent channels so that they form an equivalence class [89]. note that there is no dependency between channels belonging to the same class. now, it is possible to deﬁne the concept of class dependency. there is a dependency from class ki to class kj if there exist two channels ci ∈ ki and cj ∈ kj such that there is a dependency from ci to cj . we can represent class dependencies by means of a graph. if the classes contain all the channels, we deﬁne the class dependency graph. if we restrict our attention to classes formed by the set of channels c1 supplied by a routing subfunction r1, we deﬁne the extended class dependency graph [89].
a connected routing function r for an interconnection network i is deadlockfree if there exist a connected routing subfunction r1 and an equivalence relation ∀x,y∈n r1(x, y), such that the extended class dependency
the existence of a dependency between two classes does not imply the existence of a dependency between every pair of channels in those classes. thus, the existence of cycles in the extended class dependency graph does not imply that the extended channel dependency graph has cycles. as a consequence, theorem 3.3 only supplies a sufﬁcient condition. however, this is enough to prove deadlock freedom.
it is not always possible to establish an equivalence relation between channels. however, when possible, it considerably simpliﬁes the analysis of deadlock freedom. the channels that can be grouped into the same class usually have some common topological properties, for instance, channels crossing the same dimension of a hypercube in the same direction. classes are not a property of the topology alone. they also depend on the routing function. the next example shows the deﬁnition of equivalence classes in a 2-d mesh.
consider the extended channel dependency graph drawn in figure 3.10. there is not any dependency between channels b01, b34, and b67. however, there are dependencies from b01, b34, and b67 to b12, b45, and b78. channels b01, b34, and b67 can be grouped into the same class. the same can be done with channels b12, b45, and b78. suppose that we deﬁne classes such that all the horizontal (vertical) channels in the same column (row) and direction are in the same class. it is easy to see that there is not any dependency between channels belonging to the same class. let us denote the classes containing east (x-positive) channels as x0+, x1+, starting from the left. for instance, x0+ = {b01, b34, b67}. similarly, classes containing west channels are denoted as x0−, x1−, starting from the right.also, classes containing north channels are denoted as y 0+, y 1+, starting from the bottom, and classes containing south
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
from the point of view of router performance we are interested in two parameters [57]. when a message ﬁrst arrives at a router, it must be examined to determine the output channel over which the message is to be forwarded. this is referred to as the routing delay and typically includes the time to set the switch. once a path has been established through a router by the switch, we are interested in the rate at which messages can be forwarded through the switch. this rate is determined by the propagation delay through the switch (intrarouter delay) and the signaling rate for synchronizing the transfer of data between the input and output buffers. this delay has been characterized as the internal ﬂow control latency [57]. similarly, the delay across the physical links (interrouter delay) is referred to as the external ﬂow control latency. the routing delay and ﬂow control delays collectively determine the achievable message latency through the switch and, along with contention by messages for links, determine the network throughput.
the following section addresses some basic concepts in the implementation of the switching layer, assuming the generic router model shown in figure 2.1. the remainder of the chapter focuses on alternative implementations of the switching layer.
switching layers can be distinguished by the implementation and relative timing of ﬂow control operations and switching techniques. in addition, these operations may be overlapped with the time to make routing decisions.
each virtual network implementation is well represented by the canonical organization shown in figure 7.13. a 3 × 3 nonblocking switch (reference the multistage topologies in chapter 1) can be implemented with four 2 × 2 switches. using the partitioned design in figure 7.13, the four crossbar switches can be implemented using twelve 2 × 2 switching elements, rather than having a 9 × 9 full crossbar switch with 11-bit channels at each port. this represents an area savings of approximately 95%.
the ﬁrst two ﬂits of the message header contain the offsets in the x and y direction, respectively. the routing units perform sign and zero checks to determine if the message is to continue in the same dimension, transition to the next dimension, or be ejected. output link controllers perform arbitration between the two virtual channels to grant access to the physical link. if the router does not have access to the bidirectional channel, there is a delay before the token is requested and transferred from the other side of the channel.
these wormhole routers exploit the routing restrictions of dimension-order routing to realize efﬁcient router architectures. if adaptive routing is permitted, the internal router architectures begin to become more complex, especially as the number of dimensions and the number of virtual channels increase. this is simply a direct result of the increased routing ﬂexibility. however, the idea of using partitioned data paths to reduce the implementation complexity can still be applied if adaptivity can be controlled. the advantages of the partitioned data path found in dimension-ordered routers can be combined with limited adaptive routing to achieve a more balanced, less complex design. the planar-adaptive routing technique described in chapter 4 is an attempt to balance hardware complexity with potential advantages of increased routing freedom. the basic strategy is to adaptively route in a sequence of adaptive planes. the block diagram of a 2-d router is shown in figure 7.14 [11]. note how the data path is partitioned. each crossbar provides the switching for the increasing or decreasing network. messages only travel in one or the other. higher-dimensional networks can be supported by providing the same basic organization in each successive pair of dimensions. this technique can also be generalized to support higher degrees of adaptivity by increasing the number of dimensions within which adaptive routing is permitted. figure 7.14 illustrates the organization for adaptive routing in two dimensions at a time. alternatives may permit adaptive routing in three dimensions at a time. a 5-d network may permit adaptive routing in successive 3-d cubes. these routers are referred to as f -ﬂat routers, where the parameter f signiﬁes the number of dimensions within which adaptive routing is permitted at any one time. aoyama and chien [11] use this baseline design to study the performance impact of adaptive routing via the design of f -ﬂat routers. while performance may be expected to improve with increasing adaptivity, the hardware complexity certainly grows with increasing f .
this idea of using partitioned data paths to reduce internal router complexity can be used in fully adaptive routers as well, in order to offset the competing factors of increased adaptivity and low router delay. the internal router architecture can be designed to fully exploit the expected routing behavior as well as the capabilities of the underlying routing algorithm to achieve the highest possible performance. for instance, if most packets tend not to change dimensions or virtual channel classes frequently during routing (i.e., routing locality in dimension or in virtual channel class), then it is not necessary for the internal data path to provide packets with direct access to all output virtual channels, even in fully
request_for_data(src_vnn, dest_vnn) {   request_complete = 0;   am_request_4(src_vnn,request_handler, &arg0);   while (request_complete = 0)     am_poll(); }
figure 8.6 illustrates the logical ﬂow of information through handlers in an implementation of the active message paradigm. process 0 makes a request for remote data. the source handler initializes a completion ﬂag (request complete) and calls the active message procedure am request 4(). the source now polls waiting for completion of the remote read. the message contains the name of the handler to be invoked by the destination process on message reception. this handler responds to the request message with a read operation to return the data value. this handler runs to completion. note that no buffering is necessary at this point. similarly, reception of the message at the host invokes the reply handler, which assigns the message contents to a local memory location. buffering and copying of message contents are avoided. the request handler within the destination process is only permitted access to the network to reply to messages, while the reply handler at the source process is prevented from accessing the network. this helps prevent cyclic dependencies and certain deadlock situations.
a second example of a streamlined messaging layer implementation is the fast messages (fm) library from the university of illinois [264]. the fm procedures are similar to active messages in that a handler is speciﬁed within the message. however, there is no notion of request-reply message pairs. there are also no restrictions on the actions that a handler can take. thus, program properties such as deadlock freedom are the responsibility of,
the destination node. in the worst case, establishing a path in an n-stage bmin requires crossing 2n − 1 stages. this behavior closely resembles that of the bene˘s network. indeed, the baseline bmin can be considered as a folded bene˘s network.
as shown in figure 1.23, a butterﬂy bmin with turnaround routing can be viewed as a fat tree [201]. in a fat tree, processors are located at leaves, and internal vertices are switches. transmission bandwidth between switches is increased by adding more links in parallel as switches become closer to the root switch. when a message is routed from one processor to another, it is sent up (in the forward direction) the tree to the least common ancestor of the two processors, and then sent down (in the backward direction) to the destination. such a tree routing well explains the turnaround routing mentioned above.
myricom myrinet: supports regular and irregular topologies, 8 × 8 crossbar switch, 9-bit channels, full-duplex, 160 mbytes/s per link. thinking machines cm-5: fat tree topology, 4-bit bidirectional channels at 40 mhz, aggregate bandwidth in each direction of 20 mbytes/s. inmos c104: supports regular and irregular topologies, 32 × 32 crossbar switch, serial links, 100 mbits/s per link. ibm sp2: crossbar switches supporting omega network topologies with bidirectional, 16-bit channels at 150mhz, 300 mbytes/s in each direction. sgi spider: router with 20-bit bidirectional channels operating on both edges, 200 mhz clock, aggregate raw data rate of 1 gbyte/s. offers support for conﬁgurations as nonblocking multistage network topologies as well as irregular topologies. tandem servernet: irregular topologies, 8-bit bidirectional channels at 50 mhz, 50 mbytes/s per link.
in this section, we brieﬂy describe some network topologies that do not ﬁt into the classes described above. in general, hybrid networks combine mechanisms from shared-medium networks and direct or indirect networks. therefore, they increase bandwidth with respect to shared-medium networks and reduce the distance between nodes with respect to direct and indirect networks. there exist well-established applications of hybrid networks. this is the case for bridged lans. however, for systems requiring very high performance, direct and indirect networks achieve better scalability than hybrid networks because point-topoint links are simpler and faster than shared-medium buses. most high-performance
one such approach to reduce the number of functional nodes that must be marked as faulty was introduced by chalasani and boppana [49] and builds on the concept of fault rings to support more ﬂexible routing around fault regions. intuitively a fault ring is the sequence of links and nodes that are adjacent to and surround a fault region. rectangular fault regions will produce rectangular fault rings. figure 6.19 shows an example of two fault rings and a fault chain. if a fault region includes boundary nodes, the fault ring reduces to a fault chain. the key idea is to have messages that are blocked by a fault region misrouted along the fault rings and fault chains. for oblivious routing in 2-d meshes with nonoverlapping fault rings, and with no fault chains, two virtual channels sufﬁce for deadlock-free routing around fault rings. these two channels form two virtual networks c0 and c1, each with acyclic channel dependencies, and messages travel in only one network (see exercise 6.1).
however, this solution is inadequate in the presence of overlapping fault rings and fault chains. the reason is that the two virtual networks are no longer sufﬁcient to remove cyclic dependencies between channels. messages traveling in distinct fault rings share virtual channels as shown in figure 6.19. as a result additional dependencies are introduced between the two virtual networks, leading to cycles. furthermore, the occurrence of fault chains may cause messages that are initially routed toward the boundary nodes to be reversed. if we employed fault-tolerant planar-adaptive routing, such occurrences would introduce dependencies between the d1,0+ and d1,0− virtual channels in dimension 1, creating cyclic dependencies between them.
the solution proposed by chalasani and boppana [49] is to provide additional virtual channels to create disjoint acyclic virtual networks. the effect is to separate the trafﬁc of the two fault rings that traverse the shared links. this can be achieved by providing two
hardware support. the occurrence of faults during this update period necessitates complex synchronization protocols. furthermore, the increased storage and computation time for globally optimal routing decisions have a signiﬁcant impact on performance. at the other extreme, fault information is limited to the status of adjacent nodes. with only local fault information, routing decisions are relatively simple and can be computed quickly. also, updating the fault information of neighboring nodes can be performed simply. however, messages may be forwarded to a portion of the network with faulty components, ultimately leading to longer paths. in practice, routing algorithm design is typically a compromise between purely local and purely global fault status information. figure 6.4 shows an example of the utility of having nonlocal fault information. the dashed line shows the path that would have been taken by a message from s to d. the solid line represents a path that the message could have been routed along had knowledge of the location of the faulty nodes been made available at the source. finally, information aggregated at a node may be more than a simple list of faulty links and nodes in the network or local neighborhood. for example, status information at a node may indicate the existence of a path(s) to all nodes at a distance of k links. such a representation of node status implicitly encodes the location and distribution of faults. this is in contrast to an explicit list of faulty nodes or links within a k-neighborhood. computation of such properties is a global operation, and we must be concerned about handling faults during that computation interval.
developing algorithms for acquiring and maintaining fault information within a neighborhood is the problem of fault diagnosis. this is an important problem since diagnosis algorithms executing on-line consume network bandwidth and must provide timely updates to be useful. the notions of diagnosis within network fault models have been formalized in [27, 28]. in k-neighborhood diagnosis, each node records the status
theorem 6.1 is only applicable to physical channels if they are not split into virtual channels. if virtual channels are used, the theorem is only valid for virtual channels. however, it can be easily extended to support the failure of physical channels by considering that all the virtual channels belonging to a faulty physical channel will become faulty at the same time. theorem 6.1 is based on theorem 3.1. therefore, it is valid for the same switching techniques as theorem 3.1, as long as edge buffers are used.
the structure of fault-tolerant routing algorithms is a natural consequence of the types of faults that can occur and our ability to diagnose them. the patterns of component failures and expectations about the behavior of processors and routers in the presence of these failures determines the approaches to achieve deadlock and livelock freedom. this information is captured in the fault model. the fault-tolerant computing literature is extensive and thorough in the deﬁnition of fault models for the treatment of faulty digital systems. in this section we will focus on common fault models that have been employed in the design of fault-tolerant routing algorithms for reliable interprocessor networks.
one of the ﬁrst considerations is the level at which components are diagnosed as having failed. typically, detection mechanisms are assumed to have identiﬁed one of two classes of faults. either the entire processing element (pe) and its associated router can fail, or any communication channel may fail. the former is referred to as a node failure, and the latter as a link failure. on a node failure, all physical links incident on the failed node are also marked faulty at adjacent routers. when a physical link fails, all virtual channels on that particular physical link are marked faulty. note that many types of failures will simply manifest themselves as link or node failures. for example, the failure of the link controller, or the virtual channel buffers, appears as a link failure. on the other hand, the failure of the router control unit or the associated pe effectively appears as a node failure. even software errors in the messaging layer can lead to message handlers “locking up” the local interface and rendering the attached router inoperative, effectively resulting in a node fault. hence, this failure model is not as restrictive as it may ﬁrst appear.
this model of individual link and node failures leads to patterns of failed components. adjacent faulty links and faulty nodes are coalesced into fault regions. generally, it is assumed that fault regions do not disconnect the network, since each connected network component can be treated as a distinct network. constraints may now be placed on the structure of these fault regions. the most common constraint employed is that these regions be convex. as will become apparent in this chapter, concave regions present unique difﬁculties for fault-tolerant routing algorithms. some examples of fault regions are illustrated in figure 6.3. convex regions may be further constrained to be block fault regions—regions whose shape is rectangular. this distinction is meaningful only in some topologies, whereas in other topologies, convex faults imply a block structure. given a pattern of random faults in a multidimensional k-ary n-cube, rectangular fault regions can be constructed by marking some functioning nodes as faulty to ﬁll out the fault regions.
theorem 6.1 is only applicable to physical channels if they are not split into virtual channels. if virtual channels are used, the theorem is only valid for virtual channels. however, it can be easily extended to support the failure of physical channels by considering that all the virtual channels belonging to a faulty physical channel will become faulty at the same time. theorem 6.1 is based on theorem 3.1. therefore, it is valid for the same switching techniques as theorem 3.1, as long as edge buffers are used.
the structure of fault-tolerant routing algorithms is a natural consequence of the types of faults that can occur and our ability to diagnose them. the patterns of component failures and expectations about the behavior of processors and routers in the presence of these failures determines the approaches to achieve deadlock and livelock freedom. this information is captured in the fault model. the fault-tolerant computing literature is extensive and thorough in the deﬁnition of fault models for the treatment of faulty digital systems. in this section we will focus on common fault models that have been employed in the design of fault-tolerant routing algorithms for reliable interprocessor networks.
one of the ﬁrst considerations is the level at which components are diagnosed as having failed. typically, detection mechanisms are assumed to have identiﬁed one of two classes of faults. either the entire processing element (pe) and its associated router can fail, or any communication channel may fail. the former is referred to as a node failure, and the latter as a link failure. on a node failure, all physical links incident on the failed node are also marked faulty at adjacent routers. when a physical link fails, all virtual channels on that particular physical link are marked faulty. note that many types of failures will simply manifest themselves as link or node failures. for example, the failure of the link controller, or the virtual channel buffers, appears as a link failure. on the other hand, the failure of the router control unit or the associated pe effectively appears as a node failure. even software errors in the messaging layer can lead to message handlers “locking up” the local interface and rendering the attached router inoperative, effectively resulting in a node fault. hence, this failure model is not as restrictive as it may ﬁrst appear.
this model of individual link and node failures leads to patterns of failed components. adjacent faulty links and faulty nodes are coalesced into fault regions. generally, it is assumed that fault regions do not disconnect the network, since each connected network component can be treated as a distinct network. constraints may now be placed on the structure of these fault regions. the most common constraint employed is that these regions be convex. as will become apparent in this chapter, concave regions present unique difﬁculties for fault-tolerant routing algorithms. some examples of fault regions are illustrated in figure 6.3. convex regions may be further constrained to be block fault regions—regions whose shape is rectangular. this distinction is meaningful only in some topologies, whereas in other topologies, convex faults imply a block structure. given a pattern of random faults in a multidimensional k-ary n-cube, rectangular fault regions can be constructed by marking some functioning nodes as faulty to ﬁll out the fault regions.
one such approach to reduce the number of functional nodes that must be marked as faulty was introduced by chalasani and boppana [49] and builds on the concept of fault rings to support more ﬂexible routing around fault regions. intuitively a fault ring is the sequence of links and nodes that are adjacent to and surround a fault region. rectangular fault regions will produce rectangular fault rings. figure 6.19 shows an example of two fault rings and a fault chain. if a fault region includes boundary nodes, the fault ring reduces to a fault chain. the key idea is to have messages that are blocked by a fault region misrouted along the fault rings and fault chains. for oblivious routing in 2-d meshes with nonoverlapping fault rings, and with no fault chains, two virtual channels sufﬁce for deadlock-free routing around fault rings. these two channels form two virtual networks c0 and c1, each with acyclic channel dependencies, and messages travel in only one network (see exercise 6.1).
however, this solution is inadequate in the presence of overlapping fault rings and fault chains. the reason is that the two virtual networks are no longer sufﬁcient to remove cyclic dependencies between channels. messages traveling in distinct fault rings share virtual channels as shown in figure 6.19. as a result additional dependencies are introduced between the two virtual networks, leading to cycles. furthermore, the occurrence of fault chains may cause messages that are initially routed toward the boundary nodes to be reversed. if we employed fault-tolerant planar-adaptive routing, such occurrences would introduce dependencies between the d1,0+ and d1,0− virtual channels in dimension 1, creating cyclic dependencies between them.
the solution proposed by chalasani and boppana [49] is to provide additional virtual channels to create disjoint acyclic virtual networks. the effect is to separate the trafﬁc of the two fault rings that traverse the shared links. this can be achieved by providing two
in this chapter we study routing algorithms. routing algorithms establish the path followed by each message or packet. the list of routing algorithms proposed in the literature is almost endless. we clearly cannot do justice to all of these algorithms developed to meet many distinct requirements. we will focus on a representative set of approaches, being biased toward those being used or proposed in modern and future multiprocessor interconnects. thus, we hope to equip you with an understanding of the basic principles that can be used to study the spectrum of existing algorithms. routing algorithms for wormhole switching are also valid for other switching techniques. thus, unless explicitly stated, the routing algorithms presented in this chapter are valid for all the switching techniques. speciﬁc proposals for some switching techniques will also be presented. special emphasis is given to design methodologies because they provide a simple and structured way to design a wide variety of routing algorithms for different topologies.
deadlock and livelock freedom. ability to guarantee that packets will not block or wander across the network forever. this issue was discussed in depth in chapter 3.
fault tolerance. ability to route packets in the presence of faulty components. although it seems that fault tolerance implies adaptivity, this is not necessarily true. fault tolerance can be achieved without adaptivity by routing a packet in two or more phases, storing it in some intermediate nodes. fault tolerance also requires some additional hardware mechanisms, as will be detailed in chapter 6.
a routing algorithm is said to be f fault recoverable if, for any f failed components in the network, a message that is undeliverable will not hold network resources indeﬁnitely. if a network is fault recoverable, the faults will not induce deadlock.
ideally, we would like the network to be f fault tolerant for large values of f . practically, however, we may be satisﬁed with a system that is f fault recoverable for large values of f and f1 fault tolerant for f1 << f , so that only the functionality of a small part of the network suffers as a result of a failed component. certainly, we want to avoid the situation where a few faults may cause a catastrophic failure of the entire network, or equivalently deadlocked conﬁgurations of messages. finally, the next deﬁnition gives the redundancy level of the network.
a routing algorithm has a redundancy level equal to r iff, after removing any set of r channels, the routing function remains connected and deadlock-free, and there exists a set of r + 1 channels such that, after removing them, the routing function is no longer connected or it is not deadlock-free.
note that an algorithm has a redundancy level equal to r iff it is r fault tolerant and it is not r + 1 fault tolerant. the analysis of the redundancy level of the network is complex. fortunately, there are some theoretical results that guarantee the absence of deadlock in the whole network by analyzing the behavior of the routing function r in a subset of channels c1 (see section 3.1.3). additionally, that behavior does not change when some channels not belonging to c1 are removed. that theory can be used to guarantee the absence of deadlock when some channels fail.
suppose that there exists a routing subfunction r1 that satisﬁes the conditions of theorem 3.1. let c1 be the subset of channels supplied by r1. now, let us consider the effects of removing some channels from the network. it is easy to see that removing channels not belonging to c1 does not add indirect dependencies between the channels belonging to c1. it may actually remove indirect dependencies. in fact, when all the channels not belonging to c1 are removed, there are no indirect dependencies between the channels belonging to c1. therefore, removing channels not belonging to c1 does not introduce cycles in the extended channel dependency graph of r1. however, removing channels may disconnect the routing function. fortunately, if a routing function is connected when it is restricted to use the channels in c1, it will remain connected when some channels not belonging to c1 are removed. thus, if a routing function r satisﬁes the conditions proposed by theorem 3.1, it will still satisfy them after removing some or all of the channels not belonging to c1. in other words, it will remain connected and deadlock-free. so, we can conclude that all the channels not belonging to c1 are redundant. we can also reason in the opposite way. in general, there will exist several routing subfunctions satisfying the conditions proposed by theorem 3.1. we will restrict our attention to the minimally connected routing subfunctions satisfying those conditions because they require a minimum number of channels to guarantee deadlock freedom. let r1, r2, . . . , rk be all the minimally connected routing subfunctions satisfying the
as we might expect, solutions to these problems are dependent upon the type and pattern of the faulty components and the network topology. the next section shows how direct networks possess natural redundancy that can be exploited, by deﬁning the redundancy level of the routing function. the remaining sections will demonstrate how the issues mentioned above are addressed in the context of distinct switching techniques.
this section presents a theoretical basis to answer a fundamental question regarding faulttolerant routing in direct networks: what is the maximum number of simultaneous faulty channels tolerated by the routing algorithm? this question has been analyzed in [95] by deﬁning the redundancy level of the routing function and proposing a necessary and sufﬁcient condition to compute its value.
if a routing function tolerates f faulty channels, it must remain connected and deadlock-free for any number of faulty channels less than or equal to f . note that, in addition to connectivity, deadlock freedom is also required. otherwise, the network could reach a deadlocked conﬁguration when some channels fail.
the following deﬁnitions are required to support the discussion of the behavior of fault-tolerant routing algorithms. in conjunction with the fault model, the following terminology can be used to discuss and compare fault-tolerant routing algorithms.
a network is said to be connected with respect to a routing algorithm if the routing function can route a message between any pair of nonfaulty routing nodes.
this deﬁnition is useful since a network may be connected in the graph-theoretical sense, where a physical path exists between a pair of nodes. however, routing restrictions may result in a routing function that precludes selection of links along that path.
this deﬁnition indicates the conditions to support the failure of a given channel. fault-tolerant routing algorithms should be designed in such a way that all the channels are redundant, thus avoiding a single point of failure. however, even in this case, we cannot guarantee that the network will support two simultaneous faults. this issue is addressed by the following deﬁnitions.
a routing algorithm is said to be f fault tolerant if, for any f failed components in the network, the routing function is still connected and deadlock-free.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
and channel between the routers. in this case the coefﬁcient of the second term and the pipeline cycle time would be (ts + tw). note that the unit of message ﬂow control is a packet. therefore, even though the message may cut through the router, sufﬁcient buffer space must be allocated for a complete packet in case the header is blocked.
the need to buffer complete packets within a router can make it difﬁcult to construct small, compact, and fast routers. in wormhole switching, message packets are also pipelined through the network. however, the buffer requirements within the routers are substantially reduced over the requirements for vct switching. a message packet is broken up into ﬂits. the ﬂit is the unit of message ﬂow control, and input and output buffers at a router are typically large enough to store a few ﬂits. for example, the message buffers in the cray t3d are 1 ﬂit deep, and each ﬂit is comprised of eight 16-bit phits. the message is pipelined through the network at the ﬂit level and is typically too large to be completely buffered within a router. thus, at any instant in time a blocked message occupies buffers in several routers. the time-space diagram of a wormhole-switched message is shown in figure 2.11. the clear rectangles illustrate the propagation of single ﬂits across the physical channel. the shaded rectangles illustrate the propagation of header ﬂits across the physical channel. routing delays and intrarouter propagation of the header ﬂits are also captured in this ﬁgure. the primary difference between wormhole switching and vct switching is that, in the former, the unit of message ﬂow control is a single ﬂit and, as a consequence, small buffers can be used. just a few ﬂits need to be buffered at a router.
in the absence of blocking, the message packet is pipelined through the network. however, the blocking characteristics are very different from that of vct. if the required
through the router with no further involvement with the routing and arbitration unit, but may compete for physical channel bandwidth with other virtual channels.
the ﬂow control latency determines the rate at which ﬂits can be transmitted along the path. in the terminology of pipelines, the ﬂow control latency is the message pipeline stage time. from the ﬁgure, we can see that the ﬂow control latency experienced by a data ﬂit is the sum of the delay through the channel ﬂow control, the time to drive the ﬂit through the crossbar, and the multiplexing delay experienced through the link controller, as well as the time to read and write the fifo buffers. the delay through the link controller is often referred to as the ﬂit multiplexing delay or channel multiplexing delay. a general model for these delays is quite difﬁcult to derive since the latencies are sensitive to the implementation. one approach to developing such a model for wormholeswitched networks is described in [57]. a canonical model of a wormhole-switched router is developed, containing units for address decoding, ﬂow control, and switching. implementations of the various components (e.g., tree of gates, or selectors for each output) are modeled and expressions for the implementation complexity of each component are derived, but parameterized by technology-dependent constants. the parameterized model is illustrated in table 7.6. by instantiating the constants with values based on a particular implementation technology, realistic delays can be computed for comparing alternative designs. in [57], the values are instantiated for a class of implementations based on gate arrays to derive estimates of ﬂow control latency through the router. the utility of the model stems from the application to many classes of routers. other router architectures using similar components for partially adaptive or fully adaptive routing can be constructed, and the parameterized model used to estimate and compare intrarouter latencies. in fact, comparisons are possible across technology implementations.
a major determinant of the intrarouter delays is the size and structure of the switch. the use of a crossbar switch to connect all input virtual channels to all output virtual channels is feasible for low-dimensional networks with a small number of virtual channels. a 2-d network with four virtual channels per link would require a 16 × 16 crossbar, and a 3-d network would require a 24 × 24 switch. considering that these channels may be byte or word wide, it is apparent that alternatives begin to become attractive. one alternative is to have the switch size determined by the number of physical channels. the data rate
orphaned data ﬂits. in one approach, header information is maintained through the routers that contain ﬂits of the message. when messages are interrupted, a new message can be constructed with the stored header information, and the (relatively) smaller message can be forwarded along an alternative path to the destination. this will be referred to as ﬂit-level recovery. messages are injected only once into the network and recover from link-level dynamic and transient failures. alternatively, the blocked data ﬂits can be recovered and discarded from the network, the router buffers freed, and the message retransmitted from the source. in this case, the source and destination must synchronize message delivery or drop messages. this requires that the message be buffered at the source until it can be asserted that it has been delivered either explicitly through acknowledgments or implicitly through other mechanisms. this will be referred to as message-level recovery to denote the level at which recovery takes place. implementations of each basic approach are described in the following.
reliable message delivery can be handled by end-to-end protocols that utilize acknowledgments and message retransmission to synchronize delivery of messages. messages remain buffered at the source until it can be asserted that the message has been delivered. if we consider packet switching, more efﬁcient alternatives exist for ensuring reliable end-toend message delivery. error detection and retransmission/rerouting can be implemented at the link level. a message must be successfully transmitted across a link before it is forwarded. with some a priori knowledge of the component failure rates, fault-tolerant routing algorithms can be developed where packets are injected into the network exactly once and guaranteed to be delivered to the destination by rerouting around faulty components. the difﬁculty in applying the same approach to wormhole-switched messages has been that the message packets cannot be completely buffered at each router and are
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
r3, a copied signal is transmitted to r1 so that r1 may invalidate its copy of the ﬂit. in this way exactly two copies of each data ﬂit are maintained in the network at all times. a more detailed description of the architecture of the router implementation can be found in chapter 7.
an alternative to ﬂit-level recovery is to ﬁnd and discard the interrupted message components and retransmit the message from the source. recovery is at the level of complete messages rather than at the ﬂit level. the pcs-based solutions exploit the fact that a separate control network comprised of the control channels exists. consider a message pipeline where a fault is detected on a reserved virtual link or physical channel as shown in figure 6.39. the link is marked faulty. the link controller at the source end of the faulty virtual link introduces a release ﬂit (referred to as kill ﬂit in [123]) into the complementary virtual control channel of the virtual link upstream from the fault. this release ﬂit is routed back to the source router. the link controller at the destination end of the faulty virtual link introduces a release ﬂit into the corresponding virtual control channel of the virtual link downstream from the failed virtual link. this release ﬂit is propagated along toward the destination. when a release ﬂit arrives at a node, the input and output virtual links associated with the message are released, and the ﬂit is propagated toward the source (or destination). if multiple faults occur in one message pipeline, this mechanism is applied recursively to fault-free segments.
when a release ﬂit arrives at an intermediate router node, the set of actions taken by that node will depend on the state of the interrupted message pipeline in that node. we have the following rules that are to be implemented by each router node. in all of the following steps it is implicit that virtual links and associated buffers are released. if two control ﬂits for the same message arrive at a router node (from opposite directions along the path), the two control ﬂits collide at that router node. a ﬂit in progress toward the destination (source) is referred to as a forward (reverse) ﬂit. the following rules govern the collision of control ﬂits and are based on the assumption [123] that paths are removed by message acknowledgments from the destination rather than by the last ﬂit of the message. thus, a path is not removed until the last ﬂit has been delivered to the destination. this behavior guarantees message delivery in the presence of dynamic or transient faults.
1. if a forward release ﬂit collides with a reverse release ﬂit, remove both release ﬂits from the network. this may occur with multiple faults within the same message pipeline.
3. if a release ﬂit reaches the source, inform the message handler that the message transmission has failed. the handler may choose to retransmit or to invoke some higher-level fault-handling algorithm.
the reliable router chip is targeted for fault-tolerant operation in 2-d mesh topologies [75]. the block diagram of the reliable router is shown in figure 7.23. there are six input channels corresponding to the four physical directions in the 2-d mesh, and two additional physical ports: the local processor interface and a separate port for diagnostics. the input and output channels are connected through a full crossbar switch, although some input/output connections may be prohibited by the routing function.
while message packets can be of arbitrary length, the ﬂit length is 64 bits. there are four ﬂit types: head, data, tail, and token. the format of the head ﬂit is shown in figure 7.24. the size of the physical channel or phit size is 23 bits. the channel structure is illustrated in figure 7.23. to permit the use of chip carriers with fewer than 300 pins, these physical channels utilize half-duplex channels with simultaneous bidirectional signaling. flits are transferred in one direction across the physical channel as four 23-bit phits called frames, producing 92-bit transfers. the format of a data ﬂit and its constituent frames are shown in figure 7.25. the 28 bits in excess of the ﬂit size are used for byte parity bits (bp), kind of ﬂit (kind), virtual channel identiﬁcation (vci), ﬂow control to implement the unique token protocol (copied kind, copied vci, freed), to communicate link status information (u/d, pe), and two user bits (usr1, usr0). for a given direction of transfer, the clock is transmitted along with the four data frames as illustrated in the ﬁgure. data are driven on both edges of the clock. to enable the receiver to distinguish between the four frames and reassemble them into a ﬂit, the transmitting side of the channel also sends a pulse on the txphase signal, which has the relative timing as shown. the ﬂit is then assembled and presented to the routing logic. this reassembly process takes two cycles. each router runs off a locally generated 100 mhz clock, removing the problems with distributing a single global clock. reassembled ﬂits pass through a synchronization module for transferring ﬂits from the transmit clock domain to the receive clock domain with a worst-case penalty of one cycle (see [75] for a detailed description of the data synchronization protocol). the aggregate physical bandwidth in one direction across the channel is 3.2 gbits/s.
this variant of wormhole switching is employed in the sp2 interconnection network: a bidirectional, multistage interconnection network constructed from 4 × 4 bidirectional buffered crossbar switches. two stages of four switches each are organized into a frame that supports up to 16 processors. the architecture of a frame is shown in figure 7.46. each crossbar switch has four bidirectional inputs and four bidirectional outputs, and therefore operates as an 8 × 8 crossbar. in this two-stage organization, except for processors on the same switch, there are four paths between every pair of processors connected to the same frame. the use of bidirectional links avoids the need for costly (i.e., long) wraparound links from the outputs back to the inputs. multiple frames can be connected as shown in figure 7.46 to connect larger numbers of processors, either by directly connecting frames (up to 5 frames or 80 processors) or by cascading frames together. in the cascaded organization, switch frames are used in the last stage to provide wraparound links as shown in figure 7.46. as the systems grow larger, the number of paths between any pair of processors grows. for example, the 128-node conﬁguration provides 16 paths between any pair of frames.
the sp2 physical channel appears as shown in figure 7.47. the data channel is 8 bits wide, with a single-bit tag signal and a single-bit token signal in the reverse direction. with the addition of a clock signal, the channel is 11 bits wide. sp2 ﬂits are matched to
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
even representing latency as a function of trafﬁc. when running simulations with synthetic workloads, the applied load (also known as offered trafﬁc, generation rate, or injection rate) is an input parameter while latency and accepted trafﬁc are measurements. so, latency-trafﬁc graphs do not represent functions. it should be noted that the network may be unstable when accepted trafﬁc reaches its maximum value. in this case, increasing the applied load may reduce the accepted trafﬁc until a stable point is reached. as a consequence, for some values of the accepted trafﬁc there exist two values for the latency, clearly indicating that the graph does not represent a function.
in the presence of faults, both performance and reliability are important. when presenting performance plots, the chaos normal form (cnf) format (to be described below) should be preferred in order to analyze accepted trafﬁc as a function of applied load. plots can be represented for different values of the number of faults. in this case, accepted trafﬁc can be smaller than applied load because the network is saturated or because some messages cannot be delivered in the presence of faults. another interesting measure is the probability of message delivery as a function of the number of failures.
the next sections describe two standard formats to represent performance results. these formats were proposed at pcrcw’94. the cnf requires paired accepted trafﬁc versus applied load and latency versus applied load graphs. the burton normal form (bnf) uses a single latency versus accepted trafﬁc graph. use of only latency (including source queuing) versus applied load is discouraged because it is impossible to gain any data about performance above saturation using such graphs.
cnf graphs display accepted trafﬁc on one graph and network latency on a second graph. in both graphs, the x-axis corresponds to normalized applied load. by using two graphs, the latency is shown both below and above saturation, and the accepted trafﬁc above saturation is visible. while bnf graphs show the same data, cnf graphs are more clear in their presentation of the data.
pipeline can modify the header before forwarding the packet. if the router pipeline detects a single-bit error, it corrects the error and reports it back to the operating system via an interrupt. however, it does not correct double-bit errors. instead, if it detects a double-bit error, the 21364 alerts every reachable 21364 of the occurrence of such an error and enters into an error recovery mode.
the most challenging component of the 21364 router is the arbitration mechanism that schedules the dispatch of packets arriving at its input ports. to avoid making the arbitration mechanism a central bottleneck, the 21364 breaks the arbitration logic into local and global arbitration. there are 16 local arbiters, two for each input port. there are seven global arbiters, one for each output port. in each cycle, a local arbiter may speculatively schedule a packet for dispatch to an output port. two cycles following the local arbitration, each global arbiter selects one out of up to seven packets speculatively scheduled for dispatch through the output port. once such a selection is made, all ﬂits in the x (crossbar) stage follow the input port to the output port connection.
to ensure fairness, the local and global arbiters use a least recently selected (lrs) scheme to select a packet. each local arbiter uses the lrs scheme to select both a class (among the several packet classes) and a virtual channel (among vc0, vc1, and adaptive) within the class. similarly, the global arbiter uses the lrs policy to select an input port (among the several input ports that each output port sees).
the nominated packet is valid at the input buffer and has not been dispatched yet. the necessary dispatch path from the input buffer to the output port is free. the packet is dispatched in only one of the routes allowed. the target router, i/o chip, or local resource (in the next hop) has a free input buffer in the speciﬁc virtual channel. the target output port is free. the antistarvation mechanism is not blocking the packet. a read io packet does not pass a write io packet.
a global arbiter selects packets speculatively scheduled for dispatch through its output port. the local arbiters speculatively schedule packets not selected by any global arbiter again in subsequent cycles.
in addition to the pipeline latency, there are a total of six cycles of synchronization delay, pad receiver and driver delay, and transport delay from the pins to the router and from the router back to the pins. thus, the on-chip, pin-to-pin latency from a network input to a network output is 13 cycles. at 1.2 ghz, this leads to a pin-to-pin latency of 10.8 ns.
the network links that connect the different 21364 chips run at 0.8 ghz, which is 33% slower than the internal router clock. the 21364 chip runs synchronously with the outgoing links, but asynchronously with the incoming links. the 21364 sends its clock
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
individually acknowledged. however, due to the length of the cable, up to 23 characters may be in transit in both directions at any given time. as in the sp2, these delays are accommodated with sufﬁcient buffering at the receiver. the stop and go control characters are generated at the receiver and multiplexed along the reverse channel (data may be ﬂowing along this channel) to implement ﬂow control. this slack buffer [315] is organized as shown in figure 7.50(c). as the buffer ﬁlls up from the bottom, the stop control character is transmitted to the source when the buffer crosses the stop line. there is enough remaining buffer space for all of the ﬂits in transit as well as the ﬂits that will be injected before the stop character arrives at the source. once the buffer has drained sufﬁciently (i.e., crosses the go line), a go character is transmitted and the sender resumes data ﬂow. the placement of the stop and go points within the buffer are designed to prevent constant oscillation between stop and go states, and reduce ﬂow control trafﬁc. a separate control character (gap) is used to signify the end-of-message. to enable detection of faulty links or deadlocked messages, non-idle characters must be periodically transmitted across each channel. longer timeout intervals are used to detect potentially deadlocked packets. for example, this may occur over a nonfaulty link due to bit errors in the header. in this case, after 50 ms the blocked part of the packet is dropped, and a forward reset signal is generated to the receiver and the link buffers reset.
myrinet employs wormhole switching with source routing. the message packets may be of variable length and are structured as shown in figure 7.50(c). the ﬁrst few ﬂits of the message header contain the address of the switch ports on intermediate switches. in a manner similar to the sp2 switches, each ﬂit addresses one of the output ports in a switch. each switch strips off the leading ﬂit as the message arrives and uses the contents to address an output port. conﬂicts for the same output port are resolved using a recirculating token arbitration mechanism to ensure fairness. the remaining ﬂits of the message are transmitted
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
on the other hand, path-based routing requires establishing an ordered list of destination addresses for each copy of a message. however, tree-based routing does not require any ordering among destinations. in many cases, the ordered destination list(s) can be computed at compile time. in some other cases, the compiler does not help because messages are dynamically generated by hardware. for example, messages are generated by the cache controller in dsms with coherent caches. in this case, a clever organization of the cache directory may considerably reduce the time required to prepare the ordered destination list [69]. finally, when the list of destinations has to be explicitly ordered for each multidestination message, path-based routing should be preferred over tree-based routing only if the additional cost of computing the ordered destination list(s) is compensated by a reduction in message latency.
in order to reduce the number of channels used for a given multicast, the subpath between the source and one of the destinations in a multicast path is not necessarily in a shortest path. therefore, in path-based routing the average distance between the source and each destination is generally longer than that of tree-based routing. note that in wormhole switching the network latency is almost independent of the length of a path. moreover, path-based routing does not require replicating messages at each intermediate node, implying a less complex router.
a suitable network partitioning strategy for path-based routing is based on hamiltonian paths. a hamiltonian path visits every node in a graph exactly once [146]; a 2-d mesh has many hamiltonian paths. thus, each node u in a network is assigned a label, ?(u). in a network with n nodes, the assignment of the label to a node is based on the position of that node in a hamiltonian path, where the ﬁrst node in the path is labeled 0 and the last node in the path is labeled n − 1. figure 5.24(a) shows a possible labeling in a 4 × 3 mesh, in which each node is represented by its integer coordinate (x, y). the labeling effectively divides the network into two subnetworks. the high-channel subnetwork contains all of the channels whose direction is from lower-labeled nodes to higher-labeled nodes, and the low-channel subnetwork contains all of the channels whose direction is from higherlabeled nodes to lower-labeled nodes.
unicast as well as multicast communication will use the labeling for routing. that is, a unicast message will follow a path based on the labeling instead of using xy routing. if the label of the destination node is greater than the label of the source node, the routing always takes place in the high-channel network; otherwise, it will take place in the low-channel network. the label assignment function ? for an m × n mesh can be expressed in terms of the
once the set of valid paths for multidestination messages has been determined, it is necessary to deﬁne routing algorithms for collective communication. the hierarchical leader-based (hl) scheme has been proposed in [269] to implement multicast and broadcast. given a multicast destination set, this scheme tries to group the destinations in a hierarchical manner so that the minimum number of messages is needed to cover all the destinations. since the multidestination messages conform to paths supported by the base routing, the grouping scheme takes into account this routing scheme and the spatial positions of destination nodes to achieve the best grouping. once the grouping is achieved, multicast and broadcast take place by traversing nodes from the source in a reverse hierarchy. consider a multicast pattern from a source s with a destination set d. let l0 denote the set d ∪ {s}. the hierarchical scheme, in its ﬁrst step, partitions the set l0 into disjoint subsets with a leader node representing each subset. the leader node is chosen in such a way that it can forward a message to the members of its set using a single multidestination message under the brcp model. for example, if a 2-d mesh implements xy routing, then the partitioning is done such that the nodes in each set lie on a row, column, or row-column.
let the leaders obtained by the above ﬁrst-step partitioning be termed as level-1 leaders and identiﬁed by a set l1. this set l1 can be further partitioned into disjoint subsets with a set of level-2 leaders. this process of hierarchical grouping is continued as long as it is proﬁtable, as indicated below. assuming that the grouping is carried out for m steps, there will be m sets of leaders, satisfying that lm ⊂ lm−1 ⊂ · · · ⊂ l1 ⊂ l0.
after the grouping is achieved, the multicast takes place in two phases. in the ﬁrst phase, the source node performs unicast-based multicast (see section 5.7) to the set lm. the second phase involves m steps of multidestination message passing. it starts with the leaders in the set lm and propagates down the hierarchical grouping in a reverse fashion to cover the lower-level leaders and, ﬁnally, all the nodes in destination set d.
as mentioned above, the hierarchical grouping is continued as long as it is proﬁtable in order to reduce the multicast latency. as start-up latency dominates communication latency in networks using pipelined data transmission, the minimum multicast latency is usually achieved by minimizing the number of communication steps. as will be seen in
section 5.7, unicast-based multicast requires(cid:12)log2(|lm|+ 1)(cid:13) steps to reach all the leaders in the set lm, assuming that the source node does not belong to that set. therefore, the size of the set lm should be reduced. however, while going from lm−1 to lm, one additional step of multidestination communication is introduced into the multicast latency. hence, it can be seen that when (cid:12)log2(|lm−1| + 1)(cid:13) > (cid:12)log2(|lm| + 1)(cid:13) + 1, it is proﬁtable to go through an additional level of grouping. the previous expression assumes that the source node does not belong to lm−1. if the source node belongs to lm−1 or lm, then the cardinal of the corresponding set must be reduced by one unit.
figure 5.37 shows an example of multicast routing in a 2-d mesh using xy routing under the hierarchical leader-based scheme. given a destination set, the ﬁrst-level grouping can be done along rows (dimension 0) to obtain the level-1 leaders as shown in the ﬁgure. the level-1 leaders can be grouped along columns
on the other hand, path-based routing requires establishing an ordered list of destination addresses for each copy of a message. however, tree-based routing does not require any ordering among destinations. in many cases, the ordered destination list(s) can be computed at compile time. in some other cases, the compiler does not help because messages are dynamically generated by hardware. for example, messages are generated by the cache controller in dsms with coherent caches. in this case, a clever organization of the cache directory may considerably reduce the time required to prepare the ordered destination list [69]. finally, when the list of destinations has to be explicitly ordered for each multidestination message, path-based routing should be preferred over tree-based routing only if the additional cost of computing the ordered destination list(s) is compensated by a reduction in message latency.
in order to reduce the number of channels used for a given multicast, the subpath between the source and one of the destinations in a multicast path is not necessarily in a shortest path. therefore, in path-based routing the average distance between the source and each destination is generally longer than that of tree-based routing. note that in wormhole switching the network latency is almost independent of the length of a path. moreover, path-based routing does not require replicating messages at each intermediate node, implying a less complex router.
a suitable network partitioning strategy for path-based routing is based on hamiltonian paths. a hamiltonian path visits every node in a graph exactly once [146]; a 2-d mesh has many hamiltonian paths. thus, each node u in a network is assigned a label, ?(u). in a network with n nodes, the assignment of the label to a node is based on the position of that node in a hamiltonian path, where the ﬁrst node in the path is labeled 0 and the last node in the path is labeled n − 1. figure 5.24(a) shows a possible labeling in a 4 × 3 mesh, in which each node is represented by its integer coordinate (x, y). the labeling effectively divides the network into two subnetworks. the high-channel subnetwork contains all of the channels whose direction is from lower-labeled nodes to higher-labeled nodes, and the low-channel subnetwork contains all of the channels whose direction is from higherlabeled nodes to lower-labeled nodes.
unicast as well as multicast communication will use the labeling for routing. that is, a unicast message will follow a path based on the labeling instead of using xy routing. if the label of the destination node is greater than the label of the source node, the routing always takes place in the high-channel network; otherwise, it will take place in the low-channel network. the label assignment function ? for an m × n mesh can be expressed in terms of the
these routing algorithms were designed for saf networks using central queues. deadlocks are avoided by splitting buffers into several classes and restricting packets to move from one buffer to another in such a way that buffer class is never decremented. gopal proposed several fully adaptive minimal routing algorithms based on buffer classes [133]. these algorithms are known as hop algorithms.
the simplest hop algorithm starts by injecting a packet into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a hop to another node, it moves to a buffer of class i + 1. this routing algorithm is known as the positive-hop algorithm. deadlocks are avoided by using a buffer of a higher class every time a packet requests a new buffer. by doing so, cyclic dependencies between resources are prevented. a packet that has completed i hops will use a buffer of class i. since the routing algorithm only supplies minimal paths, the maximum number of hops taken by a packet is limited by the diameter of the network. if the network diameter is denoted by d, a minimum of d + 1 buffers per node are required to avoid deadlock. the main advantage of the positive-hop algorithm is that it is valid for any topology. however, the number of buffers required for fully adaptive deadlock-free routing is very high, and this number depends on network size.
the minimum number of buffers per node can be reduced by allowing packets to move between buffers of the same class. in this case, classes must be deﬁned such that packets moving between buffers of the same class cannot form cycles. in the negative-hop routing algorithm, the network is partitioned into several subsets in such a way that no subset contains two adjacent nodes. if s is the number of subsets, then subsets are labeled 0, 1, . . . , s − 1, and nodes in subset i are labeled i. hops from a node with a higher label to a node with a lower label are negative. otherwise, hops are nonnegative. when a packet is injected, it is stored into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a negative hop, it moves to a buffer of class i + 1. if a packet stored in a buffer of class i takes a nonnegative hop, then it requests a buffer of the same class. thus, a packet that has completed i negative hops will use a buffer of class i. there is not any cyclic dependency between buffers. effectively, a cycle starting at node a must return to node a and contains at least another node b. if b has a lower label than a, some hop between a and b (possibly through intermediate nodes) is negative, and the buffer class is increased. if b has a higher label than a, some hop between b and a (possibly through intermediate nodes) is negative, and the buffer class is increased. as a consequence, packets cannot wait for buffers cyclically, thus avoiding deadlocks. if d is the network diameter and s is the number of subsets, then the maximum number of negative hops that can be taken by a packet is hn = (cid:12)d(s − 1)/s(cid:13). the minimum number of buffers per node required to avoid deadlock is hn + 1. figure 4.14 shows a partition scheme for k-ary 2-cubes with even k. black and white circles correspond to nodes of subsets 0 and 1, respectively.
although the negative-hop routing algorithm requires approximately half the buffers required by the positive-hop algorithm in the best case, this number is still high. it can be improved by partitioning the network into subsets and numbering the partitions, such
free if and only if there exists a restricted channel waiting graph that is wait-connected and has no true cycles [309]. this condition is valid for incoherent routing functions and for routing functions deﬁned on c × n. however, it proposes a dynamic condition for deadlock avoidance, thus requiring the analysis of all the packet injection sequences to determine whether a cycle is reachable (true cycle). true cycles can be identiﬁed by using the algorithm proposed in [309]. this algorithm has nonpolynomial complexity. when all the cycles are true cycles, this theorem is equivalent to theorem 3.1. the theory proposed in [309] has been generalized in [307], supporting saf, vct, and wormhole switching. basically, the theory proposed in [307] replaces the channel waiting graph by a buffer waiting graph.
up to now, nobody has proposed static necessary and sufﬁcient conditions for deadlock-free routing for incoherent routing functions and for routing functions deﬁned on c × n. this is a theoretical open problem. however, as mentioned in previous sections, it is of very little practical interest because the cases where theorem 3.1 cannot be applied are very rare. remember that this theorem can be used to prove deadlock freedom for incoherent routing functions and for routing functions deﬁned on c × n. in these cases it becomes a sufﬁcient condition.
unlike wormhole switching, saf and vct switching provide more buffer resources when packets are blocked. a single central or edge buffer is enough to store a whole packet. as a consequence, it is much simpler to avoid deadlock.
a simple technique, known as deﬂection routing [137] or hot potato routing, is based on the following idea: the number of input channels is equal to the number of output channels. thus, an incoming packet will always ﬁnd a free output channel.
the set of input and output channels includes memory ports. if a node is not injecting any packet into the network, then every incoming packet will ﬁnd a free output channel. if several options are available, a channel belonging to a minimal path is selected. otherwise, the packet is misrouted. if a node is injecting a packet into the network, it may happen that all the output channels connecting to other nodes are busy. the only free output channel is the memory port. in this case, if another packet arrives at the node, it is buffered. buffered packets are reinjected into the network before injecting any new packet at that node.
deﬂection routing has two limitations. first, it requires storing the packet into the current node when all the output channels connecting to other nodes are busy. thus, it cannot be applied to wormhole switching. second, when all the output channels belonging to minimal paths are busy, the packet is misrouted. this increases packet latency and bandwidth consumption, and may produce livelock. the main advantages are its simplicity and ﬂexibility. deﬂection routing can be used in any topology, provided that the number of input and output channels per node is the same.
deﬂection routing was initially proposed for communication networks. it has been shown to be a viable alternative for networks using vct switching. misrouting has a small impact on performance [188]. livelock will be analyzed in section 3.7.
one of the nice features of the traditional mins (tmins) above is that there is a simple algorithm for ﬁnding a path of length logk n between any input/output pair. however, if a link becomes congested or fails, the unique path property can easily disrupt the communication between some input and output pairs. the congestion of packets over some channels causes the known hot spot problem [275]. many solutions have been proposed to resolve the hot spot problem. a popular approach is to provide multiple routing paths between any source and destination pair so as to reduce network congestion as well as to achieve fault tolerance. these methods usually require additional hardware, such as extra stages or additional channels.
the use of additional channels gives rise to dilated mins. in a d-dilated min (dmin), each switch is replaced by a d-dilated switch. in this switch, each port has d channels. by using replicated channels, dmins offer substantial network throughput improvement [191]. the routing tag of a dmin can be determined by the destination address as mentioned for tmins. within the network switches, packets destined for a particular output port are randomly distributed to one of the free channels of that port. if all channels are busy, the packet is blocked.
another approach for the design of mins consists of allowing for bidirectional communication. in this case, each port of the switch has dual channels. in a butterﬂy bidirectional min (bmin) built with k × k switches, source address s and destination address d are represented by k-ary numbers sn−1 . . . s1s0 and dn−1 . . . d1d0, respectively. the function firstdifference(s, d) returns t, the position where the ﬁrst (leftmost) different digit appears between sn−1 . . . s1s0 and dn−1 . . . d1d0.
switches. if several switches exist, they are connected between them by using point-topoint links. in this case, any communication between communicating devices requires transmitting the information through one or more switches. finally, hybrid approaches are possible. these network classes and the corresponding subclasses will be described in the following sections.
the least complex interconnect structure is one in which the transmission medium is shared by all communicating devices. in such shared-medium networks, only one device is allowed to use the network at a time. every device attached to the network has requester, driver, and receiver circuits to handle the passing of addresses and data. the network itself is usually passive, since the network itself does not generate messages.
an important issue here is the arbitration strategy that determines the mastership of the shared-medium network to resolve network access conﬂicts. a unique characteristic of a shared medium is its ability to support atomic broadcast, in which all devices on the medium can monitor network activities and receive the information transmitted on the shared medium. this property is important to efﬁciently support many applications requiring one-to-all or one-to-many communication services, such as barrier synchronization and snoopy cache coherence protocols. due to limited network bandwidth, a single shared medium can only support a limited number of devices before the medium becomes a bottleneck.
shared-medium networks constitute a well-established technology.additionally, their limited bandwidth restricts their use in multiprocessors. so, these networks will not be covered in this book, but we will present a short introduction in the following sections. there are two major classes of shared-medium networks: local area networks, mainly used to construct computer networks that span physical distances no longer than a few kilometers, and backplane buses, mainly used for internal communication in uniprocessors and multiprocessors.
high-speed lans can be used as a networking backbone to interconnect computers to provide an integrated parallel and distributed computing environment. physically, a sharedmedium lan uses copper wires or ﬁber optics in a bit-serial fashion as the transmission medium. the network topology is either a bus or a ring. depending on the arbitration mechanism used, different lans have been commercially available. for performance and implementation reasons, it is impractical to have a centralized control or to have some ﬁxed access assignment to determine the bus master who can access the bus. three major classes of lans based on distributed control are described below.
hence, it is not possible to cross every dimension from every node. crossing a given dimension from a given node may require moving in another dimension ﬁrst.
the most interesting property of strictly orthogonal topologies is that routing is very simple. thus, the routing algorithm can be efﬁciently implemented in hardware. effectively, in a strictly orthogonal topology, nodes can be numbered by using their coordinates in the n-dimensional space. since each link traverses a single dimension and every node has at least one link crossing each dimension, the distance between two nodes can be computed as the sum of dimension offsets. also, the displacement along a given link only modiﬁes the offset in the corresponding dimension. taking into account that it is possible to cross any dimension from any node in the network, routing can be easily implemented by selecting a link that decrements the absolute value of the offset in some dimension. the set of dimension offsets can be stored in the packet header and updated (by adding or subtracting one unit) every time the packet is successfully routed at some intermediate node. if the topology is not strictly orthogonal, however, routing may become much more complex.
the most popular direct networks are the n-dimensional mesh, the k-ary n-cube or torus, and the hypercube. all of them are strictly orthogonal. formally, an n-dimensional mesh has k0 × k1 × · · · × kn−2 × kn−1 nodes, ki nodes along each dimension i, where ki ≥ 2 and 0 ≤ i ≤ n − 1. each node x is identiﬁed by n coordinates, (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ ki − 1 for 0 ≤ i ≤ n − 1. two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = xj ± 1. thus, nodes have from n to 2n neighbors, depending on their location in the mesh. therefore, this topology is not regular.
in a bidirectional k-ary n-cube [70], all nodes have the same number of neighbors. the deﬁnition of a k-ary n-cube differs from that of an n-dimensional mesh in that all of the ki are equal to k and two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = (xj ± 1) mod k. the change to modular arithmetic in the deﬁnition adds wraparound channels to the k-ary n-cube, giving it regularity and symmetry. every node has n neighbors if k = 2, and 2n neighbors if k > 2. when n = 1, the k-ary n-cube collapses to a bidirectional ring with k nodes.
another topology with regularity and symmetry is the hypercube, which is a special case of both n-dimensional meshes and k-ary n-cubes. a hypercube is an n-dimensional mesh in which ki = 2 for 0 ≤ i ≤ n − 1, or a 2-ary n-cube, also referred to as a binary n-cube.
figure 1.5(a) depicts a binary 4-cube or 16-node hypercube. figure 1.5(b) illustrates a 3-ary 2-cube or two-dimensional (2-d) torus. figure 1.5(c) shows a 3-ary threedimensional (3-d) mesh, resulting by removing the wraparound channels from a 3-ary 3-cube.
two conﬂicting requirements of a direct network are that it must accommodate a large number of nodes while maintaining a low network latency. this issue will be addressed in chapter 7.
multiprocessor has 5 × 5 nonblocking crossbars at the lower level in the hierarchy, connecting four functional blocks and one i/o interface to form clusters or hypernodes. each functional block consists of two processors, two memory banks, and interfaces. these hypernodes are connected by a second-level coherent toroidal interconnect made out of multiple rings using scalable coherent interface (sci). each ring connects one functional block from all the hypernodes. at the lower level of the hierarchy, the crossbars allow all the processors within a hypernode to access the interleaved memory modules in that hypernode. at the higher level, the rings implement a cache coherence protocol.
many other hybrid topologies have been proposed [25, 337, 349]. among them, a particularly interesting class is the hypermesh [336]. a hypermesh is a regular topology consisting of a set of nodes arranged into several dimensions. instead of having direct connections to the neighbors in each dimension, each node is connected to all the nodes in each dimension through a bus. there are several ways to implement a hypermesh. the most straightforward way consists of connecting all the nodes in each dimension through a shared bus. figure 1.27 shows a 2-d hypermesh. in this network, multiple buses are arranged in two dimensions. each node is connected to one bus in each dimension. this topology was proposed by wittie [349] and was referred to as a spanning-bus hypercube. this topology has a very low diameter, and the average distance between nodes scales very well with network size. however, the overall network bandwidth does not scale well. additionally, the frequent changes of bus mastership incur signiﬁcant overheads.
multiprocessor has 5 × 5 nonblocking crossbars at the lower level in the hierarchy, connecting four functional blocks and one i/o interface to form clusters or hypernodes. each functional block consists of two processors, two memory banks, and interfaces. these hypernodes are connected by a second-level coherent toroidal interconnect made out of multiple rings using scalable coherent interface (sci). each ring connects one functional block from all the hypernodes. at the lower level of the hierarchy, the crossbars allow all the processors within a hypernode to access the interleaved memory modules in that hypernode. at the higher level, the rings implement a cache coherence protocol.
many other hybrid topologies have been proposed [25, 337, 349]. among them, a particularly interesting class is the hypermesh [336]. a hypermesh is a regular topology consisting of a set of nodes arranged into several dimensions. instead of having direct connections to the neighbors in each dimension, each node is connected to all the nodes in each dimension through a bus. there are several ways to implement a hypermesh. the most straightforward way consists of connecting all the nodes in each dimension through a shared bus. figure 1.27 shows a 2-d hypermesh. in this network, multiple buses are arranged in two dimensions. each node is connected to one bus in each dimension. this topology was proposed by wittie [349] and was referred to as a spanning-bus hypercube. this topology has a very low diameter, and the average distance between nodes scales very well with network size. however, the overall network bandwidth does not scale well. additionally, the frequent changes of bus mastership incur signiﬁcant overheads.
this variant of wormhole switching is employed in the sp2 interconnection network: a bidirectional, multistage interconnection network constructed from 4 × 4 bidirectional buffered crossbar switches. two stages of four switches each are organized into a frame that supports up to 16 processors. the architecture of a frame is shown in figure 7.46. each crossbar switch has four bidirectional inputs and four bidirectional outputs, and therefore operates as an 8 × 8 crossbar. in this two-stage organization, except for processors on the same switch, there are four paths between every pair of processors connected to the same frame. the use of bidirectional links avoids the need for costly (i.e., long) wraparound links from the outputs back to the inputs. multiple frames can be connected as shown in figure 7.46 to connect larger numbers of processors, either by directly connecting frames (up to 5 frames or 80 processors) or by cascading frames together. in the cascaded organization, switch frames are used in the last stage to provide wraparound links as shown in figure 7.46. as the systems grow larger, the number of paths between any pair of processors grows. for example, the 128-node conﬁguration provides 16 paths between any pair of frames.
the sp2 physical channel appears as shown in figure 7.47. the data channel is 8 bits wide, with a single-bit tag signal and a single-bit token signal in the reverse direction. with the addition of a clock signal, the channel is 11 bits wide. sp2 ﬂits are matched to
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
figure 4.7 allowed paths in (a) fully adaptive routing in three-dimensional networks, (b) planar-adaptive routing in three-dimensional networks, and (c) planar-adaptive routing in four-dimensional networks.
until the offset in dimension di is reduced to zero. thus, in order to offer alternative routing choices for as long as possible, a higher priority is given to channels in dimension di while routing in plane ai.
as deﬁned, planar-adaptive routing requires three virtual channels per physical channel to avoid deadlocks in meshes and six virtual channels to avoid deadlocks in tori. in what follows, we analyze meshes in more detail. channels in the ﬁrst and last dimension need only one and two virtual channels, respectively. let di,j be the set of virtual channels j crossing dimension i of the network. this set can be decomposed into two subsets, one in the positive direction and one in the negative direction. let di,j+ and di,j− denote the positive and negative direction channels, respectively.
in order to avoid deadlock, the set of virtual channels in ai is divided into two classes: increasing and decreasing networks (figure 4.8). the increasing network is formed by
although there are many similarities between interconnection networks for multicomputers and dsms, it is important to keep in mind that performance requirements may be very different. messages are usually very short when dsms are used. additionally, network latency is important because memory access time depends on that latency. however, messages are typically longer and less frequent when using multicomputers. usually the programmer is able to adjust the granularity of message communication in a multicomputer. on the other hand, interconnection networks for multicomputers and nows are mainly used for message passing. however, the geographical distribution of workstations usually imposes constraints on the way processors are connected. also, individual processors may be connected to or disconnected from the network at any time, thus imposing additional design constraints.
interconnection networks play a major role in the performance of modern parallel computers. there are many factors that may affect the choice of an appropriate interconnection network for the underlying parallel computer. these factors include the following:
1. performance requirements. processes executing in different processors synchronize and communicate through the interconnection network. these operations are usually performed by explicit message passing or by accessing shared variables. message latency is the time elapsed between the time a message is generated at its source node and the time the message is delivered at its destination node. message latency directly affects processor idle time and memory access time to remote memory locations. also, the network may saturate—it may be unable to deliver the ﬂow of messages injected by the nodes, limiting the effective computing power of a parallel computer. the maximum amount of information delivered by the network per time unit deﬁnes the throughput of that network.
2. scalability. a scalable architecture implies that as more processors are added, their memory bandwidth, i/o bandwidth, and network bandwidth should increase proportionally. otherwise the components whose bandwidth does not scale may become a bottleneck for the rest of the system, decreasing the overall efﬁciency accordingly.
3. incremental expandability. customers are unlikely to purchase a parallel computer with a full set of processors and memories. as the budget permits, more processors and memories may be added until a system’s maximum conﬁguration is reached. in some interconnection networks, the number of processors must be a power of 2, which makes them difﬁcult to expand. in other cases, expandability is provided at the cost of wasting resources. for example, a network designed for a maximum
in the path it followed from the ﬁrst channel up to the channel containing its header. thus, there is also a sequence of dependencies between the adjacent channels belonging to the path reserved by the packet. the information offered by the dependency between nonadjacent channels is redundant.
however, the application of theorem 3.1 requires the deﬁnition of a routing subfunction in such a way that the channels supplied by it could be used as escape channels from cyclic dependencies. when we restrict our attention to the escape channels, it may happen that a packet reserved a set of adjacent channels ci , ci+1, . . . , ck−1, ck in such a way that ci and ck are escape channels but ci+1, . . . , ck−1 are not. in this case, the dependency from ci to ck is important because the information offered by it is not redundant. if ci is an escape channel for the packet destination, we will refer to this kind of channel dependency as indirect dependency [86]. if ci is not an escape channel for the packet destination but is an escape channel for some other destinations, the dependency is referred to as indirect cross-dependency [92, 97]. in both cases, the channel ck requested by the packet must be an escape channel for it. indirect cross-dependencies are the indirect counterpart of direct cross-dependencies. to illustrate these dependencies, we present another example.
consider a unidirectional ring using wormhole switching with six nodes denoted ni , i = {0, 1, 2, 3, 4, 5}, and three channels connecting each pair of adjacent nodes. let cai, cbi, and ch i , i = {0, 1, 2, 3, 4, 5}, be the outgoing channels from node ni. the routing function can be stated as follows: if the current node ni is equal to the destination node nj , store the packet. otherwise, use either cai or cbi , ∀j (cid:16)= i, or ch i , ∀j > i. this routing function is identical to the one for example 3.4, except that cbi channels have been added. figure 3.7 shows the network.
suppose that we deﬁne a routing subfunction in the same way as in example 3.4: cai channels are supplied by the routing subfunction when the packet destination is lower than the current node. ch i channels are always supplied by the routing subfunction. consider a packet destined for n4 whose header is at n1. it can use channels ch 1, cb2, and ch 3 to reach n4. thus, while the packet is holding ch 1 and cb2, it requests ch 3. as cb2 is not an escape channel, there is an indirect dependency from ch 1 to ch 3. again, consider the packet destined for n4
that is, it is possible to establish a path from si to dj for messages destined for some node x, and ci and cj are the ﬁrst and last channels in that path and the only ones in that path supplied by r1 for the destination of the message. therefore, cj can be requested after using ci by some messages. as ci and cj are not adjacent, some other channels not supplied by r1 for the destination of the message are reserved while establishing the path between them. those channels are supplied by r.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of adjacent channels ci , cj ∈ c1, there is a direct cross-dependency from ci to cj iff
that is, cj can be requested immediately after using ci by messages destined for some node y, cj is supplied by r1 for the destination of the message, and ci cannot be supplied by r1 for that destination. however, ci is supplied by r1 for some other destination(s).
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect crossdependency from ci to cj iff
that is, it is possible to establish a path from si to dj for messages destined for some node y, ci and cj are the ﬁrst and last channels in that path, cj is the only channel in that path supplied by r1 for the destination of the message, and ci cannot be supplied by r1 for that destination. however, ci is supplied by r1 for some other destination(s). therefore, cj can be requested after using ci by some messages. as ci and cj are not adjacent, some other channels not supplied by r1 for the destination of the message are reserved while establishing the path between them.
in the path it followed from the ﬁrst channel up to the channel containing its header. thus, there is also a sequence of dependencies between the adjacent channels belonging to the path reserved by the packet. the information offered by the dependency between nonadjacent channels is redundant.
however, the application of theorem 3.1 requires the deﬁnition of a routing subfunction in such a way that the channels supplied by it could be used as escape channels from cyclic dependencies. when we restrict our attention to the escape channels, it may happen that a packet reserved a set of adjacent channels ci , ci+1, . . . , ck−1, ck in such a way that ci and ck are escape channels but ci+1, . . . , ck−1 are not. in this case, the dependency from ci to ck is important because the information offered by it is not redundant. if ci is an escape channel for the packet destination, we will refer to this kind of channel dependency as indirect dependency [86]. if ci is not an escape channel for the packet destination but is an escape channel for some other destinations, the dependency is referred to as indirect cross-dependency [92, 97]. in both cases, the channel ck requested by the packet must be an escape channel for it. indirect cross-dependencies are the indirect counterpart of direct cross-dependencies. to illustrate these dependencies, we present another example.
consider a unidirectional ring using wormhole switching with six nodes denoted ni , i = {0, 1, 2, 3, 4, 5}, and three channels connecting each pair of adjacent nodes. let cai, cbi, and ch i , i = {0, 1, 2, 3, 4, 5}, be the outgoing channels from node ni. the routing function can be stated as follows: if the current node ni is equal to the destination node nj , store the packet. otherwise, use either cai or cbi , ∀j (cid:16)= i, or ch i , ∀j > i. this routing function is identical to the one for example 3.4, except that cbi channels have been added. figure 3.7 shows the network.
suppose that we deﬁne a routing subfunction in the same way as in example 3.4: cai channels are supplied by the routing subfunction when the packet destination is lower than the current node. ch i channels are always supplied by the routing subfunction. consider a packet destined for n4 whose header is at n1. it can use channels ch 1, cb2, and ch 3 to reach n4. thus, while the packet is holding ch 1 and cb2, it requests ch 3. as cb2 is not an escape channel, there is an indirect dependency from ch 1 to ch 3. again, consider the packet destined for n4
for each channel, the queue capacity is not exceeded and all the ﬂits stored in the queue (if any) can reach the channel from the previous node using the routing function.
a deadlocked conﬁguration for a given interconnection network i and routing function r is a nonempty legal conﬁguration verifying the following conditions:
in a deadlocked conﬁguration there is no message whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (see assumption 5). data and tail ﬂits cannot advance because the next channel reserved by their message header has a full queue. no condition is imposed on empty channels. it must be noticed that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
a routing function r for an interconnection network i is deadlock-free iff there is not any deadlocked conﬁguration for that routing function on that network.
a routing subfunction r1 for a given routing function r is a routing function deﬁned on the same domain as r that supplies a subset of the channels supplied by r:
thus, r1 is a restriction of r. it should be noted that this deﬁnition allows the restriction of channel routing capability instead of simply removing channels. in other words, it is possible to restrict the use of a channel ci when r1 routes messages to some destinations while still allowing the use of ci when routing messages to other destinations.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect dependency from ci to cj iff
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
to reliability, interconnection networks should have a modular design, allowing hot upgrades and repairs. nodes can also fail or be removed from the network. in particular, a node can be powered off in a network of workstations. thus, nows usually require some reconﬁguration algorithm for the automatic reconﬁguration of the network when a node is powered on or off.
9. expected workloads. users of a general-purpose machine may have very different requirements. if the kind of applications that will be executed in the parallel computer are known in advance, it may be possible to extract some information on usual communication patterns, message sizes, network load, and so on. that information can be used for the optimization of some design parameters. when it is not possible to get information on expected workloads, network design should be robust; that is, design parameters should be selected in such a way that performance is good over a wide range of trafﬁc conditions.
10. cost constraints. finally, it is obvious that the “best” network may be too expensive. design decisions often are trade-offs between cost and other design factors. fortunately, cost is not always directly proportional to performance. using commodity components whenever possible may considerably reduce the overall cost.
among other criteria, interconnection networks have been traditionally classiﬁed according to the operating mode (synchronous or asynchronous) and network control (centralized, decentralized, or distributed). nowadays, multicomputers, multiprocessors, and nows dominate the parallel computing market. all of these architectures implement asynchronous networks with distributed control. therefore, we will focus on other criteria that are currently more signiﬁcant.
a classiﬁcation scheme is shown in figure 1.2, which categorizes the known interconnection networks into four major classes based primarily on network topology: shared-medium networks, direct networks, indirect networks, and hybrid networks. for each class, the ﬁgure shows a hierarchy of subclasses, also indicating some real implementations for most of them. this classiﬁcation scheme is based on the classiﬁcation proposed in [253], and it mainly focuses on networks that have been implemented. it is by no means complete, as other new and innovative interconnection networks may emerge as technology further advances, such as mobile communication and optical interconnections. in shared-medium networks, the transmission medium is shared by all communicating devices. an alternative to this approach consists of having point-to-point links directly connecting each communicating device to a (usually small) subset of other communicating devices in the network. in this case, any communication between nonneighboring devices requires transmitting the information through several intermediate devices. these networks are known as direct networks. instead of directly connecting the communicating devices between them, indirect networks connect those devices by means of one or more
mit reliable router: 2-d mesh, 23-bit links (16-bit data), 200 mhz, 400 mbytes/s per link per direction, bidirectional signaling, reliable transmission. chaos router: 2-d torus topology, bidirectional 8-bit links, 180 mhz, 360 mbytes/s in each direction. intel ipsc-2 hypercube: binary hypercube topology, bit-serial channels at 2.8 mbytes/s.
indirect or switch-based networks are another major class of interconnection networks. instead of providing a direct connection among some nodes, the communication between any two nodes has to be carried through some switches. each node has a network adapter that connects to a network switch. each switch can have a set of ports. each port consists of one input and one output link. a (possibly empty) set of ports in each switch is either connected to processors or left open, whereas the remaining ports are connected to ports of other switches to provide connectivity between the processors. the interconnection of those switches deﬁnes various network topologies.
switch-based networks have evolved considerably over time. a wide range of topologies has been proposed, ranging from regular topologies used in array processors and shared-memory uma multiprocessors to the irregular topologies currently used in nows. both network classes will be covered in this book. regular topologies have regular connection patterns between switches, while irregular topologies do not follow any predeﬁned pattern. figures 1.19 and 1.21 show several switch-based networks with regular topology. the corresponding connection patterns will be studied later. figure 1.8 shows a typical switch-based network with irregular topology. both network classes can be further classiﬁed according to the number of switches a message has to traverse before reaching its destination. although this classiﬁcation is not important in the case of irregular topologies, it makes a big difference in the case of regular networks because some speciﬁc properties can be derived for each network class.
indirect networks can also be modeled by a graph g(n, c), where n is the set of switches and c is the set of unidirectional or bidirectional links between the switches. for the analysis of most properties, it is not necessary to explicitly include processing nodes in the graph. although a similar model can be used for direct and indirect networks, a few differences exist between them. each switch in an indirect network may be connected to zero, one, or more processors. obviously, only the switches connected to some processor can be the source or the destination of a message. additionally, transmitting a message from a node to another node requires crossing the link between the source node and the switch connected to it, and the link between the last switch in the path and the destination
tiprocessors implement latency-hiding mechanisms and application performance mainly depends on the throughput achievable by the interconnection network, then adaptive routing is expected to achieve higher performance than deterministic routing. in the case of using adaptive routing, the additional cost of implementing fully adaptive routing should be kept small. therefore, routing algorithms that require few resources to avoid deadlock or to recover from it, like the ones evaluated in this chapter, should be preferred. for these routing algorithms, the additional complexity of fully adaptive routing usually produces a small reduction in clock frequency. number of virtual channels. in wormhole switching, when no virtual channels are used, blocked messages do not allow other messages to use the bandwidth of the physical channels they are occupying. adding the ﬁrst additional virtual channel usually increases throughput considerably at the expense of a small increase in latency. on the other hand, adding more virtual channels produces a much smaller increment in throughput while increasing hardware delays considerably. for deterministic routing in meshes, two virtual channels provide a good trade-off. for tori, the partially adaptive algorithm evaluated in this chapter with two virtual channels also provides a good trade-off, achieving the advantages of channel multiplexing without increasing the number of virtual channels with respect to the deterministic algorithm. if fully adaptive routing is preferred, the minimum number of virtual channels should be used. fully adaptive routing requires a minimum of two (three) virtual channels to avoid deadlock in meshes (tori). again, for applications that require low latency and produce a relatively small amount of trafﬁc, adding virtual channels does not help. virtual channels only increase performance when applications beneﬁt from a higher network throughput. hardware support for collective communication. adding hardware support for multidestination message passing usually reduces the latency of collective communication operations with respect to software algorithms. however, this reduction is very small (if any) when the number of participating nodes is small. when many nodes participate and trafﬁc is only composed of multidestination messages, latency reduction ranges from 2 to 7, depending on several parameters. in real applications, trafﬁc for collective communication operations usually represents a much smaller fraction of network trafﬁc. also, the number of participating nodes may vary considerably from one application to another. in general, this number is small except for broadcast and barrier synchronization. in summary, whether adding hardware support for collective communication is worth its cost depends on the application requirements. injection limitation mechanism. when fully adaptive routing is used, network interfaces should include some mechanism to limit the injection of new messages when the network is heavily loaded. otherwise, increasing applied load above the saturation point may degrade performance severely. in some cases, the start-up latency is so high that it effectively limits the injection rate. when the start-up latency does not prevent network saturation, simple mechanisms like restricting
even representing latency as a function of trafﬁc. when running simulations with synthetic workloads, the applied load (also known as offered trafﬁc, generation rate, or injection rate) is an input parameter while latency and accepted trafﬁc are measurements. so, latency-trafﬁc graphs do not represent functions. it should be noted that the network may be unstable when accepted trafﬁc reaches its maximum value. in this case, increasing the applied load may reduce the accepted trafﬁc until a stable point is reached. as a consequence, for some values of the accepted trafﬁc there exist two values for the latency, clearly indicating that the graph does not represent a function.
in the presence of faults, both performance and reliability are important. when presenting performance plots, the chaos normal form (cnf) format (to be described below) should be preferred in order to analyze accepted trafﬁc as a function of applied load. plots can be represented for different values of the number of faults. in this case, accepted trafﬁc can be smaller than applied load because the network is saturated or because some messages cannot be delivered in the presence of faults. another interesting measure is the probability of message delivery as a function of the number of failures.
the next sections describe two standard formats to represent performance results. these formats were proposed at pcrcw’94. the cnf requires paired accepted trafﬁc versus applied load and latency versus applied load graphs. the burton normal form (bnf) uses a single latency versus accepted trafﬁc graph. use of only latency (including source queuing) versus applied load is discouraged because it is impossible to gain any data about performance above saturation using such graphs.
cnf graphs display accepted trafﬁc on one graph and network latency on a second graph. in both graphs, the x-axis corresponds to normalized applied load. by using two graphs, the latency is shown both below and above saturation, and the accepted trafﬁc above saturation is visible. while bnf graphs show the same data, cnf graphs are more clear in their presentation of the data.
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
latency through the chip is estimated to be 40 ns, while the latency from the input on a router to the input on the adjacent router is estimated to be 50 ns.
routers supporting vct switching share many of the attributes of wormhole routers. the principal distinguishing feature is the availability of sufﬁcient space for buffering complete message packets. the following examples discuss four router architectures implementing vct switching.
the chaos router chip is an example of a router designed for vct switching for operation in a 2-d mesh. to reduce pin count, each channel is a 16-bit, half-duplex bidirectional channel. the chip was designed for a cycle time of 15 ns. the latency for the common case of cut-through routing (no misrouting) is four cycles from input to output. this performance compares favorably to that found in more compact oblivious routers and has been achieved by keeping the misrouting and buffering logic off the critical path. a block diagram of the chaos router is shown in figure 7.31.
the router is comprised of the router core and the multiqueue [32]. the core connects input frames to output frames through a crossbar switch. each frame can store one 20-ﬂit message packet. each ﬂit is 16 bits wide and corresponds to the width of the physical channel. the architecture of the core is very similar to that of contemporary oblivious routers, with the additional ability to buffer complete 20-ﬂit packets in each input and output frame. under low-load conditions, the majority of the trafﬁc ﬂows through this core,
this function requires updating the channel status register. this register is a centralized resource. otherwise, several packets may simultaneously reserve the same output channel. so when two or more packets compete for the same output channel, some arbitration is required.
the traditional scheduling policy for the allocation of the routing control unit (or for granting access to the channel status register) is round-robin among input channels. in this scheduling policy, input channel buffers form a logical circular list. after routing the header stored in a buffer, the pointer to the current buffer is advanced to the next input buffer where a packet header is ready to be routed. the routing function is computed for that header, supplying a set of candidate output channels. then, one of these channels is selected, if available, and the packet is routed to that channel. this scheduling policy is referred to as input driven [116]. it is simple, but when the network is heavily loaded a high percentage of routing operations may fail because all of the requested output channels are busy.
an alternative scheduling policy consists of selecting a packet for which there is a free output channel. in this strategy, output channels form a logical circular list. after routing a packet, the pointer to the current output channel is advanced to the next free output channel. the router tries to ﬁnd a packet in an input buffer that needs to be routed to the current output channel. if packets are found, it selects one to be routed to the current output channel. if no packet is found, the pointer to the current output channel is advanced to the next free output channel. this scheduling policy is referred to as output driven [116]. outputdriven strategies may be more complex than input-driven ones. however, performance is usually higher when the network is heavily loaded [116]. output-driven scheduling can be implemented by replicating the circuit computing the routing function at each input channel, and adding a register to each output channel to store information about the set of input channels requesting that output channel. although output-driven scheduling usually improves performance over input-driven scheduling, the difference between them is much smaller when channels are randomly selected [116]. indeed, a random selection usually performs better than round-robin when input-driven scheduling is used. finally, it should be noted that most selection functions described in the previous section cannot be implemented with output-driven scheduling.
from an engineering point of view, the “best” routing algorithm is either the one that maximizes performance or the one that maximizes the performance/cost ratio, depending on the design goals. a detailed quantitative analysis must consider the impact of the routing algorithm on node design. even if we do not consider the cost, a more complex routing algorithm may increase channel utilization at the expense of a higher propagation delay and a lower clock frequency. a slower clock also implies a proportional reduction in channel bandwidth if channels are driven synchronously. thus, router design must be considered to make performance evaluation more realistic. this is the reason why
the teraﬂops system is an architecture being designed by intel corporation in an effort to produce a machine capable of a sustained performance of 1012 ﬂoating-point operations per second and a peak performance of 1.8 × 1012 ﬂoating-point operations per second. cavallino is the name for the network interface chip and router that forms the communication fabric for this machine [48]. the network is a k-ary 3-cube, with a distinct radix in each dimension. up to 8 hops are permitted in the z dimension, 256 hops in the x dimension, and 64 hops in the y dimension. the router has six ports, and the interface design supports 2-d conﬁgurations where the two ports in the z dimension are connected to two processing nodes rather than other routers. this architecture provides a ﬂexible basis for constructing a variety of topologies.
unlike most other routers, cavallino uses simultaneous bidirectional signaling. physical channels are half duplex. a receiver interprets incoming data with respect to the reference levels received and the data being driven to determine the value being received. the physical channel is 16 bits wide (phits). message ﬂits are 64 bits and require four clock cycles to traverse the channel. three additional signals across the channel provide virtual channel and ﬂow control information. the buffer status transmitted back to the sending virtual channel is interpreted as almost full rather than an exact number of available locations. this scheme allows for some slack in the protocols to tolerate signaling errors. the physical channel operates at 200 mhz, providing an aggregate data rate of 400 mbytes/s in each direction across the link. four virtual channels are multiplexed in each direction across a physical link.
the internal structure of the router is shown in figure 7.15.a message enters along one virtual channel and is destined for one output virtual channel. the routing header contains offsets in each dimension. in fact, routing order is z − x − y − z. using offsets permits fast routing decisions within each node as a message either turns to a new dimension or continues along the same dimension. the router is input buffered. with six input links and four virtual channels/link, a 24-to-1 arbitration is required on each crossbar output. the speed of operation and the length of the signal paths led to the adoption of a two-stage arbitration scheme. the ﬁrst stage arbitrates between corresponding virtual channels from the six inputs (e.g., a 6-to-1 arbiter for virtual channel 0 on each link). the second stage is a 4-to-1 arbitration for all channels with valid data competing for the same physical output link. the internal data paths are 16 bits, matched to the output links. the ports were designed such that they could be reset while the network was operational. this would enable removal and replacement of routers and cpu boards while the network was active. a block diagram of the network interface is shown in figure 7.16. the router side of the interface utilizes portions of the router architecture conﬁgured as a three-port design. one port is used for message injection and ejection, while two other ports are used for connection to two routers. this ﬂexibility permits the construction of more complex topologies. this is often desired for the purposes of fault tolerance or simply increased bandwidth. each output virtual channel is 384 bytes deep, while each input virtual channel is 256 bytes deep. there are two additional interesting components of the interface. the ﬁrst is the remote memory access (rma) engine. the rma engine
within a group can be thought of as possessing a communicator for the communication domain within which it is a member. the communicator is logically a set of links to other processes within the same group (i.e., communication domain) and is referred to as an intracommunicator. the relationship between intracommunicators in a communication domain is illustrated in figure 8.8.
the notion of a communicator provides a very simple concept with which to structure groups of processes. often it is desirable to allocate one subgroup of processes to a speciﬁc subtask while another subgroup of processes is tasked with a distinct computation. processes within subgroups will communicate among themselves. there will also usually be communication between processes in distinct subgroups. in mpi terminology, each subgroup will have intracommunicators for communication within the subgroup. each process must also possess intercommunicators for communication between subgroups. logically the intercommunicator used by processes within one group can be thought of as links to processes within the other group and vice versa. this information captured within intercommunicators is illustrated in figure 8.9.
the linear rank of a process within a group provides no information about the structure of the problem, and a great deal of mental bookkeeping is often required to orchestrate interprocessor communication to follow the structure of the problem. we often keep a mental map or even a physical drawing of how processes must communicate. to facilitate such thinking, mpi provides for the speciﬁcation and use of virtual topologies of processes within a group. this virtual topology can be used for realizing efﬁcient assignments of processes to processors or for writing scalable programs. a substantial body of work exists on algorithms for mapping parallel programs onto parallel architectures. optimal mapping
6. the route taken by a message depends on its destination and the status of output channels (free or busy). at a given node, an adaptive routing function supplies a set of output channels based on the current and destination nodes. a selection from this set is made based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied. if all the output channels are busy, the message header will be routed again until it is able to reserve a channel, thus getting the ﬁrst channel that becomes free.
8. the routing function may allow messages to follow nonminimal paths. 9. for each source/destination pair, the routing function will supply at least one minimal path. this assumption is only required to prove the necessary condition for deadlock freedom.
an interconnection network i is a strongly connected directed multigraph, i = g(n, c). the vertices of the multigraph n represent the set of processing nodes. the arcs of the multigraph c represent the set of communication channels. more than a single channel is allowed to connect a given pair of nodes. each channel ci has an associated queue with capacity cap(ci ). the source and destination nodes of channel ci are denoted si and di, respectively. for the sake of simplicity, an enumeration of arbitrary channels is denoted c1, c2, . . . , ck instead of cn1 , cn2 , . . . , cnk . an enumeration does not imply any channel ordering. let f be the set of valid channel status, f = {free, busy}. an adaptive routing function r : n × n → p(c), where p(c) is the power set of c, supplies a set of alternative output channels to send a message from the current node nc to the destination node nd, r(nc, nd ) = {c1, c2, . . . , cp}. by deﬁnition, r(n, n) = ∅, ∀n ∈ n. a selection function s : p(c × f ) → c selects a free output channel (if any) from the set supplied by the routing function. from the deﬁnition, s takes into account the status of all the channels belonging to the set supplied by the routing function. the selection can be random or based on static or dynamic priorities.
network interface have the same capacity as the individual links, then a radix of 8 in each dimension can be expected to produce a balanced demand on the links and ports.
the router architecture is based on ﬁxed-path, dimension-order wormhole switching. the organization of the relevant components is shown in figure 7.18. there are four unidirectional virtual channels in each direction over a physical link. these four virtual channels are partitioned into two virtual networks with two virtual channels/link. one network is referred to as the request network and transmits request packets. the second network is the reply network carrying only reply packets. the two virtual channels within each network prevent deadlock due to the wraparound channels as described below. adjacent routers communicate over a bidirectional, full-duplex physical channel, with each direction comprised of 16 data bits and 8 control bits as shown in figure 7.18. the 4 forward control channel bits are used to identify the type of message packet (request or response) and the destination virtual channel buffer. the four acknowledge control signals are used for ﬂow control and signify empty buffers on the receiving router. due to the time required for channel ﬂow control (e.g., generation of acknowledgments), as long as messages are integral multiples of 16 bits (see message formats below), full link utilization can be achieved. otherwise some idle physical channel cycles are possible. the router is physically partitioned into three switches as shown in the ﬁgure. messages are routed to ﬁrst correct the offset in the x, then the y , and ﬁnally the z dimension. the ﬁrst switch is connected to the injection channel from the network interface and is used to forward the message in the x+ or x− direction. when the message has completed traversal in the x dimension, the message moves to the next switch in the intermediate router for forwarding along the y dimension, and then to the z dimension. message transition between switches is via interdimensional virtual channels. for the sake of clarity the ﬁgure omits some of the virtual channels.
from the point of view of router performance we are interested in two parameters [57]. when a message ﬁrst arrives at a router, it must be examined to determine the output channel over which the message is to be forwarded. this is referred to as the routing delay and typically includes the time to set the switch. once a path has been established through a router by the switch, we are interested in the rate at which messages can be forwarded through the switch. this rate is determined by the propagation delay through the switch (intrarouter delay) and the signaling rate for synchronizing the transfer of data between the input and output buffers. this delay has been characterized as the internal ﬂow control latency [57]. similarly, the delay across the physical links (interrouter delay) is referred to as the external ﬂow control latency. the routing delay and ﬂow control delays collectively determine the achievable message latency through the switch and, along with contention by messages for links, determine the network throughput.
the following section addresses some basic concepts in the implementation of the switching layer, assuming the generic router model shown in figure 2.1. the remainder of the chapter focuses on alternative implementations of the switching layer.
switching layers can be distinguished by the implementation and relative timing of ﬂow control operations and switching techniques. in addition, these operations may be overlapped with the time to make routing decisions.
the offset in one dimension before routing in the next one. a routing example is shown in figure 4.2. note that dimension-order routing can be executed at the source node, storing information about turns (changes of dimension) in the header. this is the street-sign routing algorithm described above. dimension-order routing can also be executed in a distributed manner. at each intermediate node, the routing algorithm supplies an output channel crossing the lowest dimension for which the offset is not null.
some manufacturers supply building blocks for parallel computers, so that different topologies can be built by using the same chips [227].also, some parallel computers feature reconﬁgurable topologies, so that the user can change the topology even dynamically at run time [112]. finally, some manufacturers allow the ﬁnal users to build the topology that best ﬁts their needs, allowing the use of irregular topologies [257]. in all these cases, especially for irregular topologies, it is very difﬁcult to derive a routing algorithm based on a ﬁnite-state machine. an alternative implementation approach consists of using table lookup [338]. this is a traditional approach used in computer networks.
an obvious implementation of table-lookup routing is to place a routing table at each node, with the number of entries in the table equal to the number of nodes in the network. once again, routing can be performed either at the source node or at each intermediate node. in the ﬁrst case, given a destination node address, the corresponding entry in the table indicates the whole path to reach that node. in the second case, each table entry indicates which outgoing channel should be used to forward the packet toward its destination. however, such an implementation is only practical for very small systems because table size increases linearly with network size. one way to reduce the table size for distributed routing is to deﬁne a range of addresses to be associated with each output channel. in this case, each routing table requires only one entry per output channel. each entry contains an interval of destination addresses, speciﬁed by indicating its bounds. this routing technique is called interval routing [199] and has been implemented in the inmos t9000 transputer and its associated router c104 [227, 228]. an important issue in interval routing is how to assign appropriate labels to nodes so that a single interval per output channel is enough to route all the packets and the resulting routing algorithm is minimal and deadlock-free. see exercise 4.2 for an example of interval routing.
within a group can be thought of as possessing a communicator for the communication domain within which it is a member. the communicator is logically a set of links to other processes within the same group (i.e., communication domain) and is referred to as an intracommunicator. the relationship between intracommunicators in a communication domain is illustrated in figure 8.8.
the notion of a communicator provides a very simple concept with which to structure groups of processes. often it is desirable to allocate one subgroup of processes to a speciﬁc subtask while another subgroup of processes is tasked with a distinct computation. processes within subgroups will communicate among themselves. there will also usually be communication between processes in distinct subgroups. in mpi terminology, each subgroup will have intracommunicators for communication within the subgroup. each process must also possess intercommunicators for communication between subgroups. logically the intercommunicator used by processes within one group can be thought of as links to processes within the other group and vice versa. this information captured within intercommunicators is illustrated in figure 8.9.
the linear rank of a process within a group provides no information about the structure of the problem, and a great deal of mental bookkeeping is often required to orchestrate interprocessor communication to follow the structure of the problem. we often keep a mental map or even a physical drawing of how processes must communicate. to facilitate such thinking, mpi provides for the speciﬁcation and use of virtual topologies of processes within a group. this virtual topology can be used for realizing efﬁcient assignments of processes to processors or for writing scalable programs. a substantial body of work exists on algorithms for mapping parallel programs onto parallel architectures. optimal mapping
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
hence, it is not possible to cross every dimension from every node. crossing a given dimension from a given node may require moving in another dimension ﬁrst.
the most interesting property of strictly orthogonal topologies is that routing is very simple. thus, the routing algorithm can be efﬁciently implemented in hardware. effectively, in a strictly orthogonal topology, nodes can be numbered by using their coordinates in the n-dimensional space. since each link traverses a single dimension and every node has at least one link crossing each dimension, the distance between two nodes can be computed as the sum of dimension offsets. also, the displacement along a given link only modiﬁes the offset in the corresponding dimension. taking into account that it is possible to cross any dimension from any node in the network, routing can be easily implemented by selecting a link that decrements the absolute value of the offset in some dimension. the set of dimension offsets can be stored in the packet header and updated (by adding or subtracting one unit) every time the packet is successfully routed at some intermediate node. if the topology is not strictly orthogonal, however, routing may become much more complex.
the most popular direct networks are the n-dimensional mesh, the k-ary n-cube or torus, and the hypercube. all of them are strictly orthogonal. formally, an n-dimensional mesh has k0 × k1 × · · · × kn−2 × kn−1 nodes, ki nodes along each dimension i, where ki ≥ 2 and 0 ≤ i ≤ n − 1. each node x is identiﬁed by n coordinates, (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ ki − 1 for 0 ≤ i ≤ n − 1. two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = xj ± 1. thus, nodes have from n to 2n neighbors, depending on their location in the mesh. therefore, this topology is not regular.
in a bidirectional k-ary n-cube [70], all nodes have the same number of neighbors. the deﬁnition of a k-ary n-cube differs from that of an n-dimensional mesh in that all of the ki are equal to k and two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = (xj ± 1) mod k. the change to modular arithmetic in the deﬁnition adds wraparound channels to the k-ary n-cube, giving it regularity and symmetry. every node has n neighbors if k = 2, and 2n neighbors if k > 2. when n = 1, the k-ary n-cube collapses to a bidirectional ring with k nodes.
another topology with regularity and symmetry is the hypercube, which is a special case of both n-dimensional meshes and k-ary n-cubes. a hypercube is an n-dimensional mesh in which ki = 2 for 0 ≤ i ≤ n − 1, or a 2-ary n-cube, also referred to as a binary n-cube.
figure 1.5(a) depicts a binary 4-cube or 16-node hypercube. figure 1.5(b) illustrates a 3-ary 2-cube or two-dimensional (2-d) torus. figure 1.5(c) shows a 3-ary threedimensional (3-d) mesh, resulting by removing the wraparound channels from a 3-ary 3-cube.
two conﬂicting requirements of a direct network are that it must accommodate a large number of nodes while maintaining a low network latency. this issue will be addressed in chapter 7.
that each node has a list of faulty links and nodes within a k-neighborhood [198], that is, all faulty components within a distance of k links. the preceding routing algorithm might be modiﬁed as follows. consider an intermediate node x = (xn−1, xn−2, . . . , x1, x0) that receives a message with a coordinate sequence (c1, c2, . . . , cr ) that describes the remaining sequence of dimensions to be traversed to the destination. from the coordinate sequence we can construct the addresses of all nodes on a minimum-length path to the destination. for example, from intermediate node 0110, the coordinate sequence (0, 2, 3) describes a path through nodes 0111, 0011, and 1011. by examining all possible r! orderings of the coordinate sequence, we can generate all paths to the destination. if the ﬁrst k elements of any such path are nonfaulty, the message can be forwarded along this path. the complexity of this operation can be reduced by examining only the r disjoint paths to the destination. these disjoint paths correspond to the following coordinate sequences: (c1, c2, . . . , cr ), (c2, c3, . . . , cr , c1), (c3, c4, . . . , cr , c1, c2), and so on.
rather than explicitly maintaining a list of faulty components in a k-neighborhood, an alternative method for expanding the extent of fault information is by encoding the state of a node. lee and hayes [198] introduced the notion of unsafe node. a nonfaulty node is unsafe if it is adjacent to two or more faulty or unsafe nodes. each node now maintains two lists: one of faulty adjacent nodes and one for unsafe adjacent nodes. with a ﬁxed set of faulty components, the status of a node may be faulty, nonfaulty, or unsafe. the status can be computed for all nodes in parallel in o(log3 n ) steps for n nodes. each node examines the state of its neighbors and changes its state if two or more are unsafe/faulty. a node need only transmit state information to the neighboring nodes if the local state has changed. this labeling algorithm will produce rectangular regions of nodes marked as faulty or unsafe. figure 6.8 shows an example of the result of the labeling process in a 2-d mesh.
with fault distributions encoded in the state of the nodes, the approach toward routing now can be further modiﬁed. the unsafe status of nodes serves as a warning that messages may become trapped or delayed if routed via these nodes. therefore, the routing algorithm
options node, physical or virtual link block, convex, concave, random static, dynamic, transient mttr, mtbf inoperable, receive, transmit local, global, k-neighborhood, encoded
of all faulty nodes within distance k. in practice it may not be possible to determine the status of all nodes within distance k since some nodes may be unreachable due to faults. this leads to the less restrictive notion of k-reachability diagnosis [28]. in this case, each nonfaulty node can determine the status of each faulty node within distance k that is reachable via nonfaulty nodes. algorithms for diagnosis are a function of the type of faults; dynamic fault environments will result in diagnosis algorithms different from those that occur in static fault environments.
in summary, the power and ﬂexibility of fault-tolerant routing algorithms are heavily inﬂuenced by the characteristics of the fault model. the model in turn speciﬁes alternatives for one or more attributes as discussed in this section. examples are provided in table 6.1.
this section describes several useful paradigms for rerouting message packets in the presence of faulty network components. in saf and vct networks the issue of deadlock avoidance becomes one of buffer management. packets cannot make forward progress if there are no buffers available at an adjacent router. unless otherwise stated, the techniques described in this section generally address the issue of rerouting around faulty components under the assumption that the issue of deadlock freedom is guaranteed by appropriate buffer management techniques such as those described in chapter 4. rerouting around faulty components necessitates nonminimal routing, and therefore the routing algorithms must ensure that livelock is avoided.
the early research on fault-tolerant routing in saf and vct networks largely focused on high-dimensional networks such as the binary hypercube topology. this was in part due to the dependence of message latency on distance and the low average internode distance in such topologies. additional beneﬁts included low diameter, logarithmic number of ports per router, and an interconnection structure that matched the communication topology of many parallel algorithms. in principle, the techniques developed for binary hypercubes can be suitably extended to the more general k-ary n-cube and often the multidimensional mesh topologies.
r3, a copied signal is transmitted to r1 so that r1 may invalidate its copy of the ﬂit. in this way exactly two copies of each data ﬂit are maintained in the network at all times. a more detailed description of the architecture of the router implementation can be found in chapter 7.
an alternative to ﬂit-level recovery is to ﬁnd and discard the interrupted message components and retransmit the message from the source. recovery is at the level of complete messages rather than at the ﬂit level. the pcs-based solutions exploit the fact that a separate control network comprised of the control channels exists. consider a message pipeline where a fault is detected on a reserved virtual link or physical channel as shown in figure 6.39. the link is marked faulty. the link controller at the source end of the faulty virtual link introduces a release ﬂit (referred to as kill ﬂit in [123]) into the complementary virtual control channel of the virtual link upstream from the fault. this release ﬂit is routed back to the source router. the link controller at the destination end of the faulty virtual link introduces a release ﬂit into the corresponding virtual control channel of the virtual link downstream from the failed virtual link. this release ﬂit is propagated along toward the destination. when a release ﬂit arrives at a node, the input and output virtual links associated with the message are released, and the ﬂit is propagated toward the source (or destination). if multiple faults occur in one message pipeline, this mechanism is applied recursively to fault-free segments.
when a release ﬂit arrives at an intermediate router node, the set of actions taken by that node will depend on the state of the interrupted message pipeline in that node. we have the following rules that are to be implemented by each router node. in all of the following steps it is implicit that virtual links and associated buffers are released. if two control ﬂits for the same message arrive at a router node (from opposite directions along the path), the two control ﬂits collide at that router node. a ﬂit in progress toward the destination (source) is referred to as a forward (reverse) ﬂit. the following rules govern the collision of control ﬂits and are based on the assumption [123] that paths are removed by message acknowledgments from the destination rather than by the last ﬂit of the message. thus, a path is not removed until the last ﬂit has been delivered to the destination. this behavior guarantees message delivery in the presence of dynamic or transient faults.
1. if a forward release ﬂit collides with a reverse release ﬂit, remove both release ﬂits from the network. this may occur with multiple faults within the same message pipeline.
3. if a release ﬂit reaches the source, inform the message handler that the message transmission has failed. the handler may choose to retransmit or to invoke some higher-level fault-handling algorithm.
deadlocks might be for certain network conﬁgurations and to understand the parameters that most inﬂuence deadlock formation. for example, wormhole switching is more prone to deadlock than are other switching techniques [347]. this is because each packet may hold several channel resources spanning multiple nodes in the network while being blocked. we therefore concentrate on wormhole-switched recovery schemes.
recent work by pinkston and warnakulasuriya on characterizing deadlocks in interconnection networks has shown that a number of interrelated factors inﬂuence the probability of deadlock formation [283, 347].among the more inﬂuential of these factors is the routing freedom, the number of blocked packets, and the number of resource dependency cycles. routing freedom corresponds to the number of routing options available to a packet being routed at a given node within the network and can be increased by adding physical channels, adding virtual channels, and/or increasing the adaptivity of the routing algorithm. it has been quantitatively shown that as the number of network resources (physical and virtual channels) and the routing options allowed on them by the routing function increase (routing freedom), the number of packets that tend to block signiﬁcantly decreases, the number of resource dependency cycles decreases, and the resulting probability of deadlock also decreases exponentially. this is due to the fact that deadlocks require highly correlated patterns of cycles, the complexity of which increases with routing freedom. a conclusion of pinkston and warnakulasuriya’s work is that deadlocks in interconnection networks can be highly improbable when sufﬁcient routing freedom is provided by the network and fully exploited by the routing function. in fact, it has been shown that as few as two virtual channels per physical channel are sufﬁcient to virtually eliminate all deadlocks up to and beyond saturation in 2-d toroidal networks when using unrestricted fully adaptive routing [283, 347]. this veriﬁes previous empirical results that estimated that deadlocks are infrequent [9, 179]. however, the frequency of deadlock increases considerably when no virtual channels are used.
the network state reﬂecting resource allocations and requests existing at a particular point in time can be depicted by the channel wait-for graph (cwg). the nodes of this graph are the virtual channels reserved and/or requested by some packet(s). the solid arcs point to the next virtual channel reserved by the corresponding packet. the dashed arcs in the cwg point to the alternative virtual channels a blocked packet may acquire in order to continue routing. the highly correlated resource dependency pattern required to form a deadlocked conﬁguration can be analyzed with the help of the cwg. a knot is a set of nodes in the graph such that from each node in the knot it is possible to reach every other node in the knot. the existence of a knot is a necessary and sufﬁcient condition for deadlock [283]. note that checking the existence of knots requires global information and cannot be efﬁciently done at run time.
deadlocks can be characterized by deadlock set, resource set, and knot cycle density attributes. the deadlock set is the set of packets that own the virtual channels involved in the knot. the resource set is the set of all virtual channels owned by members of
the deadlock set. the knot cycle density represents the number of unique cycles within the knot. the knot cycle density indicates complexity in correlated resource dependency required for deadlock. deadlocks with smaller knot cycle densities are likely to affect local regions of the network only, whereas those having larger knot cycle densities are likely to affect more global regions of the network and will therefore be more harmful.
figure 3.14 shows eight packets (m1, m2, m3, m4, m5, m6, m7, and m8) being routed minimal-adaptively in a network with two virtual channels per physical channel. the source and destination nodes of packet mi are labeled si and di, respectively. packet m1 has acquired channels vc0 and vc1, and requires either vc3 or vc11 to continue. similarly, all other packets have also acquired two virtual channels each and are waiting to acquire one of two possible alternative channels in order to proceed. all of the packets depicted here are waiting for resources held by other packets in the group and therefore will be blocked indeﬁnitely, thus constituting a deadlock. the cwg for the packets in this example is also depicted in figure 3.14. the deadlock set is the set {m0 . . . m7}. the resource set is {vc0 . . . vc15}. the knot cycle density is 24.
a cyclic nondeadlocked conﬁguration in which multiple cycles exist but do not form a deadlock is depicted in figure 3.15. the dependencies are similar to those
although there are many similarities between interconnection networks for multicomputers and dsms, it is important to keep in mind that performance requirements may be very different. messages are usually very short when dsms are used. additionally, network latency is important because memory access time depends on that latency. however, messages are typically longer and less frequent when using multicomputers. usually the programmer is able to adjust the granularity of message communication in a multicomputer. on the other hand, interconnection networks for multicomputers and nows are mainly used for message passing. however, the geographical distribution of workstations usually imposes constraints on the way processors are connected. also, individual processors may be connected to or disconnected from the network at any time, thus imposing additional design constraints.
interconnection networks play a major role in the performance of modern parallel computers. there are many factors that may affect the choice of an appropriate interconnection network for the underlying parallel computer. these factors include the following:
1. performance requirements. processes executing in different processors synchronize and communicate through the interconnection network. these operations are usually performed by explicit message passing or by accessing shared variables. message latency is the time elapsed between the time a message is generated at its source node and the time the message is delivered at its destination node. message latency directly affects processor idle time and memory access time to remote memory locations. also, the network may saturate—it may be unable to deliver the ﬂow of messages injected by the nodes, limiting the effective computing power of a parallel computer. the maximum amount of information delivered by the network per time unit deﬁnes the throughput of that network.
2. scalability. a scalable architecture implies that as more processors are added, their memory bandwidth, i/o bandwidth, and network bandwidth should increase proportionally. otherwise the components whose bandwidth does not scale may become a bottleneck for the rest of the system, decreasing the overall efﬁciency accordingly.
3. incremental expandability. customers are unlikely to purchase a parallel computer with a full set of processors and memories. as the budget permits, more processors and memories may be added until a system’s maximum conﬁguration is reached. in some interconnection networks, the number of processors must be a power of 2, which makes them difﬁcult to expand. in other cases, expandability is provided at the cost of wasting resources. for example, a network designed for a maximum
affected by the number of dimensions of the network, the routing algorithm, and the number of virtual channels per physical channel. the evaluation presented in section 9.10 considers a very detailed model, trying to be as close as possible to the real behavior of the networks. as will be seen, conclusions are a bit different when the impact of design parameters on clock frequency is considered.
in addition to unicast messages, many parallel applications perform some collective communication operations. providing support for those operations may reduce latency signiﬁcantly. collective communication schemes range from software approaches based on unicast messages to speciﬁc hardware support for multidestination messages. these schemes are evaluated in section 9.11.
finally, most network evaluations only focus on performance. however, reliability is also important. a wide spectrum of routing protocols has been proposed to tolerate faulty components in the network, ranging from software approaches for wormhole switching to very resilient mechanisms based on different switching techniques. thus, we will also present some results concerning network fault tolerance in section 9.13.
this section introduces performance metrics and deﬁnes some standard ways to illustrate performance results. in the absence of faults, the most important performance metrics of an interconnection network are latency and throughput.
latency is the time elapsed from when the message transmission is initiated until the message is received at the destination node. this general deﬁnition is vague and can be interpreted in different ways. if the study only considers the network hardware, latency is usually deﬁned as the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. if the study also considers the injection queues, the queuing time at the source node is added to the latency. this queuing time is usually negligible unless the network is close to its saturation point. when the messaging layer is also being considered, latency is deﬁned as the time elapsed from when the system call to send a message is initiated at the source node until the system call to receive that message returns control to the user program at the destination node.
latency can also be deﬁned for collective communication operations. in this case, latency is usually measured from when the operation starts at some node until all the nodes involved in that operation have completed their task. for example, when only the network hardware is being considered, the latency of a multicast operation is the time elapsed from when the ﬁrst fragment of the message header is injected into the network at the source node until the last unit of information is received at the last destination node.
the latency of individual messages is not important, especially when the study is performed using synthetic workloads. in most cases, the designer is interested in the average value of the latency. the standard deviation is also important because the execution time of parallel programs may increase considerably if some messages experience a much
once the set of valid paths for multidestination messages has been determined, it is necessary to deﬁne routing algorithms for collective communication. the hierarchical leader-based (hl) scheme has been proposed in [269] to implement multicast and broadcast. given a multicast destination set, this scheme tries to group the destinations in a hierarchical manner so that the minimum number of messages is needed to cover all the destinations. since the multidestination messages conform to paths supported by the base routing, the grouping scheme takes into account this routing scheme and the spatial positions of destination nodes to achieve the best grouping. once the grouping is achieved, multicast and broadcast take place by traversing nodes from the source in a reverse hierarchy. consider a multicast pattern from a source s with a destination set d. let l0 denote the set d ∪ {s}. the hierarchical scheme, in its ﬁrst step, partitions the set l0 into disjoint subsets with a leader node representing each subset. the leader node is chosen in such a way that it can forward a message to the members of its set using a single multidestination message under the brcp model. for example, if a 2-d mesh implements xy routing, then the partitioning is done such that the nodes in each set lie on a row, column, or row-column.
let the leaders obtained by the above ﬁrst-step partitioning be termed as level-1 leaders and identiﬁed by a set l1. this set l1 can be further partitioned into disjoint subsets with a set of level-2 leaders. this process of hierarchical grouping is continued as long as it is proﬁtable, as indicated below. assuming that the grouping is carried out for m steps, there will be m sets of leaders, satisfying that lm ⊂ lm−1 ⊂ · · · ⊂ l1 ⊂ l0.
after the grouping is achieved, the multicast takes place in two phases. in the ﬁrst phase, the source node performs unicast-based multicast (see section 5.7) to the set lm. the second phase involves m steps of multidestination message passing. it starts with the leaders in the set lm and propagates down the hierarchical grouping in a reverse fashion to cover the lower-level leaders and, ﬁnally, all the nodes in destination set d.
as mentioned above, the hierarchical grouping is continued as long as it is proﬁtable in order to reduce the multicast latency. as start-up latency dominates communication latency in networks using pipelined data transmission, the minimum multicast latency is usually achieved by minimizing the number of communication steps. as will be seen in
section 5.7, unicast-based multicast requires(cid:12)log2(|lm|+ 1)(cid:13) steps to reach all the leaders in the set lm, assuming that the source node does not belong to that set. therefore, the size of the set lm should be reduced. however, while going from lm−1 to lm, one additional step of multidestination communication is introduced into the multicast latency. hence, it can be seen that when (cid:12)log2(|lm−1| + 1)(cid:13) > (cid:12)log2(|lm| + 1)(cid:13) + 1, it is proﬁtable to go through an additional level of grouping. the previous expression assumes that the source node does not belong to lm−1. if the source node belongs to lm−1 or lm, then the cardinal of the corresponding set must be reduced by one unit.
figure 5.37 shows an example of multicast routing in a 2-d mesh using xy routing under the hierarchical leader-based scheme. given a destination set, the ﬁrst-level grouping can be done along rows (dimension 0) to obtain the level-1 leaders as shown in the ﬁgure. the level-1 leaders can be grouped along columns
that messages can travel larger distances. in particular, when l is equal to 1/e, we obtain an exponential distribution. as l approaches one, the distribution function  approaches the uniform distribution. conversely, as l approaches zero,  approaches a nearest-neighbor communication pattern. it should be noted that the decreasing probability distribution is adequate for the analysis of networks of different sizes. simply, decay(l, dmax) should be computed for each network.
the distributions described above exhibit different degrees of spatial locality but have no temporal locality. recently, several speciﬁc communication patterns between pairs of nodes have been used to evaluate the performance of interconnection networks: bit reversal, perfect shufﬂe, butterﬂy, matrix transpose, and complement. these communication patterns take into account the permutations that are usually performed in parallel numerical algorithms [175, 200, 239]. in these patterns, the destination node for the messages generated by a given node is always the same. therefore, the utilization factor of all the network links is not uniform. however, these distributions achieve the maximum degree of temporal locality. these communication patterns can be deﬁned as follows:
bit reversal. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, a1, . . . , an−2, an−1. perfect shufﬂe. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−2, an−3, . . . , a0, an−1 (rotate left 1 bit). butterﬂy. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, an−2, . . . , a1, an−1 (swap the most and least signiﬁcant bits). matrix transpose. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a n 2 complement. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−1, an−2, . . . , a1, a0.
finally, a distribution based on a least recently used stack model has been proposed in [288] to model temporal locality. in this model, each node has its own stack containing the m nodes that were most recently sent messages. for each position in the stack there is a probability of sending a message to the node in that position. the sum of probabilities for nodes in the stack is less than one. therefore, a node not currently in the stack may be chosen as the destination for the next transmission. in this case, after sending the message, its destination node will be included in the stack, replacing the least recently used destination.
for synthetic workloads, the injection rate is usually the same for all the nodes. in most cases, each node is chosen to generate messages according to an exponential distribution. the parameter λ of this distribution is referred to as the injection rate. other possible distributions include a uniform distribution within an interval, bursty trafﬁc, and traces from parallel applications. for the uniform distribution, the injection rate is the mean value of the interval. bursty trafﬁc can be generated either by injecting a burst of messages every
note that a conﬁguration describes the state of an interconnection network at a given time. thus, we do not need to consider conﬁgurations describing impossible situations. a conﬁguration is legal if it describes a possible situation. in particular, we should not consider conﬁgurations in which buffer capacity is exceeded. also, a packet cannot occupy a given channel unless the routing function supplies it for the packet destination. figure 3.3 shows an illegal conﬁguration for the above-deﬁned routing function r because a packet followed a nonminimal path, and r only forwards packets following minimal paths.
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
once the set of valid paths for multidestination messages has been determined, it is necessary to deﬁne routing algorithms for collective communication. the hierarchical leader-based (hl) scheme has been proposed in [269] to implement multicast and broadcast. given a multicast destination set, this scheme tries to group the destinations in a hierarchical manner so that the minimum number of messages is needed to cover all the destinations. since the multidestination messages conform to paths supported by the base routing, the grouping scheme takes into account this routing scheme and the spatial positions of destination nodes to achieve the best grouping. once the grouping is achieved, multicast and broadcast take place by traversing nodes from the source in a reverse hierarchy. consider a multicast pattern from a source s with a destination set d. let l0 denote the set d ∪ {s}. the hierarchical scheme, in its ﬁrst step, partitions the set l0 into disjoint subsets with a leader node representing each subset. the leader node is chosen in such a way that it can forward a message to the members of its set using a single multidestination message under the brcp model. for example, if a 2-d mesh implements xy routing, then the partitioning is done such that the nodes in each set lie on a row, column, or row-column.
let the leaders obtained by the above ﬁrst-step partitioning be termed as level-1 leaders and identiﬁed by a set l1. this set l1 can be further partitioned into disjoint subsets with a set of level-2 leaders. this process of hierarchical grouping is continued as long as it is proﬁtable, as indicated below. assuming that the grouping is carried out for m steps, there will be m sets of leaders, satisfying that lm ⊂ lm−1 ⊂ · · · ⊂ l1 ⊂ l0.
after the grouping is achieved, the multicast takes place in two phases. in the ﬁrst phase, the source node performs unicast-based multicast (see section 5.7) to the set lm. the second phase involves m steps of multidestination message passing. it starts with the leaders in the set lm and propagates down the hierarchical grouping in a reverse fashion to cover the lower-level leaders and, ﬁnally, all the nodes in destination set d.
as mentioned above, the hierarchical grouping is continued as long as it is proﬁtable in order to reduce the multicast latency. as start-up latency dominates communication latency in networks using pipelined data transmission, the minimum multicast latency is usually achieved by minimizing the number of communication steps. as will be seen in
section 5.7, unicast-based multicast requires(cid:12)log2(|lm|+ 1)(cid:13) steps to reach all the leaders in the set lm, assuming that the source node does not belong to that set. therefore, the size of the set lm should be reduced. however, while going from lm−1 to lm, one additional step of multidestination communication is introduced into the multicast latency. hence, it can be seen that when (cid:12)log2(|lm−1| + 1)(cid:13) > (cid:12)log2(|lm| + 1)(cid:13) + 1, it is proﬁtable to go through an additional level of grouping. the previous expression assumes that the source node does not belong to lm−1. if the source node belongs to lm−1 or lm, then the cardinal of the corresponding set must be reduced by one unit.
figure 5.37 shows an example of multicast routing in a 2-d mesh using xy routing under the hierarchical leader-based scheme. given a destination set, the ﬁrst-level grouping can be done along rows (dimension 0) to obtain the level-1 leaders as shown in the ﬁgure. the level-1 leaders can be grouped along columns
once the set of valid paths for multidestination messages has been determined, it is necessary to deﬁne routing algorithms for collective communication. the hierarchical leader-based (hl) scheme has been proposed in [269] to implement multicast and broadcast. given a multicast destination set, this scheme tries to group the destinations in a hierarchical manner so that the minimum number of messages is needed to cover all the destinations. since the multidestination messages conform to paths supported by the base routing, the grouping scheme takes into account this routing scheme and the spatial positions of destination nodes to achieve the best grouping. once the grouping is achieved, multicast and broadcast take place by traversing nodes from the source in a reverse hierarchy. consider a multicast pattern from a source s with a destination set d. let l0 denote the set d ∪ {s}. the hierarchical scheme, in its ﬁrst step, partitions the set l0 into disjoint subsets with a leader node representing each subset. the leader node is chosen in such a way that it can forward a message to the members of its set using a single multidestination message under the brcp model. for example, if a 2-d mesh implements xy routing, then the partitioning is done such that the nodes in each set lie on a row, column, or row-column.
let the leaders obtained by the above ﬁrst-step partitioning be termed as level-1 leaders and identiﬁed by a set l1. this set l1 can be further partitioned into disjoint subsets with a set of level-2 leaders. this process of hierarchical grouping is continued as long as it is proﬁtable, as indicated below. assuming that the grouping is carried out for m steps, there will be m sets of leaders, satisfying that lm ⊂ lm−1 ⊂ · · · ⊂ l1 ⊂ l0.
after the grouping is achieved, the multicast takes place in two phases. in the ﬁrst phase, the source node performs unicast-based multicast (see section 5.7) to the set lm. the second phase involves m steps of multidestination message passing. it starts with the leaders in the set lm and propagates down the hierarchical grouping in a reverse fashion to cover the lower-level leaders and, ﬁnally, all the nodes in destination set d.
as mentioned above, the hierarchical grouping is continued as long as it is proﬁtable in order to reduce the multicast latency. as start-up latency dominates communication latency in networks using pipelined data transmission, the minimum multicast latency is usually achieved by minimizing the number of communication steps. as will be seen in
section 5.7, unicast-based multicast requires(cid:12)log2(|lm|+ 1)(cid:13) steps to reach all the leaders in the set lm, assuming that the source node does not belong to that set. therefore, the size of the set lm should be reduced. however, while going from lm−1 to lm, one additional step of multidestination communication is introduced into the multicast latency. hence, it can be seen that when (cid:12)log2(|lm−1| + 1)(cid:13) > (cid:12)log2(|lm| + 1)(cid:13) + 1, it is proﬁtable to go through an additional level of grouping. the previous expression assumes that the source node does not belong to lm−1. if the source node belongs to lm−1 or lm, then the cardinal of the corresponding set must be reduced by one unit.
figure 5.37 shows an example of multicast routing in a 2-d mesh using xy routing under the hierarchical leader-based scheme. given a destination set, the ﬁrst-level grouping can be done along rows (dimension 0) to obtain the level-1 leaders as shown in the ﬁgure. the level-1 leaders can be grouped along columns
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
theorem 6.1 is only applicable to physical channels if they are not split into virtual channels. if virtual channels are used, the theorem is only valid for virtual channels. however, it can be easily extended to support the failure of physical channels by considering that all the virtual channels belonging to a faulty physical channel will become faulty at the same time. theorem 6.1 is based on theorem 3.1. therefore, it is valid for the same switching techniques as theorem 3.1, as long as edge buffers are used.
the structure of fault-tolerant routing algorithms is a natural consequence of the types of faults that can occur and our ability to diagnose them. the patterns of component failures and expectations about the behavior of processors and routers in the presence of these failures determines the approaches to achieve deadlock and livelock freedom. this information is captured in the fault model. the fault-tolerant computing literature is extensive and thorough in the deﬁnition of fault models for the treatment of faulty digital systems. in this section we will focus on common fault models that have been employed in the design of fault-tolerant routing algorithms for reliable interprocessor networks.
one of the ﬁrst considerations is the level at which components are diagnosed as having failed. typically, detection mechanisms are assumed to have identiﬁed one of two classes of faults. either the entire processing element (pe) and its associated router can fail, or any communication channel may fail. the former is referred to as a node failure, and the latter as a link failure. on a node failure, all physical links incident on the failed node are also marked faulty at adjacent routers. when a physical link fails, all virtual channels on that particular physical link are marked faulty. note that many types of failures will simply manifest themselves as link or node failures. for example, the failure of the link controller, or the virtual channel buffers, appears as a link failure. on the other hand, the failure of the router control unit or the associated pe effectively appears as a node failure. even software errors in the messaging layer can lead to message handlers “locking up” the local interface and rendering the attached router inoperative, effectively resulting in a node fault. hence, this failure model is not as restrictive as it may ﬁrst appear.
this model of individual link and node failures leads to patterns of failed components. adjacent faulty links and faulty nodes are coalesced into fault regions. generally, it is assumed that fault regions do not disconnect the network, since each connected network component can be treated as a distinct network. constraints may now be placed on the structure of these fault regions. the most common constraint employed is that these regions be convex. as will become apparent in this chapter, concave regions present unique difﬁculties for fault-tolerant routing algorithms. some examples of fault regions are illustrated in figure 6.3. convex regions may be further constrained to be block fault regions—regions whose shape is rectangular. this distinction is meaningful only in some topologies, whereas in other topologies, convex faults imply a block structure. given a pattern of random faults in a multidimensional k-ary n-cube, rectangular fault regions can be constructed by marking some functioning nodes as faulty to ﬁll out the fault regions.
the nodes of an interconnection network send and receive messages or packets through the network interface. both messages and packets carry information about the destination node. thus, the techniques described in this chapter can be applied to both of them indistinctly. without loss of generality, in what follows we will only refer to packets.
in direct networks, packets usually travel across several intermediate nodes before reaching the destination. in switch-based networks, packets usually traverse several switches before reaching the destination. however, it may happen that some packets are not able to reach their destinations, even if there exist fault-free paths connecting the source and destination nodes for every packet. assuming that the routing algorithm is able to use those paths, there are several situations that may prevent packet delivery. this chapter studies those situations and proposes techniques to guarantee packet delivery.
as seen in chapter 2, some buffers are required to store fragments of each packet, or even the whole packet, at each intermediate node or switch. however, buffer storage is not free. thus, buffer capacity is ﬁnite. as each packet whose header has not already arrived at its destination requests some buffers while keeping the buffers currently storing the packet, a deadlock may arise. a deadlock occurs when some packets cannot advance toward their destination because the buffers requested by them are full. all the packets involved in a deadlocked conﬁguration are blocked forever. note that a packet may be permanently blocked in the network because the destination node does not consume it. this kind of deadlock is produced by the application, and it is beyond the scope of this book. in this chapter, we will assume that packets are always consumed by the destination node in ﬁnite time. therefore, in a deadlocked conﬁguration, a set of packets is blocked forever. every packet is requesting resources held by other packet(s) while holding resources requested by other packet(s).
a different situation arises when some packets are not able to reach their destination, even if they never block permanently. a packet may be traveling around its destination node, never reaching it because the channels required to do so are occupied by other packets. this situation is known as livelock. it can only occur when packets are allowed to follow nonminimal paths.
finally, a packet may be permanently stopped if trafﬁc is intense and the resources requested by it are always granted to other packets also requesting them. this situation is
deadlock freedom by the existence of a routing subfunction that is connected and has no cyclic dependencies between resources. disha’s routing subfunction is usually deﬁned by the deadlock buffers, but, for concurrent recovery discussed below, it may also make use of some edge virtual channels. an approach similar to disha was proposed in [88, 94, 274], where the channels supplied by the routing subfunction can only be selected if packets are waiting for longer than a threshold. the circuit required to measure packet waiting time is identical to the circuit for deadlock detection in disha. however, the routing algorithms proposed in [88, 94, 274] use dedicated virtual channels to route packets waiting for longer than a threshold.
disha can also be designed so that several deadlocks can be recovered from concurrently (disha concurrent [10]). in this case, the token logic is no longer necessary. instead, an arbiter is required in each node so that simultaneous requests for the use of the deadlock buffer coming from different input channels are handled [281]. for concurrent recovery, more than one deadlock buffer may be required at each node for some network topologies, but even this is still the minimum required by progressive recovery. exercise 3.6 shows how simultaneous deadlock recovery can be achieved on meshes by using a single deadlock buffer and some edge buffers.
in general, simultaneous deadlock recovery can be achieved on any topology with hamiltonian paths by using two deadlock buffers per node. nodes are labeled according to their position in the hamiltonian path. one set of deadlock buffers is used by deadlocked packets whose destination node has a higher label than the current node. the second set of deadlock buffers is used by deadlocked packets whose destination node has a lower label than the current node. all edge virtual channels can be used for fully adaptive routing. as deadlock buffers form a connected routing subfunction without cyclic dependencies between them, all deadlocked packets are guaranteed to be delivered by using deadlock buffers. similarly, simultaneous deadlock recovery can be achieved on any topology with an embedded spanning tree by using two deadlock buffers per node.
the simplest way to avoid livelock is by limiting misrouting. minimal routing algorithms are a special case of limited misrouting in which no misrouting is allowed. the number of channels reserved by a packet is upper bounded by the network diameter. if there is no deadlock, the packet is guaranteed to be delivered in the absence of faults. minimal routing algorithms usually achieve a higher performance when wormhole switching is used because packets do not consume more channel bandwidth than the minimum amount required.
an important reason to use misrouting is routing around faulty components. if only minimal paths are allowed, and the link connecting the current and destination nodes is faulty, the packet is not able to advance. given a maximum number of faults that do not disconnect the network, it has been shown that limited misrouting is enough to reach all the destinations [126]. by limiting misrouting, there is also an upper bound for the number
in this chapter we study routing algorithms. routing algorithms establish the path followed by each message or packet. the list of routing algorithms proposed in the literature is almost endless. we clearly cannot do justice to all of these algorithms developed to meet many distinct requirements. we will focus on a representative set of approaches, being biased toward those being used or proposed in modern and future multiprocessor interconnects. thus, we hope to equip you with an understanding of the basic principles that can be used to study the spectrum of existing algorithms. routing algorithms for wormhole switching are also valid for other switching techniques. thus, unless explicitly stated, the routing algorithms presented in this chapter are valid for all the switching techniques. speciﬁc proposals for some switching techniques will also be presented. special emphasis is given to design methodologies because they provide a simple and structured way to design a wide variety of routing algorithms for different topologies.
deadlock and livelock freedom. ability to guarantee that packets will not block or wander across the network forever. this issue was discussed in depth in chapter 3.
fault tolerance. ability to route packets in the presence of faulty components. although it seems that fault tolerance implies adaptivity, this is not necessarily true. fault tolerance can be achieved without adaptivity by routing a packet in two or more phases, storing it in some intermediate nodes. fault tolerance also requires some additional hardware mechanisms, as will be detailed in chapter 6.
pipeline can modify the header before forwarding the packet. if the router pipeline detects a single-bit error, it corrects the error and reports it back to the operating system via an interrupt. however, it does not correct double-bit errors. instead, if it detects a double-bit error, the 21364 alerts every reachable 21364 of the occurrence of such an error and enters into an error recovery mode.
the most challenging component of the 21364 router is the arbitration mechanism that schedules the dispatch of packets arriving at its input ports. to avoid making the arbitration mechanism a central bottleneck, the 21364 breaks the arbitration logic into local and global arbitration. there are 16 local arbiters, two for each input port. there are seven global arbiters, one for each output port. in each cycle, a local arbiter may speculatively schedule a packet for dispatch to an output port. two cycles following the local arbitration, each global arbiter selects one out of up to seven packets speculatively scheduled for dispatch through the output port. once such a selection is made, all ﬂits in the x (crossbar) stage follow the input port to the output port connection.
to ensure fairness, the local and global arbiters use a least recently selected (lrs) scheme to select a packet. each local arbiter uses the lrs scheme to select both a class (among the several packet classes) and a virtual channel (among vc0, vc1, and adaptive) within the class. similarly, the global arbiter uses the lrs policy to select an input port (among the several input ports that each output port sees).
the nominated packet is valid at the input buffer and has not been dispatched yet. the necessary dispatch path from the input buffer to the output port is free. the packet is dispatched in only one of the routes allowed. the target router, i/o chip, or local resource (in the next hop) has a free input buffer in the speciﬁc virtual channel. the target output port is free. the antistarvation mechanism is not blocking the packet. a read io packet does not pass a write io packet.
a global arbiter selects packets speculatively scheduled for dispatch through its output port. the local arbiters speculatively schedule packets not selected by any global arbiter again in subsequent cycles.
in addition to the pipeline latency, there are a total of six cycles of synchronization delay, pad receiver and driver delay, and transport delay from the pins to the router and from the router back to the pins. thus, the on-chip, pin-to-pin latency from a network input to a network output is 13 cycles. at 1.2 ghz, this leads to a pin-to-pin latency of 10.8 ns.
the network links that connect the different 21364 chips run at 0.8 ghz, which is 33% slower than the internal router clock. the 21364 chip runs synchronously with the outgoing links, but asynchronously with the incoming links. the 21364 sends its clock
commonly operated as fifo queues. therefore, once a message occupies a buffer for a channel, no other message can access the physical channel, even if the message is blocked. alternatively, a physical channel may support several logical or virtual channels multiplexed across the physical channel. each unidirectional virtual channel is realized by an independently managed pair of message buffers as illustrated in figure 2.17. this ﬁgure shows two unidirectional virtual channels in each direction across the physical channel. consider wormhole switching with a message in each virtual channel. each message can share the physical channel on a ﬂit-by-ﬂit basis. the physical channel protocol must be able to distinguish between the virtual channels using the physical channel. logically, each virtual channel operates as if each were using a distinct physical channel operating at half the speed. virtual channels were originally introduced to solve the problem of deadlock in wormhole-switched networks. deadlock is a network state where no messages can advance because each message requires a channel occupied by another message. this issue is discussed in detail in chapter 3.
virtual channels can also be used to improve message latency and network throughput. by allowing messages to share a physical channel, messages can make progress rather than remain blocked. for example, figure 2.18 shows two messages crossing the physical channel between routers r1 and r2. with no virtual channels, message a will prevent message b from advancing until the transmission of message a has been completed. however, in the ﬁgure, there are two single-ﬂit virtual channels multiplexed over each physical channel. by multiplexing the two messages on a ﬂit-by-ﬂit basis, both messages continue to make progress. the rate at which each message is forwarded is nominally onehalf the rate achievable when the channel is not shared. in effect, the use of virtual channels decouples the physical channels from message buffers, allowing multiple messages to share a physical channel in the same manner that multiple programs may share a cpu. the overall time a message spends blocked at a router waiting for a free channel is reduced, leading to an overall reduction in individual message latency. there are two speciﬁc cases where such sharing of the physical link bandwidth is particularly beneﬁcial. consider the case where message a is temporarily blocked downstream from the current node. with an appropriate physical channel ﬂow control protocol, message b can make use of the
signaling between two routers across a single signal line. thus, full-duplex, bidirectional communication of a single bit between two routers can be realized with one pin (signal) rather than two signals. a logic 1 (0) is transmitted as positive (negative) current. the transmitted signal is the superposition of these two signals. each transmitter generates a reference signal that is subtracted from the superimposed signal to generate the received signal. the result is a considerable savings over the number of input/output pins required and consequent reduction in the packaging cost. such optimizations at the physical level are also clearly independent of and beneﬁt all switching techniques.
application environments that exhibit locality in interprocessor communication are particularly good candidates for application-speciﬁc optimizations. for example, systolic computation makes use of ﬁne-grained, pipelined, parallel transmission of multiple data streams through a ﬁxed communication topology such as multidimensional mesh or hexagonal interconnection topologies. in such cases, it is beneﬁcial to set up interprocessor communication paths once to be shared by many successive data elements (i.e., messages). the intel iwarp chip was designed to support such systolic communication through message pathways: long-lived communication paths [39]. rather than set up and remove network paths each time data are to be communicated, paths through the network persist for long periods of time. special messages called pathway begin markers are used to reserve virtual channels (referred to as logical channels in iwarp) and set up interprocessor communication paths. messages are periodically injected into the network to use these existing paths utilizing wormhole switching. on completion of the computation, the paths are explicitly removed by other control messages. unlike conventional wormhole switching, the last ﬂit of the message does not cause the routers to tear down the path. the overhead of preparing a node for message transmission is incurred once and amortized over all messages that use the path. such optimizations are possible due to the regular, predictable nature of systolic communication. the basic switching technique is wormhole, but it is applied in a manner to optimize the characteristics of the applications.
not only ﬁne-grained parallel algorithms exhibit communication locality that can be exploited by switching techniques. studies of vlsi cad programs and programs from linear algebra have shown that coarse-grained message-passing applications can also exhibit sufﬁcient communication locality to beneﬁt from long-lived communication paths and justify the design of an enhanced wormhole router [158]. as in the iwarp chip, paths are set up and shared by multiple messages until they are explicitly removed. the basic wormhole-switching technique is modiﬁed to prevent the path from being removed when the ﬁrst message has been successfully received at the destination. in this case interprocessor paths can even be shared between applications in a multiprogrammed parallel architecture. message ﬂow control must avoid deadlock as well as ensuring that reserved paths do not preclude new messages from being injected into the network.
the need for reliable message transmission has also led to proposals for enhancing switching techniques. for example, one way to ensure message delivery is to buffer the message at the source until it can be asserted that the message has been received at the destination. receipt at the destination can be determined through the use of message acknowledgments. in this case, paths are removed by explicit control signals/messages generated by the source/destination rather than by a part of the message. alternatively,
on the other hand, path-based routing requires establishing an ordered list of destination addresses for each copy of a message. however, tree-based routing does not require any ordering among destinations. in many cases, the ordered destination list(s) can be computed at compile time. in some other cases, the compiler does not help because messages are dynamically generated by hardware. for example, messages are generated by the cache controller in dsms with coherent caches. in this case, a clever organization of the cache directory may considerably reduce the time required to prepare the ordered destination list [69]. finally, when the list of destinations has to be explicitly ordered for each multidestination message, path-based routing should be preferred over tree-based routing only if the additional cost of computing the ordered destination list(s) is compensated by a reduction in message latency.
in order to reduce the number of channels used for a given multicast, the subpath between the source and one of the destinations in a multicast path is not necessarily in a shortest path. therefore, in path-based routing the average distance between the source and each destination is generally longer than that of tree-based routing. note that in wormhole switching the network latency is almost independent of the length of a path. moreover, path-based routing does not require replicating messages at each intermediate node, implying a less complex router.
a suitable network partitioning strategy for path-based routing is based on hamiltonian paths. a hamiltonian path visits every node in a graph exactly once [146]; a 2-d mesh has many hamiltonian paths. thus, each node u in a network is assigned a label, ?(u). in a network with n nodes, the assignment of the label to a node is based on the position of that node in a hamiltonian path, where the ﬁrst node in the path is labeled 0 and the last node in the path is labeled n − 1. figure 5.24(a) shows a possible labeling in a 4 × 3 mesh, in which each node is represented by its integer coordinate (x, y). the labeling effectively divides the network into two subnetworks. the high-channel subnetwork contains all of the channels whose direction is from lower-labeled nodes to higher-labeled nodes, and the low-channel subnetwork contains all of the channels whose direction is from higherlabeled nodes to lower-labeled nodes.
unicast as well as multicast communication will use the labeling for routing. that is, a unicast message will follow a path based on the labeling instead of using xy routing. if the label of the destination node is greater than the label of the source node, the routing always takes place in the high-channel network; otherwise, it will take place in the low-channel network. the label assignment function ? for an m × n mesh can be expressed in terms of the
vct switching improved the performance of packet switching by enabling pipelined message ﬂow while retaining the ability to buffer complete message packets. wormhole switching provided further reductions in latency by permitting small buffer vct so that routing could be completely handled within single-chip routers, therefore providing low latency necessary for tightly coupled parallel processing. this trend toward increased message pipelining is continued with the development of the mad postman switching mechanism in an attempt to realize the minimal possible routing latency per node.
the technique is best understood in the context of bit-serial physical channels. consider a 2-d mesh network with message packets that have a 2-ﬂit header. routing is dimension order: messages are ﬁrst routed completely along dimension 0 and then along dimension 1. the leading header ﬂit contains the destination address of a node in dimension 0. when the message reaches this node, the ﬁrst header ﬂit is discarded and the message is forwarded along dimension 1. the second header ﬂit contains the destination in dimension 1. in vct and wormhole switching, ﬂits cannot be forwarded until the header ﬂits required for routing have been received in their entirety at the router. if we have 8-bit ﬂits, transmission of the ﬁrst header ﬂit across a bit-serial physical channel will take 8 cycles. note that only the ﬁrst header ﬂit is required for routing along dimension 0. assuming a 1-cycle delay to select the output channel at each intermediate router, the minimum latency for the header to reach a destination router three links away is 27 cycles (no turns in the path) or 9 + 9 + 17 = 35 cycles (one turn in the path). the mad postman attempts to reduce the per-node latency further by pipelining at the bit level. when a header ﬂit starts arriving at a router, it is assumed that the message will be continuing along the same dimension. therefore, header bits are forwarded to the output link in the same dimension as soon as they are received (assuming that the output channel is free). each bit of the header is also buffered locally. once the last bit of the ﬁrst ﬂit of the header has been received, the router can examine this ﬂit and determine if the message should indeed proceed further along this dimension. if it is to proceed along the second dimension, the remainder of the message starting with the second ﬂit of the header is transmitted to the output along the second dimension. if the message has arrived at its destination, it is delivered to the local processor. in essence, the message is ﬁrst delivered to an output channel and the address is checked later, hence the name of this switching technique. this strategy can work very well in 2-d networks since a message will make at most one turn from one dimension to another and we can encode each dimension offset in 1 header ﬂit. the common case of messages continuing along the same dimension is made very fast. a time-space diagram of a message transmitted over three links using the mad postman switching technique is illustrated in figure 2.14.
some constraints must be placed on the organization of the header. an example is shown in figure 2.15, wherein each dimension offset is encoded in a header ﬂit, and these ﬂits are ordered according to the order of traversal. for example, when the message packet has completely traversed the ﬁrst dimension, the router can start transmitting in the second dimension with the start of the ﬁrst bit of the second header ﬂit. the ﬁrst ﬂit has effectively been stripped off the message, but continues to traverse the ﬁrst dimension. such a ﬂit
adaptive double-y (mad-y). it improves adaptivity with respect to the double-y algorithm. basically, mad-y allows packets using y 1 channels to turn to the x+ direction and packets using x− channels to turn and use y 2 channels. figures 4.23 and 4.24 show the turns allowed by the double-y and mad-y algorithms, respectively.
the mad-y algorithm has the maximum adaptivity that can be obtained without introducing cyclic dependencies between channels. however, as shown in theorem 3.1, cycles do not necessarily produce deadlock. thus, the mad-y algorithm can be improved. it was done by schwiebert and jayasimha, who proposed the opt-y algorithm [306, 308]. this algorithm is deadlock-free and optimal with respect to the number of routing restrictions on the virtual channels for deadlock-avoidance-based routing. basically, the opt-y algorithm allows all the turns between x and y 2 channels as well as turns between x+ and y 1 channels. turns from y 1 to x− channels are prohibited. turns from x− to y 1 channels as well as 0-degree turns between y 1 and y 2 channels are restricted. these turns are only allowed when the packet has completed its movement along x− channels (the x-offset is zero or positive). figure 4.25 shows the turns allowed by the opt-y algorithm.
deﬁning a routing algorithm by describing the allowed and prohibited turns makes it difﬁcult to understand how routing decisions are taken at a given node. the opt-y algorithm is described in figure 4.26 using pseudocode.
network addresses to routes and performs the header generation. in any myrinet network, one of the interfaces serves as the mapper: it is responsible for sending mapping packets to other interfaces. this map of the network is distributed to all of the other interfaces. now all routes can be created locally by each interface. the network maps are computed in a manner that is guaranteed to provide only deadlock-free routes. the attractive feature of the myrinet network is that should the mapper interface be removed or faulty links separate the network, a new mapper is selected automatically. periodic remapping of the network is performed to detect faulty links and switches, and this information is distributed throughout the network.
myrinet designs evolved to a three-chip set. the single-chip crossbar is used for system area networks (sans) where the distances are less than 3 meters. for longer distances, custom vlsi chips convert to formats suitable for distances of up to 8 meters typically used in lans. a third chip provides a 32-bit fifo interface for conﬁgurations as classical multicomputer nodes. switches can be conﬁgured with combinations of san and lan ports, producing conﬁgurations with varying bisection bandwidth and power requirements. the building blocks are very ﬂexible and permit a variety of multiprocessor conﬁgurations as well as opportunities for subsequently growing an existing network. for example, a myrinet switch conﬁgured as a lan switch has a cut-through latency of 300 ns, while a san switch has a cut-through latency of 100 ns. thus, small tightly coupled multiprocessor systems can be conﬁgured with commercial processors to be used as parallel engines. alternatively, low-latency communication can be achieved within existing workstation and pc clusters that may be distributed over a larger area.
servernet [15, 155, 156, 157] is a san that provides the interconnection fabric for supporting interprocessor, processor-i/o, and i/o-i/o communication. the goal is to provide a ﬂexible interconnect fabric that can reliably provide scalable bandwidth.
that messages can travel larger distances. in particular, when l is equal to 1/e, we obtain an exponential distribution. as l approaches one, the distribution function  approaches the uniform distribution. conversely, as l approaches zero,  approaches a nearest-neighbor communication pattern. it should be noted that the decreasing probability distribution is adequate for the analysis of networks of different sizes. simply, decay(l, dmax) should be computed for each network.
the distributions described above exhibit different degrees of spatial locality but have no temporal locality. recently, several speciﬁc communication patterns between pairs of nodes have been used to evaluate the performance of interconnection networks: bit reversal, perfect shufﬂe, butterﬂy, matrix transpose, and complement. these communication patterns take into account the permutations that are usually performed in parallel numerical algorithms [175, 200, 239]. in these patterns, the destination node for the messages generated by a given node is always the same. therefore, the utilization factor of all the network links is not uniform. however, these distributions achieve the maximum degree of temporal locality. these communication patterns can be deﬁned as follows:
bit reversal. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, a1, . . . , an−2, an−1. perfect shufﬂe. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−2, an−3, . . . , a0, an−1 (rotate left 1 bit). butterﬂy. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a0, an−2, . . . , a1, an−1 (swap the most and least signiﬁcant bits). matrix transpose. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node a n 2 complement. the node with binary coordinates an−1, an−2, . . . , a1, a0 communicates with the node an−1, an−2, . . . , a1, a0.
finally, a distribution based on a least recently used stack model has been proposed in [288] to model temporal locality. in this model, each node has its own stack containing the m nodes that were most recently sent messages. for each position in the stack there is a probability of sending a message to the node in that position. the sum of probabilities for nodes in the stack is less than one. therefore, a node not currently in the stack may be chosen as the destination for the next transmission. in this case, after sending the message, its destination node will be included in the stack, replacing the least recently used destination.
for synthetic workloads, the injection rate is usually the same for all the nodes. in most cases, each node is chosen to generate messages according to an exponential distribution. the parameter λ of this distribution is referred to as the injection rate. other possible distributions include a uniform distribution within an interval, bursty trafﬁc, and traces from parallel applications. for the uniform distribution, the injection rate is the mean value of the interval. bursty trafﬁc can be generated either by injecting a burst of messages every
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
for long messages that are not split into small packets, nonminimal routing algorithms are not interesting because bandwidth is wasted every time a long message reserves a nonminimal path. note that this is not the case for short packets because misrouting is an alternative to waiting for a free minimal path and channels are reserved for a short period of time.
however, proﬁtable backtracking algorithms may outperform progressive algorithms when messages are long because they look for alternative minimal paths instead of waiting for a free channel. the main drawback of backtracking algorithms is the overhead produced by the circuits required to keep track of the header history, thus avoiding searching a path twice.also, note that backtracking algorithms cannot be easily combined with wormhole or vct switching. they are usually used on networks using some variant of circuit switching. as data do not follow the header immediately, some additional performance degradation is produced.
in summary, the optimal routing algorithm depends on trafﬁc conditions. for uniform or local trafﬁc, deterministic routing algorithms are the best choice. for irregular trafﬁc or hot spots, adaptive routers usually outperform deterministic ones. however, as we will see in chapter 9, the impact of the routing algorithm on performance is small when compared with other parameters like trafﬁc characteristics.
this chapter aims at describing the most interesting routing algorithms and design methodologies proposed up to now. references to the original work have been included along with the descriptions. also, some proposals were presented in chapter 3 while describing deadlock-handling mechanisms. although the number of routing algorithms proposed in the literature is very high, most of them have some points in common with other proposals. for example, several algorithms proposed in the literature [136, 331] are equivalent or very similar to those obtained by using methodology 4.1. other proposals tried to maximize adaptivity, very much like the algorithms proposed in section 4.5.1. this is the case for the mesh-route algorithm proposed in [42]. also, some proposals combine previously proposed methodologies in an interesting way. the positive-ﬁrst, negative-ﬁrst routing algorithm for 2-d meshes proposed in [344] combines two algorithms from the turn model in a balanced way, trying to distribute trafﬁc more evenly.
a detailed description of every proposed routing algorithm is beyond the scope of this book. the interested reader can refer to [318] for a survey on multistage interconnection networks and to [263] for a brief survey on switch-based networks. routing algorithms for direct interconnection networks are surveyed in [125, 241, 255]. these surveys mainly focus on distributed routing using wormhole switching. however, source routing and table-lookup routing are also covered in [255]. additionally, backtracking protocols are covered in [125]. nevertheless, this chapter covers several proposals that have not been surveyed elsewhere.
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
using normalized delays will represent performance relative to this improved base 2-d case. let the switch delay and routing delay be represented as integer multiples of the wire delay in a 2-d network: s and r, respectively. from the earlier discussion, we know the value of tw relative to a 2-d network. thus, the latency expression can be written as follows (for networks where k is even):
the above expression is based on a linear delay model for wires. for short wires, delay is logarithmic [70] and, for comparison purposes, it is useful (though perhaps unrealistic) to have a model based on constant wire delay. the above expression is normalized to the wire delay in a 2-d mesh. therefore, in the constant delay model, wire delay simply remains at 1. in the logarithmic delay model, delay is a logarithmic function of wire length
the latency expressions can be viewed as the sum of two components: a distance component, which is the delay experienced by the header ﬂit, and a message component, which is the delay due to the length of the message. the expressions also include terms representing physical constraints (wire length, channel widths), network topology (dimensions, average distance), applications (message lengths), and router architecture (routing and switching delays). although in the above expression routing and switching delays are constants, we could model them as an increasing function of network dimension. this appeals to intuition since a larger number of dimensions may make routing decisions more complex and increase the delay through internal networks that must switch between a larger number of router inputs and outputs. we revisit this issue in the exercises at the end of this chapter.
the above expression provides us with insight into selecting the appropriate number of dimensions for a ﬁxed number of nodes. the minimum latency is realized when the component due to distance is equal to the component due to message length. the choice of parameters such as the switching and routing delays, channel widths, and message lengths determine the dimension at which this minimum is realized. the parameters themselves are not independent and are related by implementation constraints. examples of such analysis are presented in the following sections.
with process groups, communicators, and logical topologies we have a set of logical concepts with which to describe the point-to-point and collective communication semantics of mpi. the basic constructs of mpi are best illustrated by examining an example mpi function. consider the following blocking send procedure [325]:
the above procedure transmits count number of entries starting at address buf to node dest. note that the message size is speciﬁed as a number of entries and not as a number of bytes. usage in conjunction with the datatype argument promotes a portable implementation. the language support provides deﬁnitions for a host of predeﬁned data types (e.g., for the c language there exist mpi char, mpi float, mpi byte, etc.). facilities are also available for the user to deﬁne new data types.
in addition to specifying the source of message data, the call also speciﬁes the dest, tag, and comm ﬁelds, which with the addition of the source address form the message envelope. the comm argument is the communicator, and dest is the rank of the receiving process within this communication domain. if the communicator is an intercommunicator, then dest is the rank of the process in the destination domain (we must be careful since processes can belong to more than one domain and have different ranks in each domain). thus, comm and dest serve to uniquely identify the destination process. the tag is simply an integer-valued argument that can be used by the receiving processes to distinguish between messages.
the blocking semantics are with respect to buffer usage rather than the temporal relationship between send and matching receive operations. when a blocking send procedure returns, this does not necessarily imply that the matching receive procedure has started executing. a blocking send call returns as soon as the send buffer can be safely reused. depending upon the implementation, this may be the case after the message has been copied directly into the receive buffer, or after it has been copied into a system buffer for subsequent transmission. in the latter case the send may return before the matching receive has even begun executing, which may run counter to our expectations for blocking operations. a blocking receive returns when the buffer contains the message data. the blocking receive is shown below:
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
resources in a cyclic way. if we prevent packets from waiting on some resources when blocked, we can prevent deadlocks. however, the additional routing ﬂexibility offered by this model is usually small.
models based on waiting channels are more general than the ones based on channel dependencies. in fact, the latter are particular cases of the former when a packet is allowed to wait on all the channels supplied by the routing function. another particular case of interest arises when a blocked packet always waits on a single channel. in this case, this model matches the behavior of routers that buffer blocked packets in queues associated with output channels. note that those routers do not use wormhole switching because they buffer whole packets.
although models based on waiting channels are more general than the ones based on channel dependencies, note that adaptive routing is useful because it offers alternative paths when the network is congested. in order to maximize performance, a blocked packet should be able to reserve the ﬁrst valid channel that becomes available. thus, restricting the set of routing options when a packet is blocked does not seem to be an attractive choice from the performance point of view.
a model that increases routing ﬂexibility in the absence of contention is based on the wait-for graph [73]. this graph indicates the resources that blocked packets are waiting for. packets are even allowed to use nonminimal paths as long as they are not blocked. however, blocked packets are not allowed to wait for channels held by other packets in a cyclic way. thus, some blocked packets are obliged to use deterministic routing until delivered if they produce a cycle in the wait-for graph.
an interesting model based on waiting channels is the message ﬂow model [207, 209]. in this model, a routing function is deadlock-free if and only if all the channels are deadlock-immune. a channel is deadlock-immune if every packet that reserves that channel is guaranteed to be delivered. the model starts by analyzing the channels for which a packet reserving them is immediately delivered. those channels are deadlockimmune. then the model analyzes the remaining channels step by step. in each step, the channels adjacent to the ones considered in the previous step are analyzed. a channel is deadlock-immune if for all the alternative paths a packet can follow, the next channel to be reserved is also deadlock-immune. waiting channels play a role similar to routing subfunctions, serving as escape paths when a packet is blocked.
another model uses a channel waiting graph [309]. this graph captures the relationship between channels in the same way as the channel dependency graph. however, this model does not distinguish between routing and selection functions. thus, it considers the dynamic evolution of the network because the selection function takes into account channel status. two theorems are proposed. the ﬁrst one considers that every packet has a single waiting channel. it states that a routing algorithm is deadlock-free if it is waitconnected and there are no cycles in the channel waiting graph. the routing algorithm is wait-connected if it is connected by using only waiting channels.
but the most important result is a necessary and sufﬁcient condition for deadlock-free routing. this condition assumes that a packet can wait on any channel supplied by the routing algorithm. it uses the concept of true cycles. a cycle is a true cycle if it is reachable, starting from an empty network. the theorem states that a routing algorithm is deadlock-
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
signaling between two routers across a single signal line. thus, full-duplex, bidirectional communication of a single bit between two routers can be realized with one pin (signal) rather than two signals. a logic 1 (0) is transmitted as positive (negative) current. the transmitted signal is the superposition of these two signals. each transmitter generates a reference signal that is subtracted from the superimposed signal to generate the received signal. the result is a considerable savings over the number of input/output pins required and consequent reduction in the packaging cost. such optimizations at the physical level are also clearly independent of and beneﬁt all switching techniques.
application environments that exhibit locality in interprocessor communication are particularly good candidates for application-speciﬁc optimizations. for example, systolic computation makes use of ﬁne-grained, pipelined, parallel transmission of multiple data streams through a ﬁxed communication topology such as multidimensional mesh or hexagonal interconnection topologies. in such cases, it is beneﬁcial to set up interprocessor communication paths once to be shared by many successive data elements (i.e., messages). the intel iwarp chip was designed to support such systolic communication through message pathways: long-lived communication paths [39]. rather than set up and remove network paths each time data are to be communicated, paths through the network persist for long periods of time. special messages called pathway begin markers are used to reserve virtual channels (referred to as logical channels in iwarp) and set up interprocessor communication paths. messages are periodically injected into the network to use these existing paths utilizing wormhole switching. on completion of the computation, the paths are explicitly removed by other control messages. unlike conventional wormhole switching, the last ﬂit of the message does not cause the routers to tear down the path. the overhead of preparing a node for message transmission is incurred once and amortized over all messages that use the path. such optimizations are possible due to the regular, predictable nature of systolic communication. the basic switching technique is wormhole, but it is applied in a manner to optimize the characteristics of the applications.
not only ﬁne-grained parallel algorithms exhibit communication locality that can be exploited by switching techniques. studies of vlsi cad programs and programs from linear algebra have shown that coarse-grained message-passing applications can also exhibit sufﬁcient communication locality to beneﬁt from long-lived communication paths and justify the design of an enhanced wormhole router [158]. as in the iwarp chip, paths are set up and shared by multiple messages until they are explicitly removed. the basic wormhole-switching technique is modiﬁed to prevent the path from being removed when the ﬁrst message has been successfully received at the destination. in this case interprocessor paths can even be shared between applications in a multiprogrammed parallel architecture. message ﬂow control must avoid deadlock as well as ensuring that reserved paths do not preclude new messages from being injected into the network.
the need for reliable message transmission has also led to proposals for enhancing switching techniques. for example, one way to ensure message delivery is to buffer the message at the source until it can be asserted that the message has been received at the destination. receipt at the destination can be determined through the use of message acknowledgments. in this case, paths are removed by explicit control signals/messages generated by the source/destination rather than by a part of the message. alternatively,
orphaned data ﬂits. in one approach, header information is maintained through the routers that contain ﬂits of the message. when messages are interrupted, a new message can be constructed with the stored header information, and the (relatively) smaller message can be forwarded along an alternative path to the destination. this will be referred to as ﬂit-level recovery. messages are injected only once into the network and recover from link-level dynamic and transient failures. alternatively, the blocked data ﬂits can be recovered and discarded from the network, the router buffers freed, and the message retransmitted from the source. in this case, the source and destination must synchronize message delivery or drop messages. this requires that the message be buffered at the source until it can be asserted that it has been delivered either explicitly through acknowledgments or implicitly through other mechanisms. this will be referred to as message-level recovery to denote the level at which recovery takes place. implementations of each basic approach are described in the following.
reliable message delivery can be handled by end-to-end protocols that utilize acknowledgments and message retransmission to synchronize delivery of messages. messages remain buffered at the source until it can be asserted that the message has been delivered. if we consider packet switching, more efﬁcient alternatives exist for ensuring reliable end-toend message delivery. error detection and retransmission/rerouting can be implemented at the link level. a message must be successfully transmitted across a link before it is forwarded. with some a priori knowledge of the component failure rates, fault-tolerant routing algorithms can be developed where packets are injected into the network exactly once and guaranteed to be delivered to the destination by rerouting around faulty components. the difﬁculty in applying the same approach to wormhole-switched messages has been that the message packets cannot be completely buffered at each router and are
the majority of this text has been concentrated on issues facing the design of the interconnection network: switching, routing, and supporting router architectures. however, from the user’s perspective the experienced latency/throughput performance is not only a function of the network, but also of the operations performed at the source and destination processors to inject and receive messages from the network. end-to-end performance can be measured from data located in the memory of the sending process to data in the memory of the receiving process. this includes the process of preparing data for transmission (e.g., computation of packet headers and check sums) and injecting/ejecting messages into/out of the network. in most systems, a large segment of this functionality is implemented in software: the messaging layer. with the dramatic improvements in raw link bandwidths, the time on the wire experienced by messages has become overshadowed by the software overheads at the sender/receiver.
these software overheads are determined by the functionality to be provided by the messaging layer and the hardware features that are available in the network and node interfaces to support this desired functionality. for example, the overhead in transferring message data from the local memory to the network interface is determined by the internal node design and the availability of services such as dma, interrupts, or memorymapped interfaces. this chapter ﬁrst discusses the major functions that are provided by current message layers and then discusses how the implementation of these functions are affected by the services provided by the network hardware and the internal node architecture. the services of this messaging layer are the basis on which user-level message-passing libraries can be constructed. the chapter focuses on the message-passing programming model and does not include issues concerning the support of shared-memory abstractions on distributed-memory architectures. the data transfer requirements for implementing shared-address spaces have similarities with the requirements of messagepassing implementations, and several current research efforts are studying the integration of shared-memory and message-passing communication within a common framework. however, there appears to be a greater consensus on the requirements for supporting message-passing models, and this chapter focuses on examples from this domain.
generally, an application programming interface (api) provides access to the services of the messaging layer through a set of library procedures and functions. a desirable api is
the implementation of the unique token protocol for reliable transmission necessitates some special handling and architectural support. the token at the end of a message must be forwarded only after the corresponding ﬂit queue has successfully emptied. retransmission on failure involves generation of duplicate tokens. finally, ﬂow control must span two routers to ensure that a duplicate copy of each data ﬂit is maintained at all times in adjacent routers. when a data ﬂit is successfully transmitted across a channel, the copy of the data ﬂit two routers upstream must be deallocated. the relevant ﬂow control signals are passed through frames as captured in figure 7.25.
the sgi spider (scalable pipelined interconnect for distributed endpoint routing) [117] was designed with multiple missions in mind: as part of a conventional multiprocessor switch fabric, as a building block for large-scale nonblocking central switches, and as a high-bandwidth communication fabric for distributed graphics applications. the physical links are full-duplex channels with 20 data bits, a frame signal, and a differential clock signal in each direction. data are transferred on both edges of a 200 mhz clock, realizing a raw data rate of 1 gbyte/s in each direction. the chip core operates at 100 mhz, providing 80-bit units that are serialized into 20-bit phits for transmission over the channel. the organization of the spider chip is shown in figure 7.27.
the router implements four 256-byte virtual channels over each physical link. the units of buffer management and transmission are referred to as micropackets, equivalent to the notion of a ﬂit. each micropacket is comprised of 128 bits of data, 8 bits of sequencing
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
1. blocking. a connection between a free input/output pair is not always possible because of conﬂicts with the existing connections. typically, there is a unique
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
2. bidirectional mins. channels and switches are bidirectional. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches.
additionally, each channel may be either multiplexed or replaced by two or more channels. in the latter case, the network is referred to as dilated min. obviously, the number of ports of each switch must increase accordingly.
unidirectional multistage interconnection networks the basic building blocks of unidirectional mins are unidirectional switches. an a × b switch is a crossbar network with a inputs and b outputs. if each input port is allowed to connect to exactly one output port, at most min{a, b} connections can be supported simultaneously. if each input port is allowed to connect to many output ports, a more complicated design is needed to support the so-called one-to-many or multicast communication. in the broadcast mode or one-to-all communication, each input port is allowed to connect to all output ports. figure 1.18 shows four possible states of a 2 × 2 switch. the last two states are used to support one-to-many and one-to-all communications. in mins with n = m, it is common to use switches with the same number of input and output ports, that is, a = b. if n > m, switches with a > b will be used. such switches are also called concentration switches. in the case of n < m, distribution switches with a < b will be used. it can be shown that with n input and output ports, a unidirectional min with k × k switches requires at least (cid:12)logk n(cid:13) stages to allow a connection path between any input
port and any output port. by having additional stages, more connection paths may be used to deliver a message between an input port and an output port at the expense of extra hardware cost. every path through the min crosses all the stages. therefore, all the paths have the same length.
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
the advent of vlsi permitted the integration of hardware for thousands of switches into a single chip. however, the number of pins on a vlsi chip cannot exceed a few hundred, which restricts the size of the largest crossbar that can be integrated into a single vlsi chip. large crossbars can be realized by partitioning them into smaller crossbars, each one implemented using a single chip. thus, a full crossbar of size n × n can be implemented with (n/n)(n/n) n × n crossbars.
multistage interconnection networks (mins) connect input devices to output devices through a number of switch stages, where each switch is a crossbar network. the number of stages and the connection patterns between stages determine the routing capability of the networks.
mins were initially proposed for telephone networks and later for array processors. in these cases, a central controller establishes the path from input to output. in cases where the number of inputs equals the number of outputs, each input synchronously transmits a message to one output, and each output receives a message from exactly one input. such unicast communication patterns can be represented as a permutation of the input addresses. for this application, mins have been popular as alignment networks for storing and accessing arrays in parallel from memory banks. array storage is typically skewed to permit conﬂict-free access, and the network is used to unscramble the arrays during access. these networks can also be conﬁgured with the number of inputs greater than the number of outputs (concentrators) and vice versa (expanders). on the other hand, in asynchronous multiprocessors, centralized control and permutation routing are infeasible. in this case, a routing algorithm is required to establish the path across the stages of a min.
depending on the interconnection scheme employed between two adjacent stages and the number of stages, various mins have been proposed. mins are good for constructing parallel computers with hundreds of processors and have been used in some commercial machines.
there are many ways to interconnect adjacent stages. figure 1.11 shows a generalized multistage interconnection network with n inputs and m outputs. it has g stages, g0 to gg−1. as shown in figure 1.12, each stage, say, gi, has wi switches of size ai,j × bi,j , where 1 ≤ j ≤ wi. thus, stage gi has pi inputs and qi outputs, where
the connection between two adjacent stages, gi−1 and gi, denoted ci, deﬁnes the connection pattern for pi = qi−1 links, where p0 = n and qg−1 = m. a min thus can be represented as
a connection pattern ci (pi ) deﬁnes how those pi links should be connected between the qi−1 = pi outputs from stage gi−1 and the pi inputs to stage gi. different connection patterns give different characteristics and topological properties of mins. the links are labeled from 0 to pi − 1 at ci.
in practice, all the switches will be identical, thus amortizing the design cost. banyan networks are a class of mins with the property that there is a unique path between any pair of source and destination [132]. an n-node (n = kn) delta network is a subclass of banyan networks, which is constructed from identical k × k switches in n stages, where each stage contains n k switches. many of the known mins, such as omega, ﬂip, cube, butterﬂy, and baseline, belong to the class of delta networks [273] and have been shown
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
is usually done taking into account the status of output channels at the current node. obviously, the selection is performed in such a way that a free channel (if any) is supplied. however, when several output channels are available, some policy is required to select one of them. policies may have different goals, like balancing the use of resources, reserving some bandwidth for high-priority packets, or even delaying the use of resources that are exclusively used for deadlock avoidance. regardless of the main goal of the policy, the selection function should give preference to channels belonging to minimal paths when the routing function is nonminimal. otherwise the selection function may produce livelock. in this section we present some selection functions proposed in the literature for several purposes.
in [73], three different selection functions were proposed for n-dimensional meshes using wormhole switching with the goal of maximizing performance: minimum congestion, maximum ﬂexibility, and straight lines. in minimum congestion, a virtual channel is selected in the dimension and direction with the most available virtual channels. this selection function tries to balance the use of virtual channels in different physical channels. the motivation for this selection function is that packet transmission is pipelined. hence, ﬂit transmission rate is limited by the slowest stage in the pipeline. balancing the use of virtual channels also balances the bandwidth allocated to different virtual channels. in maximum ﬂexibility, a virtual channel is selected in the dimension with the greatest distance to travel to the destination. this selection function tries to maximize the number of routing choices as a packet approaches its destination. in meshes, this selection function has the side effect of concentrating trafﬁc in the central part of the network bisection, therefore producing an uneven channel utilization and degrading performance. finally, in straight lines, a virtual channel is selected in the dimension closest to the current dimension. so, the packet will continue traveling in the same dimension whenever possible. this selection function tries to route packets in dimension order unless the requested channels in the corresponding dimension are busy. in meshes, this selection function achieves a good distribution of trafﬁc across the network bisection. these selection functions were evaluated in [73] for 2-d meshes, showing that minimum congestion achieves the lowest latency and highest throughput. straight lines achieves similar performance. however, maximum ﬂexibility achieves much worse results. these results may change for other topologies and routing functions, but in general minimum congestion is a good choice for the reason mentioned above.
for routing functions that allow cyclic dependencies between channels, the selection function should give preference to adaptive channels over channels used to escape from deadlocks [91]. by doing so, escape channels are only used when all the remaining channels are busy, therefore increasing the probability of escape channels being available when they are required to escape from deadlock. the selection among adaptive channels can be done by using any of the strategies described above. note that if escape channels are not free when requested, it does not mean that the routing function is not deadlock-free. escape channels are guaranteed to become free sooner or later. however, performance may degrade if packets take long to escape from deadlock. in order to reduce the utilization of escape channels and increase their availability to escape from deadlock, it is possible to delay the use of escape channels by using a timeout. in this case, the selection function can
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
the receiver uses the message envelope to specify the sources from which messages will be received. only messages whose envelopes match the receiver’s request will be accepted. receivers also control message reception via type matching. the source ﬁeld may be a wild card (e.g., the predeﬁned string mpi any source). in this instance, a message may be received from any source, and it may not be possible to know a priori the exact size of the message. therefore, the buffer size is an upper bound on the required storage. query functions are available to obtain information about received messages; for example, mpi get count() returns the number of received entries in the receive buffer. the status ﬁeld is a structure that contains additional information for the receiver.
there are a host of nonblocking calls that return immediately to permit overlap of communication and computation. these calls are supported by query functions that are used to determine if the operation has actually completed. such calls are a necessary prelude to the reuse of buffers used in the nonblocking calls. these functions include mpi wait() and mpi test(). a common structure is to initiate a nonblocking send operation and continue with computation that is overlapped with the transmission. when the program reaches a point where the send buffer must be reused, queries are used to establish the completion of the previously issued send operation.
it is clear that the choice of buffering strategy has a signiﬁcant impact on performance, particularly if there are known features of the application that can be exploited. to enable the programmer to inﬂuence the buffering strategy and thus performance, mpi offers several modes for send/receive operations. the preceding descriptions were that of standard mode send/receive calls. in this case, no assumption can be made with regard to whether the message is buffered or not. in buffered mode, buffering can be provided within the user program. therefore, the user can guarantee that some buffering is available. synchronous mode realizes the semantics of a rendezvous operation and ensures that a matching receive has executed. finally, in ready mode, the user can assert that the matching receive has been posted. these distinct modes enable the user to exploit some knowledge of the application implementation in inﬂuencing the choice of the message transfer protocol used in a communication operation.
when communication repeatedly occurs between processes (e.g., in loops), some of the overhead involved in the message-passing implementation may be shared across multiple messages by using features for persistent communication. calls to mpi functions are
the receiver uses the message envelope to specify the sources from which messages will be received. only messages whose envelopes match the receiver’s request will be accepted. receivers also control message reception via type matching. the source ﬁeld may be a wild card (e.g., the predeﬁned string mpi any source). in this instance, a message may be received from any source, and it may not be possible to know a priori the exact size of the message. therefore, the buffer size is an upper bound on the required storage. query functions are available to obtain information about received messages; for example, mpi get count() returns the number of received entries in the receive buffer. the status ﬁeld is a structure that contains additional information for the receiver.
there are a host of nonblocking calls that return immediately to permit overlap of communication and computation. these calls are supported by query functions that are used to determine if the operation has actually completed. such calls are a necessary prelude to the reuse of buffers used in the nonblocking calls. these functions include mpi wait() and mpi test(). a common structure is to initiate a nonblocking send operation and continue with computation that is overlapped with the transmission. when the program reaches a point where the send buffer must be reused, queries are used to establish the completion of the previously issued send operation.
it is clear that the choice of buffering strategy has a signiﬁcant impact on performance, particularly if there are known features of the application that can be exploited. to enable the programmer to inﬂuence the buffering strategy and thus performance, mpi offers several modes for send/receive operations. the preceding descriptions were that of standard mode send/receive calls. in this case, no assumption can be made with regard to whether the message is buffered or not. in buffered mode, buffering can be provided within the user program. therefore, the user can guarantee that some buffering is available. synchronous mode realizes the semantics of a rendezvous operation and ensures that a matching receive has executed. finally, in ready mode, the user can assert that the matching receive has been posted. these distinct modes enable the user to exploit some knowledge of the application implementation in inﬂuencing the choice of the message transfer protocol used in a communication operation.
when communication repeatedly occurs between processes (e.g., in loops), some of the overhead involved in the message-passing implementation may be shared across multiple messages by using features for persistent communication. calls to mpi functions are
the receiver uses the message envelope to specify the sources from which messages will be received. only messages whose envelopes match the receiver’s request will be accepted. receivers also control message reception via type matching. the source ﬁeld may be a wild card (e.g., the predeﬁned string mpi any source). in this instance, a message may be received from any source, and it may not be possible to know a priori the exact size of the message. therefore, the buffer size is an upper bound on the required storage. query functions are available to obtain information about received messages; for example, mpi get count() returns the number of received entries in the receive buffer. the status ﬁeld is a structure that contains additional information for the receiver.
there are a host of nonblocking calls that return immediately to permit overlap of communication and computation. these calls are supported by query functions that are used to determine if the operation has actually completed. such calls are a necessary prelude to the reuse of buffers used in the nonblocking calls. these functions include mpi wait() and mpi test(). a common structure is to initiate a nonblocking send operation and continue with computation that is overlapped with the transmission. when the program reaches a point where the send buffer must be reused, queries are used to establish the completion of the previously issued send operation.
it is clear that the choice of buffering strategy has a signiﬁcant impact on performance, particularly if there are known features of the application that can be exploited. to enable the programmer to inﬂuence the buffering strategy and thus performance, mpi offers several modes for send/receive operations. the preceding descriptions were that of standard mode send/receive calls. in this case, no assumption can be made with regard to whether the message is buffered or not. in buffered mode, buffering can be provided within the user program. therefore, the user can guarantee that some buffering is available. synchronous mode realizes the semantics of a rendezvous operation and ensures that a matching receive has executed. finally, in ready mode, the user can assert that the matching receive has been posted. these distinct modes enable the user to exploit some knowledge of the application implementation in inﬂuencing the choice of the message transfer protocol used in a communication operation.
when communication repeatedly occurs between processes (e.g., in loops), some of the overhead involved in the message-passing implementation may be shared across multiple messages by using features for persistent communication. calls to mpi functions are
the receiver uses the message envelope to specify the sources from which messages will be received. only messages whose envelopes match the receiver’s request will be accepted. receivers also control message reception via type matching. the source ﬁeld may be a wild card (e.g., the predeﬁned string mpi any source). in this instance, a message may be received from any source, and it may not be possible to know a priori the exact size of the message. therefore, the buffer size is an upper bound on the required storage. query functions are available to obtain information about received messages; for example, mpi get count() returns the number of received entries in the receive buffer. the status ﬁeld is a structure that contains additional information for the receiver.
there are a host of nonblocking calls that return immediately to permit overlap of communication and computation. these calls are supported by query functions that are used to determine if the operation has actually completed. such calls are a necessary prelude to the reuse of buffers used in the nonblocking calls. these functions include mpi wait() and mpi test(). a common structure is to initiate a nonblocking send operation and continue with computation that is overlapped with the transmission. when the program reaches a point where the send buffer must be reused, queries are used to establish the completion of the previously issued send operation.
it is clear that the choice of buffering strategy has a signiﬁcant impact on performance, particularly if there are known features of the application that can be exploited. to enable the programmer to inﬂuence the buffering strategy and thus performance, mpi offers several modes for send/receive operations. the preceding descriptions were that of standard mode send/receive calls. in this case, no assumption can be made with regard to whether the message is buffered or not. in buffered mode, buffering can be provided within the user program. therefore, the user can guarantee that some buffering is available. synchronous mode realizes the semantics of a rendezvous operation and ensures that a matching receive has executed. finally, in ready mode, the user can assert that the matching receive has been posted. these distinct modes enable the user to exploit some knowledge of the application implementation in inﬂuencing the choice of the message transfer protocol used in a communication operation.
when communication repeatedly occurs between processes (e.g., in loops), some of the overhead involved in the message-passing implementation may be shared across multiple messages by using features for persistent communication. calls to mpi functions are
from the programmer’s perspective, collective communication services are provided in the context of a process group. in a multicomputer, each individual application is usually allocated with a subset of processors, called a processor cluster, in order to achieve the best performance (the performance may be degraded when more processors are allocated due to increased penalty from communication overhead) and to increase the system throughput. the scheduling of processors in a multicomputer should carefully consider the trade-off between space sharing and time sharing, which is beyond the scope of this book.
from the viewpoint of system or processors, a process group only involves a subset of processors, as shown in figure 5.6 with two process groups. the four processes of process group 1 are each assigned to different processors, as are the three processes in process group 2. process 2 from group 1 and process 0 from group 2 share the same processor.
obviously, as indicated in figure 5.6, the “all” communication in a process group becomes the “many” communication, which involves an arbitrary subset of processors, from the system’s point of view. in order to efﬁciently support collective communication, it is desirable that at the processor level the system can support “one-to-one” (unicast), “oneto-many,” “many-to-one,” and “many-to-many” communication primitives in hardware. all multiprocessors directly support various forms of unicast communication, such as blocking versus nonblocking, synchronous versus asynchronous, and direct remote memory access. one-to-many communication, mainly in the form of multicast, in which the
into dl1 = {(5, 1), (5, 0)} and dl2 = {(0, 2), (0, 0)}. then, the multicast is performed using four multicast paths, as shown in figure 5.30. in this example, multipath routing requires only 21 channels, and the maximum distance from source to destination is 6 hops.
example 5.9 shows that multipath routing can offer advantages over dual-path routing in terms of generated trafﬁc and the maximum distance between the source and destination nodes. also, multipath routing usually requires fewer channels than dual-path routing. because the destinations are divided into four sets rather than two, they are reached more efﬁciently from the source, which is approximately centrally located among the sets. thus, when the network load is not high, multipath routing offers slight improvement over dualpath routing due to the fact that multipath routing introduces less trafﬁc to the network.
however, a potential disadvantage of multipath routing is not revealed until both the load and number of destinations are relatively high. when multipath routing is used to reach a relatively large set of destinations, the source node will likely send on all of its outgoing channels. until this multicast transmission is complete, any ﬂit from another multicast or unicast message that routes through that source node will be blocked at that point. in essence, the source node becomes a hot spot. in fact, every node currently sending a multicast message is likely to be a hot spot. if the load is very high, these hot spots may throttle system throughput and increase message latency. hot spots are less likely to occur in dual-path routing, accounting for its stable behavior under high loads with large destination sets. although all the outgoing channels at a node can be simultaneously busy, this can only result from two or more messages routing through that node. a detailed performance study can be found in [206, 208].
as indicated in section 3.1.3, there is a dependency between two channels when a packet or message is holding one of them, and then it requests the other channel. in path-based multicast routing, the delivery of the same message to several destinations may produce additional channel dependencies, as we show in the next example.
let us consider a 2-d mesh using xy routing. this routing algorithm prevents the use of horizontal channels after using a vertical channel. figure 5.31 shows an example of multicast routing on a 2-d mesh. a message is sent by node 0, destined for nodes 9 and 14. the xy routing algorithm ﬁrst routes the message through horizontal channels until it reaches node 1. then, it routes the message through vertical channels until the ﬁrst destination is reached (node 9). now, the message must be forwarded toward its next destination (node 14). the path requested by the xy routing algorithm contains a horizontal channel. so, there is a dependency from a vertical channel to a horizontal one. this dependency does not exist in unicast routing. it is due to the inclusion of multiple destinations in the message header. more precisely, after reaching an intermediate destination the message header is routed toward the next destination. as a consequence, it is forced to take a path that it would not follow otherwise. for this reason, this dependency is referred to as multicast dependency [90, 96].
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
u1, u2, . . . , uk denote k destination nodes, where k ≥ 1. the set k = {u0, u1, . . . , uk}, which is a subset of v (g), is called a multicast set. depending on the underlying communication paradigm and the routing method, the multicast communication problem in a multicomputer can be formulated as different graph-theoretical problems.
in some communication mechanisms, replication of an incoming message in order to be forwarded to multiple neighboring nodes may involve too much overhead and is usually undesirable. thus, the routing method does not allow each processor to replicate the message passing by. also, a multicast path model provides better performance than the tree model (to be described below) when there is contention in the network. from a switching technology point of view, the multicast path model is more suitable for wormhole switching.
the multicast communication problem becomes the problem of ﬁnding a shortest path starting from u0 and visiting all k destination nodes. this optimization problem is the ﬁnding of an optimal multicast path (omp) and is formally deﬁned below.
a multicast path (v1, v2, . . . , vn) for a multicast set k in g is a subgraph p (v , e) of g, where v (p ) = {v1, v2, . . . , vn} and e(p ) = {(vi , vi+1) : 1 ≤ i ≤ n − 1}, such that v1 = u0 and k ⊆ v (p ). an omp is a multicast path with the shortest total length.
reliable communication is essential to a message-passing system. usually, a separate acknowledgment message is sent from every destination node to the source node on receipt of a message. one way to avoid the sending of |k| separate acknowledgment messages is to have the source node itself receive a copy of the message it initiated after all destination nodes have been visited. acknowledgments are provided in the form of error bits ﬂagged by intermediate nodes when a transmission error is detected. thus, the multicast communication problem is the problem of ﬁnding a shortest cycle, called the optimal multicast cycle (omc), for k.
a multicast cycle (v1, v2, . . . , vn, v1) for k is a subgraph c(v , e) of g, where v (c) = {v1, v2, . . . , vn} and e(c) = {(vn, v1), (vi , vi+1) : 1 ≤ i ≤ n − 1}, such that k ⊆ v (c). an omc is a multicast cycle with the shortest total length.
both omc and omp assume that the message will not be replicated by any node during transmission. however, message replication can be implemented by using some hardware approach [197]. if the major concern is to minimize trafﬁc, the multicast problem becomes
of the message from the source to every destination. depending on the number of destinations, such separate addressing may require excessive time, particularly in a one-port architecture in which a local processor may send only one message at a time. assuming that the start-up latency dominates the communication latency, separate addressing achieves a communication latency that increases linearly with the number of destinations.
performance may be improved by organizing the unicast messages as a multicast tree, whereby the source node sends the message directly to a subset of the destinations, each of which forwards the message to one or more other destinations. eventually, all destinations will receive the message.
the potential advantage of tree-based communication is apparent from the performance of various broadcast methods. for example, in the spanning binomial tree algorithm described in section 5.5.2, the number of nodes that already received the broadcast message is doubled after each step. hence, assuming a one-port architecture, communication latency increases logarithmically with the number of destinations.
it is possible to reduce latency even more if the router interface has several ports, allowing nodes to inject several messages simultaneously. however, in this section we will only consider algorithms for a one-port architecture because most commercial multicomputers have a single port. see [232] and [153, 231, 295, 342] for a survey and detailed descriptions of multicast/broadcast algorithms for multiport architectures, respectively.
which types of multicast trees should be used depends on the switching technique and unicast routing algorithm. the following features are desirable in the software implementation of a multicast tree:
2. the implementation should exploit the small distance sensitivity of wormhole 3. the height of the multicast tree should be minimal. speciﬁcally, for m − 1
4. there should be no channel contention among the constituent messages of the multicast. in other words, the unicast messages involved should not simultaneously require the same channel. note that this feature does not eliminate contention with other unicast or multicast messages.
how to achieve these goals depends on the switching technique and unicast routing algorithm of the network. although the user has no control over the routing of individual
however, a parallel computer requires some kind of communication subsystem to interconnect processors, memories, disks, and other peripherals. the speciﬁc requirements of these communication subsystems depend on the architecture of the parallel computer. the simplest solution consists of connecting processors to memories and disks as if there were a single processor, using system buses and i/o buses. then, processors can be interconnected using the interfaces to local area networks. unfortunately, commodity communication subsystems have been designed to meet a different set of requirements, that is, those arising in computer networks. although networks of workstations have been proposed as an inexpensive approach to build parallel computers, the communication subsystem becomes the bottleneck in most applications.
therefore, designing high-performance interconnection networks becomes a critical issue to exploit the performance of parallel computers. moreover, as the interconnection network is the only subsystem that cannot be efﬁciently implemented by using commodity components, its design becomes very critical. this issue motivated the writing of this book. up to now, most manufacturers designed custom interconnection networks (ncube2, ncube-3, intel paragon, cray t3d, cray t3e, thinking machines corp. cm-5, nec cenju-3, ibm sp2). more recently, several high-performance switches have been developed (autonet, myrinet, servernet) and are being marketed. these switches are targeted to workstations and personal computers, offering the customer the possibility of building an inexpensive parallel computer by connecting cost-effective computers through high-performance switches. the main issues arising in the design of networks for both approaches are covered in this book.
in this section, we brieﬂy introduce the most popular parallel computer architectures. this description will focus on the role of the interconnection network. a more detailed description is beyond the scope of this book.
the idea of using commodity components for the design of parallel computers led to the development of distributed-memory multiprocessors, or multicomputers, in the early 1980s. these parallel computers consist of a set of processors, each one connected to its own local memory. processors communicate between themselves by passing messages through an interconnection network. figure 1.1(a) shows a simple scheme for this architecture. the ﬁrst commercial multicomputers utilized commodity components, including ethernet controllers to implement communication between processors. unfortunately, commodity communication subsystems were too slow, and the interconnection network became the bottleneck of those parallel computers. several research efforts led to the development of interconnection networks that are several orders of magnitude faster than ethernet networks. most of the performance gain is due to architectural rather than technological improvements.
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
a. select a safe proﬁtable adaptive output channel. stop. b. select a safe deterministic output channel. stop. c. if the safe deterministic channel is not faulty, block. d. if the deterministic channel is faulty, select an unsafe proﬁtable adaptive
able to dynamically conﬁgure the ﬂow control protocols employed by the routers. routing algorithms can be designed such that in the vicinity of faulty components messages use pcs-style ﬂow control, where controlled misrouting and backtracking can be used to avoid faults and deadlocked conﬁgurations. at the same time messages use wormholeswitching ﬂow control in fault-free portions of the network with the attendant performance advantages. messages are routed in two phases—a fault-free phase and a faulty phase. messages may transition between these two phases several times in the course of being routed between a source and destination node. routing algorithms designed based on such ﬂow control mechanisms are referred to as multiphase routing algorithms [79]. such dynamically conﬁgurable ﬂow control protocols can be conﬁgured using variants of scouting switching described in chapter 2.
the following description presents a two-phase routing algorithm. message routing proceeds in one of two phases: an optimistic phase for routing in fault-free network segments and a conservative phase for routing in faulty segments. an example of a twophase (tp) algorithm is shown in figure 6.37. the optimistic phase uses a fully adaptive, minimal, deadlock-free routing function based on dp (see section 4.4.4). the conservative phase uses a form of mb-m. the switching technique is scouting switching with a scouting distance of k.
the virtual channels on each physical link are partitioned into restricted and unrestricted partitions. fully adaptive minimal routing is permitted on the unrestricted partition (adaptive channels), while only deterministic routing is allowed on the restricted partition (deterministic channels). channels are marked as safe or unsafe [198, 351] depending on the number of faulty links/nodes within the immediate neighborhood. the deﬁnition of the extent of this neighborhood for determining the safe/unsafe designation can be quite
same message to a speciﬁc set of destinations without requiring assistance from any other processor. the approaches used to implement such functionality are highly dependent on the network topology and may affect the design of the switching strategy used in the network. before studying those approaches, some schemes to encode multiple destination addresses are described in the next section.
the header of multidestination messages must carry the addresses of the destination nodes. the header information is an overhead to the system, increasing message latency and reducing the effective network bandwidth. a good multiaddress encoding scheme should minimize the message header length, also reducing the header processing time.
in wormhole and vct switching, the routing algorithm is executed before the whole message arrives at the router. as the header may require several ﬂits to encode the destination addresses, it is desirable that the routing decision in each router could be made as soon as possible to reduce message latency. ideally, a message header should be processed on the ﬂy as header ﬂits arrive. when the number of destination addresses is variable, it is inefﬁcient to use a counter to indicate the number of destinations. such a counter should be placed at the beginning of a message header. since the value of the counter may be changed at a router if the destination set is split into several subsets, it would prevent the processing of message headers on the ﬂy. an alternative approach is to have an end-of-header (eoh) ﬂit to indicate the end of a header. another approach consists of using 1 bit in each ﬂit to distinguish between header and data ﬂits.
figure 5.9 shows ﬁve different encoding schemes, namely, all-destination encoding, bit string encoding, multiple-region broadcast encoding, multiple-region stride encoding, and multiple-region bit string encoding. these schemes were proposed in [54].
the all-destination encoding is a simple scheme in which all destination addresses are carried by the header. this encoding scheme has two important advantages. first, the same routing hardware used for unicast messages can be used for multidestination messages. second, the message header can be processed on the ﬂy as address ﬂits arrive. this scheme is good for a small number of addresses because the header length is proportional to the number of addresses. however, it produces a signiﬁcant overhead when the number of destinations is large.
one way to limit the size of the header is to encode destination addresses as a bit string, where each bit corresponds to a destination ranged between node b and node e, as shown in figure 5.9(b). since the length of the string in a system is predeﬁned, the eoh ﬁeld is not required. this encoding scheme is good when the average number of destinations is large. however, it is inefﬁcient when the system is large and the number of destinations is small. the main drawback of the bit string encoding scheme is that a router usually has to buffer the entire bit string in order to make the routing decision and to generate the output bit string(s). additionally, address decoding cannot be done with the same routing hardware as for unicast messages. finally, the length of the string usually depends on network size, limiting the scalability of the system.
same message to a speciﬁc set of destinations without requiring assistance from any other processor. the approaches used to implement such functionality are highly dependent on the network topology and may affect the design of the switching strategy used in the network. before studying those approaches, some schemes to encode multiple destination addresses are described in the next section.
the header of multidestination messages must carry the addresses of the destination nodes. the header information is an overhead to the system, increasing message latency and reducing the effective network bandwidth. a good multiaddress encoding scheme should minimize the message header length, also reducing the header processing time.
in wormhole and vct switching, the routing algorithm is executed before the whole message arrives at the router. as the header may require several ﬂits to encode the destination addresses, it is desirable that the routing decision in each router could be made as soon as possible to reduce message latency. ideally, a message header should be processed on the ﬂy as header ﬂits arrive. when the number of destination addresses is variable, it is inefﬁcient to use a counter to indicate the number of destinations. such a counter should be placed at the beginning of a message header. since the value of the counter may be changed at a router if the destination set is split into several subsets, it would prevent the processing of message headers on the ﬂy. an alternative approach is to have an end-of-header (eoh) ﬂit to indicate the end of a header. another approach consists of using 1 bit in each ﬂit to distinguish between header and data ﬂits.
figure 5.9 shows ﬁve different encoding schemes, namely, all-destination encoding, bit string encoding, multiple-region broadcast encoding, multiple-region stride encoding, and multiple-region bit string encoding. these schemes were proposed in [54].
the all-destination encoding is a simple scheme in which all destination addresses are carried by the header. this encoding scheme has two important advantages. first, the same routing hardware used for unicast messages can be used for multidestination messages. second, the message header can be processed on the ﬂy as address ﬂits arrive. this scheme is good for a small number of addresses because the header length is proportional to the number of addresses. however, it produces a signiﬁcant overhead when the number of destinations is large.
one way to limit the size of the header is to encode destination addresses as a bit string, where each bit corresponds to a destination ranged between node b and node e, as shown in figure 5.9(b). since the length of the string in a system is predeﬁned, the eoh ﬁeld is not required. this encoding scheme is good when the average number of destinations is large. however, it is inefﬁcient when the system is large and the number of destinations is small. the main drawback of the bit string encoding scheme is that a router usually has to buffer the entire bit string in order to make the routing decision and to generate the output bit string(s). additionally, address decoding cannot be done with the same routing hardware as for unicast messages. finally, the length of the string usually depends on network size, limiting the scalability of the system.
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
node. therefore, the distance between two nodes is the distance between the switches directly connected to those nodes plus two units. similarly, the diameter is the maximum distance between two switches connected to some node plus two units. it may be argued that it is not necessary to add two units because direct networks also have internal links between routers and processing nodes. however, those links are external in the case of indirect networks. this gives a consistent view of the diameter as the maximum number of external links between two processing nodes. in particular, the distance between two nodes connected through a single switch is two instead of zero.
similar to direct networks, an indirect network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the switches are interconnected by channels and can be modeled by a graph as indicated above. for indirect networks with n nodes, the ideal topology would connect those nodes through a single n × n switch. such a switch is known as a crossbar. although using a single n × n crossbar is much cheaper than using a fully connected direct network topology (requiring n routers, each one having an internal n × n crossbar), the cost is still prohibitive for large networks. similar to direct networks, the number of physical connections of a switch is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of crossbar networks for large network sizes. as a consequence, many alternative topologies have been proposed. in these topologies, messages may have to traverse several switches before reaching the destination node. in regular networks, these switches are usually identical and have been traditionally organized as a set of stages. each stage (but the input/output stages) is only connected to the previous and next stages using regular connection patterns. input/output stages are connected to the nodes as well as to another stage in the network. these networks are referred to as multistage interconnection networks and have different properties depending on the number of stages and how those stages are arranged.
the advent of vlsi permitted the integration of hardware for thousands of switches into a single chip. however, the number of pins on a vlsi chip cannot exceed a few hundred, which restricts the size of the largest crossbar that can be integrated into a single vlsi chip. large crossbars can be realized by partitioning them into smaller crossbars, each one implemented using a single chip. thus, a full crossbar of size n × n can be implemented with (n/n)(n/n) n × n crossbars.
multistage interconnection networks (mins) connect input devices to output devices through a number of switch stages, where each switch is a crossbar network. the number of stages and the connection patterns between stages determine the routing capability of the networks.
mins were initially proposed for telephone networks and later for array processors. in these cases, a central controller establishes the path from input to output. in cases where the number of inputs equals the number of outputs, each input synchronously transmits a message to one output, and each output receives a message from exactly one input. such unicast communication patterns can be represented as a permutation of the input addresses. for this application, mins have been popular as alignment networks for storing and accessing arrays in parallel from memory banks. array storage is typically skewed to permit conﬂict-free access, and the network is used to unscramble the arrays during access. these networks can also be conﬁgured with the number of inputs greater than the number of outputs (concentrators) and vice versa (expanders). on the other hand, in asynchronous multiprocessors, centralized control and permutation routing are infeasible. in this case, a routing algorithm is required to establish the path across the stages of a min.
depending on the interconnection scheme employed between two adjacent stages and the number of stages, various mins have been proposed. mins are good for constructing parallel computers with hundreds of processors and have been used in some commercial machines.
like the cray t3d, the sp2 employs source routing. the route tables are statically constructed and stored in memory to be used by the messaging software. the construction and use of the routes are performed in a manner that prevents deadlock and promotes balanced use of multiple paths between pairs of processors. in this manner an attempt is made to fully use the available wire bandwidth.
at the time of this writing the next generation of ibm switches were becoming available. the new switches utilize 16-bit ﬂits, and the crossbar path is 16 bits wide. internally the switches operate at 75 mhz as opposed to 40 mhz in the current router. the physical channel structure remains the same, except that differential signaling results in doubling the number of signals per port to 22. the physical channel also operates at 150 mhz. chunks are now 128 bits wide, and the central queue can store 256 chunks for a total of 4 kbytes. since the ﬂit size and clock speed have doubled, so have the buffers on the input ports. flow control and buffer management necessitate 63-ﬂit buffers, or 126 bytes. with the wider internal switch data paths and higher clock rates, the memory-tomemory bandwidth, including micro channel transfers, is approaching 100 mbytes/s for large messages.
the last decade has seen a relentless and large (close to doubling every 1.5 years) improvement in the price/performance ratio of workstations and personal computers. there is no evidence of this trend fading any time soon. with the concurrent trends in portable parallel programming standards, there has been an explosive growth in research and development efforts seeking to harness these commodity parts to create commercial off-the-shelf multiprocessors. a major architectural challenge in conﬁguring such systems has been in providing the low-latency and high-throughput interprocessor communication traditionally afforded by multiprocessor backplanes and interconnects in a cluster of workstations/pcs. we are now seeing a migration of this interconnect technology into the cluster computing arena. this section discusses two examples of such networks.
myrinet is a switching fabric that grew out of two previous research efforts, the caltech mosaic project [315] and the usc/isi atomic lan [106] project. the distinguishing features of myrinet include the support of high-performance wormhole switching in arbitrary lan topologies. a myrinet network comprises host interfaces and switches. the switches may be connected to other host interfaces and other switches as illustrated in figure 7.49, leading to arbitrary topologies.
each myrinet physical link is a full-duplex link with 9-bit-wide channels operating at 80 mhz. the 9-bit character may be a data byte or one of several control characters used to implement the physical channel ﬂow control protocol. each byte transmission is
hence, it is not possible to cross every dimension from every node. crossing a given dimension from a given node may require moving in another dimension ﬁrst.
the most interesting property of strictly orthogonal topologies is that routing is very simple. thus, the routing algorithm can be efﬁciently implemented in hardware. effectively, in a strictly orthogonal topology, nodes can be numbered by using their coordinates in the n-dimensional space. since each link traverses a single dimension and every node has at least one link crossing each dimension, the distance between two nodes can be computed as the sum of dimension offsets. also, the displacement along a given link only modiﬁes the offset in the corresponding dimension. taking into account that it is possible to cross any dimension from any node in the network, routing can be easily implemented by selecting a link that decrements the absolute value of the offset in some dimension. the set of dimension offsets can be stored in the packet header and updated (by adding or subtracting one unit) every time the packet is successfully routed at some intermediate node. if the topology is not strictly orthogonal, however, routing may become much more complex.
the most popular direct networks are the n-dimensional mesh, the k-ary n-cube or torus, and the hypercube. all of them are strictly orthogonal. formally, an n-dimensional mesh has k0 × k1 × · · · × kn−2 × kn−1 nodes, ki nodes along each dimension i, where ki ≥ 2 and 0 ≤ i ≤ n − 1. each node x is identiﬁed by n coordinates, (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ ki − 1 for 0 ≤ i ≤ n − 1. two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = xj ± 1. thus, nodes have from n to 2n neighbors, depending on their location in the mesh. therefore, this topology is not regular.
in a bidirectional k-ary n-cube [70], all nodes have the same number of neighbors. the deﬁnition of a k-ary n-cube differs from that of an n-dimensional mesh in that all of the ki are equal to k and two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = (xj ± 1) mod k. the change to modular arithmetic in the deﬁnition adds wraparound channels to the k-ary n-cube, giving it regularity and symmetry. every node has n neighbors if k = 2, and 2n neighbors if k > 2. when n = 1, the k-ary n-cube collapses to a bidirectional ring with k nodes.
another topology with regularity and symmetry is the hypercube, which is a special case of both n-dimensional meshes and k-ary n-cubes. a hypercube is an n-dimensional mesh in which ki = 2 for 0 ≤ i ≤ n − 1, or a 2-ary n-cube, also referred to as a binary n-cube.
figure 1.5(a) depicts a binary 4-cube or 16-node hypercube. figure 1.5(b) illustrates a 3-ary 2-cube or two-dimensional (2-d) torus. figure 1.5(c) shows a 3-ary threedimensional (3-d) mesh, resulting by removing the wraparound channels from a 3-ary 3-cube.
two conﬂicting requirements of a direct network are that it must accommodate a large number of nodes while maintaining a low network latency. this issue will be addressed in chapter 7.
suggesting the corresponding west-ﬁrst routing algorithm: route a packet ﬁrst west, if necessary, and then adaptively south, east, and north. the two turns prohibited in figure 4.9(c) are the two turns to the west. therefore, in order to travel west, a packet must begin in that direction. figure 4.10 shows the minimal west-ﬁrst routing algorithm for 2-d meshes, where select() is the selection function deﬁned in section 3.1.2. this function returns a free channel (if any) from the set of channels passed as parameters. see exercise 4.5 for a nonminimal version of this algorithm. three example paths for the west-ﬁrst algorithm are shown in figure 4.11. the channels marked as unavailable are either faulty or being used by other packets. one of the paths shown is minimal, while the other two paths are nonminimal, resulting from routing around unavailable channels. because cycles are avoided, west-ﬁrst routing is deadlock-free. for minimal routing, the algorithm is fully adaptive if the destination is on the right-hand side (east) of the source; otherwise, it is deterministic. if nonminimal routing is allowed, the algorithm is adaptive in either case. however, it is not fully adaptive.
there are other ways to select six turns so as to prohibit cycles. however, the selection of the two prohibited turns may not be arbitrary [129]. if turns are prohibited as in figure 4.12, deadlock is still possible. figure 4.12(a) shows that the three remaining left turns are equivalent to the prohibited right turn, and figure 4.12(b) shows that the three remaining right turns are equivalent to the prohibited left turn. figure 4.12(c) illustrates how cycles may still occur. of the 16 different ways to prohibit two turns, 12 prevent deadlock and only 3 are unique if symmetry is taken into account. these three combinations correspond to the west-ﬁrst, north-last, and negative-ﬁrst routing algorithms. the northlast routing algorithm does not allow turns from north to east or from north to west. the negative-ﬁrst routing algorithm does not allow turns from north to west or from east to south.
in addition to 2-d mesh networks, the turn model can be used to develop partially adaptive routing algorithms for n-dimensional meshes, for k-ary n-cubes, and for hypercubes [129]. by applying the turn model to the hypercube, an adaptive routing algorithm, namely, p-cube routing, can be developed. let s = sn−1sn−2 . . . s0 and d = dn−1dn−2 . . . d0 be the source and destination nodes, respectively, in a binary n-cube. the set e consists of all the dimension numbers in which s and d differ. the size of e is the hamming distance between s and d. thus, i ∈ e if si (cid:16)= di. e is divided into two disjoint subsets, e0 and e1, where i ∈ e0 if si = 0 and di = 1, and j ∈ e1 if sj = 1 and
the turn model described in chapter 4 can be modiﬁed to handle the occurrence of faulty components. the example described here is the nonminimal version of the negativeﬁrst routing algorithm for 2-d meshes. recall that this algorithm operates in two phases: the message is routed in the negative direction in each of the dimensions in the ﬁrst phase, and then messages are routed in the positive directions in the second phase. the fault-tolerant nonminimal version routes adaptively in the negative direction, even farther west or south than the destination. for example, this is the path taken by message a in figure 6.31. during this phase the message is routed adaptively and around faulty nodes or links. the exception occurs when a message being routed along the edge of the mesh in the negative direction encounters a faulty node. in the second phase, the message is routed
these routing algorithms were designed for saf networks using central queues. deadlocks are avoided by splitting buffers into several classes and restricting packets to move from one buffer to another in such a way that buffer class is never decremented. gopal proposed several fully adaptive minimal routing algorithms based on buffer classes [133]. these algorithms are known as hop algorithms.
the simplest hop algorithm starts by injecting a packet into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a hop to another node, it moves to a buffer of class i + 1. this routing algorithm is known as the positive-hop algorithm. deadlocks are avoided by using a buffer of a higher class every time a packet requests a new buffer. by doing so, cyclic dependencies between resources are prevented. a packet that has completed i hops will use a buffer of class i. since the routing algorithm only supplies minimal paths, the maximum number of hops taken by a packet is limited by the diameter of the network. if the network diameter is denoted by d, a minimum of d + 1 buffers per node are required to avoid deadlock. the main advantage of the positive-hop algorithm is that it is valid for any topology. however, the number of buffers required for fully adaptive deadlock-free routing is very high, and this number depends on network size.
the minimum number of buffers per node can be reduced by allowing packets to move between buffers of the same class. in this case, classes must be deﬁned such that packets moving between buffers of the same class cannot form cycles. in the negative-hop routing algorithm, the network is partitioned into several subsets in such a way that no subset contains two adjacent nodes. if s is the number of subsets, then subsets are labeled 0, 1, . . . , s − 1, and nodes in subset i are labeled i. hops from a node with a higher label to a node with a lower label are negative. otherwise, hops are nonnegative. when a packet is injected, it is stored into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a negative hop, it moves to a buffer of class i + 1. if a packet stored in a buffer of class i takes a nonnegative hop, then it requests a buffer of the same class. thus, a packet that has completed i negative hops will use a buffer of class i. there is not any cyclic dependency between buffers. effectively, a cycle starting at node a must return to node a and contains at least another node b. if b has a lower label than a, some hop between a and b (possibly through intermediate nodes) is negative, and the buffer class is increased. if b has a higher label than a, some hop between b and a (possibly through intermediate nodes) is negative, and the buffer class is increased. as a consequence, packets cannot wait for buffers cyclically, thus avoiding deadlocks. if d is the network diameter and s is the number of subsets, then the maximum number of negative hops that can be taken by a packet is hn = (cid:12)d(s − 1)/s(cid:13). the minimum number of buffers per node required to avoid deadlock is hn + 1. figure 4.14 shows a partition scheme for k-ary 2-cubes with even k. black and white circles correspond to nodes of subsets 0 and 1, respectively.
although the negative-hop routing algorithm requires approximately half the buffers required by the positive-hop algorithm in the best case, this number is still high. it can be improved by partitioning the network into subsets and numbering the partitions, such
higher latency than the average value. a high value of the standard deviation usually indicates that some messages are blocked for a long time in the network. the peak value of the latency can also help in identifying these situations.
latency is measured in time units. however, when comparing several design choices, the absolute value is not important. as many comparisons are performed by using network simulators, latency can be measured in simulator clock cycles. unless otherwise stated, the latency plots presented in this chapter for unicast messages measure the average value of the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. in most cases, the simulator clock cycle is the unit of measurement. however, in section 9.10, latency is measured in nanoseconds.
throughput is the maximum amount of information delivered per time unit. it can also be deﬁned as the maximum trafﬁc accepted by the network, where trafﬁc, or accepted trafﬁc is the amount of information delivered per time unit. throughput could be measured in messages per second or messages per clock cycle, depending on whether absolute or relative timing is used. however, throughput would depend on message and network size. so, throughput is usually normalized, dividing it by message size and network size. as a result, throughput can be measured in bits per node and microsecond, or in bits per node and clock cycle. again, when comparing different design choices by simulation, and assuming that channel width is equal to ﬂit size, throughput can be measured in ﬂits per node and clock cycle. alternatively, accepted trafﬁc and throughput can be measured as a fraction of network capacity. a uniformly loaded network is operating at capacity if the most heavily loaded channel is used 100% of the time [72]. again, network capacity depends on the communication pattern.
a standard way to measure accepted trafﬁc and throughput was proposed at the workshop on parallel computer routing and communication (pcrcw’94). it consists of representing them as a fraction of the network capacity for a uniform distribution of destinations, assuming that the most heavily loaded channels are located in the network bisection. this network capacity is referred to as normalized bandwidth. so, regardless of the communication pattern used, it is recommended to measure applied load, accepted trafﬁc, and throughput as a fraction of normalized bandwidth. normalized bandwidth can be easily derived by considering that 50% of uniform random trafﬁc crosses the bisection of the network. thus, if a network has bisection bandwidth b bits/s, each node in an n-node network can inject 2b/n bits/s at the maximum load. unless otherwise stated, accepted trafﬁc and throughput are measured as a fraction of normalized bandwidth. while this is acceptable when comparing different design choices in the same network, it should be taken into account that those choices may lead to different clock cycles. in this case, each set of design parameters may produce a different bisection bandwidth, therefore invalidating the normalized bandwidth as a trafﬁc unit. in that case, accepted trafﬁc and throughput can be measured in bits (ﬂits) per node and microsecond. we use this unit in section 9.10.
a common misconception consists of using throughput instead of trafﬁc. as mentioned above, throughput is the maximum accepted trafﬁc. another misconception consists of considering throughput or trafﬁc as input parameters instead of measurements,
in order to make the theoretical results as general as possible, we assume no restriction about packet generation rate, packet destinations, and packet length. also, we assume no restriction on the paths supplied by the routing algorithm. both minimal and nonminimal paths are allowed. however, for performance reasons, a routing algorithm should supply at least one channel belonging to a minimal path at each intermediate node. additionally, we are going to focus on deadlocks produced by the interconnection network. thus, we assume that packets will be consumed at their destination nodes in ﬁnite time.
several switching techniques can be used. each of them will be considered as a particular case of the general theory. however, a few speciﬁc assumptions are required for some switching techniques. for saf and vct switching, we assume that edge buffers are used. central queues will be considered in section 3.2.3. for wormhole switching, we assume that a queue cannot contain ﬂits belonging to different packets. after accepting a tail ﬂit, a queue must be emptied before accepting another header ﬂit. when a virtual channel has queues at both ends, both queues must be emptied before accepting another header ﬂit. thus, when a packet is blocked, its header ﬂit will always occupy the head of a queue. also, for every path p that can be established by a routing function r, all subpaths of p are also paths of r. the routing functions satisfying the latter property will be referred to as coherent. for mad postman switching, we assume the same restrictions as for wormhole switching. additionally, dead ﬂits are removed from the network as soon as they are blocked.
a conﬁguration is an assignment of a set of packets or ﬂits to each queue. before analyzing how to avoid deadlocks, we are going to present a deadlocked conﬁguration by using an example.
consider a 2-d mesh with bidirectional channels. the routing function r forwards packets following any minimal path. this routing function is not deadlock-free. figure 3.2 shows a deadlocked conﬁguration. dashed incomplete boxes represent nodes of a 3 × 3 mesh. dashed boxes represent switches. solid boxes represent packet buffers or ﬂit buffers, depending on the switching technique used. the number inside each buffer indicates the destination node. solid arrows indicate the channel requested by the packet or the header at the queue head. as packets are allowed to follow all the minimal paths, packets wait for each other in a cyclic way. additionally, there is no alternative path for the packets in the ﬁgure because packets are only allowed to follow minimal paths. as all the buffers are full, no packet can advance.
so, a deadlocked conﬁguration is a conﬁguration in which some packets are blocked forever, waiting for resources that will never be granted because they are held by other packets. the conﬁguration described in example 3.1 would also be deadlocked if there were some additional packets traveling across the network that are not blocked. a deadlocked conﬁguration in which all the packets are blocked is referred to as canonical. given a deadlocked conﬁguration, the corresponding canonical conﬁguration can be obtained by stopping packet injection at all the nodes, and waiting for the delivery of
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
programming multicomputers is not an easy task. the programmer has to take care of distributing code and data among the processors in an efﬁcient way, invoking messagepassing calls whenever some data are needed by other processors. on the other hand, shared-memory multiprocessors provide a single memory space to all the processors, simplifying the task of exchanging data among processors. access to shared memory has been traditionally implemented by using an interconnection network between processors and memory (figure 1.1(b)). this architecture is referred to as uniform memory access (uma) architecture. it is not scalable because memory access time includes the latency of the interconnection network, and this latency increases with system size.
more recently, shared-memory multiprocessors followed some trends previously established for multicomputers. in particular, memory has been physically distributed among processors, therefore reducing the memory access time for local accesses and increasing scalability. these parallel computers are referred to as distributed sharedmemory multiprocessors (dsms). accesses to remote memory are performed through an interconnection network, very much like in multicomputers. the main difference between dsms and multicomputers is that messages are initiated by memory accesses rather than by calling a system function. in order to reduce memory latency, each processor has several levels of cache memory, thus matching the speed of processors and memories. this architecture provides nonuniform memory access (numa) time. indeed, most of the nonuniformity is due to the different access time between caches and main memories, rather than the different access time between local and remote memories. the main problem arising in dsms is cache coherence. several hardware and software cache coherence protocols have been proposed. these protocols produce additional trafﬁc through the interconnection network.
the use of custom interconnects makes multicomputers and dsms quite expensive. so, networks of workstations (nows) have been proposed as an inexpensive approach to build parallel computers. nows take advantage of recent developments in lans. in particular, the use of atm switches has been proposed to implement nows. however, atm switches are still expensive, which has motivated the development of high-performance
continuously transmitted when channels are not in use to support error detection. on-chip circuitry and external interfaces support static conﬁguration in response to errors: ports can be selectively disabled and ﬂushed, and the route tables used for source routing can be recomputed to avoid faulty regions. an interesting feature of the chip is the presence of counters to record message counts for analysis of the overall network behavior.
the r2 router evolved as a router for a second-generation system in support of lowlatency, high-performance interprocessor communication. the current network prototype provides for interprocessor communication between hewlett-packard commercial workstations. the r2 interconnect topology is a 2-d hexagonal interconnect, and the current router is designed to support a system of 91 nodes. each router has seven ports: six to neighboring routers and one to a local network interface (referred to as the fabric interface component, or fic). the topology is wrapped; that is, routers at the edge of the network have wraparound links to routers at the opposite end of the dimension. the network topology is shown in figure 7.34. for clarity, only one wrapped link is illustrated. as in multidimensional tori, the network topology provides multiple shortest paths between each pair of routers. however, unlike tori, there are also two no-farther paths. when a message traverses a link along a no-farther path, the distance to the destination is neither increased nor decreased. the result is an increase in the number of routing options available at an intermediate router.
the physical channel is organized as a half-duplex channel with 16 bits for data and 9 bits for control. one side of each link is a router port with an arbiter that governs channel mastership. the routers and channels are fully self-timed, eliminating the need for a global clock. there are no local clocks, and resynchronization with the local node processor clocks is performed in the network interface. the rate at which the r2 ports can toggle is estimated to be approximately 5–10 ns, corresponding to an equivalent synchronous transmission rate of 100–200 mhz.
message packets may be of variable length and up to a maximum of 160 bytes. each packet has a 3-byte header that contains addressing information and a 16-byte protocol
while the architecture of the routers is certainly related to the topology of the interconnect, their design is primarily inﬂuenced by the switching technique that they are designed to support. generic issues can be broadly classiﬁed into those dealing with internal data and control paths and those dealing with the design of the interrouter physical links. speciﬁc design choices include internal switch design, buffer management, use of virtual channels, and physical channel ﬂow control strategies. the remainder of the chapter presents descriptions of several modern router architectures as examples of speciﬁc design choices. rather than present a comprehensive coverage of available architectures, we have tried to select illustrative examples from the range of currently available architectures.
for a ﬁxed number of nodes, the choice of the number of dimensions of a k-ary n-cube represents a fundamental trade-off between network diameter and node degree. this choice of network dimension also places different demands on physical resources such as wiring area and number of chip i/os. for a given implementation technology, practical constraints on these physical resources will determine architectural features such as channel widths and, as a result, determine the no-load message latency—message latency in the absence of trafﬁc. similarly, these constraints will also determine the degree of congestion in the network for a given communication pattern, although accurate modeling of such dynamic behavior is more difﬁcult. it thus becomes important to model the relationships between physical constraints and topology and the resulting impact on performance. network optimization is the process of utilizing these models in selecting topologies that best match the physical constraints of the implementation. the following subsections discuss the dominant constraints and the construction of analytical models that incorporate their effects on the no-load message latency.
one of the physical constraints facing the implementation of interconnection networks is the available wiring area. the available wiring area is determined by the packaging technology; that is, whether the network resides on a chip, multichip module, or printed circuit board. in particular, vlsi systems are generally wire limited: the silicon area required by these systems is determined by the interconnect area, and the performance is limited by the delay of these interconnections. since these networks must be implemented 2 3 , while in three dimensions, for an n-node network the available wiring space grows as n the network trafﬁc can grow as n. clearly machines cannot be scaled to arbitrarily large sizes without eventually encountering wiring limits. the choice of network dimension is inﬂuenced by how well the resulting topology makes use of the available wiring area. such an analysis requires performance measures that can relate network topology to the wiring constraints.
theorem 6.1 is only applicable to physical channels if they are not split into virtual channels. if virtual channels are used, the theorem is only valid for virtual channels. however, it can be easily extended to support the failure of physical channels by considering that all the virtual channels belonging to a faulty physical channel will become faulty at the same time. theorem 6.1 is based on theorem 3.1. therefore, it is valid for the same switching techniques as theorem 3.1, as long as edge buffers are used.
the structure of fault-tolerant routing algorithms is a natural consequence of the types of faults that can occur and our ability to diagnose them. the patterns of component failures and expectations about the behavior of processors and routers in the presence of these failures determines the approaches to achieve deadlock and livelock freedom. this information is captured in the fault model. the fault-tolerant computing literature is extensive and thorough in the deﬁnition of fault models for the treatment of faulty digital systems. in this section we will focus on common fault models that have been employed in the design of fault-tolerant routing algorithms for reliable interprocessor networks.
one of the ﬁrst considerations is the level at which components are diagnosed as having failed. typically, detection mechanisms are assumed to have identiﬁed one of two classes of faults. either the entire processing element (pe) and its associated router can fail, or any communication channel may fail. the former is referred to as a node failure, and the latter as a link failure. on a node failure, all physical links incident on the failed node are also marked faulty at adjacent routers. when a physical link fails, all virtual channels on that particular physical link are marked faulty. note that many types of failures will simply manifest themselves as link or node failures. for example, the failure of the link controller, or the virtual channel buffers, appears as a link failure. on the other hand, the failure of the router control unit or the associated pe effectively appears as a node failure. even software errors in the messaging layer can lead to message handlers “locking up” the local interface and rendering the attached router inoperative, effectively resulting in a node fault. hence, this failure model is not as restrictive as it may ﬁrst appear.
this model of individual link and node failures leads to patterns of failed components. adjacent faulty links and faulty nodes are coalesced into fault regions. generally, it is assumed that fault regions do not disconnect the network, since each connected network component can be treated as a distinct network. constraints may now be placed on the structure of these fault regions. the most common constraint employed is that these regions be convex. as will become apparent in this chapter, concave regions present unique difﬁculties for fault-tolerant routing algorithms. some examples of fault regions are illustrated in figure 6.3. convex regions may be further constrained to be block fault regions—regions whose shape is rectangular. this distinction is meaningful only in some topologies, whereas in other topologies, convex faults imply a block structure. given a pattern of random faults in a multidimensional k-ary n-cube, rectangular fault regions can be constructed by marking some functioning nodes as faulty to ﬁll out the fault regions.
channels are used is not particularly important. however, it is central to the evaluation of performance since channel widths directly impact message latency. we will see in later sections that assumptions about how these pins are used must be unambiguous and clearly stated.
in addition to wiring space, a second physical constraint is the number of i/os available per router. this is referred to as the node size. the construction of networks under a constant channel width for an arbitrary number of dimensions is impractical since the node size is linear in the number of dimensions. for example, consider a 2-d network and assume a baseline channel width of 16 data bits, 8 control bits, and full-duplex channels (i.e., two unidirectional channels between adjacent nodes). this would correspond to 192 signal pins, not including power, ground, and some miscellaneous i/os. this is certainly feasible in current technology. for an n-dimensional network, the router will have 48 pins in each direction in each dimension, for a total pin-out of 96n. if we consider a 10-d network with the same channel width, we will require 960 pins, or close to 500 pins just for the channels in a more modest 5-d network. as we progress to 32-bit channels, the feasible network dimension becomes smaller. thus, practical considerations place limits on channel width as a function of dimensionality [1, 3].
even if we consider the bisection width constraints and networks being wiring area limited, it is likely that before networks encounter a bisection width limit, they may encounter the pin-out limit. this is evident from the following observation. consider the implementation of a k-ary n-cube with the bisection width constrained to be n bits. the maximum channel width determined by this bisection width is given by
note that this is the maximum channel width permitted under this bisection width constraint. a router does not have to provide this number of pins across each channel. however, if it does, the router will have a channel width of w bits between adjacent routers in each direction in each dimension, for a total pin-out of 2nw = nk. a network of
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
programming multicomputers is not an easy task. the programmer has to take care of distributing code and data among the processors in an efﬁcient way, invoking messagepassing calls whenever some data are needed by other processors. on the other hand, shared-memory multiprocessors provide a single memory space to all the processors, simplifying the task of exchanging data among processors. access to shared memory has been traditionally implemented by using an interconnection network between processors and memory (figure 1.1(b)). this architecture is referred to as uniform memory access (uma) architecture. it is not scalable because memory access time includes the latency of the interconnection network, and this latency increases with system size.
more recently, shared-memory multiprocessors followed some trends previously established for multicomputers. in particular, memory has been physically distributed among processors, therefore reducing the memory access time for local accesses and increasing scalability. these parallel computers are referred to as distributed sharedmemory multiprocessors (dsms). accesses to remote memory are performed through an interconnection network, very much like in multicomputers. the main difference between dsms and multicomputers is that messages are initiated by memory accesses rather than by calling a system function. in order to reduce memory latency, each processor has several levels of cache memory, thus matching the speed of processors and memories. this architecture provides nonuniform memory access (numa) time. indeed, most of the nonuniformity is due to the different access time between caches and main memories, rather than the different access time between local and remote memories. the main problem arising in dsms is cache coherence. several hardware and software cache coherence protocols have been proposed. these protocols produce additional trafﬁc through the interconnection network.
the use of custom interconnects makes multicomputers and dsms quite expensive. so, networks of workstations (nows) have been proposed as an inexpensive approach to build parallel computers. nows take advantage of recent developments in lans. in particular, the use of atm switches has been proposed to implement nows. however, atm switches are still expensive, which has motivated the development of high-performance
higher latency than the average value. a high value of the standard deviation usually indicates that some messages are blocked for a long time in the network. the peak value of the latency can also help in identifying these situations.
latency is measured in time units. however, when comparing several design choices, the absolute value is not important. as many comparisons are performed by using network simulators, latency can be measured in simulator clock cycles. unless otherwise stated, the latency plots presented in this chapter for unicast messages measure the average value of the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. in most cases, the simulator clock cycle is the unit of measurement. however, in section 9.10, latency is measured in nanoseconds.
throughput is the maximum amount of information delivered per time unit. it can also be deﬁned as the maximum trafﬁc accepted by the network, where trafﬁc, or accepted trafﬁc is the amount of information delivered per time unit. throughput could be measured in messages per second or messages per clock cycle, depending on whether absolute or relative timing is used. however, throughput would depend on message and network size. so, throughput is usually normalized, dividing it by message size and network size. as a result, throughput can be measured in bits per node and microsecond, or in bits per node and clock cycle. again, when comparing different design choices by simulation, and assuming that channel width is equal to ﬂit size, throughput can be measured in ﬂits per node and clock cycle. alternatively, accepted trafﬁc and throughput can be measured as a fraction of network capacity. a uniformly loaded network is operating at capacity if the most heavily loaded channel is used 100% of the time [72]. again, network capacity depends on the communication pattern.
a standard way to measure accepted trafﬁc and throughput was proposed at the workshop on parallel computer routing and communication (pcrcw’94). it consists of representing them as a fraction of the network capacity for a uniform distribution of destinations, assuming that the most heavily loaded channels are located in the network bisection. this network capacity is referred to as normalized bandwidth. so, regardless of the communication pattern used, it is recommended to measure applied load, accepted trafﬁc, and throughput as a fraction of normalized bandwidth. normalized bandwidth can be easily derived by considering that 50% of uniform random trafﬁc crosses the bisection of the network. thus, if a network has bisection bandwidth b bits/s, each node in an n-node network can inject 2b/n bits/s at the maximum load. unless otherwise stated, accepted trafﬁc and throughput are measured as a fraction of normalized bandwidth. while this is acceptable when comparing different design choices in the same network, it should be taken into account that those choices may lead to different clock cycles. in this case, each set of design parameters may produce a different bisection bandwidth, therefore invalidating the normalized bandwidth as a trafﬁc unit. in that case, accepted trafﬁc and throughput can be measured in bits (ﬂits) per node and microsecond. we use this unit in section 9.10.
a common misconception consists of using throughput instead of trafﬁc. as mentioned above, throughput is the maximum accepted trafﬁc. another misconception consists of considering throughput or trafﬁc as input parameters instead of measurements,
suggesting the corresponding west-ﬁrst routing algorithm: route a packet ﬁrst west, if necessary, and then adaptively south, east, and north. the two turns prohibited in figure 4.9(c) are the two turns to the west. therefore, in order to travel west, a packet must begin in that direction. figure 4.10 shows the minimal west-ﬁrst routing algorithm for 2-d meshes, where select() is the selection function deﬁned in section 3.1.2. this function returns a free channel (if any) from the set of channels passed as parameters. see exercise 4.5 for a nonminimal version of this algorithm. three example paths for the west-ﬁrst algorithm are shown in figure 4.11. the channels marked as unavailable are either faulty or being used by other packets. one of the paths shown is minimal, while the other two paths are nonminimal, resulting from routing around unavailable channels. because cycles are avoided, west-ﬁrst routing is deadlock-free. for minimal routing, the algorithm is fully adaptive if the destination is on the right-hand side (east) of the source; otherwise, it is deterministic. if nonminimal routing is allowed, the algorithm is adaptive in either case. however, it is not fully adaptive.
there are other ways to select six turns so as to prohibit cycles. however, the selection of the two prohibited turns may not be arbitrary [129]. if turns are prohibited as in figure 4.12, deadlock is still possible. figure 4.12(a) shows that the three remaining left turns are equivalent to the prohibited right turn, and figure 4.12(b) shows that the three remaining right turns are equivalent to the prohibited left turn. figure 4.12(c) illustrates how cycles may still occur. of the 16 different ways to prohibit two turns, 12 prevent deadlock and only 3 are unique if symmetry is taken into account. these three combinations correspond to the west-ﬁrst, north-last, and negative-ﬁrst routing algorithms. the northlast routing algorithm does not allow turns from north to east or from north to west. the negative-ﬁrst routing algorithm does not allow turns from north to west or from east to south.
in addition to 2-d mesh networks, the turn model can be used to develop partially adaptive routing algorithms for n-dimensional meshes, for k-ary n-cubes, and for hypercubes [129]. by applying the turn model to the hypercube, an adaptive routing algorithm, namely, p-cube routing, can be developed. let s = sn−1sn−2 . . . s0 and d = dn−1dn−2 . . . d0 be the source and destination nodes, respectively, in a binary n-cube. the set e consists of all the dimension numbers in which s and d differ. the size of e is the hamming distance between s and d. thus, i ∈ e if si (cid:16)= di. e is divided into two disjoint subsets, e0 and e1, where i ∈ e0 if si = 0 and di = 1, and j ∈ e1 if sj = 1 and
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
even representing latency as a function of trafﬁc. when running simulations with synthetic workloads, the applied load (also known as offered trafﬁc, generation rate, or injection rate) is an input parameter while latency and accepted trafﬁc are measurements. so, latency-trafﬁc graphs do not represent functions. it should be noted that the network may be unstable when accepted trafﬁc reaches its maximum value. in this case, increasing the applied load may reduce the accepted trafﬁc until a stable point is reached. as a consequence, for some values of the accepted trafﬁc there exist two values for the latency, clearly indicating that the graph does not represent a function.
in the presence of faults, both performance and reliability are important. when presenting performance plots, the chaos normal form (cnf) format (to be described below) should be preferred in order to analyze accepted trafﬁc as a function of applied load. plots can be represented for different values of the number of faults. in this case, accepted trafﬁc can be smaller than applied load because the network is saturated or because some messages cannot be delivered in the presence of faults. another interesting measure is the probability of message delivery as a function of the number of failures.
the next sections describe two standard formats to represent performance results. these formats were proposed at pcrcw’94. the cnf requires paired accepted trafﬁc versus applied load and latency versus applied load graphs. the burton normal form (bnf) uses a single latency versus accepted trafﬁc graph. use of only latency (including source queuing) versus applied load is discouraged because it is impossible to gain any data about performance above saturation using such graphs.
cnf graphs display accepted trafﬁc on one graph and network latency on a second graph. in both graphs, the x-axis corresponds to normalized applied load. by using two graphs, the latency is shown both below and above saturation, and the accepted trafﬁc above saturation is visible. while bnf graphs show the same data, cnf graphs are more clear in their presentation of the data.
butterﬂy permutation β k be β k 0 . pattern ci is described by the (n − i)th butterﬂy permutation β k connection pattern c0 is selected to be σ k. 4. omega network. in an omega network, connection pattern ci is described by the perfect k-shufﬂe permutation σ k for 0 ≤ i ≤ n − 1. connection pattern cn is selected to be β k 0 . thus, all the connection patterns but the last one are identical. the last connection pattern produces no permutation.
the topological equivalence of these mins can be viewed as follows: consider that each input link to the ﬁrst stage is numbered using a string of n digits sn−1sn−2 . . . s1s0, where 0 ≤ si ≤ k − 1, for 0 ≤ i ≤ n − 1. the least signiﬁcant digit s0 gives the address of the input port at the corresponding switch, and the address of the switch is given by sn−1sn−2 . . . s1. at each stage, a given switch is able to connect any input port with any output port. this can be viewed as changing the value of the least signiﬁcant digit of the address. in order to be able to connect any input to any output of the network, it should be possible to change the value of all the digits. as each switch is only able to change the value of the least signiﬁcant digit of the address, connection patterns between stages are deﬁned in such a way that the position of digits is permuted, and after n stages all the digits have occupied the least signiﬁcant position. therefore, the above-deﬁned mins differ in the order in which address digits occupy the least signiﬁcant position. figure 1.19 shows the topology of four 16 × 16 unidirectional multistage interconnection networks: (a) baseline network, (b) butterﬂy network, (c) cube network, and (d) omega network.
figure 1.20 illustrates a bidirectional switch in which each port is associated with a pair of unidirectional channels in opposite directions. this implies that information can be transmitted simultaneously in opposite directions between neighboring switches. for ease of explanation, it is assumed that processor nodes are on the left-hand side of the network, as shown in figure 1.21. a bidirectional switch supports three types of connections: forward, backward, and turnaround (see figure 1.20). as turnaround connections between ports at the same side of a switch are possible, paths have different lengths. an eight-node butterﬂy bidirectional min (bmin) is illustrated in figure 1.21.
paths are established in bmins by crossing stages in the forward direction, then establishing a turnaround connection, and ﬁnally crossing stages in the backward direction. this is usually referred to as turnaround routing. figure 1.22 shows two alternative paths from node s to node d in an eight-node butterﬂy bmin. when crossing stages in the forward direction, several paths are possible. each switch can select any of its output ports. however, once the turnaround connection is crossed, a single path is available up to
adaptive double-y (mad-y). it improves adaptivity with respect to the double-y algorithm. basically, mad-y allows packets using y 1 channels to turn to the x+ direction and packets using x− channels to turn and use y 2 channels. figures 4.23 and 4.24 show the turns allowed by the double-y and mad-y algorithms, respectively.
the mad-y algorithm has the maximum adaptivity that can be obtained without introducing cyclic dependencies between channels. however, as shown in theorem 3.1, cycles do not necessarily produce deadlock. thus, the mad-y algorithm can be improved. it was done by schwiebert and jayasimha, who proposed the opt-y algorithm [306, 308]. this algorithm is deadlock-free and optimal with respect to the number of routing restrictions on the virtual channels for deadlock-avoidance-based routing. basically, the opt-y algorithm allows all the turns between x and y 2 channels as well as turns between x+ and y 1 channels. turns from y 1 to x− channels are prohibited. turns from x− to y 1 channels as well as 0-degree turns between y 1 and y 2 channels are restricted. these turns are only allowed when the packet has completed its movement along x− channels (the x-offset is zero or positive). figure 4.25 shows the turns allowed by the opt-y algorithm.
deﬁning a routing algorithm by describing the allowed and prohibited turns makes it difﬁcult to understand how routing decisions are taken at a given node. the opt-y algorithm is described in figure 4.26 using pseudocode.
u1, u2, . . . , uk denote k destination nodes, where k ≥ 1. the set k = {u0, u1, . . . , uk}, which is a subset of v (g), is called a multicast set. depending on the underlying communication paradigm and the routing method, the multicast communication problem in a multicomputer can be formulated as different graph-theoretical problems.
in some communication mechanisms, replication of an incoming message in order to be forwarded to multiple neighboring nodes may involve too much overhead and is usually undesirable. thus, the routing method does not allow each processor to replicate the message passing by. also, a multicast path model provides better performance than the tree model (to be described below) when there is contention in the network. from a switching technology point of view, the multicast path model is more suitable for wormhole switching.
the multicast communication problem becomes the problem of ﬁnding a shortest path starting from u0 and visiting all k destination nodes. this optimization problem is the ﬁnding of an optimal multicast path (omp) and is formally deﬁned below.
a multicast path (v1, v2, . . . , vn) for a multicast set k in g is a subgraph p (v , e) of g, where v (p ) = {v1, v2, . . . , vn} and e(p ) = {(vi , vi+1) : 1 ≤ i ≤ n − 1}, such that v1 = u0 and k ⊆ v (p ). an omp is a multicast path with the shortest total length.
reliable communication is essential to a message-passing system. usually, a separate acknowledgment message is sent from every destination node to the source node on receipt of a message. one way to avoid the sending of |k| separate acknowledgment messages is to have the source node itself receive a copy of the message it initiated after all destination nodes have been visited. acknowledgments are provided in the form of error bits ﬂagged by intermediate nodes when a transmission error is detected. thus, the multicast communication problem is the problem of ﬁnding a shortest cycle, called the optimal multicast cycle (omc), for k.
a multicast cycle (v1, v2, . . . , vn, v1) for k is a subgraph c(v , e) of g, where v (c) = {v1, v2, . . . , vn} and e(c) = {(vn, v1), (vi , vi+1) : 1 ≤ i ≤ n − 1}, such that k ⊆ v (c). an omc is a multicast cycle with the shortest total length.
both omc and omp assume that the message will not be replicated by any node during transmission. however, message replication can be implemented by using some hardware approach [197]. if the major concern is to minimize trafﬁc, the multicast problem becomes
u1, u2, . . . , uk denote k destination nodes, where k ≥ 1. the set k = {u0, u1, . . . , uk}, which is a subset of v (g), is called a multicast set. depending on the underlying communication paradigm and the routing method, the multicast communication problem in a multicomputer can be formulated as different graph-theoretical problems.
in some communication mechanisms, replication of an incoming message in order to be forwarded to multiple neighboring nodes may involve too much overhead and is usually undesirable. thus, the routing method does not allow each processor to replicate the message passing by. also, a multicast path model provides better performance than the tree model (to be described below) when there is contention in the network. from a switching technology point of view, the multicast path model is more suitable for wormhole switching.
the multicast communication problem becomes the problem of ﬁnding a shortest path starting from u0 and visiting all k destination nodes. this optimization problem is the ﬁnding of an optimal multicast path (omp) and is formally deﬁned below.
a multicast path (v1, v2, . . . , vn) for a multicast set k in g is a subgraph p (v , e) of g, where v (p ) = {v1, v2, . . . , vn} and e(p ) = {(vi , vi+1) : 1 ≤ i ≤ n − 1}, such that v1 = u0 and k ⊆ v (p ). an omp is a multicast path with the shortest total length.
reliable communication is essential to a message-passing system. usually, a separate acknowledgment message is sent from every destination node to the source node on receipt of a message. one way to avoid the sending of |k| separate acknowledgment messages is to have the source node itself receive a copy of the message it initiated after all destination nodes have been visited. acknowledgments are provided in the form of error bits ﬂagged by intermediate nodes when a transmission error is detected. thus, the multicast communication problem is the problem of ﬁnding a shortest cycle, called the optimal multicast cycle (omc), for k.
a multicast cycle (v1, v2, . . . , vn, v1) for k is a subgraph c(v , e) of g, where v (c) = {v1, v2, . . . , vn} and e(c) = {(vn, v1), (vi , vi+1) : 1 ≤ i ≤ n − 1}, such that k ⊆ v (c). an omc is a multicast cycle with the shortest total length.
both omc and omp assume that the message will not be replicated by any node during transmission. however, message replication can be implemented by using some hardware approach [197]. if the major concern is to minimize trafﬁc, the multicast problem becomes
a steiner tree, s(v , e), for a multicast set k is a subtree of g such that k ⊆ v (s). a minimal steiner tree (mst) is a steiner tree with a minimal total length.
in the steiner tree problem, it is not necessary to use a shortest path from the source to a destination. if the distance between two nodes is not a major factor in the communication time, such as in vct, wormhole, and circuit switching, the above optimization problem is appropriate. however, if the distance is a major factor in the communication time, such as in saf switching, then we may like to minimize time ﬁrst, then trafﬁc. the multicast communication problem is then modeled as an optimal multicast tree (omt). the omt problem was originally deﬁned in [195].
an omt, t (v , e), for k is a subtree of g such that (a) k ⊆ v (t ), (b) dt (u0, ui ) = dg(u0, ui ), for 1 ≤ i ≤ k, and (c) |e(t )| is as small as possible.
apparently, the complexity of each of the above optimization problems is directly dependent on the underlying host graph. the above graph optimization problems for the popular hypercube and 2-d mesh topologies were studied in [210, 212], showing that the omc and omp problems are np-complete for those topologies. also, it was shown that the mst and omt problems are np-complete for the hypercube topology [60, 135]. the mst problem for the 2-d mesh topology is equivalent to the rectilinear steiner tree problem, which is np-complete [120].
the np-completeness results indicate the necessity to develop heuristic multicast communication algorithms for popular interconnection topologies. multicast communication may be supported in hardware, software, or both. sections 5.5 and 5.7 will address hardware and software implementations, respectively, of multicast.
hardware support of multicast communication in multicomputers requires increased functionality within the routers. this functionality may include interpretation of multicast addresses (or group id) and forwarding of messages onto multiple outgoing channels (replication). the result is the capability for one local processor to efﬁciently send the
the evolution of switching techniques was naturally inﬂuenced by the need for better performance. vct switching introduced pipelined message transmission, and wormhole switching further contributed reduced buffer requirements in conjunction with ﬁne-grained pipelining. the mad postman switching technique carried pipelining to the bit level to maximize performance. in packet switching and vct, messages are completely buffered at a node. as a result, the messages consume network bandwidth proportional to the network load. on the other hand, wormhole-switched messages may block occupying buffers and channels across multiple routers, precluding access to the network bandwidth by other messages. thus, while average message latency can be low, the network saturates at a fraction of the maximum available bandwidth, and the variance of message latency can be high. the use of virtual channels decouples the physical channel from blocked messages, thus reducing the blocking delays experienced by messages and enabling a larger fraction of the available bandwidth to be utilized. however, the increasing multiplexing of multiple messages increases the delay experienced by data ﬂits. furthermore, multiple virtual channels can increase the ﬂow control latency through the router and across the physical channel, producing upward pressure on average message latency.
the effects of wormhole switching on individual messages can be highly unpredictable. since buffer requirements are low, contention in the network can substantially increase the latency of a message in parts of the network. packet switching tends to have more predictable latency characteristics, particularly at low loads since messages are buffered at each node. vct operates like wormhole switching at low loads and approximates packet switching at high loads where link contention forces packets to be buffered at each node. thus, at low loads we expect to see wormhole-switching techniques providing superior latency/throughput relative to packet-switched networks, while at high loads we expect to see packet-switched schemes perform better. as expected, the performance of vct approaches that of wormhole switching at low loads and that of packet switching at high loads. more detailed performance comparisons can be found in chapter 9.
these switching techniques can be characterized as optimistic in the sense that buffer resources and links are allocated as soon as they become available, regardless of the state of progress of the remainder of the message. in contrast, pipelined circuit switching and scouting switching may be characterized as conservative. data ﬂits are transmitted only after it is clear that ﬂits can make forward progress. these ﬂow control protocols are motivated by fault tolerance concerns. bws seeks to improve the fraction of available bandwidth that can be exploited by wormhole switching by buffering groups of ﬂits.
in packet switching, error detection and retransmission can be performed on a linkby-link basis. packets may be adaptively routed around faulty regions of the network. when messages are pipelined over several links, error recovery and control becomes complicated. error detection and retransmission (if feasible) must be performed by higherlevel protocols operating between the source and destination, rather than at the level of the physical link. if network routers or links have failed, message progress can be indeﬁnitely halted, with messages occupying buffer and channel resources. this can lead to deadlocked conﬁgurations of messages and eventually failure of the network.
the paradigm adopted throughout the preceding examples has been to characterize messages by the direction of traversal in the network and therefore the direction and virtual channels occupied by these messages when misrouted around a fault ring. the addition of virtual channels for each message type ensures that these messages occupy disjoint virtual networks and therefore channel resources. since the usage of resources within a network is orchestrated to be acyclic and transitions made by messages between networks remain acyclic, routing can be guaranteed to be deadlock-free. however, the addition of virtual channels affects the speed and complexity of the routers.arbitration between virtual channels and the multiplexing of virtual channels across the physical channel can have a substantial impact on the ﬂow control latency through the router [57]. this motivated investigations of solutions that did not rely on many (or any) virtual channels.
origin-based fault-tolerant routing is a paradigm that enables fault-tolerant routing in mesh networks under a similar fault model, but without the addition of virtual channels [145]. the basic fault-free form of origin-based routing follows the paradigm of several adaptive routing algorithms proposed for binary hypercubes and the turn model proposed for more general direct network topologies. each message progresses through two phases. in the ﬁrst phase, the message is adaptively routed toward a special node. on reaching this node, the message is adaptively routed to the destination in the second phase. in previous applications of this paradigm to binary hypercubes, this special node could be the zenith node whose address is given by the logical or of the source and destination addresses. messages are ﬁrst routed adaptively to their zenith and then adaptively toward the destination [184]. this phase ordering prevents the formation of cycles in the channel dependency graphs. variants of this approach have also been proposed, including the relaxation of ordering restrictions on the phases [59]. in origin-based routing in mesh networks this special node is the node that is designated as the origin according to the mesh coordinates. while the approach is directly extensible to multidimensional networks, the following description deals with 2-d meshes.
all of the physical channels are partitioned into two disjoint networks. the in network consists of all of the unidirectional channels that are directed toward the origin, while the out network consists of all of the unidirectional channels directed away from the origin. the orientation of a channel can be determined by the node at the receiving end of the channel. if this node is closer to the origin than the sending end of the channel, the channel is in the in network. otherwise it is in the out network. the outbox for a node in the mesh is the submesh comprised of all nodes on a shortest path to the origin. an example of an outbox for a destination node d is shown in figure 6.24. messages are routed in two phases. in the ﬁrst phase messages are routed adaptively toward the destination/origin, over any shortest path using channels in the in network. when the header ﬂit arrives at any node in the outbox for the destination, the message is now routed adaptively toward the destination using only channels in the out network. as shown in the example in figure 6.24, the resulting complete path may not be a minimal path. the choice of minimal paths is easily enforced by restricting messages traversing the in network to use channels that take the message closer to the destination. the result of enforcing such a restriction on the same (source, destination) pair is also shown in figure 6.24. the choice of the origin node is important. since all messages are ﬁrst routed toward the origin, hot spots can
latency through the chip is estimated to be 40 ns, while the latency from the input on a router to the input on the adjacent router is estimated to be 50 ns.
routers supporting vct switching share many of the attributes of wormhole routers. the principal distinguishing feature is the availability of sufﬁcient space for buffering complete message packets. the following examples discuss four router architectures implementing vct switching.
the chaos router chip is an example of a router designed for vct switching for operation in a 2-d mesh. to reduce pin count, each channel is a 16-bit, half-duplex bidirectional channel. the chip was designed for a cycle time of 15 ns. the latency for the common case of cut-through routing (no misrouting) is four cycles from input to output. this performance compares favorably to that found in more compact oblivious routers and has been achieved by keeping the misrouting and buffering logic off the critical path. a block diagram of the chaos router is shown in figure 7.31.
the router is comprised of the router core and the multiqueue [32]. the core connects input frames to output frames through a crossbar switch. each frame can store one 20-ﬂit message packet. each ﬂit is 16 bits wide and corresponds to the width of the physical channel. the architecture of the core is very similar to that of contemporary oblivious routers, with the additional ability to buffer complete 20-ﬂit packets in each input and output frame. under low-load conditions, the majority of the trafﬁc ﬂows through this core,
this function requires updating the channel status register. this register is a centralized resource. otherwise, several packets may simultaneously reserve the same output channel. so when two or more packets compete for the same output channel, some arbitration is required.
the traditional scheduling policy for the allocation of the routing control unit (or for granting access to the channel status register) is round-robin among input channels. in this scheduling policy, input channel buffers form a logical circular list. after routing the header stored in a buffer, the pointer to the current buffer is advanced to the next input buffer where a packet header is ready to be routed. the routing function is computed for that header, supplying a set of candidate output channels. then, one of these channels is selected, if available, and the packet is routed to that channel. this scheduling policy is referred to as input driven [116]. it is simple, but when the network is heavily loaded a high percentage of routing operations may fail because all of the requested output channels are busy.
an alternative scheduling policy consists of selecting a packet for which there is a free output channel. in this strategy, output channels form a logical circular list. after routing a packet, the pointer to the current output channel is advanced to the next free output channel. the router tries to ﬁnd a packet in an input buffer that needs to be routed to the current output channel. if packets are found, it selects one to be routed to the current output channel. if no packet is found, the pointer to the current output channel is advanced to the next free output channel. this scheduling policy is referred to as output driven [116]. outputdriven strategies may be more complex than input-driven ones. however, performance is usually higher when the network is heavily loaded [116]. output-driven scheduling can be implemented by replicating the circuit computing the routing function at each input channel, and adding a register to each output channel to store information about the set of input channels requesting that output channel. although output-driven scheduling usually improves performance over input-driven scheduling, the difference between them is much smaller when channels are randomly selected [116]. indeed, a random selection usually performs better than round-robin when input-driven scheduling is used. finally, it should be noted that most selection functions described in the previous section cannot be implemented with output-driven scheduling.
from an engineering point of view, the “best” routing algorithm is either the one that maximizes performance or the one that maximizes the performance/cost ratio, depending on the design goals. a detailed quantitative analysis must consider the impact of the routing algorithm on node design. even if we do not consider the cost, a more complex routing algorithm may increase channel utilization at the expense of a higher propagation delay and a lower clock frequency. a slower clock also implies a proportional reduction in channel bandwidth if channels are driven synchronously. thus, router design must be considered to make performance evaluation more realistic. this is the reason why
suggesting the corresponding west-ﬁrst routing algorithm: route a packet ﬁrst west, if necessary, and then adaptively south, east, and north. the two turns prohibited in figure 4.9(c) are the two turns to the west. therefore, in order to travel west, a packet must begin in that direction. figure 4.10 shows the minimal west-ﬁrst routing algorithm for 2-d meshes, where select() is the selection function deﬁned in section 3.1.2. this function returns a free channel (if any) from the set of channels passed as parameters. see exercise 4.5 for a nonminimal version of this algorithm. three example paths for the west-ﬁrst algorithm are shown in figure 4.11. the channels marked as unavailable are either faulty or being used by other packets. one of the paths shown is minimal, while the other two paths are nonminimal, resulting from routing around unavailable channels. because cycles are avoided, west-ﬁrst routing is deadlock-free. for minimal routing, the algorithm is fully adaptive if the destination is on the right-hand side (east) of the source; otherwise, it is deterministic. if nonminimal routing is allowed, the algorithm is adaptive in either case. however, it is not fully adaptive.
there are other ways to select six turns so as to prohibit cycles. however, the selection of the two prohibited turns may not be arbitrary [129]. if turns are prohibited as in figure 4.12, deadlock is still possible. figure 4.12(a) shows that the three remaining left turns are equivalent to the prohibited right turn, and figure 4.12(b) shows that the three remaining right turns are equivalent to the prohibited left turn. figure 4.12(c) illustrates how cycles may still occur. of the 16 different ways to prohibit two turns, 12 prevent deadlock and only 3 are unique if symmetry is taken into account. these three combinations correspond to the west-ﬁrst, north-last, and negative-ﬁrst routing algorithms. the northlast routing algorithm does not allow turns from north to east or from north to west. the negative-ﬁrst routing algorithm does not allow turns from north to west or from east to south.
in addition to 2-d mesh networks, the turn model can be used to develop partially adaptive routing algorithms for n-dimensional meshes, for k-ary n-cubes, and for hypercubes [129]. by applying the turn model to the hypercube, an adaptive routing algorithm, namely, p-cube routing, can be developed. let s = sn−1sn−2 . . . s0 and d = dn−1dn−2 . . . d0 be the source and destination nodes, respectively, in a binary n-cube. the set e consists of all the dimension numbers in which s and d differ. the size of e is the hamming distance between s and d. thus, i ∈ e if si (cid:16)= di. e is divided into two disjoint subsets, e0 and e1, where i ∈ e0 if si = 0 and di = 1, and j ∈ e1 if sj = 1 and
increases with increasing network dimension. the optimal number of dimensions is determined as a result of speciﬁc values of router architecture and implementation technology that ﬁx the values of α and β.
it is important to note that these models provide the optimal number of dimensions in the absence of contention. this analysis must be coupled with detailed performance analysis, usually via simulation, as described in chapter 9, or analytical models of contention (e.g., as in [1, 3, 127]).
it is clear that physical constraints have a substantial impact on the performance of the interconnection network. in practice, electronic packaging is a major determinant of these physical constraints. by packaging we refer to the hierarchy of interconnections between computational elements. systems start with communicating elements sharing a silicon surface. a bare die may then be packaged in a single-chip carrier or a multichip module [303], which in turn may be mounted on one or both sides of a printed circuit board. multiple boards are mounted in a card cage, and multiple card cages may be connected, and so on. this is graphically illustrated in figure 7.5. the materials and fabrication technology at each level of this hierarchy are distinct, leading to interconnects with very different physical characteristics and constraints. for example, wire pitch may vary from a few micrometers on a die to a few hundred micrometers on a printed circuit board. such parameters clearly determine available wiring area and therefore achievable bisection bandwidth. with multichip modules and area array i/os, the number of i/os available out of a die becomes proportional to chip area rather than chip perimeter [302]. as the preceding analysis has demonstrated, the choice of topology is strongly inﬂuenced by such physical constraints and therefore by the packaging technology.
packaging technology continues to evolve, keeping pace with rapid advances in semiconductor technology. the advent of new packaging technologies (e.g., low-cost multichip modules, 3-d chip stacks, etc.) will lead to major changes in the relationship between these physical constraints, and as a result signiﬁcantly impact the cost, performance, and reliability of the next generation of systems in general and multiprocessor interconnects in particular [83, 122]. an accurate analysis of the effect of packaging technology and options will enable the identiﬁcation of topologies that effectively utilize available packaging technology, where “form ﬁts function” [70].
from the point of view of the network topology, the fundamental packaging question concerns the placement of die, package, board, and subsystem boundaries and the subsequent performance impact of these choices. die boundaries may be determined in part by technology as we see advances that place multiple processors and the associated routers and interconnect on a single chip. the placement of other boundaries such as multichip carriers and boards are determined by trade-offs in cost, performance, thermal density, reliability, pin-out, and so on. for example, the network used to connect 16 processors on a single chip may be quite different from that used on a printed circuit board to connect 64 such chips. therefore, these partitioning decisions are critical. to make the
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
request class (3 ﬂits). a processor or i/o device uses a request packet to obtain data in and/or ownership of a cache block or up to 64 bytes of i/o data.
forward class (3 ﬂits). a memory controller (mc1 or mc2) uses a forward packet to forward a request packet to the current owner or sharer (processor or i/o device) of a cache block.
block response class (18 or 19 ﬂits).a processor or i/o device uses a block response packet to return the data requested by a request class packet or to send a modiﬁed cache block back to memory.
nonblock response class (2 or 3 ﬂits).a processor, memory controller, or i/o device uses a nonblock response packet to acknowledge coherence protocol actions, such as a request for a cache block.
special class (1 or 3 ﬂits). these are packets used by the network and coherence protocol. the special class includes noop packets, which can carry buffer deallocation information between routers.
the packet header identiﬁes the packet class and function. the header also contains routing information for the packet, (optionally) the physical address of the cache block or the data block in i/o space corresponding to this packet, and ﬂow control information between neighboring routers. besides the header, the block response and write io packets also contain 16 ﬂits (64 bytes) of data.
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
delay through the entire circuit is negligible compared to clock cycle. hence, tdat a does not depend on that delay. the factor of 2 in the setup cost represents the time for the forward progress of the header and the return of the acknowledgment. the use of b hz as the channel speed represents the transmission across a hardwired path from source to destination.
in circuit switching, the complete message is transmitted after the circuit has been set up. alternatively, the message can be partitioned and transmitted as ﬁxed-length packets, for example, 128 bytes. the ﬁrst few bytes of a packet contain routing and control information and are referred to as the packet header. each packet is individually routed from source to destination. this technique is referred to as packet switching. a packet is completely buffered at each intermediate node before it is forwarded to the next node. this is the reason why this switching technique is also referred to as store-and-forward (saf) switching. the header information is extracted by the intermediate router and used to determine the output link over which the packet is to be forwarded. a time-space diagram of the progress of a packet across three links is shown in figure 2.8. from the ﬁgure we can see that the latency experienced by a packet is proportional to the distance between the source and destination nodes. note that the ﬁgure has omitted the packet latency, ts , through the router. packet switching is advantageous when messages are short and frequent. unlike circuit switching, where a segment of a reserved path may be idle for a signiﬁcant period of time, a communication link is fully utilized when there are data to be transmitted. many packets belonging to a message can be in the network simultaneously even if the ﬁrst packet has not yet arrived at the destination. however, splitting a message into packets produces some overhead. in addition to the time required at source and destination nodes, every packet must be routed at each intermediate node. an example of the format of a data packet header is shown in figure 2.9. this is the header format used in the jpl
software messaging layer. a high percentage of the communication latency in multicomputers is produced by the overhead in the software messaging layer. reducing or hiding this overhead is likely to have a higher impact on performance than the remaining design parameters, especially when messages are relatively short. the use of vcc can eliminate or hide most of the software overhead by overlapping path setup with computation, and caching and retaining virtual circuits for use by multiple messages. vcc complements the use of techniques to reduce the software overhead, like active messages. it should be noted that software overhead is so high in some multicomputers that it makes no sense improving other design parameters. however, once this overhead has been removed or hidden, the remaining design parameters become much more important.
software support for collective communication. collective communication operations beneﬁt considerably from using speciﬁc algorithms. using separate addressing, latency increases linearly with the number of participating nodes. however, when algorithms for collective communication are implemented in software, latency is considerably reduced, increasing logarithmically with the number of participating nodes. this improvement is achieved with no additional hardware cost and no overhead for unicast messages.
number of ports. if the software overhead has been removed or hidden, the number of ports has a considerable inﬂuence on performance, especially when messages are sent locally. if the number of ports is too small, the network interface is likely to be a bottleneck for the network. the optimal number of ports heavily depends on the spatial locality of trafﬁc patterns.
switching technique. nowadays, most commercial and experimental parallel computers implement wormhole switching. although vct switching achieves a higher throughput, the performance improvement is small when virtual channels are used in wormhole switching. additionally, vct switching requires splitting messages into packets not exceeding buffer capacity. if messages are longer than buffer size, wormhole switching should be preferred. however, when messages are shorter than or equal to buffer size, vct switching performs slightly better than wormhole switching, also simplifying deadlock avoidance. this is the case for distributed, shared-memory multiprocessors. vct switching is also preferable when it is not easy to avoid deadlock in wormhole switching (multicast routing in multistage networks) or in some applications requiring real-time communication [294]. finally, a combination of wormhole switching and circuit switching with wave-pipelined switches and channels has the potential to increase performance, especially when messages are very long [101].
packet size. for pipelined switching techniques, ﬁlling the pipeline produces some overhead. also, routing a header usually takes longer than transmitting a data ﬂit
delay through the entire circuit is negligible compared to clock cycle. hence, tdat a does not depend on that delay. the factor of 2 in the setup cost represents the time for the forward progress of the header and the return of the acknowledgment. the use of b hz as the channel speed represents the transmission across a hardwired path from source to destination.
in circuit switching, the complete message is transmitted after the circuit has been set up. alternatively, the message can be partitioned and transmitted as ﬁxed-length packets, for example, 128 bytes. the ﬁrst few bytes of a packet contain routing and control information and are referred to as the packet header. each packet is individually routed from source to destination. this technique is referred to as packet switching. a packet is completely buffered at each intermediate node before it is forwarded to the next node. this is the reason why this switching technique is also referred to as store-and-forward (saf) switching. the header information is extracted by the intermediate router and used to determine the output link over which the packet is to be forwarded. a time-space diagram of the progress of a packet across three links is shown in figure 2.8. from the ﬁgure we can see that the latency experienced by a packet is proportional to the distance between the source and destination nodes. note that the ﬁgure has omitted the packet latency, ts , through the router. packet switching is advantageous when messages are short and frequent. unlike circuit switching, where a segment of a reserved path may be idle for a signiﬁcant period of time, a communication link is fully utilized when there are data to be transmitted. many packets belonging to a message can be in the network simultaneously even if the ﬁrst packet has not yet arrived at the destination. however, splitting a message into packets produces some overhead. in addition to the time required at source and destination nodes, every packet must be routed at each intermediate node. an example of the format of a data packet header is shown in figure 2.9. this is the header format used in the jpl
one that presents clear semantics, is independent of a speciﬁc platform, and presents highlevel abstractions that are easy to use. the message passing interface (mpi) represents a standard message-passing api that has evolved out of the collective efforts of many people in academia, industry, and government over the past several years. it captures many of the desirable features of a message-passing api gleaned from collective experience with the development of several message-passing libraries. it is rapidly becoming the api of choice for portable parallel message-passing programming. therefore, a brief introduction to mpi is provided in the latter half of this chapter. a full introduction to the mpi standard can be found in many excellent texts and papers (e.g., [138, 325]). an understanding of the design and implementation of the messaging layer can help motivate and clarify the semantics of the mpi interface and aid in developing efﬁcient mpi-based programs.
the design of the messaging layer follows from an identiﬁcation of the functions that must be performed in the delivery of a message. an example of the ﬂow of data from the location in the source node’s memory to the destination node’s memory location is illustrated in figure 8.1. while this represents only one implementation, it is fairly generic and captures functions found in many implementations. let us suppose that a user process starts with a single word to be transmitted to a process being executed on another node. transmission of this word is initiated via a call to a message-passing procedure such as send(buf, nbytes, dest), where buf contains the nbytes of data to be transmitted to node dest. a message packet must be created (packetization) with a header containing information required to correctly route the packet to its destination. this packet may also include other information (e.g., crc codes) in addition to the data. access to the network interface may not be immediately available. therefore, the message may be buffered in system
5. if a forward release ﬂit collides with a setup acknowledgment, the acknowledgment is removed from the network and the release ﬂit is allowed to continue along the path toward the destination.
release ﬂits are injected into the network only on the occurrence of faults. since faults are a dynamic phenomenon, they may occur at any time during path setup, message transmission, or message acknowledgment. the interaction of control ﬂits produces the actions deﬁned above to ensure proper recovery. in the fault-free case each virtual circuit contains at most one control ﬂit at a time. therefore, it is necessary to add additional buffer space for one reverse and one forward release ﬂit per virtual control channel. since nonheader control ﬂits are allowed to pass blocked headers, release ﬂits will not wait and therefore cannot create chains of blocked ﬂits as shown in figure 6.1. header ﬂits using backtracking pcs algorithms do not block on faults. thus, no ﬂit in the network blocks indeﬁnitely on a fault. as a result, release ﬂits cannot induce deadlock in the presence of faults, and the existing routing algorithm is deadlock-free.
as long as a path exists between the source and the destination, if a fault interrupts the message, both the source and the destination will be notiﬁed. however, it is possible that an inconsistent state can develop between a source and a destination where the destination has received the message intact, but the source believes the message was lost and must retransmit. this situation develops when a dynamic fault occurs in the virtual circuit after the last ﬂit is delivered, but before the ﬁnal message acknowledgment reaches the source. this situation can be remedied at the operating system level by assigning identiﬁcation tags to messages. if the source is prevented from sending the next message until the previous message has been successfully received, the destination can detect and discard duplicate messages relatively easily.
the above approach requires the use of virtual channels to support recovery trafﬁc. the use of virtual channels can complicate routing decisions and channel control, leading to an increase in the ﬂow control latency through the router and across the channel. fault-tolerant implementations of compressionless routing have been proposed [179], motivated by a desire to simplify router design to enable adaptive routing and fault recovery with speeds comparable to oblivious routers. compressionless routing exploits the small amount of buffering within the network routers to be able to indirectly determine when the header of a message has reached the destination. for example, assume that each router can only buffer two ﬂits at the input and output of a router. if a message is to traverse three routers to the destination, once the 12th ﬂit has been injected into the network the source node can assert that the header has been received into the destination node queue. figure 6.41 illustrates this case, with the exception that the message is actually comprised of 10 ﬂits. when the last data ﬂit is injected into the network, the header will not have reached the destination node. therefore, the message is padded with pad ﬂits. the number of pad ﬂits that must be added is a function of the distance to the destination and the message size.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
a copy of the source message across several separate multicast paths, each path for each subset of the destination nodes. this multidestination routing scheme will be referred to as path-based routing.
in path-based routing, the header of each copy of a message consists of multiple destinations. the source node arranges these destinations as an ordered list, depending on their intended order of traversal. as soon as the message is injected into the network, it is routed based on the address in the leading header ﬂit corresponding to the ﬁrst destination. once the message header reaches the router of the ﬁrst destination node, the ﬂit containing this address is removed by the router. now the message is routed to the node whose address is contained in the next header ﬂit. this address corresponds to the second destination in the ordered list. while the ﬂits are being forwarded by the router of the ﬁrst destination node to its adjacent router, they are also copied ﬂit by ﬂit to the delivery buffer of this node. this process is carried out in each intermediate destination node of the ordered list. when the message reaches the last destination node, it is not routed any further and is completely consumed by that node.
the routing hardware requires a few changes with respect to the router model described in section 2.1 to support path-based routing. we assume that each destination address is encoded in a different ﬂit. also, we assume that a single bit is used in each ﬂit to distinguish between destination addresses and data ﬂits. some control logic is required to discard the current destination address and transmit the next destination ﬂit to the routing and arbitration unit. sending a single clock pulse to the corresponding input channel buffer so that ﬂits advance one position is enough to discard the previous destination address. then, the routing and arbitration unit is reset so that it looks for a destination address ﬂit at the header of a buffer and starts a routing operation again. also, note that if the switch is a crossbar and it is implemented as a set of multiplexors (one for each switch output), it is possible to select the same input from several outputs. therefore, the switch can be easily conﬁgured so that ﬂits are simultaneously forwarded to the next router and copied to the delivery buffer of the current node. finally, the delivery buffer should be designed in such a way that ﬂits containing destination addresses are automatically discarded and no ﬂow control is required. this is to avoid conﬂicts with ﬂow control signals from the next node when messages are simultaneously forwarded to the next router and copied to the delivery buffer.
a simple analysis shows that the probability that a message is blocked in path-based routing is lower than that for tree-based routing. suppose that the probability that a message is blocked in each channel is p. assume that at a certain level of the tree-based routing the total number of branches is k. the tree requires that all k channels be available at the same time. the probability that a message is blocked at this level would be 1 − (1 − p)k. on the other hand, the probability of blocking for path-based routing is p since each multicast path only requests one channel at one moment. an important property of pathbased routing is that, when one path is blocked, it will not block the message delivery on the other paths. for example, consider a 6-cube and suppose that p is 0.1. the second level of the broadcast routing tree has 6 branches. the probability that a message is blocked at this level is 1 − (1 − 0.1)6, which is 0.47, while the probability is only 0.1 for path-based routing.
when switches have the same number of input and output ports, mins also have the same number of input and output ports. since there is a one-to-one correspondence between inputs and outputs, the connections between stages are also called permutations. five basic permutations are deﬁned below. although these permutations were originally deﬁned for networks with 2 × 2 switches, for most deﬁnitions we assume that the network is built by using k × k switches, and that there are n = kn inputs and outputs, where n is an integer. however, some of these permutations are only deﬁned for the case where n is a power of 2. with n = kn ports, let x = xn−1xn−2 . . . x0 be an arbitrary port number, 0 ≤ x ≤ n − 1, where 0 ≤ xi ≤ k − 1, 0 ≤ i ≤ n − 1.
this can be achieved in a fully distributed manner. the nodes are generally assumed to possess some ability for self-test as well as the ability to test neighboring nodes. we can envision an approach where, in one step, each node performs a self-test and interrogates the status of its neighbors. if neighbors or links in two or more dimensions are faulty, the node transitions to a faulty state, even though it may be nonfaulty. this diagnosis step is repeated. after a ﬁnite number of steps bounded by the diameter of the network, fault regions will have been created and will be rectangular in shape. in multidimensional meshes and tori under the block fault model, fault-free nodes are adjacent to at most one faulty node (i.e., along only one dimension). note that single component faults correspond to the block fault model with block sizes of 1.
the block fault model is particularly well suited to evolving trends in packaging technology and the use of low-dimensional networks. we will continue to see subnetworks implemented in chips, multichip modules (mcms), and boards. failures within these components will produce block faults at the chip, board, and mcm level. construction of block fault regions often naturally falls along chip, mcm, and board boundaries. for example, if submeshes are implemented on a chip, failure of two or more processors on a chip may result in marking the chip as faulty, leading to a block fault region comprised of all of the processors on the chip. the advantages in doing so include simpler solutions to deadlock- and livelock-free routing.
failures may be either static or dynamic. static failures are present in the network when the system is powered on. dynamic failures appear at random during the operation of the system. both types of faults are generally considered to be permanent; that is, they remain in the system until it is repaired.alternatively, faults may be transient.as integrated circuit feature sizes continue to decrease and speeds continue to increase, problems arise with soft errors that are transient or dynamic in nature. the difﬁculty of designing for such faults is that they often cannot be reproduced. for example, soft errors that occur in ﬂight tests of avionics hardware often cannot be reproduced on the ground. in addition,
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
the ith butterﬂy permutation interchanges the zeroth and ith digits of the index. figure 1.14 shows the butterﬂy permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that β k deﬁnes a straight one-to-one permutation and is also called identity permutation, i .
cube permutation the ith cube permutation ei, for 0 ≤ i ≤ n − 1, is deﬁned only for k = 2 by ei (xn−1 . . . xi+1xi xi−1 . . . x0) = xn−1 . . . xi+1xi xi−1 . . . x0
the ith cube permutation complements the ith bit of the index. figure 1.15 shows the cube permutation for i = 0, 1, and 2 with n = 8. e0 is also called the exchange permutation.
the ith baseline permutation performs a cyclic shifting of the i + 1 least signiﬁcant digits in the index to the right for one position. figure 1.16 shows the baseline permutation for k = 2, and i = 0, 1, and 2 with n = 8. note that δk 0 also deﬁnes the identity permutation i .
the perfect k-shufﬂe permutation performs a cyclic shifting of the digits in x to the left for one position. for k = 2, this action corresponds to perfectly shufﬂing a deck of n cards, as demonstrated in figure 1.13(a) for the case of n = 8. the perfect shufﬂe cuts the deck into two halves from the center and intermixes them evenly. the inverse perfect shufﬂe permutation does the opposite, as deﬁned by
this permutation is usually referred to as bit reversal, clearly indicating that it was proposed for k = 2. however, its deﬁnition is also valid for k > 2. figure 1.13(c) demonstrates a bit reversal permutation for the case of k = 2 and n = 8.
when switches have the same number of input and output ports, mins also have the same number of input and output ports. since there is a one-to-one correspondence between inputs and outputs, the connections between stages are also called permutations. five basic permutations are deﬁned below. although these permutations were originally deﬁned for networks with 2 × 2 switches, for most deﬁnitions we assume that the network is built by using k × k switches, and that there are n = kn inputs and outputs, where n is an integer. however, some of these permutations are only deﬁned for the case where n is a power of 2. with n = kn ports, let x = xn−1xn−2 . . . x0 be an arbitrary port number, 0 ≤ x ≤ n − 1, where 0 ≤ xi ≤ k − 1, 0 ≤ i ≤ n − 1.
when switches have the same number of input and output ports, mins also have the same number of input and output ports. since there is a one-to-one correspondence between inputs and outputs, the connections between stages are also called permutations. five basic permutations are deﬁned below. although these permutations were originally deﬁned for networks with 2 × 2 switches, for most deﬁnitions we assume that the network is built by using k × k switches, and that there are n = kn inputs and outputs, where n is an integer. however, some of these permutations are only deﬁned for the case where n is a power of 2. with n = kn ports, let x = xn−1xn−2 . . . x0 be an arbitrary port number, 0 ≤ x ≤ n − 1, where 0 ≤ xi ≤ k − 1, 0 ≤ i ≤ n − 1.
the receiver uses the message envelope to specify the sources from which messages will be received. only messages whose envelopes match the receiver’s request will be accepted. receivers also control message reception via type matching. the source ﬁeld may be a wild card (e.g., the predeﬁned string mpi any source). in this instance, a message may be received from any source, and it may not be possible to know a priori the exact size of the message. therefore, the buffer size is an upper bound on the required storage. query functions are available to obtain information about received messages; for example, mpi get count() returns the number of received entries in the receive buffer. the status ﬁeld is a structure that contains additional information for the receiver.
there are a host of nonblocking calls that return immediately to permit overlap of communication and computation. these calls are supported by query functions that are used to determine if the operation has actually completed. such calls are a necessary prelude to the reuse of buffers used in the nonblocking calls. these functions include mpi wait() and mpi test(). a common structure is to initiate a nonblocking send operation and continue with computation that is overlapped with the transmission. when the program reaches a point where the send buffer must be reused, queries are used to establish the completion of the previously issued send operation.
it is clear that the choice of buffering strategy has a signiﬁcant impact on performance, particularly if there are known features of the application that can be exploited. to enable the programmer to inﬂuence the buffering strategy and thus performance, mpi offers several modes for send/receive operations. the preceding descriptions were that of standard mode send/receive calls. in this case, no assumption can be made with regard to whether the message is buffered or not. in buffered mode, buffering can be provided within the user program. therefore, the user can guarantee that some buffering is available. synchronous mode realizes the semantics of a rendezvous operation and ensures that a matching receive has executed. finally, in ready mode, the user can assert that the matching receive has been posted. these distinct modes enable the user to exploit some knowledge of the application implementation in inﬂuencing the choice of the message transfer protocol used in a communication operation.
when communication repeatedly occurs between processes (e.g., in loops), some of the overhead involved in the message-passing implementation may be shared across multiple messages by using features for persistent communication. calls to mpi functions are
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
number of virtual channels as the deterministic algorithm. when more than four virtual channels are used, the negative effects of channel multiplexing mentioned above outweigh the beneﬁts.
figure 9.18 shows the behavior of the fully adaptive algorithm with three, four, six, and eight virtual channels. note that two virtual channels are used for deadlock avoidance and the remaining channels are used for fully adaptive routing. in this case, latency is very similar regardless of the number of virtual channels. note that the selection function selects the output channel for a message in such a way that channel multiplexing is minimized. as a consequence, channel multiplexing is more uniform and channel utilization is higher, obtaining a higher throughput than the other routing algorithms. as indicated in section 9.4.1, throughput decreases when the fully adaptive algorithm reaches the saturation point. this degradation can be clearly observed in figure 9.18. also, as indicated in [91], it can be seen that increasing the number of virtual channels increases throughput and even removes performance degradation.
network hardware has become very fast. in some cases, network performance may be limited by the bandwidth available at the source and destination nodes to inject and deliver messages, respectively. in this section, we analyze the effect of that bandwidth. as fully adaptive algorithms achieve a higher throughput, this issue is more critical when adaptive routing is used. therefore, we will restrict our study to fully adaptive algorithms. injection and delivery channels are usually referred to as ports. in this study, we assume that each port has a bandwidth equal to the channel bandwidth.
software messaging layer. a high percentage of the communication latency in multicomputers is produced by the overhead in the software messaging layer. reducing or hiding this overhead is likely to have a higher impact on performance than the remaining design parameters, especially when messages are relatively short. the use of vcc can eliminate or hide most of the software overhead by overlapping path setup with computation, and caching and retaining virtual circuits for use by multiple messages. vcc complements the use of techniques to reduce the software overhead, like active messages. it should be noted that software overhead is so high in some multicomputers that it makes no sense improving other design parameters. however, once this overhead has been removed or hidden, the remaining design parameters become much more important.
software support for collective communication. collective communication operations beneﬁt considerably from using speciﬁc algorithms. using separate addressing, latency increases linearly with the number of participating nodes. however, when algorithms for collective communication are implemented in software, latency is considerably reduced, increasing logarithmically with the number of participating nodes. this improvement is achieved with no additional hardware cost and no overhead for unicast messages.
number of ports. if the software overhead has been removed or hidden, the number of ports has a considerable inﬂuence on performance, especially when messages are sent locally. if the number of ports is too small, the network interface is likely to be a bottleneck for the network. the optimal number of ports heavily depends on the spatial locality of trafﬁc patterns.
switching technique. nowadays, most commercial and experimental parallel computers implement wormhole switching. although vct switching achieves a higher throughput, the performance improvement is small when virtual channels are used in wormhole switching. additionally, vct switching requires splitting messages into packets not exceeding buffer capacity. if messages are longer than buffer size, wormhole switching should be preferred. however, when messages are shorter than or equal to buffer size, vct switching performs slightly better than wormhole switching, also simplifying deadlock avoidance. this is the case for distributed, shared-memory multiprocessors. vct switching is also preferable when it is not easy to avoid deadlock in wormhole switching (multicast routing in multistage networks) or in some applications requiring real-time communication [294]. finally, a combination of wormhole switching and circuit switching with wave-pipelined switches and channels has the potential to increase performance, especially when messages are very long [101].
packet size. for pipelined switching techniques, ﬁlling the pipeline produces some overhead. also, routing a header usually takes longer than transmitting a data ﬂit
these routing algorithms were designed for saf networks using central queues. deadlocks are avoided by splitting buffers into several classes and restricting packets to move from one buffer to another in such a way that buffer class is never decremented. gopal proposed several fully adaptive minimal routing algorithms based on buffer classes [133]. these algorithms are known as hop algorithms.
the simplest hop algorithm starts by injecting a packet into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a hop to another node, it moves to a buffer of class i + 1. this routing algorithm is known as the positive-hop algorithm. deadlocks are avoided by using a buffer of a higher class every time a packet requests a new buffer. by doing so, cyclic dependencies between resources are prevented. a packet that has completed i hops will use a buffer of class i. since the routing algorithm only supplies minimal paths, the maximum number of hops taken by a packet is limited by the diameter of the network. if the network diameter is denoted by d, a minimum of d + 1 buffers per node are required to avoid deadlock. the main advantage of the positive-hop algorithm is that it is valid for any topology. however, the number of buffers required for fully adaptive deadlock-free routing is very high, and this number depends on network size.
the minimum number of buffers per node can be reduced by allowing packets to move between buffers of the same class. in this case, classes must be deﬁned such that packets moving between buffers of the same class cannot form cycles. in the negative-hop routing algorithm, the network is partitioned into several subsets in such a way that no subset contains two adjacent nodes. if s is the number of subsets, then subsets are labeled 0, 1, . . . , s − 1, and nodes in subset i are labeled i. hops from a node with a higher label to a node with a lower label are negative. otherwise, hops are nonnegative. when a packet is injected, it is stored into the buffer of class 0 at the current node. every time a packet stored in a buffer of class i takes a negative hop, it moves to a buffer of class i + 1. if a packet stored in a buffer of class i takes a nonnegative hop, then it requests a buffer of the same class. thus, a packet that has completed i negative hops will use a buffer of class i. there is not any cyclic dependency between buffers. effectively, a cycle starting at node a must return to node a and contains at least another node b. if b has a lower label than a, some hop between a and b (possibly through intermediate nodes) is negative, and the buffer class is increased. if b has a higher label than a, some hop between b and a (possibly through intermediate nodes) is negative, and the buffer class is increased. as a consequence, packets cannot wait for buffers cyclically, thus avoiding deadlocks. if d is the network diameter and s is the number of subsets, then the maximum number of negative hops that can be taken by a packet is hn = (cid:12)d(s − 1)/s(cid:13). the minimum number of buffers per node required to avoid deadlock is hn + 1. figure 4.14 shows a partition scheme for k-ary 2-cubes with even k. black and white circles correspond to nodes of subsets 0 and 1, respectively.
although the negative-hop routing algorithm requires approximately half the buffers required by the positive-hop algorithm in the best case, this number is still high. it can be improved by partitioning the network into subsets and numbering the partitions, such
algorithm: fault-tolerant routing using safety levels input: message (message, destination), tag = current node ⊕ destination, and d is the distance to the destination procedure: 1. if tag = 0, then the destination has been reached. 2. if there is at least one preferred neighbor with a safety level ≥ d − 1, send 3. if there is at least one spare neighbhor with a safety level ≥ d + 1, update the
this example illustrates how the use of unsafe labels affects routing with the fault pattern shown in figure 6.8(a). figure 6.9 shows nodes that become labeled unsafe. the solid line illustrates the path that is now taken by a message that is routed from s1 to d1. note that the routing of messages through unsafe nodes is avoided. the ﬁgure also shows paths taken by messages from nodes s2 to d2 and from nodes s3 to d3. note that the former message is forwarded through an unsafe node. such routing decisions are preferred to misrouting. the message originating from node s3 must be misrouted since all shortest paths to the destination are blocked by faults.
the concept of a safe node captures the fault distribution within a neighborhood. a node is safe if all of the neighboring nodes and links are nonfaulty. this concept of safety can be extended to capture multiple distinct levels of safety in binary hypercubes [351]. a node has a safety level of k if any node at a distance of k can be reached by a path of length k. safety levels can be viewed as a form of nonlocal state information. the location and distribution of faults is encoded in the safety level of a node. the exact locations of the faulty nodes are not speciﬁed. however, in practice, knowledge of fault locations is for the purpose of ﬁnding (preferably) minimal-length paths. safety levels can be used to realize this goal by exploiting the following properties. if the safety level of a node is s, then for any message received by this node and destined for a node no more than distance s away, at least one neighbor on a shortest path to the destination will have a safety level of (s − 1). the neighbors on a shortest path to the destination will be referred to as preferred neighbors [351] and the remaining neighbors as spare neighbors. clearly, if the message cannot be delivered to the destination, the safety levels of all of the preferred neighbors will be < d − 1, where d is the distance to the destination, and that of the spare neighbors will be < d. once the safety levels have been computed for each node, a message may be routed using the sequence of steps shown in figure 6.11.
a faulty node has a safety level of 0. a safe node has a safety level of n (the diameter of the network). all nonfaulty nodes are initialized to a safety level of n. the levels of
the ﬁrst term in tset up is the time taken for the header ﬂit to reach the destination. the second term is the time taken for the acknowledgment ﬂit to reach the source node. we then have tdat a as the time for pipelining the data ﬂits into the destination network interface. the ﬁrst term is the time for the ﬁrst data ﬂit to reach the destination. the second term is the time required to receive the remaining ﬂits. the message pipeline cycle time is determined by the maximum of the switch delay and wire delay.
scouting switching is a hybrid message ﬂow control mechanism that can be dynamically conﬁgured to provide speciﬁc trade-offs between fault tolerance and performance. in pcs the ﬁrst data ﬂit is injected into the network only after the complete path has been set up. in an attempt to reduce pcs path setup time overhead, in scouting switching the ﬁrst data ﬂit is constrained to remain at least k links behind the routing header. when k = 0, the ﬂow control is equivalent to wormhole switching, while large values can ensure path setup prior to data transmission (if a path exists). intermediate values of k permit the data ﬂits to follow the header at a distance, while still allowing the header to backtrack if the need arises. therefore, when the header reaches the destination, the ﬁrst data ﬂit arrives shortly thereafter rather than immediately (as in wormhole switching). figure 2.24 illustrates a time-space diagram for messages being pipelined over three links using scouting switching (k = 2). the parameter, k, is referred to as the scouting distance or probe lead. every time a channel is successfully reserved by the routing header, a positive acknowledgment is returned in the opposite direction.as a particular case, positive acknowledgments are continuously transmitted when the routing header has reached the destination node. associated with each virtual channel is a programmable counter. the counter associated with the virtual channel reserved by a header is incremented when a positive acknowledgment is received and is decremented when a negative acknowledgment is received. when the value of the counter is equal to k, data ﬂits are allowed to advance. as acknowledgments ﬂow in the direction opposite to the routing header, the gap between the header and the ﬁrst data ﬂit can grow up to a maximum of 2k − 1 links while the header is advancing. if the routing header backtracks, a negative acknowledgment is transmitted. for performance reasons, when k = 0 no acknowledgments are sent across the channels. in this case, data ﬂits immediately follow the header ﬂit and ﬂow control is equivalent to wormhole switching. for example, in figure 2.25 a message is being transmitted between nodes a and g and k = 2. the initial path attempted by the header is row ﬁrst. data ﬂits remain at least
deﬁned within a context called the process group. a unique group id is associated with each distinct process group. members of a process group may not be ﬁxed. during different phases of a computation, new members may be added to the process group, and old members may be removed from the process group.
various collective communication services have been identiﬁed for processes within a process group. providing such services can simplify the programming effort and facilitate efﬁcient implementation. consider a process group, g, with n processes, p1, p2, . . . , pn. we assume that all processes will be involved in collective communication, although it is possible that some processes are disabled or masked off from participation. four basic types of collective communication services within the context of a process group are described below.
in this category, each process can send at most one message and receive at most one message. if each process has to send exactly one message and receive exactly one message, there are n! different permutation or communication patterns. figure 5.1(a) shows a circular shift communication pattern in which pi sends a message to pi+1 for 1 ≤ i ≤ n − 1 and pn delivers its message to p1. in some applications, with no need of wraparound, a shift communication pattern can be supported in which p1 only sends a message and pn only receives a message, as shown in figure 5.1(b).
in one-to-all communication, one process is identiﬁed as the sender (called the root) and all processes in the group are receivers. there are some variations. in some designs, the sender may or may not be a member of the process group. also, if the sender is a member of the process group, it may or may not be a receiver. here we assume that the sender is a
from the programmer’s perspective, collective communication services are provided in the context of a process group. in a multicomputer, each individual application is usually allocated with a subset of processors, called a processor cluster, in order to achieve the best performance (the performance may be degraded when more processors are allocated due to increased penalty from communication overhead) and to increase the system throughput. the scheduling of processors in a multicomputer should carefully consider the trade-off between space sharing and time sharing, which is beyond the scope of this book.
from the viewpoint of system or processors, a process group only involves a subset of processors, as shown in figure 5.6 with two process groups. the four processes of process group 1 are each assigned to different processors, as are the three processes in process group 2. process 2 from group 1 and process 0 from group 2 share the same processor.
obviously, as indicated in figure 5.6, the “all” communication in a process group becomes the “many” communication, which involves an arbitrary subset of processors, from the system’s point of view. in order to efﬁciently support collective communication, it is desirable that at the processor level the system can support “one-to-one” (unicast), “oneto-many,” “many-to-one,” and “many-to-many” communication primitives in hardware. all multiprocessors directly support various forms of unicast communication, such as blocking versus nonblocking, synchronous versus asynchronous, and direct remote memory access. one-to-many communication, mainly in the form of multicast, in which the
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
with input channel requests. conﬂicts for the same output must be arbitrated (in logarithmic time), and if relative addressing is used, a header must be selected. if the requested buffer(s) is (are) busy, the incoming message remains in the input buffer until a requested output becomes free. the ﬁgure shows a full crossbar that connects all input virtual channels to all output virtual channels. alternatively, the router may use a design where full connectivity is only provided between physical channels, and virtual channels arbitrate for crossbar input ports. fast arbitration policies are crucial to maintaining a low ﬂow control latency through the switch.
buffers. these are fifo buffers for storing messages in transit. in the above model, a buffer is associated with both the input physical channels and output physical channels. the buffer size is an integral number of ﬂow control units. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). in vct switching sufﬁcient buffer space is available for a complete message packet. for a ﬁxed buffer size, insertion and removal from the buffer is usually not on the router critical path.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
while the above router model is representative of routers constructed to date, router architectures are evolving with different implementations. the routing and arbitration unit may be replicated to reduce arbitration time, and channels may share ports on the crossbar. the basic functions appear largely unaltered but with distinct implementations to match the current generation of technology.
if the routers support adaptive routing, the presence of multiple choices makes the routing decision more complex. there is a need to generate information corresponding to these choices and to select among these choices. this naturally incurs some overhead in time as well as resources (e.g., chip area). similarly, the use of virtual channels, while reducing header blocking delays in the network, makes the link controllers more complex by requiring arbitration and more complex ﬂow control mechanisms.
the two main measures of intrarouter performance [57] are the routing latency or header latency and the ﬂow control latency. from figure 7.7 it is apparent that the latency experienced by the header ﬂit(s) through a router is comprised of several components. after the header has been received, it must be decoded and the connection request generated. since multiple headers may arrive simultaneously, the routing and arbitration unit arbitrates among multiple requests, and one of the headers is selected and routed. this involves computing an output and, if it is available, setting the crossbar. the updated header and subsequent data may now be driven through the switch and will experience some multiplexing delay through the link controllers as they multiplex multiple virtual channels across the physical channel. once a path has been set up, data ﬂits may now ﬂow
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
of the packets to break the deadlock. however, because heuristic detection mechanisms operate locally and in parallel, several nodes may detect deadlock concurrently and release several buffers in the same deadlocked conﬁguration. in the worst case, it may happen that all the packets involved in a deadlock release the buffers they occupy. these overheads suggest that deadlock-recovery-based routing can beneﬁt from highly selective deadlock detection mechanisms. for instance, turn selection criteria could also be enforced to limit a packet’s eligibility to use recovery resources [279].
the most important limitation of heuristic deadlock detection mechanisms arises when packets have different lengths. the optimal value of the timeout for deadlock detection heavily depends on packet length unless some type of physical channel monitoring of neighboring nodes is implemented. when a packet is blocked waiting for channels occupied by long packets, the selected value for the timeout should be high in order to minimize false deadlock detection. as a consequence, deadlocked packets have to wait for a long time until deadlock is detected. in these situations, latency becomes much less predictable. the poor behavior of current deadlock detection mechanisms considerably limits the practical applicability of deadlock recovery techniques. some current research efforts aim at improving deadlock detection techniques [226].
once deadlocks are detected and packets made eligible to recover, there are several alternative actions that can be taken to release the buffer resources occupied by deadlocked packets. deadlock recovery techniques can be classiﬁed as progressive or regressive. progressive techniques deallocate resources from other (normal) packets and reassign them to deadlocked packets for quick delivery. regressive techniques deallocate resources from deadlocked packets, usually killing them (abort-and-retry). the set of possible actions taken depends on where deadlocks are detected.
if deadlocks are detected at the source node, regressive deadlock recovery is usually used.a packet can be killed by sending a control signal that releases buffers and propagates along the path reserved by the header. this is the solution proposed in compressionless routing [179]. after a random delay, the packet is injected again into the network. this reinjection requires a packet buffer associated with each injection port. note that a packet that is not really deadlocked may resume advancement and even start delivering ﬂits at the destination after the source node presumes it is deadlocked. thus, this situation also requires a packet buffer associated with each delivery port to store fragments of packets that should be killed if a kill signal reaches the destination node. if the entire packet is consumed without receiving a kill signal, it is delivered. obviously, this use of packet buffers associated with ports restricts packet size.
if deadlocks are detected at an intermediate node containing the header, then both regressive and progressive deadlock recovery are possible. in regressive recovery, a deadlocked packet can be killed by propagating a backward control signal that releases buffers from the node containing the header back to the source by following the path reserved by the packet in the opposite direction. instead of killing a deadlocked packet,
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
continuously transmitted when channels are not in use to support error detection. on-chip circuitry and external interfaces support static conﬁguration in response to errors: ports can be selectively disabled and ﬂushed, and the route tables used for source routing can be recomputed to avoid faulty regions. an interesting feature of the chip is the presence of counters to record message counts for analysis of the overall network behavior.
the r2 router evolved as a router for a second-generation system in support of lowlatency, high-performance interprocessor communication. the current network prototype provides for interprocessor communication between hewlett-packard commercial workstations. the r2 interconnect topology is a 2-d hexagonal interconnect, and the current router is designed to support a system of 91 nodes. each router has seven ports: six to neighboring routers and one to a local network interface (referred to as the fabric interface component, or fic). the topology is wrapped; that is, routers at the edge of the network have wraparound links to routers at the opposite end of the dimension. the network topology is shown in figure 7.34. for clarity, only one wrapped link is illustrated. as in multidimensional tori, the network topology provides multiple shortest paths between each pair of routers. however, unlike tori, there are also two no-farther paths. when a message traverses a link along a no-farther path, the distance to the destination is neither increased nor decreased. the result is an increase in the number of routing options available at an intermediate router.
the physical channel is organized as a half-duplex channel with 16 bits for data and 9 bits for control. one side of each link is a router port with an arbiter that governs channel mastership. the routers and channels are fully self-timed, eliminating the need for a global clock. there are no local clocks, and resynchronization with the local node processor clocks is performed in the network interface. the rate at which the r2 ports can toggle is estimated to be approximately 5–10 ns, corresponding to an equivalent synchronous transmission rate of 100–200 mhz.
message packets may be of variable length and up to a maximum of 160 bytes. each packet has a 3-byte header that contains addressing information and a 16-byte protocol
no packet has already arrived at its destination node. packets cannot advance because the queues for all the alternative output channels supplied by the routing function are full.
there is no packet whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (remember, we make the assumption that a queue cannot contain ﬂits belonging to different packets). data ﬂits cannot advance because the next channel reserved by their packet header has a full queue. note that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
in some cases, a conﬁguration cannot be reached by routing packets starting from an empty network. this situation arises when two or more packets require the use of the same channel at the same time to reach the conﬁguration. a conﬁguration that can be reached by routing packets starting from an empty network is reachable or routable [63]. it should be noted that by deﬁning the domain of the routing function as n × n, every legal conﬁguration is also reachable. effectively, as the routing function has no memory of the path followed by each packet, we can consider that, for any legal conﬁguration, a packet stored in a channel queue was generated by the source node of that channel. in wormhole switching, we can consider that the packet was generated by the source node of the channel containing the last ﬂit of the packet. this is important because when all the legal conﬁgurations are reachable, we do not need to consider the dynamic evolution of the network leading to those conﬁgurations. we can simply consider legal conﬁgurations, regardless of the packet injection sequence required to reach them. when all the legal conﬁgurations are reachable, a routing function is deadlock-free if and only if there is not any deadlocked conﬁguration for that routing function.
a routing function r is connected if it is able to establish a path between every pair of nodes x and y using channels belonging to the sets supplied by r. it is obvious that a routing function must be connected, and most authors implicitly assume this property. however, we mention it explicitly because we will use a restricted routing function to prove deadlock freedom, and restricting a routing function may disconnect it.
the theoretical model of deadlock avoidance we are going to present relies on the concept of channel dependency [77]. other approaches are possible. they will be brieﬂy described in section 3.3. when a packet is holding a channel, and then it requests the use of another channel, there is a dependency between those channels. both channels are in one of the
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
and the messages sent within a particular step do not contend for common channels. in practice, however, the message-passing steps of the multicast operation may not be ideally synchronized, and contention may arise among messages sent in different steps. as indicated in section 5.2, start-up latency includes system call time at both the source and destination nodes; these latencies are termed the sending latency and receiving latency, respectively. if these latencies are large relative to the network latency, messages can be sent concurrently, but in different steps. for example, assuming that both sending latency and receiving latency have the same value, denoted by t, and that network latency is negligible, the labels in figure 5.51(d) indicate when a copy of the message will enter and leave each node. assuming a one-port architecture, the latency between two consecutive sends at a particular node is t. leaf nodes in the tree do not send messages, and therefore encounter only receiving latency. for other destinations (intermediate nodes in the tree), both receiving latency and sending latency are incurred. under these conditions, node (1, 0) may not have ﬁnished receiving the message from node (2,1) until after node (2, 1) has ﬁnished sending to node (3, 2) and started sending to node (1, 3). if node (1,0) sends to node (1,2) at this time, 3t, then contention will occur for the [(1, 1), (1, 2)] channel. the multicast tree in figure 5.51(e), which is based on the methods presented in the following sections, is contention-free regardless of message length or receiving latency.
developing an algorithm that produces minimum-time, contention-free multicast implementations for a speciﬁc system requires a detailed understanding of potential conﬂicts among messages, which in turn are dependent on the routing algorithm used. this section formulates a method to avoid contention among unicast messages under the most common routing algorithm for wormhole-switched n-dimensional meshes, namely, dimensionorder routing. this method was presented in [234].
a few preliminaries are in order. a node address x in a ﬁnite n-dimensional mesh is represented by σn−1(x)σn−2(x) . . . σ0(x). under a minimal deterministic routing algorithm, all messages transmitted from a node x to a node y will follow a unique shortest path between the two nodes. let such a path be represented as p (x, y) = (x; z1, z2, . . . , zk; y), where the zis are the sequence of intermediate routers visited by the message. in order to simplify the presentation, we let z0 = x and zk+1 = y.
in order to characterize contention among messages transmitted under dimensionorder routing, an ordering on nodes in an n-dimensional mesh is needed. the multicast algorithms described herein are based on lexicographic ordering of the source and destination nodes according to their address components. actually, two such orderings are possible: one in which the subscripts of address components increase from right to left, and another in which the subscripts are reversed. which ordering is appropriate for multicasting in a given system depends on whether addresses are resolved, under dimension-order routing, in decreasing or increasing dimension order. here it is assumed
a steiner tree, s(v , e), for a multicast set k is a subtree of g such that k ⊆ v (s). a minimal steiner tree (mst) is a steiner tree with a minimal total length.
in the steiner tree problem, it is not necessary to use a shortest path from the source to a destination. if the distance between two nodes is not a major factor in the communication time, such as in vct, wormhole, and circuit switching, the above optimization problem is appropriate. however, if the distance is a major factor in the communication time, such as in saf switching, then we may like to minimize time ﬁrst, then trafﬁc. the multicast communication problem is then modeled as an optimal multicast tree (omt). the omt problem was originally deﬁned in [195].
an omt, t (v , e), for k is a subtree of g such that (a) k ⊆ v (t ), (b) dt (u0, ui ) = dg(u0, ui ), for 1 ≤ i ≤ k, and (c) |e(t )| is as small as possible.
apparently, the complexity of each of the above optimization problems is directly dependent on the underlying host graph. the above graph optimization problems for the popular hypercube and 2-d mesh topologies were studied in [210, 212], showing that the omc and omp problems are np-complete for those topologies. also, it was shown that the mst and omt problems are np-complete for the hypercube topology [60, 135]. the mst problem for the 2-d mesh topology is equivalent to the rectilinear steiner tree problem, which is np-complete [120].
the np-completeness results indicate the necessity to develop heuristic multicast communication algorithms for popular interconnection topologies. multicast communication may be supported in hardware, software, or both. sections 5.5 and 5.7 will address hardware and software implementations, respectively, of multicast.
hardware support of multicast communication in multicomputers requires increased functionality within the routers. this functionality may include interpretation of multicast addresses (or group id) and forwarding of messages onto multiple outgoing channels (replication). the result is the capability for one local processor to efﬁciently send the
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
a routing algorithm is said to be f fault recoverable if, for any f failed components in the network, a message that is undeliverable will not hold network resources indeﬁnitely. if a network is fault recoverable, the faults will not induce deadlock.
ideally, we would like the network to be f fault tolerant for large values of f . practically, however, we may be satisﬁed with a system that is f fault recoverable for large values of f and f1 fault tolerant for f1 << f , so that only the functionality of a small part of the network suffers as a result of a failed component. certainly, we want to avoid the situation where a few faults may cause a catastrophic failure of the entire network, or equivalently deadlocked conﬁgurations of messages. finally, the next deﬁnition gives the redundancy level of the network.
a routing algorithm has a redundancy level equal to r iff, after removing any set of r channels, the routing function remains connected and deadlock-free, and there exists a set of r + 1 channels such that, after removing them, the routing function is no longer connected or it is not deadlock-free.
note that an algorithm has a redundancy level equal to r iff it is r fault tolerant and it is not r + 1 fault tolerant. the analysis of the redundancy level of the network is complex. fortunately, there are some theoretical results that guarantee the absence of deadlock in the whole network by analyzing the behavior of the routing function r in a subset of channels c1 (see section 3.1.3). additionally, that behavior does not change when some channels not belonging to c1 are removed. that theory can be used to guarantee the absence of deadlock when some channels fail.
suppose that there exists a routing subfunction r1 that satisﬁes the conditions of theorem 3.1. let c1 be the subset of channels supplied by r1. now, let us consider the effects of removing some channels from the network. it is easy to see that removing channels not belonging to c1 does not add indirect dependencies between the channels belonging to c1. it may actually remove indirect dependencies. in fact, when all the channels not belonging to c1 are removed, there are no indirect dependencies between the channels belonging to c1. therefore, removing channels not belonging to c1 does not introduce cycles in the extended channel dependency graph of r1. however, removing channels may disconnect the routing function. fortunately, if a routing function is connected when it is restricted to use the channels in c1, it will remain connected when some channels not belonging to c1 are removed. thus, if a routing function r satisﬁes the conditions proposed by theorem 3.1, it will still satisfy them after removing some or all of the channels not belonging to c1. in other words, it will remain connected and deadlock-free. so, we can conclude that all the channels not belonging to c1 are redundant. we can also reason in the opposite way. in general, there will exist several routing subfunctions satisfying the conditions proposed by theorem 3.1. we will restrict our attention to the minimally connected routing subfunctions satisfying those conditions because they require a minimum number of channels to guarantee deadlock freedom. let r1, r2, . . . , rk be all the minimally connected routing subfunctions satisfying the
as we might expect, solutions to these problems are dependent upon the type and pattern of the faulty components and the network topology. the next section shows how direct networks possess natural redundancy that can be exploited, by deﬁning the redundancy level of the routing function. the remaining sections will demonstrate how the issues mentioned above are addressed in the context of distinct switching techniques.
this section presents a theoretical basis to answer a fundamental question regarding faulttolerant routing in direct networks: what is the maximum number of simultaneous faulty channels tolerated by the routing algorithm? this question has been analyzed in [95] by deﬁning the redundancy level of the routing function and proposing a necessary and sufﬁcient condition to compute its value.
if a routing function tolerates f faulty channels, it must remain connected and deadlock-free for any number of faulty channels less than or equal to f . note that, in addition to connectivity, deadlock freedom is also required. otherwise, the network could reach a deadlocked conﬁguration when some channels fail.
the following deﬁnitions are required to support the discussion of the behavior of fault-tolerant routing algorithms. in conjunction with the fault model, the following terminology can be used to discuss and compare fault-tolerant routing algorithms.
a network is said to be connected with respect to a routing algorithm if the routing function can route a message between any pair of nonfaulty routing nodes.
this deﬁnition is useful since a network may be connected in the graph-theoretical sense, where a physical path exists between a pair of nodes. however, routing restrictions may result in a routing function that precludes selection of links along that path.
this deﬁnition indicates the conditions to support the failure of a given channel. fault-tolerant routing algorithms should be designed in such a way that all the channels are redundant, thus avoiding a single point of failure. however, even in this case, we cannot guarantee that the network will support two simultaneous faults. this issue is addressed by the following deﬁnitions.
a routing algorithm is said to be f fault tolerant if, for any f failed components in the network, the routing function is still connected and deadlock-free.
figure 5.9 multiaddress encoding schemes: (a) all-destination encoding, (b) bit string encoding, (c) multiple-region broadcast encoding, (d) multiple-region stride encoding, and (e) multiple-region bit string encoding.
the remaining encoding schemes try to optimize the header length by considering ranges of addresses or regions. in figure 5.9(c), each region is speciﬁed by two ﬁelds: the beginning and ending addresses of the region. within each region, the message is broadcast to all the addresses in the range. in some applications, a node may send a message to a set of destination addresses that have a constant distance between two adjacent addresses. a suitable encoding scheme for those applications consists of adding a stride to the deﬁnition of each region, as shown in figure 5.9(d). finally, if the destination addresses are irregularly distributed but can be grouped into regions, each region can be speciﬁed by a bit string, in addition to the beginning and ending addresses (see figure 5.9(e)). the main drawback of encoding schemes based on regions is that the routing hardware required to decode addresses is complex. also, several header ﬂits may be required to encode each region. those ﬂits should reach the router before starting a routing operation.
one approach to multicast routing is to deliver the message along a common path as far as possible, then replicate the message and forward each copy on a different channel bound for a unique set of destination nodes. the path followed by each copy may further branch in this manner until the message is delivered to every destination node. in such tree-based routing, the destination set is partitioned at the source, and separate copies are sent on one
of the packets to break the deadlock. however, because heuristic detection mechanisms operate locally and in parallel, several nodes may detect deadlock concurrently and release several buffers in the same deadlocked conﬁguration. in the worst case, it may happen that all the packets involved in a deadlock release the buffers they occupy. these overheads suggest that deadlock-recovery-based routing can beneﬁt from highly selective deadlock detection mechanisms. for instance, turn selection criteria could also be enforced to limit a packet’s eligibility to use recovery resources [279].
the most important limitation of heuristic deadlock detection mechanisms arises when packets have different lengths. the optimal value of the timeout for deadlock detection heavily depends on packet length unless some type of physical channel monitoring of neighboring nodes is implemented. when a packet is blocked waiting for channels occupied by long packets, the selected value for the timeout should be high in order to minimize false deadlock detection. as a consequence, deadlocked packets have to wait for a long time until deadlock is detected. in these situations, latency becomes much less predictable. the poor behavior of current deadlock detection mechanisms considerably limits the practical applicability of deadlock recovery techniques. some current research efforts aim at improving deadlock detection techniques [226].
once deadlocks are detected and packets made eligible to recover, there are several alternative actions that can be taken to release the buffer resources occupied by deadlocked packets. deadlock recovery techniques can be classiﬁed as progressive or regressive. progressive techniques deallocate resources from other (normal) packets and reassign them to deadlocked packets for quick delivery. regressive techniques deallocate resources from deadlocked packets, usually killing them (abort-and-retry). the set of possible actions taken depends on where deadlocks are detected.
if deadlocks are detected at the source node, regressive deadlock recovery is usually used.a packet can be killed by sending a control signal that releases buffers and propagates along the path reserved by the header. this is the solution proposed in compressionless routing [179]. after a random delay, the packet is injected again into the network. this reinjection requires a packet buffer associated with each injection port. note that a packet that is not really deadlocked may resume advancement and even start delivering ﬂits at the destination after the source node presumes it is deadlocked. thus, this situation also requires a packet buffer associated with each delivery port to store fragments of packets that should be killed if a kill signal reaches the destination node. if the entire packet is consumed without receiving a kill signal, it is delivered. obviously, this use of packet buffers associated with ports restricts packet size.
if deadlocks are detected at an intermediate node containing the header, then both regressive and progressive deadlock recovery are possible. in regressive recovery, a deadlocked packet can be killed by propagating a backward control signal that releases buffers from the node containing the header back to the source by following the path reserved by the packet in the opposite direction. instead of killing a deadlocked packet,
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
r3, a copied signal is transmitted to r1 so that r1 may invalidate its copy of the ﬂit. in this way exactly two copies of each data ﬂit are maintained in the network at all times. a more detailed description of the architecture of the router implementation can be found in chapter 7.
an alternative to ﬂit-level recovery is to ﬁnd and discard the interrupted message components and retransmit the message from the source. recovery is at the level of complete messages rather than at the ﬂit level. the pcs-based solutions exploit the fact that a separate control network comprised of the control channels exists. consider a message pipeline where a fault is detected on a reserved virtual link or physical channel as shown in figure 6.39. the link is marked faulty. the link controller at the source end of the faulty virtual link introduces a release ﬂit (referred to as kill ﬂit in [123]) into the complementary virtual control channel of the virtual link upstream from the fault. this release ﬂit is routed back to the source router. the link controller at the destination end of the faulty virtual link introduces a release ﬂit into the corresponding virtual control channel of the virtual link downstream from the failed virtual link. this release ﬂit is propagated along toward the destination. when a release ﬂit arrives at a node, the input and output virtual links associated with the message are released, and the ﬂit is propagated toward the source (or destination). if multiple faults occur in one message pipeline, this mechanism is applied recursively to fault-free segments.
when a release ﬂit arrives at an intermediate router node, the set of actions taken by that node will depend on the state of the interrupted message pipeline in that node. we have the following rules that are to be implemented by each router node. in all of the following steps it is implicit that virtual links and associated buffers are released. if two control ﬂits for the same message arrive at a router node (from opposite directions along the path), the two control ﬂits collide at that router node. a ﬂit in progress toward the destination (source) is referred to as a forward (reverse) ﬂit. the following rules govern the collision of control ﬂits and are based on the assumption [123] that paths are removed by message acknowledgments from the destination rather than by the last ﬂit of the message. thus, a path is not removed until the last ﬂit has been delivered to the destination. this behavior guarantees message delivery in the presence of dynamic or transient faults.
1. if a forward release ﬂit collides with a reverse release ﬂit, remove both release ﬂits from the network. this may occur with multiple faults within the same message pipeline.
3. if a release ﬂit reaches the source, inform the message handler that the message transmission has failed. the handler may choose to retransmit or to invoke some higher-level fault-handling algorithm.
the aggressive tp performs considerably better. this is due to the fact that with k > 0, substantial acknowledgment ﬂit trafﬁc can be introduced into the network, dominating the effect of an increased number of detours.
when dynamic faults occur, messages may become interrupted. in section 6.7 a special type of control ﬂit, called a release ﬂit, was introduced to permit distributed recovery in the presence of dynamic faults.
these control ﬂits release any reserved buffers, notify the source that the message was not delivered, and notify the destination to ignore the message currently being received. if we are also interested in guaranteeing message delivery in the presence of dynamic faults, the complete path must be held until the last ﬂit is delivered to the destination. a message acknowledgment sent from the destination removes the path and ﬂushes the copy of the message at the source. here we are only interested in the impact on the performance of tp. figure 9.64 illustrates the overhead of this recovery and reliable message delivery mechanism.
the additional message acknowledgment introduces additional control ﬂit trafﬁc into the system. message acknowledgments tend to have a throttling effect on the injection of new messages. as a result, tp routing using the mechanism saturates at lower network
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
injected messages to use some predetermined virtual channel(s) or waiting until the number of free output virtual channels at a node is higher than a threshold are enough. injection limitation mechanisms are especially recommended when using routing algorithms that allow cyclic dependencies between channels, and to limit the frequency of deadlock when using deadlock recovery mechanisms.
buffer size. for wormhole switching and short messages, increasing buffer size above a certain threshold does not improve performance signiﬁcantly. for long messages (or packets), increasing buffer size increases performance because blocked messages occupy fewer channels. however, when messages are very long, increasing buffer size only helps if buffers are deep enough to allow blocked messages to leave the source node and release some channels. it should be noted that communication locality may prevent most messages from leaving the source node before reaching their destination even when using deep buffers. therefore, in most cases small buffers are enough to achieve good performance. however, this analysis assumes that ﬂits are individually acknowledged. indeed, buffer size in wormhole switching is mainly determined by the requirements of the ﬂow control mechanism. optimizations like block acknowledgments require a certain buffer capacity to perform efﬁciently. moreover, when channels are pipelined, buffers must be deep enough to store all the ﬂits in transit across the physical link plus the ﬂits injected into the link during the propagation of the ﬂow control signals. some additional capacity is required for the ﬂow control mechanism to operate without introducing bubbles in the message pipeline. thus, when channels are pipelined, buffer size mainly depends on the degree of channel pipelining. for vct switching, throughput increases considerably when moving from one to two packet buffers. adding more buffers yields diminishing returns.
reliability/performance trade-offs. an interconnection network should be reliable. depending on the intended applications and the relative value of mtbf and mttr, different trade-offs are possible. when mttr << mtbf, the probability of the second or the third fault occurring before the ﬁrst fault is repaired is very low. in such environments, software-based rerouting is a cost-effective and viable alternative. this technique supports many fault patterns and requires minimum hardware support. however, performance degrades signiﬁcantly when faults occur, increasing latency for messages that meet faults by a factor of 2–4. when faults are more frequent or performance degradation is not acceptable, a more complex hardware support is required. the fault tolerance properties of the routing algorithm are constrained by the underlying switching technique, as indicated in the next item.
switching technique. if performance is more important than reliability, fault tolerance should be achieved without modifying the switching technique. in this case, additional resources (usually virtual channels) are required to implement
the reliable router chip is targeted for fault-tolerant operation in 2-d mesh topologies [75]. the block diagram of the reliable router is shown in figure 7.23. there are six input channels corresponding to the four physical directions in the 2-d mesh, and two additional physical ports: the local processor interface and a separate port for diagnostics. the input and output channels are connected through a full crossbar switch, although some input/output connections may be prohibited by the routing function.
while message packets can be of arbitrary length, the ﬂit length is 64 bits. there are four ﬂit types: head, data, tail, and token. the format of the head ﬂit is shown in figure 7.24. the size of the physical channel or phit size is 23 bits. the channel structure is illustrated in figure 7.23. to permit the use of chip carriers with fewer than 300 pins, these physical channels utilize half-duplex channels with simultaneous bidirectional signaling. flits are transferred in one direction across the physical channel as four 23-bit phits called frames, producing 92-bit transfers. the format of a data ﬂit and its constituent frames are shown in figure 7.25. the 28 bits in excess of the ﬂit size are used for byte parity bits (bp), kind of ﬂit (kind), virtual channel identiﬁcation (vci), ﬂow control to implement the unique token protocol (copied kind, copied vci, freed), to communicate link status information (u/d, pe), and two user bits (usr1, usr0). for a given direction of transfer, the clock is transmitted along with the four data frames as illustrated in the ﬁgure. data are driven on both edges of the clock. to enable the receiver to distinguish between the four frames and reassemble them into a ﬂit, the transmitting side of the channel also sends a pulse on the txphase signal, which has the relative timing as shown. the ﬂit is then assembled and presented to the routing logic. this reassembly process takes two cycles. each router runs off a locally generated 100 mhz clock, removing the problems with distributing a single global clock. reassembled ﬂits pass through a synchronization module for transferring ﬂits from the transmit clock domain to the receive clock domain with a worst-case penalty of one cycle (see [75] for a detailed description of the data synchronization protocol). the aggregate physical bandwidth in one direction across the channel is 3.2 gbits/s.
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
network interface have the same capacity as the individual links, then a radix of 8 in each dimension can be expected to produce a balanced demand on the links and ports.
the router architecture is based on ﬁxed-path, dimension-order wormhole switching. the organization of the relevant components is shown in figure 7.18. there are four unidirectional virtual channels in each direction over a physical link. these four virtual channels are partitioned into two virtual networks with two virtual channels/link. one network is referred to as the request network and transmits request packets. the second network is the reply network carrying only reply packets. the two virtual channels within each network prevent deadlock due to the wraparound channels as described below. adjacent routers communicate over a bidirectional, full-duplex physical channel, with each direction comprised of 16 data bits and 8 control bits as shown in figure 7.18. the 4 forward control channel bits are used to identify the type of message packet (request or response) and the destination virtual channel buffer. the four acknowledge control signals are used for ﬂow control and signify empty buffers on the receiving router. due to the time required for channel ﬂow control (e.g., generation of acknowledgments), as long as messages are integral multiples of 16 bits (see message formats below), full link utilization can be achieved. otherwise some idle physical channel cycles are possible. the router is physically partitioned into three switches as shown in the ﬁgure. messages are routed to ﬁrst correct the offset in the x, then the y , and ﬁnally the z dimension. the ﬁrst switch is connected to the injection channel from the network interface and is used to forward the message in the x+ or x− direction. when the message has completed traversal in the x dimension, the message moves to the next switch in the intermediate router for forwarding along the y dimension, and then to the z dimension. message transition between switches is via interdimensional virtual channels. for the sake of clarity the ﬁgure omits some of the virtual channels.
network interface have the same capacity as the individual links, then a radix of 8 in each dimension can be expected to produce a balanced demand on the links and ports.
the router architecture is based on ﬁxed-path, dimension-order wormhole switching. the organization of the relevant components is shown in figure 7.18. there are four unidirectional virtual channels in each direction over a physical link. these four virtual channels are partitioned into two virtual networks with two virtual channels/link. one network is referred to as the request network and transmits request packets. the second network is the reply network carrying only reply packets. the two virtual channels within each network prevent deadlock due to the wraparound channels as described below. adjacent routers communicate over a bidirectional, full-duplex physical channel, with each direction comprised of 16 data bits and 8 control bits as shown in figure 7.18. the 4 forward control channel bits are used to identify the type of message packet (request or response) and the destination virtual channel buffer. the four acknowledge control signals are used for ﬂow control and signify empty buffers on the receiving router. due to the time required for channel ﬂow control (e.g., generation of acknowledgments), as long as messages are integral multiples of 16 bits (see message formats below), full link utilization can be achieved. otherwise some idle physical channel cycles are possible. the router is physically partitioned into three switches as shown in the ﬁgure. messages are routed to ﬁrst correct the offset in the x, then the y , and ﬁnally the z dimension. the ﬁrst switch is connected to the injection channel from the network interface and is used to forward the message in the x+ or x− direction. when the message has completed traversal in the x dimension, the message moves to the next switch in the intermediate router for forwarding along the y dimension, and then to the z dimension. message transition between switches is via interdimensional virtual channels. for the sake of clarity the ﬁgure omits some of the virtual channels.
and another multidestination message arrives on another virtual lane in the same set, it must wait. this waiting cannot produce deadlock because both messages follow the same direction. also, it should be noted that adaptive routing algorithms that allow cyclic dependencies between channels only require the escape channels to avoid deadlock. since the brcp model restricts routing for multidestination messages according to the paths deﬁned by escape channels, only the escape channels need to be considered when computing the number of delivery channels required for those algorithms.
the high number of delivery channels required to implement deadlock-free multidestination communication under the brcp model may restrict the applicability of this model. fortunately, current trends in network topologies recommend the use of low-dimensional meshes or tori (two or three dimensions at most). also, as shown in sections 4.2 and 4.4.4, it is possible to design deterministic and fully adaptive routing algorithms with a very small number of virtual channels. one and two virtual channels per physical channel are enough for deterministic routing in n-dimensional meshes and k-ary n-cubes, respectively. for fully adaptive routing, the requirements for escape channels are identical to those for deterministic routing. as indicated in [268], four delivery channels are enough to support deadlock-free multidestination communication under the brcp model in 2-d meshes when the base routing is either xy routing, nonminimal west-ﬁrst, or fully adaptive routing based on escape channels (described in section 4.4.4). moreover, there is very little blocking probability with all delivery channels being accessed simultaneously. hence, delivery channels can be implemented as virtual channels by multiplexing the available bandwidth at the network interface.
in this section we describe algorithms and architectural support to perform barrier synchronization, reduction, and global combining. these algorithms have been proposed by panda in [265, 266] and are based on the brcp model described in section 5.5.3.
barrier synchronization can be performed in two phases by using multidestination messages. the ﬁrst phase implements reporting by using gather messages. the second phase implements wake-up by using broadcasting messages.
consider a linear array of six processors as shown in figure 5.42. assume that four processors (p 0, p 1, p 2, and p 4) participate in a barrier. the rightmost processor p 4, after reaching its barrier point, can send a multidestination gather message. the header of this message consists of an ordered destination list (p 2, p 1, and p 0) with p 0 as the ﬁnal destination. as this message propagates toward p 0, it can gather information from processors p 2 and p 1 regarding whether they have reached the barrier or not. if this
network interface have the same capacity as the individual links, then a radix of 8 in each dimension can be expected to produce a balanced demand on the links and ports.
the router architecture is based on ﬁxed-path, dimension-order wormhole switching. the organization of the relevant components is shown in figure 7.18. there are four unidirectional virtual channels in each direction over a physical link. these four virtual channels are partitioned into two virtual networks with two virtual channels/link. one network is referred to as the request network and transmits request packets. the second network is the reply network carrying only reply packets. the two virtual channels within each network prevent deadlock due to the wraparound channels as described below. adjacent routers communicate over a bidirectional, full-duplex physical channel, with each direction comprised of 16 data bits and 8 control bits as shown in figure 7.18. the 4 forward control channel bits are used to identify the type of message packet (request or response) and the destination virtual channel buffer. the four acknowledge control signals are used for ﬂow control and signify empty buffers on the receiving router. due to the time required for channel ﬂow control (e.g., generation of acknowledgments), as long as messages are integral multiples of 16 bits (see message formats below), full link utilization can be achieved. otherwise some idle physical channel cycles are possible. the router is physically partitioned into three switches as shown in the ﬁgure. messages are routed to ﬁrst correct the offset in the x, then the y , and ﬁnally the z dimension. the ﬁrst switch is connected to the injection channel from the network interface and is used to forward the message in the x+ or x− direction. when the message has completed traversal in the x dimension, the message moves to the next switch in the intermediate router for forwarding along the y dimension, and then to the z dimension. message transition between switches is via interdimensional virtual channels. for the sake of clarity the ﬁgure omits some of the virtual channels.
network interface have the same capacity as the individual links, then a radix of 8 in each dimension can be expected to produce a balanced demand on the links and ports.
the router architecture is based on ﬁxed-path, dimension-order wormhole switching. the organization of the relevant components is shown in figure 7.18. there are four unidirectional virtual channels in each direction over a physical link. these four virtual channels are partitioned into two virtual networks with two virtual channels/link. one network is referred to as the request network and transmits request packets. the second network is the reply network carrying only reply packets. the two virtual channels within each network prevent deadlock due to the wraparound channels as described below. adjacent routers communicate over a bidirectional, full-duplex physical channel, with each direction comprised of 16 data bits and 8 control bits as shown in figure 7.18. the 4 forward control channel bits are used to identify the type of message packet (request or response) and the destination virtual channel buffer. the four acknowledge control signals are used for ﬂow control and signify empty buffers on the receiving router. due to the time required for channel ﬂow control (e.g., generation of acknowledgments), as long as messages are integral multiples of 16 bits (see message formats below), full link utilization can be achieved. otherwise some idle physical channel cycles are possible. the router is physically partitioned into three switches as shown in the ﬁgure. messages are routed to ﬁrst correct the offset in the x, then the y , and ﬁnally the z dimension. the ﬁrst switch is connected to the injection channel from the network interface and is used to forward the message in the x+ or x− direction. when the message has completed traversal in the x dimension, the message moves to the next switch in the intermediate router for forwarding along the y dimension, and then to the z dimension. message transition between switches is via interdimensional virtual channels. for the sake of clarity the ﬁgure omits some of the virtual channels.
deadlocks might be for certain network conﬁgurations and to understand the parameters that most inﬂuence deadlock formation. for example, wormhole switching is more prone to deadlock than are other switching techniques [347]. this is because each packet may hold several channel resources spanning multiple nodes in the network while being blocked. we therefore concentrate on wormhole-switched recovery schemes.
recent work by pinkston and warnakulasuriya on characterizing deadlocks in interconnection networks has shown that a number of interrelated factors inﬂuence the probability of deadlock formation [283, 347].among the more inﬂuential of these factors is the routing freedom, the number of blocked packets, and the number of resource dependency cycles. routing freedom corresponds to the number of routing options available to a packet being routed at a given node within the network and can be increased by adding physical channels, adding virtual channels, and/or increasing the adaptivity of the routing algorithm. it has been quantitatively shown that as the number of network resources (physical and virtual channels) and the routing options allowed on them by the routing function increase (routing freedom), the number of packets that tend to block signiﬁcantly decreases, the number of resource dependency cycles decreases, and the resulting probability of deadlock also decreases exponentially. this is due to the fact that deadlocks require highly correlated patterns of cycles, the complexity of which increases with routing freedom. a conclusion of pinkston and warnakulasuriya’s work is that deadlocks in interconnection networks can be highly improbable when sufﬁcient routing freedom is provided by the network and fully exploited by the routing function. in fact, it has been shown that as few as two virtual channels per physical channel are sufﬁcient to virtually eliminate all deadlocks up to and beyond saturation in 2-d toroidal networks when using unrestricted fully adaptive routing [283, 347]. this veriﬁes previous empirical results that estimated that deadlocks are infrequent [9, 179]. however, the frequency of deadlock increases considerably when no virtual channels are used.
the network state reﬂecting resource allocations and requests existing at a particular point in time can be depicted by the channel wait-for graph (cwg). the nodes of this graph are the virtual channels reserved and/or requested by some packet(s). the solid arcs point to the next virtual channel reserved by the corresponding packet. the dashed arcs in the cwg point to the alternative virtual channels a blocked packet may acquire in order to continue routing. the highly correlated resource dependency pattern required to form a deadlocked conﬁguration can be analyzed with the help of the cwg. a knot is a set of nodes in the graph such that from each node in the knot it is possible to reach every other node in the knot. the existence of a knot is a necessary and sufﬁcient condition for deadlock [283]. note that checking the existence of knots requires global information and cannot be efﬁciently done at run time.
deadlocks can be characterized by deadlock set, resource set, and knot cycle density attributes. the deadlock set is the set of packets that own the virtual channels involved in the knot. the resource set is the set of all virtual channels owned by members of
some node architectures (e.g., intel paragon) make use of a coprocessor to execute all message-passing functions. the interaction of the coprocessor with the compute processor may be interrupt driven or polled. this permits signiﬁcant overlap between the message processing and computation; however, it may do little for latency. since the message processor controls the network interface, similar considerations about protected communications arise when seeking to run message handlers in user context rather than the kernel on the coprocessor.
buffering policies are extremely important to the design of the message layer. they are crucial to both correctness as well as performance. network routing protocols remain deadlock-free under the consumption assumption: all messages destined for a node are eventually consumed. deadlock freedom proofs that rely on this assumption (as do virtually all such proofs) are based on memory of inﬁnite extent. in reality, memory is necessarily limited, and therefore some ﬂow control between senders and receivers is necessary to ensure that a ﬁxed amount of storage can be allocated, deallocated, and reused over large numbers of messages while avoiding the loss of messages. credit-based ﬂow control, windowing schemes, and so on are examples of techniques for managing a limited amount of memory.
to ensure the availability of buffer space, the intel ipsc/2 machines employed a three-trip protocol for sending long messages (> 100 bytes). an initial request message is transmitted to the receiver to allocate buffer space. on receiving a reply message, the message can be transmitted. various token protocols can be employed where each node has a number of tokens corresponding to buffers. to send a message a processor must have a token from the destination. tokens can be returned by piggybacking on other messages. the fast message library [264] uses a return-to-sender optimistic ﬂow control protocol for buffer management. packets are optimistically transmitted after allocating buffer space at the source for the packet. if the packet cannot be received due to the lack of buffer space, it is returned to the sender where it can be retransmitted, and buffer space is guaranteed to be available to receive the rejected packet. if the packet is successfully delivered, acknowledgments are used to free buffer space at the source. this scheme has the advantage of requiring buffer space proportional to the number of outstanding message packets rather than having to preallocate space proportional to the number of nodes.
the overhead of buffer management can be measured as the time spent in the message handlers to acquire and release buffers, performing status updates such as marking buffers as allocated and free, and updating data structures that track available buffers. generally memory is statically allocated to keep these management costs down.
in addition to the above functions, interprocessor communication is often expected to preserve other properties that may require additional functionality within the message layer. a detailed study of the source of software overheads in the message layer by karamcheti and chien [171] identiﬁed several functions implemented in the messaging layer to provide services that are not provided by the network, but are expected by the user-
r3, a copied signal is transmitted to r1 so that r1 may invalidate its copy of the ﬂit. in this way exactly two copies of each data ﬂit are maintained in the network at all times. a more detailed description of the architecture of the router implementation can be found in chapter 7.
an alternative to ﬂit-level recovery is to ﬁnd and discard the interrupted message components and retransmit the message from the source. recovery is at the level of complete messages rather than at the ﬂit level. the pcs-based solutions exploit the fact that a separate control network comprised of the control channels exists. consider a message pipeline where a fault is detected on a reserved virtual link or physical channel as shown in figure 6.39. the link is marked faulty. the link controller at the source end of the faulty virtual link introduces a release ﬂit (referred to as kill ﬂit in [123]) into the complementary virtual control channel of the virtual link upstream from the fault. this release ﬂit is routed back to the source router. the link controller at the destination end of the faulty virtual link introduces a release ﬂit into the corresponding virtual control channel of the virtual link downstream from the failed virtual link. this release ﬂit is propagated along toward the destination. when a release ﬂit arrives at a node, the input and output virtual links associated with the message are released, and the ﬂit is propagated toward the source (or destination). if multiple faults occur in one message pipeline, this mechanism is applied recursively to fault-free segments.
when a release ﬂit arrives at an intermediate router node, the set of actions taken by that node will depend on the state of the interrupted message pipeline in that node. we have the following rules that are to be implemented by each router node. in all of the following steps it is implicit that virtual links and associated buffers are released. if two control ﬂits for the same message arrive at a router node (from opposite directions along the path), the two control ﬂits collide at that router node. a ﬂit in progress toward the destination (source) is referred to as a forward (reverse) ﬂit. the following rules govern the collision of control ﬂits and are based on the assumption [123] that paths are removed by message acknowledgments from the destination rather than by the last ﬂit of the message. thus, a path is not removed until the last ﬂit has been delivered to the destination. this behavior guarantees message delivery in the presence of dynamic or transient faults.
1. if a forward release ﬂit collides with a reverse release ﬂit, remove both release ﬂits from the network. this may occur with multiple faults within the same message pipeline.
3. if a release ﬂit reaches the source, inform the message handler that the message transmission has failed. the handler may choose to retransmit or to invoke some higher-level fault-handling algorithm.
hence, it is not possible to cross every dimension from every node. crossing a given dimension from a given node may require moving in another dimension ﬁrst.
the most interesting property of strictly orthogonal topologies is that routing is very simple. thus, the routing algorithm can be efﬁciently implemented in hardware. effectively, in a strictly orthogonal topology, nodes can be numbered by using their coordinates in the n-dimensional space. since each link traverses a single dimension and every node has at least one link crossing each dimension, the distance between two nodes can be computed as the sum of dimension offsets. also, the displacement along a given link only modiﬁes the offset in the corresponding dimension. taking into account that it is possible to cross any dimension from any node in the network, routing can be easily implemented by selecting a link that decrements the absolute value of the offset in some dimension. the set of dimension offsets can be stored in the packet header and updated (by adding or subtracting one unit) every time the packet is successfully routed at some intermediate node. if the topology is not strictly orthogonal, however, routing may become much more complex.
the most popular direct networks are the n-dimensional mesh, the k-ary n-cube or torus, and the hypercube. all of them are strictly orthogonal. formally, an n-dimensional mesh has k0 × k1 × · · · × kn−2 × kn−1 nodes, ki nodes along each dimension i, where ki ≥ 2 and 0 ≤ i ≤ n − 1. each node x is identiﬁed by n coordinates, (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ ki − 1 for 0 ≤ i ≤ n − 1. two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = xj ± 1. thus, nodes have from n to 2n neighbors, depending on their location in the mesh. therefore, this topology is not regular.
in a bidirectional k-ary n-cube [70], all nodes have the same number of neighbors. the deﬁnition of a k-ary n-cube differs from that of an n-dimensional mesh in that all of the ki are equal to k and two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = (xj ± 1) mod k. the change to modular arithmetic in the deﬁnition adds wraparound channels to the k-ary n-cube, giving it regularity and symmetry. every node has n neighbors if k = 2, and 2n neighbors if k > 2. when n = 1, the k-ary n-cube collapses to a bidirectional ring with k nodes.
another topology with regularity and symmetry is the hypercube, which is a special case of both n-dimensional meshes and k-ary n-cubes. a hypercube is an n-dimensional mesh in which ki = 2 for 0 ≤ i ≤ n − 1, or a 2-ary n-cube, also referred to as a binary n-cube.
figure 1.5(a) depicts a binary 4-cube or 16-node hypercube. figure 1.5(b) illustrates a 3-ary 2-cube or two-dimensional (2-d) torus. figure 1.5(c) shows a 3-ary threedimensional (3-d) mesh, resulting by removing the wraparound channels from a 3-ary 3-cube.
two conﬂicting requirements of a direct network are that it must accommodate a large number of nodes while maintaining a low network latency. this issue will be addressed in chapter 7.
deﬁned within a context called the process group. a unique group id is associated with each distinct process group. members of a process group may not be ﬁxed. during different phases of a computation, new members may be added to the process group, and old members may be removed from the process group.
various collective communication services have been identiﬁed for processes within a process group. providing such services can simplify the programming effort and facilitate efﬁcient implementation. consider a process group, g, with n processes, p1, p2, . . . , pn. we assume that all processes will be involved in collective communication, although it is possible that some processes are disabled or masked off from participation. four basic types of collective communication services within the context of a process group are described below.
in this category, each process can send at most one message and receive at most one message. if each process has to send exactly one message and receive exactly one message, there are n! different permutation or communication patterns. figure 5.1(a) shows a circular shift communication pattern in which pi sends a message to pi+1 for 1 ≤ i ≤ n − 1 and pn delivers its message to p1. in some applications, with no need of wraparound, a shift communication pattern can be supported in which p1 only sends a message and pn only receives a message, as shown in figure 5.1(b).
in one-to-all communication, one process is identiﬁed as the sender (called the root) and all processes in the group are receivers. there are some variations. in some designs, the sender may or may not be a member of the process group. also, if the sender is a member of the process group, it may or may not be a receiver. here we assume that the sender is a
no packet has already arrived at its destination node. packets cannot advance because the queues for all the alternative output channels supplied by the routing function are full.
there is no packet whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (remember, we make the assumption that a queue cannot contain ﬂits belonging to different packets). data ﬂits cannot advance because the next channel reserved by their packet header has a full queue. note that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
in some cases, a conﬁguration cannot be reached by routing packets starting from an empty network. this situation arises when two or more packets require the use of the same channel at the same time to reach the conﬁguration. a conﬁguration that can be reached by routing packets starting from an empty network is reachable or routable [63]. it should be noted that by deﬁning the domain of the routing function as n × n, every legal conﬁguration is also reachable. effectively, as the routing function has no memory of the path followed by each packet, we can consider that, for any legal conﬁguration, a packet stored in a channel queue was generated by the source node of that channel. in wormhole switching, we can consider that the packet was generated by the source node of the channel containing the last ﬂit of the packet. this is important because when all the legal conﬁgurations are reachable, we do not need to consider the dynamic evolution of the network leading to those conﬁgurations. we can simply consider legal conﬁgurations, regardless of the packet injection sequence required to reach them. when all the legal conﬁgurations are reachable, a routing function is deadlock-free if and only if there is not any deadlocked conﬁguration for that routing function.
a routing function r is connected if it is able to establish a path between every pair of nodes x and y using channels belonging to the sets supplied by r. it is obvious that a routing function must be connected, and most authors implicitly assume this property. however, we mention it explicitly because we will use a restricted routing function to prove deadlock freedom, and restricting a routing function may disconnect it.
the theoretical model of deadlock avoidance we are going to present relies on the concept of channel dependency [77]. other approaches are possible. they will be brieﬂy described in section 3.3. when a packet is holding a channel, and then it requests the use of another channel, there is a dependency between those channels. both channels are in one of the
the direct network or point-to-point network is a popular network architecture that scales well to a large number of processors. a direct network consists of a set of nodes, each one being directly connected to a (usually small) subset of other nodes in the network. each node is a programmable computer with its own processor, local memory, and other supporting devices. these nodes may have different functional capabilities. for example, the set of nodes may contain vector processors, graphics processors, and i/o processors. figure 1.4 shows the architecture of a generic node. a common component of these nodes is a router, which handles message communication among nodes. for this reason, direct networks are also known as router-based networks. each router has direct connections to the router of its neighbors. usually, two neighboring nodes are connected by a pair of unidirectional channels in opposite directions. a bidirectional channel may also be used to connect two neighboring nodes. although the function of a router can be performed by the local processor, dedicated routers have been used in high-performance multicomputers, allowing overlapped computation and communication within each node. as the number of nodes in the system increases, the total communication bandwidth, memory bandwidth, and processing capability of the system also increase. thus, direct networks have been a popular interconnection architecture for constructing large-scale parallel computers. figures 1.5 through 1.7 show several direct networks. the corresponding interconnection patterns between nodes will be studied later.
each router supports some number of input and output channels. internal channels or ports connect the local processor/memory to the router. although it is common to provide only one pair of internal channels, some systems use more internal channels in order to avoid a communication bottleneck between the local processor/memory and the router [39]. external channels are used for communication between routers. by connecting input channels of one node to the output channels of other nodes, the direct network is deﬁned. unless otherwise speciﬁed, the term “channel” will refer to an external channel. two directly connected nodes are called neighboring or adjacent nodes. usually, each
pipeline can modify the header before forwarding the packet. if the router pipeline detects a single-bit error, it corrects the error and reports it back to the operating system via an interrupt. however, it does not correct double-bit errors. instead, if it detects a double-bit error, the 21364 alerts every reachable 21364 of the occurrence of such an error and enters into an error recovery mode.
the most challenging component of the 21364 router is the arbitration mechanism that schedules the dispatch of packets arriving at its input ports. to avoid making the arbitration mechanism a central bottleneck, the 21364 breaks the arbitration logic into local and global arbitration. there are 16 local arbiters, two for each input port. there are seven global arbiters, one for each output port. in each cycle, a local arbiter may speculatively schedule a packet for dispatch to an output port. two cycles following the local arbitration, each global arbiter selects one out of up to seven packets speculatively scheduled for dispatch through the output port. once such a selection is made, all ﬂits in the x (crossbar) stage follow the input port to the output port connection.
to ensure fairness, the local and global arbiters use a least recently selected (lrs) scheme to select a packet. each local arbiter uses the lrs scheme to select both a class (among the several packet classes) and a virtual channel (among vc0, vc1, and adaptive) within the class. similarly, the global arbiter uses the lrs policy to select an input port (among the several input ports that each output port sees).
the nominated packet is valid at the input buffer and has not been dispatched yet. the necessary dispatch path from the input buffer to the output port is free. the packet is dispatched in only one of the routes allowed. the target router, i/o chip, or local resource (in the next hop) has a free input buffer in the speciﬁc virtual channel. the target output port is free. the antistarvation mechanism is not blocking the packet. a read io packet does not pass a write io packet.
a global arbiter selects packets speculatively scheduled for dispatch through its output port. the local arbiters speculatively schedule packets not selected by any global arbiter again in subsequent cycles.
in addition to the pipeline latency, there are a total of six cycles of synchronization delay, pad receiver and driver delay, and transport delay from the pins to the router and from the router back to the pins. thus, the on-chip, pin-to-pin latency from a network input to a network output is 13 cycles. at 1.2 ghz, this leads to a pin-to-pin latency of 10.8 ns.
the network links that connect the different 21364 chips run at 0.8 ghz, which is 33% slower than the internal router clock. the 21364 chip runs synchronously with the outgoing links, but asynchronously with the incoming links. the 21364 sends its clock
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
with input channel requests. conﬂicts for the same output must be arbitrated (in logarithmic time), and if relative addressing is used, a header must be selected. if the requested buffer(s) is (are) busy, the incoming message remains in the input buffer until a requested output becomes free. the ﬁgure shows a full crossbar that connects all input virtual channels to all output virtual channels. alternatively, the router may use a design where full connectivity is only provided between physical channels, and virtual channels arbitrate for crossbar input ports. fast arbitration policies are crucial to maintaining a low ﬂow control latency through the switch.
buffers. these are fifo buffers for storing messages in transit. in the above model, a buffer is associated with both the input physical channels and output physical channels. the buffer size is an integral number of ﬂow control units. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). in vct switching sufﬁcient buffer space is available for a complete message packet. for a ﬁxed buffer size, insertion and removal from the buffer is usually not on the router critical path.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
while the above router model is representative of routers constructed to date, router architectures are evolving with different implementations. the routing and arbitration unit may be replicated to reduce arbitration time, and channels may share ports on the crossbar. the basic functions appear largely unaltered but with distinct implementations to match the current generation of technology.
if the routers support adaptive routing, the presence of multiple choices makes the routing decision more complex. there is a need to generate information corresponding to these choices and to select among these choices. this naturally incurs some overhead in time as well as resources (e.g., chip area). similarly, the use of virtual channels, while reducing header blocking delays in the network, makes the link controllers more complex by requiring arbitration and more complex ﬂow control mechanisms.
the two main measures of intrarouter performance [57] are the routing latency or header latency and the ﬂow control latency. from figure 7.7 it is apparent that the latency experienced by the header ﬂit(s) through a router is comprised of several components. after the header has been received, it must be decoded and the connection request generated. since multiple headers may arrive simultaneously, the routing and arbitration unit arbitrates among multiple requests, and one of the headers is selected and routed. this involves computing an output and, if it is available, setting the crossbar. the updated header and subsequent data may now be driven through the switch and will experience some multiplexing delay through the link controllers as they multiplex multiple virtual channels across the physical channel. once a path has been set up, data ﬂits may now ﬂow
architectures and emerging switched networks for workstation clusters utilize some form of cut-through switching or some variant of it (e.g., vct switching, wormhole switching, buffered wormhole switching, etc.). therefore, this chapter largely focuses on a discussion of issues and designs of such routers. while the router implementations for wormhole and vct switching differ in many of the architectural trade-offs, they share many common features derived from the use of some form of cut-through switching. the common issues and features facing the design of routers can be categorized as intrarouter or interrouter, and are discussed below. this discussion is followed by descriptions of recent router designs, emphasizing their unique features and reinforcing the commonly held trade-offs.
in an effort to capture the commonalities and enable quantiﬁable comparisons, chien [57] developed an abstract model for the architecture of routers in wormhole-switched k-ary n-cubes. this model is largely concentrated on the intrarouter architecture, while the performance of interrouter link operation is very sensitive to packaging implementations. the basic wormhole router functions can be captured in an abstract router architecture as shown in figure 7.7. we are interested in the implementation complexity of each of the components in the ﬁgure.
crossbar switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
adaptive channel. thus, a packet blocked in the adaptive channel can drop down to vc0 or vc1. however, in subsequent routers along the packet path, the packet can return to the adaptive channel if the adaptive channel is not congested.
thevc0 andvc1 virtual channels must be mapped carefully onto the physical links to create the deadlock-free escape network. the 21364 has separate rules to break deadlocks across dimensions and within a dimension. across dimensions, dimension-order routing is used to prevent deadlocks in vc0 and vc1 channels. within a dimension, the 21364 maps the vc0s and vc1s in such a way that there is at least one processor not crossed by the dependency chains formed by vc0 virtual channels. the same applies to vc1 mappings. this ensures that there is no cyclic dependency in a virtual channel within a dimension. the 21364 can choose among a variety of such virtual channel mappings because the virtual channel assignments are programmable at boot time. the simplest scheme that satisﬁes the property stated above was proposed by dally and seitz [77]. in this scheme (see example 4.1), all the processors in a dimension are numbered consecutively. then, for all source and destination processors, we can make the following virtual channel assignments: if the source is less than the destination, that source/destination pair is assigned vc0. if the source is greater than the destination, then that pair is assigned vc1.
unfortunately, in this scheme, the virtual channel-to-physical link assignments are not well balanced, which can cause underutilization of network link bandwidth under heavy loads. the 21364 searches for an optimal virtual channel-to-physical link assignment using a hill-climbing algorithm. this scheme does not incur any overhead because the algorithm is run off-line and only once for a dimension with a speciﬁc size (ranging from 2 to 16 processors).
the 21364 router table consists of three parts: a 128-entry conﬁguration table, with one entry for each destination processor in the network; a virtual channel table consisting of two 16-bit vectors, which contains the vc0 and vc1 virtual channel assignments; and an additional table to support broadcasting invalidations to clusters of processors (as required by the 21364 coherence protocol).
the 21364 router table is programmable by software at boot time, which allows software the ﬂexibility to optimize the desired routing for maximal performance and to map out faulty nodes in the network. the ﬁrst ﬂit of a packet entering from a local or i/o port accesses the conﬁguration table and sets up most of the 16-bit routing information in a packet header.
the 21364 router has nine pipeline types based on the input and output ports. an input or an output port can be of three types: local port (cache and memory controller), interprocessor port (off-chip network), and i/o. any type of input port can route packets to any type of output port, which leads to nine types of pipelines. figure 7.39 shows two such pipeline types. the ﬁrst ﬂit goes through two pipelines: the scheduling pipeline (upper pipeline) and the data pipeline (lower pipeline). second and subsequent ﬂits only go through the data pipeline.
the decode stage (dw) identiﬁes the packet class, determines the virtual channel (by accessing the virtual channel table), computes the output port, and ﬁgures out the deadlockfree direction. the decode phase also prepares the packet for subsequent operations in the pipeline.
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
with input channel requests. conﬂicts for the same output must be arbitrated (in logarithmic time), and if relative addressing is used, a header must be selected. if the requested buffer(s) is (are) busy, the incoming message remains in the input buffer until a requested output becomes free. the ﬁgure shows a full crossbar that connects all input virtual channels to all output virtual channels. alternatively, the router may use a design where full connectivity is only provided between physical channels, and virtual channels arbitrate for crossbar input ports. fast arbitration policies are crucial to maintaining a low ﬂow control latency through the switch.
buffers. these are fifo buffers for storing messages in transit. in the above model, a buffer is associated with both the input physical channels and output physical channels. the buffer size is an integral number of ﬂow control units. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). in vct switching sufﬁcient buffer space is available for a complete message packet. for a ﬁxed buffer size, insertion and removal from the buffer is usually not on the router critical path.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
while the above router model is representative of routers constructed to date, router architectures are evolving with different implementations. the routing and arbitration unit may be replicated to reduce arbitration time, and channels may share ports on the crossbar. the basic functions appear largely unaltered but with distinct implementations to match the current generation of technology.
if the routers support adaptive routing, the presence of multiple choices makes the routing decision more complex. there is a need to generate information corresponding to these choices and to select among these choices. this naturally incurs some overhead in time as well as resources (e.g., chip area). similarly, the use of virtual channels, while reducing header blocking delays in the network, makes the link controllers more complex by requiring arbitration and more complex ﬂow control mechanisms.
the two main measures of intrarouter performance [57] are the routing latency or header latency and the ﬂow control latency. from figure 7.7 it is apparent that the latency experienced by the header ﬂit(s) through a router is comprised of several components. after the header has been received, it must be decoded and the connection request generated. since multiple headers may arrive simultaneously, the routing and arbitration unit arbitrates among multiple requests, and one of the headers is selected and routed. this involves computing an output and, if it is available, setting the crossbar. the updated header and subsequent data may now be driven through the switch and will experience some multiplexing delay through the link controllers as they multiplex multiple virtual channels across the physical channel. once a path has been set up, data ﬂits may now ﬂow
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
free if and only if there exists a restricted channel waiting graph that is wait-connected and has no true cycles [309]. this condition is valid for incoherent routing functions and for routing functions deﬁned on c × n. however, it proposes a dynamic condition for deadlock avoidance, thus requiring the analysis of all the packet injection sequences to determine whether a cycle is reachable (true cycle). true cycles can be identiﬁed by using the algorithm proposed in [309]. this algorithm has nonpolynomial complexity. when all the cycles are true cycles, this theorem is equivalent to theorem 3.1. the theory proposed in [309] has been generalized in [307], supporting saf, vct, and wormhole switching. basically, the theory proposed in [307] replaces the channel waiting graph by a buffer waiting graph.
up to now, nobody has proposed static necessary and sufﬁcient conditions for deadlock-free routing for incoherent routing functions and for routing functions deﬁned on c × n. this is a theoretical open problem. however, as mentioned in previous sections, it is of very little practical interest because the cases where theorem 3.1 cannot be applied are very rare. remember that this theorem can be used to prove deadlock freedom for incoherent routing functions and for routing functions deﬁned on c × n. in these cases it becomes a sufﬁcient condition.
unlike wormhole switching, saf and vct switching provide more buffer resources when packets are blocked. a single central or edge buffer is enough to store a whole packet. as a consequence, it is much simpler to avoid deadlock.
a simple technique, known as deﬂection routing [137] or hot potato routing, is based on the following idea: the number of input channels is equal to the number of output channels. thus, an incoming packet will always ﬁnd a free output channel.
the set of input and output channels includes memory ports. if a node is not injecting any packet into the network, then every incoming packet will ﬁnd a free output channel. if several options are available, a channel belonging to a minimal path is selected. otherwise, the packet is misrouted. if a node is injecting a packet into the network, it may happen that all the output channels connecting to other nodes are busy. the only free output channel is the memory port. in this case, if another packet arrives at the node, it is buffered. buffered packets are reinjected into the network before injecting any new packet at that node.
deﬂection routing has two limitations. first, it requires storing the packet into the current node when all the output channels connecting to other nodes are busy. thus, it cannot be applied to wormhole switching. second, when all the output channels belonging to minimal paths are busy, the packet is misrouted. this increases packet latency and bandwidth consumption, and may produce livelock. the main advantages are its simplicity and ﬂexibility. deﬂection routing can be used in any topology, provided that the number of input and output channels per node is the same.
deﬂection routing was initially proposed for communication networks. it has been shown to be a viable alternative for networks using vct switching. misrouting has a small impact on performance [188]. livelock will be analyzed in section 3.7.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
five virtual channels are multiplexed in each direction over each physical channel. four of the channels essentially function as they do in the t3d (with differences identiﬁed below) while the ﬁfth channel is used for fully adaptive routing. each adaptive channel can buffer up to 22 ﬂits, while each of the regular channels can buffer up to 12 ﬂits. the channels utilize credit-based ﬂow control for buffer management with acknowledgments in the reverse direction being piggybacked on other messages or transmitted as idle ﬂits. roundrobin scheduling across active virtual channels within a group determines which channel will compete for crossbar outputs. the winning channel can request a virtual channel in the deterministic direction or an adaptive channel in the highest-ordered direction yet to be satisﬁed. if both are available, the adaptive channel is used. once an output virtual channel is allocated, it must remain allocated, while other virtual channel inputs may still request and use the same physical output channel. finally, output virtual channels must arbitrate for the physical channel. this process is also round-robin, although the adaptive channel has the lowest priority to start transmitting.
routing is fully adaptive. the deterministic paths use four virtual channels for the request/reply network as in the t3d. however, the path is determined by ordering dimensions and the direction of traversal within each dimension. this style of routing can be regarded as direction order rather than dimension order. the ordering used in the t3e is x+, y+, z+, x−, y−, and z−. in general, any other ordering would work just as well. by applying the concept of channel classes from chapter 3, it is apparent that each direction corresponds to a class of channels, and the order in which channel classes are used by a message is acyclic. thus, routing is deadlock-free. in addition, the network supports an initial hop in any positive direction and permits one ﬁnal hop in the z− direction at the end of the route. these initial and ﬁnal hops add routing ﬂexibility and are only necessary if a normal direction-order route does not exist. each message has a ﬁxed path that is determined at the source router. a message can be routed along the statically conﬁgured path, or along the adaptive virtual channel over any physical link along a minimal path to the destination. however, the implementation does not require the extended channel dependency graph to be acyclic. the adaptive virtual channels are large enough to buffer two maximal messages. therefore, indirect dependencies between deterministic channels do not exist. this permits optimizations in the manner in which virtual channels are assigned to improve virtual channel utilization and minimize imbalance due to routing restrictions. finally, adaptive routing can be turned off via control ﬁelds within the routing tables and on an individual packet basis through a single bit recorded in the message. since adaptively routed messages may arrive out of order at the destination, the latter capability enables the processor to issue sequences of messages that are delivered in order.
memory management and the network interface operation have been made more ﬂexible than the ﬁrst-generation support found in the t3d.an arbitrary number of message queues are supported in both user memory and system memory. message queues can be set up to be interrupt driven, polled, or interrupt driven based on some threshold on the number of messages. in general, message passing is more tightly coupled with operations for the support of shared-memory abstractions.
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
a routing algorithm is said to be f fault recoverable if, for any f failed components in the network, a message that is undeliverable will not hold network resources indeﬁnitely. if a network is fault recoverable, the faults will not induce deadlock.
ideally, we would like the network to be f fault tolerant for large values of f . practically, however, we may be satisﬁed with a system that is f fault recoverable for large values of f and f1 fault tolerant for f1 << f , so that only the functionality of a small part of the network suffers as a result of a failed component. certainly, we want to avoid the situation where a few faults may cause a catastrophic failure of the entire network, or equivalently deadlocked conﬁgurations of messages. finally, the next deﬁnition gives the redundancy level of the network.
a routing algorithm has a redundancy level equal to r iff, after removing any set of r channels, the routing function remains connected and deadlock-free, and there exists a set of r + 1 channels such that, after removing them, the routing function is no longer connected or it is not deadlock-free.
note that an algorithm has a redundancy level equal to r iff it is r fault tolerant and it is not r + 1 fault tolerant. the analysis of the redundancy level of the network is complex. fortunately, there are some theoretical results that guarantee the absence of deadlock in the whole network by analyzing the behavior of the routing function r in a subset of channels c1 (see section 3.1.3). additionally, that behavior does not change when some channels not belonging to c1 are removed. that theory can be used to guarantee the absence of deadlock when some channels fail.
suppose that there exists a routing subfunction r1 that satisﬁes the conditions of theorem 3.1. let c1 be the subset of channels supplied by r1. now, let us consider the effects of removing some channels from the network. it is easy to see that removing channels not belonging to c1 does not add indirect dependencies between the channels belonging to c1. it may actually remove indirect dependencies. in fact, when all the channels not belonging to c1 are removed, there are no indirect dependencies between the channels belonging to c1. therefore, removing channels not belonging to c1 does not introduce cycles in the extended channel dependency graph of r1. however, removing channels may disconnect the routing function. fortunately, if a routing function is connected when it is restricted to use the channels in c1, it will remain connected when some channels not belonging to c1 are removed. thus, if a routing function r satisﬁes the conditions proposed by theorem 3.1, it will still satisfy them after removing some or all of the channels not belonging to c1. in other words, it will remain connected and deadlock-free. so, we can conclude that all the channels not belonging to c1 are redundant. we can also reason in the opposite way. in general, there will exist several routing subfunctions satisfying the conditions proposed by theorem 3.1. we will restrict our attention to the minimally connected routing subfunctions satisfying those conditions because they require a minimum number of channels to guarantee deadlock freedom. let r1, r2, . . . , rk be all the minimally connected routing subfunctions satisfying the
as we might expect, solutions to these problems are dependent upon the type and pattern of the faulty components and the network topology. the next section shows how direct networks possess natural redundancy that can be exploited, by deﬁning the redundancy level of the routing function. the remaining sections will demonstrate how the issues mentioned above are addressed in the context of distinct switching techniques.
this section presents a theoretical basis to answer a fundamental question regarding faulttolerant routing in direct networks: what is the maximum number of simultaneous faulty channels tolerated by the routing algorithm? this question has been analyzed in [95] by deﬁning the redundancy level of the routing function and proposing a necessary and sufﬁcient condition to compute its value.
if a routing function tolerates f faulty channels, it must remain connected and deadlock-free for any number of faulty channels less than or equal to f . note that, in addition to connectivity, deadlock freedom is also required. otherwise, the network could reach a deadlocked conﬁguration when some channels fail.
the following deﬁnitions are required to support the discussion of the behavior of fault-tolerant routing algorithms. in conjunction with the fault model, the following terminology can be used to discuss and compare fault-tolerant routing algorithms.
a network is said to be connected with respect to a routing algorithm if the routing function can route a message between any pair of nonfaulty routing nodes.
this deﬁnition is useful since a network may be connected in the graph-theoretical sense, where a physical path exists between a pair of nodes. however, routing restrictions may result in a routing function that precludes selection of links along that path.
this deﬁnition indicates the conditions to support the failure of a given channel. fault-tolerant routing algorithms should be designed in such a way that all the channels are redundant, thus avoiding a single point of failure. however, even in this case, we cannot guarantee that the network will support two simultaneous faults. this issue is addressed by the following deﬁnitions.
a routing algorithm is said to be f fault tolerant if, for any f failed components in the network, the routing function is still connected and deadlock-free.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
free if and only if there exists a restricted channel waiting graph that is wait-connected and has no true cycles [309]. this condition is valid for incoherent routing functions and for routing functions deﬁned on c × n. however, it proposes a dynamic condition for deadlock avoidance, thus requiring the analysis of all the packet injection sequences to determine whether a cycle is reachable (true cycle). true cycles can be identiﬁed by using the algorithm proposed in [309]. this algorithm has nonpolynomial complexity. when all the cycles are true cycles, this theorem is equivalent to theorem 3.1. the theory proposed in [309] has been generalized in [307], supporting saf, vct, and wormhole switching. basically, the theory proposed in [307] replaces the channel waiting graph by a buffer waiting graph.
up to now, nobody has proposed static necessary and sufﬁcient conditions for deadlock-free routing for incoherent routing functions and for routing functions deﬁned on c × n. this is a theoretical open problem. however, as mentioned in previous sections, it is of very little practical interest because the cases where theorem 3.1 cannot be applied are very rare. remember that this theorem can be used to prove deadlock freedom for incoherent routing functions and for routing functions deﬁned on c × n. in these cases it becomes a sufﬁcient condition.
unlike wormhole switching, saf and vct switching provide more buffer resources when packets are blocked. a single central or edge buffer is enough to store a whole packet. as a consequence, it is much simpler to avoid deadlock.
a simple technique, known as deﬂection routing [137] or hot potato routing, is based on the following idea: the number of input channels is equal to the number of output channels. thus, an incoming packet will always ﬁnd a free output channel.
the set of input and output channels includes memory ports. if a node is not injecting any packet into the network, then every incoming packet will ﬁnd a free output channel. if several options are available, a channel belonging to a minimal path is selected. otherwise, the packet is misrouted. if a node is injecting a packet into the network, it may happen that all the output channels connecting to other nodes are busy. the only free output channel is the memory port. in this case, if another packet arrives at the node, it is buffered. buffered packets are reinjected into the network before injecting any new packet at that node.
deﬂection routing has two limitations. first, it requires storing the packet into the current node when all the output channels connecting to other nodes are busy. thus, it cannot be applied to wormhole switching. second, when all the output channels belonging to minimal paths are busy, the packet is misrouted. this increases packet latency and bandwidth consumption, and may produce livelock. the main advantages are its simplicity and ﬂexibility. deﬂection routing can be used in any topology, provided that the number of input and output channels per node is the same.
deﬂection routing was initially proposed for communication networks. it has been shown to be a viable alternative for networks using vct switching. misrouting has a small impact on performance [188]. livelock will be analyzed in section 3.7.
adaptive double-y (mad-y). it improves adaptivity with respect to the double-y algorithm. basically, mad-y allows packets using y 1 channels to turn to the x+ direction and packets using x− channels to turn and use y 2 channels. figures 4.23 and 4.24 show the turns allowed by the double-y and mad-y algorithms, respectively.
the mad-y algorithm has the maximum adaptivity that can be obtained without introducing cyclic dependencies between channels. however, as shown in theorem 3.1, cycles do not necessarily produce deadlock. thus, the mad-y algorithm can be improved. it was done by schwiebert and jayasimha, who proposed the opt-y algorithm [306, 308]. this algorithm is deadlock-free and optimal with respect to the number of routing restrictions on the virtual channels for deadlock-avoidance-based routing. basically, the opt-y algorithm allows all the turns between x and y 2 channels as well as turns between x+ and y 1 channels. turns from y 1 to x− channels are prohibited. turns from x− to y 1 channels as well as 0-degree turns between y 1 and y 2 channels are restricted. these turns are only allowed when the packet has completed its movement along x− channels (the x-offset is zero or positive). figure 4.25 shows the turns allowed by the opt-y algorithm.
deﬁning a routing algorithm by describing the allowed and prohibited turns makes it difﬁcult to understand how routing decisions are taken at a given node. the opt-y algorithm is described in figure 4.26 using pseudocode.
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
adaptive double-y (mad-y). it improves adaptivity with respect to the double-y algorithm. basically, mad-y allows packets using y 1 channels to turn to the x+ direction and packets using x− channels to turn and use y 2 channels. figures 4.23 and 4.24 show the turns allowed by the double-y and mad-y algorithms, respectively.
the mad-y algorithm has the maximum adaptivity that can be obtained without introducing cyclic dependencies between channels. however, as shown in theorem 3.1, cycles do not necessarily produce deadlock. thus, the mad-y algorithm can be improved. it was done by schwiebert and jayasimha, who proposed the opt-y algorithm [306, 308]. this algorithm is deadlock-free and optimal with respect to the number of routing restrictions on the virtual channels for deadlock-avoidance-based routing. basically, the opt-y algorithm allows all the turns between x and y 2 channels as well as turns between x+ and y 1 channels. turns from y 1 to x− channels are prohibited. turns from x− to y 1 channels as well as 0-degree turns between y 1 and y 2 channels are restricted. these turns are only allowed when the packet has completed its movement along x− channels (the x-offset is zero or positive). figure 4.25 shows the turns allowed by the opt-y algorithm.
deﬁning a routing algorithm by describing the allowed and prohibited turns makes it difﬁcult to understand how routing decisions are taken at a given node. the opt-y algorithm is described in figure 4.26 using pseudocode.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
a. select a safe proﬁtable adaptive output channel. stop. b. select a safe deterministic output channel. stop. c. if the safe deterministic channel is not faulty, block. d. if the deterministic channel is faulty, select an unsafe proﬁtable adaptive
able to dynamically conﬁgure the ﬂow control protocols employed by the routers. routing algorithms can be designed such that in the vicinity of faulty components messages use pcs-style ﬂow control, where controlled misrouting and backtracking can be used to avoid faults and deadlocked conﬁgurations. at the same time messages use wormholeswitching ﬂow control in fault-free portions of the network with the attendant performance advantages. messages are routed in two phases—a fault-free phase and a faulty phase. messages may transition between these two phases several times in the course of being routed between a source and destination node. routing algorithms designed based on such ﬂow control mechanisms are referred to as multiphase routing algorithms [79]. such dynamically conﬁgurable ﬂow control protocols can be conﬁgured using variants of scouting switching described in chapter 2.
the following description presents a two-phase routing algorithm. message routing proceeds in one of two phases: an optimistic phase for routing in fault-free network segments and a conservative phase for routing in faulty segments. an example of a twophase (tp) algorithm is shown in figure 6.37. the optimistic phase uses a fully adaptive, minimal, deadlock-free routing function based on dp (see section 4.4.4). the conservative phase uses a form of mb-m. the switching technique is scouting switching with a scouting distance of k.
the virtual channels on each physical link are partitioned into restricted and unrestricted partitions. fully adaptive minimal routing is permitted on the unrestricted partition (adaptive channels), while only deterministic routing is allowed on the restricted partition (deterministic channels). channels are marked as safe or unsafe [198, 351] depending on the number of faulty links/nodes within the immediate neighborhood. the deﬁnition of the extent of this neighborhood for determining the safe/unsafe designation can be quite
the turn model described in chapter 4 can be modiﬁed to handle the occurrence of faulty components. the example described here is the nonminimal version of the negativeﬁrst routing algorithm for 2-d meshes. recall that this algorithm operates in two phases: the message is routed in the negative direction in each of the dimensions in the ﬁrst phase, and then messages are routed in the positive directions in the second phase. the fault-tolerant nonminimal version routes adaptively in the negative direction, even farther west or south than the destination. for example, this is the path taken by message a in figure 6.31. during this phase the message is routed adaptively and around faulty nodes or links. the exception occurs when a message being routed along the edge of the mesh in the negative direction encounters a faulty node. in the second phase, the message is routed
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
adaptive double-y (mad-y). it improves adaptivity with respect to the double-y algorithm. basically, mad-y allows packets using y 1 channels to turn to the x+ direction and packets using x− channels to turn and use y 2 channels. figures 4.23 and 4.24 show the turns allowed by the double-y and mad-y algorithms, respectively.
the mad-y algorithm has the maximum adaptivity that can be obtained without introducing cyclic dependencies between channels. however, as shown in theorem 3.1, cycles do not necessarily produce deadlock. thus, the mad-y algorithm can be improved. it was done by schwiebert and jayasimha, who proposed the opt-y algorithm [306, 308]. this algorithm is deadlock-free and optimal with respect to the number of routing restrictions on the virtual channels for deadlock-avoidance-based routing. basically, the opt-y algorithm allows all the turns between x and y 2 channels as well as turns between x+ and y 1 channels. turns from y 1 to x− channels are prohibited. turns from x− to y 1 channels as well as 0-degree turns between y 1 and y 2 channels are restricted. these turns are only allowed when the packet has completed its movement along x− channels (the x-offset is zero or positive). figure 4.25 shows the turns allowed by the opt-y algorithm.
deﬁning a routing algorithm by describing the allowed and prohibited turns makes it difﬁcult to understand how routing decisions are taken at a given node. the opt-y algorithm is described in figure 4.26 using pseudocode.
the paradigm adopted throughout the preceding examples has been to characterize messages by the direction of traversal in the network and therefore the direction and virtual channels occupied by these messages when misrouted around a fault ring. the addition of virtual channels for each message type ensures that these messages occupy disjoint virtual networks and therefore channel resources. since the usage of resources within a network is orchestrated to be acyclic and transitions made by messages between networks remain acyclic, routing can be guaranteed to be deadlock-free. however, the addition of virtual channels affects the speed and complexity of the routers.arbitration between virtual channels and the multiplexing of virtual channels across the physical channel can have a substantial impact on the ﬂow control latency through the router [57]. this motivated investigations of solutions that did not rely on many (or any) virtual channels.
origin-based fault-tolerant routing is a paradigm that enables fault-tolerant routing in mesh networks under a similar fault model, but without the addition of virtual channels [145]. the basic fault-free form of origin-based routing follows the paradigm of several adaptive routing algorithms proposed for binary hypercubes and the turn model proposed for more general direct network topologies. each message progresses through two phases. in the ﬁrst phase, the message is adaptively routed toward a special node. on reaching this node, the message is adaptively routed to the destination in the second phase. in previous applications of this paradigm to binary hypercubes, this special node could be the zenith node whose address is given by the logical or of the source and destination addresses. messages are ﬁrst routed adaptively to their zenith and then adaptively toward the destination [184]. this phase ordering prevents the formation of cycles in the channel dependency graphs. variants of this approach have also been proposed, including the relaxation of ordering restrictions on the phases [59]. in origin-based routing in mesh networks this special node is the node that is designated as the origin according to the mesh coordinates. while the approach is directly extensible to multidimensional networks, the following description deals with 2-d meshes.
all of the physical channels are partitioned into two disjoint networks. the in network consists of all of the unidirectional channels that are directed toward the origin, while the out network consists of all of the unidirectional channels directed away from the origin. the orientation of a channel can be determined by the node at the receiving end of the channel. if this node is closer to the origin than the sending end of the channel, the channel is in the in network. otherwise it is in the out network. the outbox for a node in the mesh is the submesh comprised of all nodes on a shortest path to the origin. an example of an outbox for a destination node d is shown in figure 6.24. messages are routed in two phases. in the ﬁrst phase messages are routed adaptively toward the destination/origin, over any shortest path using channels in the in network. when the header ﬂit arrives at any node in the outbox for the destination, the message is now routed adaptively toward the destination using only channels in the out network. as shown in the example in figure 6.24, the resulting complete path may not be a minimal path. the choice of minimal paths is easily enforced by restricting messages traversing the in network to use channels that take the message closer to the destination. the result of enforcing such a restriction on the same (source, destination) pair is also shown in figure 6.24. the choice of the origin node is important. since all messages are ﬁrst routed toward the origin, hot spots can
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
a copy of the source message across several separate multicast paths, each path for each subset of the destination nodes. this multidestination routing scheme will be referred to as path-based routing.
in path-based routing, the header of each copy of a message consists of multiple destinations. the source node arranges these destinations as an ordered list, depending on their intended order of traversal. as soon as the message is injected into the network, it is routed based on the address in the leading header ﬂit corresponding to the ﬁrst destination. once the message header reaches the router of the ﬁrst destination node, the ﬂit containing this address is removed by the router. now the message is routed to the node whose address is contained in the next header ﬂit. this address corresponds to the second destination in the ordered list. while the ﬂits are being forwarded by the router of the ﬁrst destination node to its adjacent router, they are also copied ﬂit by ﬂit to the delivery buffer of this node. this process is carried out in each intermediate destination node of the ordered list. when the message reaches the last destination node, it is not routed any further and is completely consumed by that node.
the routing hardware requires a few changes with respect to the router model described in section 2.1 to support path-based routing. we assume that each destination address is encoded in a different ﬂit. also, we assume that a single bit is used in each ﬂit to distinguish between destination addresses and data ﬂits. some control logic is required to discard the current destination address and transmit the next destination ﬂit to the routing and arbitration unit. sending a single clock pulse to the corresponding input channel buffer so that ﬂits advance one position is enough to discard the previous destination address. then, the routing and arbitration unit is reset so that it looks for a destination address ﬂit at the header of a buffer and starts a routing operation again. also, note that if the switch is a crossbar and it is implemented as a set of multiplexors (one for each switch output), it is possible to select the same input from several outputs. therefore, the switch can be easily conﬁgured so that ﬂits are simultaneously forwarded to the next router and copied to the delivery buffer of the current node. finally, the delivery buffer should be designed in such a way that ﬂits containing destination addresses are automatically discarded and no ﬂow control is required. this is to avoid conﬂicts with ﬂow control signals from the next node when messages are simultaneously forwarded to the next router and copied to the delivery buffer.
a simple analysis shows that the probability that a message is blocked in path-based routing is lower than that for tree-based routing. suppose that the probability that a message is blocked in each channel is p. assume that at a certain level of the tree-based routing the total number of branches is k. the tree requires that all k channels be available at the same time. the probability that a message is blocked at this level would be 1 − (1 − p)k. on the other hand, the probability of blocking for path-based routing is p since each multicast path only requests one channel at one moment. an important property of pathbased routing is that, when one path is blocked, it will not block the message delivery on the other paths. for example, consider a 6-cube and suppose that p is 0.1. the second level of the broadcast routing tree has 6 branches. the probability that a message is blocked at this level is 1 − (1 − 0.1)6, which is 0.47, while the probability is only 0.1 for path-based routing.
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
message packets comprise a header and body. sample formats are shown in figure 7.19. the header is an integral number of 16-bit phits that contain routing information, control information for the remote pe, and possibly source information as well. the body makes use of check bits to detect errors. for example, a 64-bit word will be augmented with 14 check bits (hence the 5-phit body in figure 7.19) and four-word message bodies will have 56 check bits (necessitating 20-phit bodies). the messages are organized as sequences of ﬂits, with each ﬂit comprised of 8 phits. virtual channel buffers are 1 ﬂit deep.
the t3d utilizes source routing. header information identiﬁes the sequence of virtual channels that the message will traverse in each dimension. in order to understand the routing strategy, we must ﬁrst be familiar with the pe addressing scheme. while all pes have physical addresses, they also possess logical addresses and virtual addresses. logical addresses are based on the logical topology of the installation. thus, nodes can be addressed by the coordinate of their logical position in this topology and are in principle independent of their physical location. this approach permits spare nodes to be mapped into the network to replace failed nodes by assigning the spare node the same logical address. the virtual address is assigned to nodes within a partition allocated to a job. the virtual address is interpreted according to the shape of the allocated partition. for
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
figure 5.9 multiaddress encoding schemes: (a) all-destination encoding, (b) bit string encoding, (c) multiple-region broadcast encoding, (d) multiple-region stride encoding, and (e) multiple-region bit string encoding.
the remaining encoding schemes try to optimize the header length by considering ranges of addresses or regions. in figure 5.9(c), each region is speciﬁed by two ﬁelds: the beginning and ending addresses of the region. within each region, the message is broadcast to all the addresses in the range. in some applications, a node may send a message to a set of destination addresses that have a constant distance between two adjacent addresses. a suitable encoding scheme for those applications consists of adding a stride to the deﬁnition of each region, as shown in figure 5.9(d). finally, if the destination addresses are irregularly distributed but can be grouped into regions, each region can be speciﬁed by a bit string, in addition to the beginning and ending addresses (see figure 5.9(e)). the main drawback of encoding schemes based on regions is that the routing hardware required to decode addresses is complex. also, several header ﬂits may be required to encode each region. those ﬂits should reach the router before starting a routing operation.
one approach to multicast routing is to deliver the message along a common path as far as possible, then replicate the message and forward each copy on a different channel bound for a unique set of destination nodes. the path followed by each copy may further branch in this manner until the message is delivered to every destination node. in such tree-based routing, the destination set is partitioned at the source, and separate copies are sent on one
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
assume that p moves to the a queue in node (3, 1). as p is still allowed to move to the inside, it moves to the b queue in node (3, 2), reaching its destination node, and moving to the delivery queue in node (3, 2).
all the routing algorithms described in previous sections use avoidance techniques to handle deadlocks, therefore restricting routing. deadlock recovery techniques do not restrict routing to avoid deadlock. hence, routing strategies based on deadlock recovery allow maximum routing adaptivity (even beyond that proposed in [306, 308]) as well as minimum resource requirements. in particular, progressive deadlock recovery techniques, like disha (see section 3.6), decouple deadlock-handling resources from normal routing resources by dedicating minimum hardware to efﬁcient deadlock recovery in order to make the common case (i.e., no deadlocks) fast.as proposed, sequential recovery from deadlocks requires only one central ﬂit-sized buffer applicable to arbitrary network topologies [8, 9], and concurrent recovery requires at most two central buffers for any topology on which a hamiltonian path or a spanning tree can be deﬁned [10].
when routing is not restricted, no virtual channels are dedicated to avoid deadlocks. instead, virtual channels are used for the sole purpose of improving channel utilization and adaptivity. hence, true fully adaptive routing is permitted on all virtual channels within each physical channel, regardless of network topology. true fully adaptive routing can be minimal or nonminimal, depending on whether routing is restricted to minimal paths or not. note that fully adaptive routing used in the context of avoidance-based algorithms connotes full adaptivity across all physical channels but only partial adaptivity across virtual channels within a given physical channel. on the other hand, true fully adaptive routing used in the context of recovery-based algorithms connotes full adaptivity across all physical channel dimensions as well as across all virtual channels within a given physical channel. routing restrictions on virtual channels are therefore completely relaxed so that no ordering among these resources is enforced.
as an example, figure 4.28 shows a true fully adaptive minimal routing algorithm for 2-d meshes. each physical channel is assumed to be split into two virtual channels a and b. in this ﬁgure, xa+ and xb+ denote the a and b channels, respectively, in the positive direction of the x dimension. a similar notation is used for the other direction and dimension. as can be seen, no routing restrictions are enforced, except for paths to be minimal.
figure 4.28 only shows the routing algorithm. it does not include deadlock handling. this issue was covered in section 3.6. for the sake of completeness, figure 4.29 shows a ﬂow diagram of the true fully adaptive nonminimal routing algorithm implemented by disha. the shaded box corresponds to the routing algorithm described in figure 4.28 (extended to handle nonminimal routing). if after a number of tries a packet cannot access any virtual channel along any minimal path to its destination, it is allowed to access any misrouting channel except those resulting in 180-degree turns. if all minimal and misrouting channels remain busy for longer than the timeout for deadlock detection, the
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
across a switch. finally, splitting messages into packets and reassembling them at the destination node also produces some overhead. these overheads can be amortized if packets are long enough. however, once packets are long enough to amortize the overhead, it is not convenient to increase packet size even more because blocking time for some packets will be high. on the other hand, switching techniques like vct may limit packet size, especially when packet buffers are implemented in hardware. finally, it should be noted that this parameter only makes sense when messages are long. if they are shorter than the optimal packet size, messages should not be split into packets. this is the case for dsms.
deadlock-handling technique. current deadlock avoidance techniques allow fully adaptive routing across physical channels. however, some buffer resources (usually some virtual channels) must be dedicated to avoid deadlock by providing escape paths to messages blocking cyclically. on the other hand, progressive deadlock recovery techniques require a minimum amount of dedicated hardware to deliver deadlocked packets. deadlock recovery techniques do not restrict routing at all and therefore allow the use of all the virtual channels to increase routing freedom, achieving the highest performance when packets are short. however, when packets are long or have very different lengths and the network approaches the saturation point, the small bandwidth offered by the recovery hardware may saturate. in this case, some deadlocked packets may have to wait for a long time, thus degrading performance and making latency less predictable. also, recovery techniques require efﬁcient deadlock detection mechanisms. currently available detection techniques only work efﬁciently when all the packets are short and have a similar length. otherwise, many false deadlocks are detected, quickly saturating the bandwidth of the recovery hardware. the poor behavior of current deadlock detection mechanisms considerably limits the practical applicability of deadlock recovery techniques unless all the packets are short. this may change when more accurate distributed deadlock detection mechanisms are developed.
routing algorithm. for regular topologies and uniform trafﬁc, the difference between deterministic and fully adaptive routing algorithms is small. however, for switch-based networks with irregular topology and uniform trafﬁc, adaptive routing algorithms considerably improve performance over deterministic or partially adaptive ones because the latter usually route many messages across nonminimal paths. moreover, for nonuniform trafﬁc patterns, adaptive routing considerably increases throughput over deterministic routing algorithms, regardless of network topology. on the other hand, adaptive routing does not reduce latency when trafﬁc is low to moderate because contention is small and base latency is the same for deterministic and fully adaptive routing, provided that both algorithms only use minimal paths. so, for real applications, the best choice depends on the application requirements. if most applications exhibit a high degree of communication locality, fully adaptive routing does not help. if the trafﬁc produced by the applications does not saturate the network (regardless of the routing algorithm) and latency is critical, then adaptive routing will not increase performance. however, when mul-
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
consider the router model described in section 2.1. we assume that all the operations inside each router are synchronized by its local clock signal. to compute the clock frequency of each router, we will use the delay model proposed in [57]. it assumes 0.8 µm cmos gate array technology for the implementation.
routing control unit. routing a message involves the following operations: address decoding, routing decision, and header selection. the ﬁrst operation extracts the message header and generates requests of acceptable outputs based on the routing algorithm. in other words, the address decoder implements the routing function. according to [57], the address decoder delay is constant and equal to 2.7 ns. the routing decision logic takes as inputs the possible output channels generated by the address decoder and the status of the output channels. in other words, this logic implements the selection function. this circuit has a delay that grows logarithmically with the number of alternatives, or degrees of freedom, offered by the routing algorithm. representing the degrees of freedom by f , this circuit has a delay value given by 0.6 + 0.6 log f ns. finally, the routing control unit must compute the new header, depending on the output channel selected. while new headers can be computed in parallel with the routing decision, it is necessary to select the appropriate one when this decision is made. this operation has a delay that grows logarithmically with the degrees of freedom. thus, this delay will be 1.4 + 0.6 log f ns. the operations and the associated delays are shown in figure 9.31. the total routing time will be the sum of all delays, yielding tr = 2.7 + 0.6 + 0.6 log f + 1.4 + 0.6 log f = 4.7 + 1.2 log f ns
switch. the time required to transfer a ﬂit from one input channel to the corresponding output channel is the sum of the delay involved in the internal ﬂow control unit, the delay of the crossbar, and the setup time of the output channel latch. the ﬂow control unit manages the buffers, preventing overﬂow and underﬂow. it has a
from the point of view of router performance we are interested in two parameters [57]. when a message ﬁrst arrives at a router, it must be examined to determine the output channel over which the message is to be forwarded. this is referred to as the routing delay and typically includes the time to set the switch. once a path has been established through a router by the switch, we are interested in the rate at which messages can be forwarded through the switch. this rate is determined by the propagation delay through the switch (intrarouter delay) and the signaling rate for synchronizing the transfer of data between the input and output buffers. this delay has been characterized as the internal ﬂow control latency [57]. similarly, the delay across the physical links (interrouter delay) is referred to as the external ﬂow control latency. the routing delay and ﬂow control delays collectively determine the achievable message latency through the switch and, along with contention by messages for links, determine the network throughput.
the following section addresses some basic concepts in the implementation of the switching layer, assuming the generic router model shown in figure 2.1. the remainder of the chapter focuses on alternative implementations of the switching layer.
switching layers can be distinguished by the implementation and relative timing of ﬂow control operations and switching techniques. in addition, these operations may be overlapped with the time to make routing decisions.
some changes in notation. those changes will be summarized in section 3.2.3. in this case, a few central buffers deep enough to store one or more packets are used. as above, a channel will only accept a new packet if there is enough buffer space to store the whole packet. buffer space must be reserved before starting packet transmission, thus preventing other channels from reserving the same buffer space. the message ﬂow control protocol is responsible for ensuring the availability of buffer space and arbitrating between concurrent requests for space in the central queues.
as we will see in section 3.2.3, it is also possible to consider models that mix both kinds of resources, edge buffers and central queues. it will be useful in section 3.6. in this case, each node has edge buffers and central queues. the routing function determines the resource to be used in each case. this mixed model may consider either ﬂit buffers or packet buffers, depending on the switching technique. for the sake of clarity, we will restrict deﬁnitions and examples to use only edge buffers. results can be easily generalized by introducing the changes in notation indicated in section 3.2.3.
the interconnection network i is modeled by using a strongly connected directed graph with multiple arcs, i = g(n, c). the vertices of the graph n represent the set of processing nodes. the arcs of the graph c represent the set of communication channels. more than a single channel is allowed to connect a given pair of nodes. bidirectional channels are considered as two unidirectional channels. we will refer to a channel and its associated edge buffer indistinctly. the source and destination nodes of a channel ci are denoted si and di, respectively.
a routing algorithm is modeled by means of two functions: routing and selection. the routing function supplies a set of output channels based on the current and destination nodes. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied. if all the output channels are busy, the packet will be routed again until it is able to reserve a channel, thus getting the ﬁrst channel that becomes available. as we will see, the routing function determines whether the routing algorithm is deadlock-free or not. the selection function only affects performance. note that, in our model, the domain of the routing function is n × n because it only takes into account the current and destination nodes. thus, we do not consider the path followed by the packet while computing the next channel to be used. we do not even consider the input channel on which the packet arrived at the current node. the reason for this choice is that it enables the design of practical routing protocols for which speciﬁc properties can be proven. these results may not be valid for other routing functions. furthermore, this approach enables the development of methodologies for the design of fully adaptive routing protocols (covered in chapter 4). these methodologies are invalid for other routing functions. thus, this choice is motivated by engineering practice that is developed without sacriﬁce in rigor. other deﬁnitions of the routing function will be considered in section 3.2.
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
a transmission in one clock cycle. therefore, the propagation delay across this channel is denoted by tw = 1 b . this assumption will be relaxed in section 7.1.5. once a path has been set up through the router, the intrarouter delay or switching delay is denoted by ts. the router internal data paths are assumed to be matched to the channel width of w bits. thus, in ts seconds a w -bit ﬂit can be transferred from the input of the router to the output. the source and destination processors are assumed to be d links apart. the relationship between these components as they are used to compute the no-load message latency is shown in figure 2.5.
in circuit switching, a physical path from the source to the destination is reserved prior to the transmission of the data. this is realized by injecting the routing header ﬂit into the network. this routing probe contains the destination address and some additional control information. the routing probe progresses toward the destination reserving physical links as it is transmitted through intermediate routers. when the probe reaches the destination, a complete path has been set up and an acknowledgment is transmitted back to the source. the message contents may now be transmitted at the full bandwidth of the hardware path. the circuit may be released by the destination or by the last few bits of the message. in the intel ipsc/2 routers [258], the acknowledgments are multiplexed in the reverse direction on the same physical line as the message. alternatively, implementations may provide separate signal lines to transmit acknowledgment signals. a time-space diagram of the transmission of a message over three links is shown in figure 2.6. the header probe is forwarded across three links, followed by the return of the acknowledgment. the shaded boxes represent the times during which a link is busy. the space between these boxes represents the time to process the routing header plus the intrarouter propagation delays. the clear box represents the duration that the links are busy transmitting data through the
at node n2 destined for n0 can reserve ca2 and then request ca3. finally, a packet at node n3 destined for n1 can reserve ca3 and then request ca0 and ch 0.
however, the routing function is deadlock-free. although we focus on wormhole switching, the following analysis is also valid for other switching techniques. let us show that there is not any deadlocked conﬁguration by trying to build one. if there were a packet stored in the queue of channel ch 2, it would be destined for n3 and ﬂits could advance. so, ch 2 must be empty. also, if there were a packet stored in the queue of ch 1, it would be destined for n2 or n3. as ch 2 is empty, ﬂits could also advance and ch 1 must be empty. if there were ﬂits stored in the queue of ch 0, they would be destined for n1, n2, or n3. even if their header were stored in ca1 or ca2, as ch 1 and ch 2 are empty, ﬂits could advance and ch 0 must be empty.
thus, any deadlocked conﬁguration can only use channels cai. although there is a cyclic dependency between them, ca0 cannot contain ﬂits destined for n0. that conﬁguration would not be legal because n0 cannot forward packets for itself through the network. for any other destination, those ﬂits can advance because ch 1 and ch 2 are empty. again, ca0 can be emptied, thus breaking the cyclic dependency. thus, the routing function is deadlock-free.
example 3.3 shows that deadlocks can be avoided even if there are cyclic dependencies between some channels. obviously, if there were cyclic dependencies between all the channels in the network, there would be no path to escape from cycles. thus, the key idea consists of providing a path free of cyclic dependencies to escape from cycles. that path can be considered as an escape path. note that at least one packet from each cycle should be able to select the escape path at the current node, whichever its destination is. in example 3.3, for every legal conﬁguration, a packet whose header ﬂit is stored in channel ca0 must be destined for either n1, n2, or n3. in the ﬁrst case, it can be immediately delivered. in the other cases, it can use channel ch 1.
it seems that we could focus only on the escape paths and forget about the other channels to prove deadlock freedom. in order to do so, we can restrict a routing function in such a way that it only supplies channels belonging to the escape paths as routing choices. in other words, if a routing function supplies a given set of channels to route a packet from the current node toward its destination, the restricted routing function will supply a subset of those channels. the restricted routing function will be referred to as a routing subfunction. formally, if r is a routing function and r1 is a routing subfunction of r, we have
channels supplied by r1 for a given packet destination will be referred to as escape channels for that packet. note that the routing subfunction is only a mathematical tool to prove deadlock freedom. packets can be routed by using all the channels supplied by the routing function r. simply, the concept of a routing subfunction will allow us to focus on
for each channel, the queue capacity is not exceeded and all the ﬂits stored in the queue (if any) can reach the channel from the previous node using the routing function.
a deadlocked conﬁguration for a given interconnection network i and routing function r is a nonempty legal conﬁguration verifying the following conditions:
in a deadlocked conﬁguration there is no message whose header ﬂit has already arrived at its destination. header ﬂits cannot advance because the queues for all the alternative output channels supplied by the routing function are not empty (see assumption 5). data and tail ﬂits cannot advance because the next channel reserved by their message header has a full queue. no condition is imposed on empty channels. it must be noticed that a data ﬂit can be blocked at a node even if there are free output channels to reach its destination because data ﬂits must follow the path reserved by their header.
a routing function r for an interconnection network i is deadlock-free iff there is not any deadlocked conﬁguration for that routing function on that network.
a routing subfunction r1 for a given routing function r is a routing function deﬁned on the same domain as r that supplies a subset of the channels supplied by r:
thus, r1 is a restriction of r. it should be noted that this deﬁnition allows the restriction of channel routing capability instead of simply removing channels. in other words, it is possible to restrict the use of a channel ci when r1 routes messages to some destinations while still allowing the use of ci when routing messages to other destinations.
given an interconnection network i , a routing function r, a routing subfunction r1, and a pair of nonadjacent channels ci , cj ∈ c1, there is an indirect dependency from ci to cj iff
develop around the origin. congestion around the origin is minimized when it is placed at one of the corners of the mesh. in this case, origin-based routing is equivalent to the negative-ﬁrst algorithm derived from the turn model. we know from chapter 4 that such routing algorithms are only partially adaptive. placing the origin at the center improves adaptivity, but increases hot spot contention in the vicinity.
the fault-tolerant implementation of origin-based routing requires the fault regions to be square. square regions are formed by starting from the rectangular fault regions created by preceding techniques and disabling additional nodes to ensure that the fault regions are square. a ﬁnal step in setting up the mesh is for each nonfaulty node to compute and store the distance to the nearest fault region in the north, south, east, and west directions. the time required to compute this information and form the square fault regions depends on the network size, thus being bounded by the diameter of the mesh. finally an origin node is selected such that the row and column containing the origin has no faulty nodes. if this is not possible, these techniques can be applied to the largest fault-free submesh, disabling all remaining nodes. central to this routing algorithm is the identiﬁcation of a set of nodes in the diagonal band of the destination. the diagonal band of a node s is the set of all nodes in the outbox of s that lie on a tridiagonal band toward the origin. the x and y offsets from node s to any node in the diagonal band differ by at most 1. an example of a diagonal band is shown in figure 6.25. the useful property of nodes in the diagonal band is that there exists a fault-free path from any one of these nodes to the destination node using channels in the out network [145]. such nodes are referred to as safe nodes. the property of safety of nodes in the diagonal band is ensured by square fault regions. if fault regions are square, then any message at a safe node that is blocked by a fault region is guaranteed the existence of a path around the fault region, through nodes within the outbox, and reaching a destination node on the other side of the fault region. square fault regions guarantee the safety of nodes in the diagonal band.
algorithm: fault-tolerant routing using safety levels input: message (message, destination), tag = current node ⊕ destination, and d is the distance to the destination procedure: 1. if tag = 0, then the destination has been reached. 2. if there is at least one preferred neighbor with a safety level ≥ d − 1, send 3. if there is at least one spare neighbhor with a safety level ≥ d + 1, update the
this example illustrates how the use of unsafe labels affects routing with the fault pattern shown in figure 6.8(a). figure 6.9 shows nodes that become labeled unsafe. the solid line illustrates the path that is now taken by a message that is routed from s1 to d1. note that the routing of messages through unsafe nodes is avoided. the ﬁgure also shows paths taken by messages from nodes s2 to d2 and from nodes s3 to d3. note that the former message is forwarded through an unsafe node. such routing decisions are preferred to misrouting. the message originating from node s3 must be misrouted since all shortest paths to the destination are blocked by faults.
the concept of a safe node captures the fault distribution within a neighborhood. a node is safe if all of the neighboring nodes and links are nonfaulty. this concept of safety can be extended to capture multiple distinct levels of safety in binary hypercubes [351]. a node has a safety level of k if any node at a distance of k can be reached by a path of length k. safety levels can be viewed as a form of nonlocal state information. the location and distribution of faults is encoded in the safety level of a node. the exact locations of the faulty nodes are not speciﬁed. however, in practice, knowledge of fault locations is for the purpose of ﬁnding (preferably) minimal-length paths. safety levels can be used to realize this goal by exploiting the following properties. if the safety level of a node is s, then for any message received by this node and destined for a node no more than distance s away, at least one neighbor on a shortest path to the destination will have a safety level of (s − 1). the neighbors on a shortest path to the destination will be referred to as preferred neighbors [351] and the remaining neighbors as spare neighbors. clearly, if the message cannot be delivered to the destination, the safety levels of all of the preferred neighbors will be < d − 1, where d is the distance to the destination, and that of the spare neighbors will be < d. once the safety levels have been computed for each node, a message may be routed using the sequence of steps shown in figure 6.11.
a faulty node has a safety level of 0. a safe node has a safety level of n (the diameter of the network). all nonfaulty nodes are initialized to a safety level of n. the levels of
although there are many similarities between interconnection networks for multicomputers and dsms, it is important to keep in mind that performance requirements may be very different. messages are usually very short when dsms are used. additionally, network latency is important because memory access time depends on that latency. however, messages are typically longer and less frequent when using multicomputers. usually the programmer is able to adjust the granularity of message communication in a multicomputer. on the other hand, interconnection networks for multicomputers and nows are mainly used for message passing. however, the geographical distribution of workstations usually imposes constraints on the way processors are connected. also, individual processors may be connected to or disconnected from the network at any time, thus imposing additional design constraints.
interconnection networks play a major role in the performance of modern parallel computers. there are many factors that may affect the choice of an appropriate interconnection network for the underlying parallel computer. these factors include the following:
1. performance requirements. processes executing in different processors synchronize and communicate through the interconnection network. these operations are usually performed by explicit message passing or by accessing shared variables. message latency is the time elapsed between the time a message is generated at its source node and the time the message is delivered at its destination node. message latency directly affects processor idle time and memory access time to remote memory locations. also, the network may saturate—it may be unable to deliver the ﬂow of messages injected by the nodes, limiting the effective computing power of a parallel computer. the maximum amount of information delivered by the network per time unit deﬁnes the throughput of that network.
2. scalability. a scalable architecture implies that as more processors are added, their memory bandwidth, i/o bandwidth, and network bandwidth should increase proportionally. otherwise the components whose bandwidth does not scale may become a bottleneck for the rest of the system, decreasing the overall efﬁciency accordingly.
3. incremental expandability. customers are unlikely to purchase a parallel computer with a full set of processors and memories. as the budget permits, more processors and memories may be added until a system’s maximum conﬁguration is reached. in some interconnection networks, the number of processors must be a power of 2, which makes them difﬁcult to expand. in other cases, expandability is provided at the cost of wasting resources. for example, a network designed for a maximum
in addition to the four basic types of collective communication services, some collective communication services require the combination of these basic services. some of these frequently used collective communication services, referred to as convenient or composite collective communication services, are listed below:
all combining. the result of a reduce operation is available to all processes. this is also referred to as a reduce and spread operation. the result may be broadcast to all processes after the reduce operation, or multiple reduce operations are performed with each process as a root.
barrier synchronization. a synchronization barrier is a logical point in the control ﬂow of an algorithm at which all processes in a process group must arrive before any of the processes in the group are allowed to proceed further. obviously, barrier synchronization involves a reduce operation followed by a broadcast operation.
scan. a scan operation performs a parallel preﬁx with respect to a commutative and associative combining operator on messages in a process group. figure 5.5(a) shows a parallel preﬁx operation in a four-member process group with respect to the associative combining operator f . apparently, a scan operation involves many reduce operations. the reverse (or downward) of parallel preﬁx is called parallel sufﬁx, as shown in figure 5.5(b).
collective communication services are demanded in many scientiﬁc applications. such services have been supported by several communication packages for multicomputers. however, efﬁcient implementation of various collective communication services is machine dependent. the next section will describe system support for collective communication.
in all-to-one communication, all processes in a process group are senders and one process (called the root) is identiﬁed as the sole receiver. again, there are two distinct services:
reduce. different messages from different senders are combined together to form a single message for the receiver. the combining operator is usually commutative and associative, such as addition, multiplication, maximum, minimum, and the logical or, and, and exclusive or operators. this service is also referred to as personalized combining or global combining.
gather. different messages from different senders are concatenated together for the receiver. the order of concatenation is usually dependent on the id of the senders.
the ﬁrst term in tset up is the time taken for the header ﬂit to reach the destination. the second term is the time taken for the acknowledgment ﬂit to reach the source node. we then have tdat a as the time for pipelining the data ﬂits into the destination network interface. the ﬁrst term is the time for the ﬁrst data ﬂit to reach the destination. the second term is the time required to receive the remaining ﬂits. the message pipeline cycle time is determined by the maximum of the switch delay and wire delay.
scouting switching is a hybrid message ﬂow control mechanism that can be dynamically conﬁgured to provide speciﬁc trade-offs between fault tolerance and performance. in pcs the ﬁrst data ﬂit is injected into the network only after the complete path has been set up. in an attempt to reduce pcs path setup time overhead, in scouting switching the ﬁrst data ﬂit is constrained to remain at least k links behind the routing header. when k = 0, the ﬂow control is equivalent to wormhole switching, while large values can ensure path setup prior to data transmission (if a path exists). intermediate values of k permit the data ﬂits to follow the header at a distance, while still allowing the header to backtrack if the need arises. therefore, when the header reaches the destination, the ﬁrst data ﬂit arrives shortly thereafter rather than immediately (as in wormhole switching). figure 2.24 illustrates a time-space diagram for messages being pipelined over three links using scouting switching (k = 2). the parameter, k, is referred to as the scouting distance or probe lead. every time a channel is successfully reserved by the routing header, a positive acknowledgment is returned in the opposite direction.as a particular case, positive acknowledgments are continuously transmitted when the routing header has reached the destination node. associated with each virtual channel is a programmable counter. the counter associated with the virtual channel reserved by a header is incremented when a positive acknowledgment is received and is decremented when a negative acknowledgment is received. when the value of the counter is equal to k, data ﬂits are allowed to advance. as acknowledgments ﬂow in the direction opposite to the routing header, the gap between the header and the ﬁrst data ﬂit can grow up to a maximum of 2k − 1 links while the header is advancing. if the routing header backtracks, a negative acknowledgment is transmitted. for performance reasons, when k = 0 no acknowledgments are sent across the channels. in this case, data ﬂits immediately follow the header ﬂit and ﬂow control is equivalent to wormhole switching. for example, in figure 2.25 a message is being transmitted between nodes a and g and k = 2. the initial path attempted by the header is row ﬁrst. data ﬂits remain at least
the ﬁrst term in tset up is the time taken for the header ﬂit to reach the destination. the second term is the time taken for the acknowledgment ﬂit to reach the source node. we then have tdat a as the time for pipelining the data ﬂits into the destination network interface. the ﬁrst term is the time for the ﬁrst data ﬂit to reach the destination. the second term is the time required to receive the remaining ﬂits. the message pipeline cycle time is determined by the maximum of the switch delay and wire delay.
scouting switching is a hybrid message ﬂow control mechanism that can be dynamically conﬁgured to provide speciﬁc trade-offs between fault tolerance and performance. in pcs the ﬁrst data ﬂit is injected into the network only after the complete path has been set up. in an attempt to reduce pcs path setup time overhead, in scouting switching the ﬁrst data ﬂit is constrained to remain at least k links behind the routing header. when k = 0, the ﬂow control is equivalent to wormhole switching, while large values can ensure path setup prior to data transmission (if a path exists). intermediate values of k permit the data ﬂits to follow the header at a distance, while still allowing the header to backtrack if the need arises. therefore, when the header reaches the destination, the ﬁrst data ﬂit arrives shortly thereafter rather than immediately (as in wormhole switching). figure 2.24 illustrates a time-space diagram for messages being pipelined over three links using scouting switching (k = 2). the parameter, k, is referred to as the scouting distance or probe lead. every time a channel is successfully reserved by the routing header, a positive acknowledgment is returned in the opposite direction.as a particular case, positive acknowledgments are continuously transmitted when the routing header has reached the destination node. associated with each virtual channel is a programmable counter. the counter associated with the virtual channel reserved by a header is incremented when a positive acknowledgment is received and is decremented when a negative acknowledgment is received. when the value of the counter is equal to k, data ﬂits are allowed to advance. as acknowledgments ﬂow in the direction opposite to the routing header, the gap between the header and the ﬁrst data ﬂit can grow up to a maximum of 2k − 1 links while the header is advancing. if the routing header backtracks, a negative acknowledgment is transmitted. for performance reasons, when k = 0 no acknowledgments are sent across the channels. in this case, data ﬂits immediately follow the header ﬂit and ﬂow control is equivalent to wormhole switching. for example, in figure 2.25 a message is being transmitted between nodes a and g and k = 2. the initial path attempted by the header is row ﬁrst. data ﬂits remain at least
some changes in notation. those changes will be summarized in section 3.2.3. in this case, a few central buffers deep enough to store one or more packets are used. as above, a channel will only accept a new packet if there is enough buffer space to store the whole packet. buffer space must be reserved before starting packet transmission, thus preventing other channels from reserving the same buffer space. the message ﬂow control protocol is responsible for ensuring the availability of buffer space and arbitrating between concurrent requests for space in the central queues.
as we will see in section 3.2.3, it is also possible to consider models that mix both kinds of resources, edge buffers and central queues. it will be useful in section 3.6. in this case, each node has edge buffers and central queues. the routing function determines the resource to be used in each case. this mixed model may consider either ﬂit buffers or packet buffers, depending on the switching technique. for the sake of clarity, we will restrict deﬁnitions and examples to use only edge buffers. results can be easily generalized by introducing the changes in notation indicated in section 3.2.3.
the interconnection network i is modeled by using a strongly connected directed graph with multiple arcs, i = g(n, c). the vertices of the graph n represent the set of processing nodes. the arcs of the graph c represent the set of communication channels. more than a single channel is allowed to connect a given pair of nodes. bidirectional channels are considered as two unidirectional channels. we will refer to a channel and its associated edge buffer indistinctly. the source and destination nodes of a channel ci are denoted si and di, respectively.
a routing algorithm is modeled by means of two functions: routing and selection. the routing function supplies a set of output channels based on the current and destination nodes. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied. if all the output channels are busy, the packet will be routed again until it is able to reserve a channel, thus getting the ﬁrst channel that becomes available. as we will see, the routing function determines whether the routing algorithm is deadlock-free or not. the selection function only affects performance. note that, in our model, the domain of the routing function is n × n because it only takes into account the current and destination nodes. thus, we do not consider the path followed by the packet while computing the next channel to be used. we do not even consider the input channel on which the packet arrived at the current node. the reason for this choice is that it enables the design of practical routing protocols for which speciﬁc properties can be proven. these results may not be valid for other routing functions. furthermore, this approach enables the development of methodologies for the design of fully adaptive routing protocols (covered in chapter 4). these methodologies are invalid for other routing functions. thus, this choice is motivated by engineering practice that is developed without sacriﬁce in rigor. other deﬁnitions of the routing function will be considered in section 3.2.
while establishing a path between source and destination nodes, the routing algorithm may supply a single path (deterministic routing). when source routing is used, this path is computed at the source node without considering network trafﬁc. when deterministic routing is implemented in a distributed way, the routing algorithm supplies a single routing choice at each intermediate node, based on current and destination node addresses. in this case, channel status is not considered while computing the output channel to be used. deterministic routing algorithms for regular topologies are simple. when implemented in hardware using a ﬁnite-state machine, routing logic is compact and fast. most commercially available multicomputers use distributed deterministic routing. deterministic routing algorithms usually perform well under uniform trafﬁc. however, performance is poor when trafﬁc is not uniform, especially when some pairs of nonneighbor nodes exchange information very frequently.
alternatively, adaptive routing algorithms consider network state while making a decision. as indicated above, it is not interesting to combine source routing and adaptive routing. thus, in what follows, we only consider distributed routing. although some authors considered nonlocal information [289], most proposals only use local information for efﬁciency reasons.as seen in chapter 3, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. a selection from this set is made by the selection function based on the status of output channels at the current node. this selection is performed in such a way that a free channel (if any) is supplied.as a consequence, adaptive routing algorithms are able to follow alternative paths instead of waiting on busy channels. thus, these algorithms increase routing ﬂexibility at the expense of a more complex and slower hardware. several experimental parallel computers use adaptive routing [74]. also, commercial machines with adaptive routing are being developed [257] or are even available in the market [313]. example 3.8 describes a distributed adaptive routing algorithm.
adaptive routing algorithms can be classiﬁed as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms allow the header to backtrack, releasing previously reserved channels. backtracking algorithms systematically search the network, backtracking as needed, using history information to ensure that no path is searched more than once. note that in most switching techniques, data immediately follow the packet header. in these switching techniques, backtracking is not possible without a very complex hardware support. however, limited backtracking (one channel) is possible with some hardware support [181]. on the other hand, pipelined circuit switching is very well suited for backtracking because data ﬂits do not follow the header immediately, giving more freedom to the header to search the network. backtracking algorithms are mainly used for fault-tolerant routing, as will be seen in chapter 6.
at a lower level, routing algorithms can be classiﬁed as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. misrouting algorithms may also supply channels that send the packet away from its destination. misrouting algorithms are based on an optimistic view of the network: taking an unproﬁtable channel is likely to bring the header to another set of proﬁtable
using new channels belonging to minimal paths, and never using original channels. when a message arrives at a switch from another switch through a new channel, the routing function gives a higher priority to the new channels belonging to minimal paths. if all of them are busy, then the routing algorithm selects an original channel belonging to a minimal path (if any). to ensure that the new routing function is deadlock-free, if none of the original channels provides minimal routing, then the original channel that provides the shortest path will be used. once a message reserves an original channel, it will be routed using this kind of channel according to the original routing function until it is delivered. this enhanced design methodology was proposed in [319] and can be applied to any routing algorithm that avoids deadlock by prohibiting cyclic dependencies between channels. in particular, it can be applied to networks using up/down routing simply by splitting physical channels into two virtual channels (or duplicating each physical channel) and using up/down routing as the original routing function. by restricting the use of original channels as indicated above, most messages are allowed to follow minimal paths, and therefore a more efﬁcient use of resources is made. also, adaptivity is considerably increased with respect to the original up/down routing algorithm. as a result, latency decreases signiﬁcantly, and the network is able to deliver a throughput several times higher than the one achieved by the original up/down routing algorithm [319] (see section 9.8).
in this section we brieﬂy describe several policies proposed in the literature for the allocation of network resources. resource allocation policies can be grouped into two classes: those required to select a resource for a packet when several resources are available, and those required to arbitrate between several packets contending for the same resource. the ﬁrst class of policies is usually required to select a channel or buffer among the options offered by an adaptive routing function. the function that performs this selection is usually referred to as a selection function. the second class of policies is usually required to allocate resources like the routing control unit (the circuit that computes the routing algorithm) or the bandwidth of a physical channel when it is multiplexed among several virtual channels. both classes of policies are studied in the next sections.
without loss of generality, let us assume that when a packet is routed, the network resources requested by that packet are the output channels at the current node. as indicated in section 4.1, adaptive routing algorithms can be decomposed into two functions: routing and selection. the routing function supplies a set of output channels based on the current node or buffer and the destination node. the selection function selects an output channel from the set of channels supplied by the routing function. this selection can be random. in this case, network state is not considered (oblivious routing). however, the selection
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
header that contains information necessary to implement a novel sender-based protocol for low-latency messaging. a 4-byte crc trailer rounds out the message packet format. two types of messages are supported: normal messages and priority messages. while normal messages may be as large as 64k packets in length, priority messages are limited to a single packet. typically priority messages are used by the operating system, and normal messages are generated by the applications.
a block diagram of the r2 router is illustrated in figure 7.35. in the ﬁgure the ports are drawn as distinct inputs and outputs to emphasize the data path, although they are bidirectional. each fifo buffer is capable of storing one maximum-length packet. there are 12 fifo buffers for normal packets and 3 fifo buffers for storing priority packets. two 7 × 15 crossbars provide switching between the buffers and the bidirectional ports. each of the fifo buffers has a routing controller. routing decisions use the destination address in the packet header and the local node id. buffer management and ordered use of buffers provides deadlock freedom. the implementation is self-timed, and the time to receive a packet header and to compute the address of an output port is estimated to take between 60 and 120 ns.
routing is controlled at the sender by specifying a number of adaptive credits. each adaptive credit permits the message packet to traverse one link that takes the packet no further from the destination. at any node, there are always two no-farther directions. the adaptive credit ﬁeld in the header is decremented each time a message takes an adaptive step, and further adaptive routing is not permitted once this value is reduced to zero. shortest paths are given priority in packet routing. priority packets are only routed along shortest paths. when all outputs for a priority packet are blocked by a normal packet, the priority packet is embedded in the normal packet byte stream being transmitted along one of the shortest paths, analogous with the implementation of virtual channels.an interesting feature of the r2 router is the support for message throttling. packet headers are monitored,
and the messages sent within a particular step do not contend for common channels. in practice, however, the message-passing steps of the multicast operation may not be ideally synchronized, and contention may arise among messages sent in different steps. as indicated in section 5.2, start-up latency includes system call time at both the source and destination nodes; these latencies are termed the sending latency and receiving latency, respectively. if these latencies are large relative to the network latency, messages can be sent concurrently, but in different steps. for example, assuming that both sending latency and receiving latency have the same value, denoted by t, and that network latency is negligible, the labels in figure 5.51(d) indicate when a copy of the message will enter and leave each node. assuming a one-port architecture, the latency between two consecutive sends at a particular node is t. leaf nodes in the tree do not send messages, and therefore encounter only receiving latency. for other destinations (intermediate nodes in the tree), both receiving latency and sending latency are incurred. under these conditions, node (1, 0) may not have ﬁnished receiving the message from node (2,1) until after node (2, 1) has ﬁnished sending to node (3, 2) and started sending to node (1, 3). if node (1,0) sends to node (1,2) at this time, 3t, then contention will occur for the [(1, 1), (1, 2)] channel. the multicast tree in figure 5.51(e), which is based on the methods presented in the following sections, is contention-free regardless of message length or receiving latency.
developing an algorithm that produces minimum-time, contention-free multicast implementations for a speciﬁc system requires a detailed understanding of potential conﬂicts among messages, which in turn are dependent on the routing algorithm used. this section formulates a method to avoid contention among unicast messages under the most common routing algorithm for wormhole-switched n-dimensional meshes, namely, dimensionorder routing. this method was presented in [234].
a few preliminaries are in order. a node address x in a ﬁnite n-dimensional mesh is represented by σn−1(x)σn−2(x) . . . σ0(x). under a minimal deterministic routing algorithm, all messages transmitted from a node x to a node y will follow a unique shortest path between the two nodes. let such a path be represented as p (x, y) = (x; z1, z2, . . . , zk; y), where the zis are the sequence of intermediate routers visited by the message. in order to simplify the presentation, we let z0 = x and zk+1 = y.
in order to characterize contention among messages transmitted under dimensionorder routing, an ordering on nodes in an n-dimensional mesh is needed. the multicast algorithms described herein are based on lexicographic ordering of the source and destination nodes according to their address components. actually, two such orderings are possible: one in which the subscripts of address components increase from right to left, and another in which the subscripts are reversed. which ordering is appropriate for multicasting in a given system depends on whether addresses are resolved, under dimension-order routing, in decreasing or increasing dimension order. here it is assumed
of the message from the source to every destination. depending on the number of destinations, such separate addressing may require excessive time, particularly in a one-port architecture in which a local processor may send only one message at a time. assuming that the start-up latency dominates the communication latency, separate addressing achieves a communication latency that increases linearly with the number of destinations.
performance may be improved by organizing the unicast messages as a multicast tree, whereby the source node sends the message directly to a subset of the destinations, each of which forwards the message to one or more other destinations. eventually, all destinations will receive the message.
the potential advantage of tree-based communication is apparent from the performance of various broadcast methods. for example, in the spanning binomial tree algorithm described in section 5.5.2, the number of nodes that already received the broadcast message is doubled after each step. hence, assuming a one-port architecture, communication latency increases logarithmically with the number of destinations.
it is possible to reduce latency even more if the router interface has several ports, allowing nodes to inject several messages simultaneously. however, in this section we will only consider algorithms for a one-port architecture because most commercial multicomputers have a single port. see [232] and [153, 231, 295, 342] for a survey and detailed descriptions of multicast/broadcast algorithms for multiport architectures, respectively.
which types of multicast trees should be used depends on the switching technique and unicast routing algorithm. the following features are desirable in the software implementation of a multicast tree:
2. the implementation should exploit the small distance sensitivity of wormhole 3. the height of the multicast tree should be minimal. speciﬁcally, for m − 1
4. there should be no channel contention among the constituent messages of the multicast. in other words, the unicast messages involved should not simultaneously require the same channel. note that this feature does not eliminate contention with other unicast or multicast messages.
how to achieve these goals depends on the switching technique and unicast routing algorithm of the network. although the user has no control over the routing of individual
network addresses to routes and performs the header generation. in any myrinet network, one of the interfaces serves as the mapper: it is responsible for sending mapping packets to other interfaces. this map of the network is distributed to all of the other interfaces. now all routes can be created locally by each interface. the network maps are computed in a manner that is guaranteed to provide only deadlock-free routes. the attractive feature of the myrinet network is that should the mapper interface be removed or faulty links separate the network, a new mapper is selected automatically. periodic remapping of the network is performed to detect faulty links and switches, and this information is distributed throughout the network.
myrinet designs evolved to a three-chip set. the single-chip crossbar is used for system area networks (sans) where the distances are less than 3 meters. for longer distances, custom vlsi chips convert to formats suitable for distances of up to 8 meters typically used in lans. a third chip provides a 32-bit fifo interface for conﬁgurations as classical multicomputer nodes. switches can be conﬁgured with combinations of san and lan ports, producing conﬁgurations with varying bisection bandwidth and power requirements. the building blocks are very ﬂexible and permit a variety of multiprocessor conﬁgurations as well as opportunities for subsequently growing an existing network. for example, a myrinet switch conﬁgured as a lan switch has a cut-through latency of 300 ns, while a san switch has a cut-through latency of 100 ns. thus, small tightly coupled multiprocessor systems can be conﬁgured with commercial processors to be used as parallel engines. alternatively, low-latency communication can be achieved within existing workstation and pc clusters that may be distributed over a larger area.
servernet [15, 155, 156, 157] is a san that provides the interconnection fabric for supporting interprocessor, processor-i/o, and i/o-i/o communication. the goal is to provide a ﬂexible interconnect fabric that can reliably provide scalable bandwidth.
the implementation of the unique token protocol for reliable transmission necessitates some special handling and architectural support. the token at the end of a message must be forwarded only after the corresponding ﬂit queue has successfully emptied. retransmission on failure involves generation of duplicate tokens. finally, ﬂow control must span two routers to ensure that a duplicate copy of each data ﬂit is maintained at all times in adjacent routers. when a data ﬂit is successfully transmitted across a channel, the copy of the data ﬂit two routers upstream must be deallocated. the relevant ﬂow control signals are passed through frames as captured in figure 7.25.
the sgi spider (scalable pipelined interconnect for distributed endpoint routing) [117] was designed with multiple missions in mind: as part of a conventional multiprocessor switch fabric, as a building block for large-scale nonblocking central switches, and as a high-bandwidth communication fabric for distributed graphics applications. the physical links are full-duplex channels with 20 data bits, a frame signal, and a differential clock signal in each direction. data are transferred on both edges of a 200 mhz clock, realizing a raw data rate of 1 gbyte/s in each direction. the chip core operates at 100 mhz, providing 80-bit units that are serialized into 20-bit phits for transmission over the channel. the organization of the spider chip is shown in figure 7.27.
the router implements four 256-byte virtual channels over each physical link. the units of buffer management and transmission are referred to as micropackets, equivalent to the notion of a ﬂit. each micropacket is comprised of 128 bits of data, 8 bits of sequencing
to reliability, interconnection networks should have a modular design, allowing hot upgrades and repairs. nodes can also fail or be removed from the network. in particular, a node can be powered off in a network of workstations. thus, nows usually require some reconﬁguration algorithm for the automatic reconﬁguration of the network when a node is powered on or off.
9. expected workloads. users of a general-purpose machine may have very different requirements. if the kind of applications that will be executed in the parallel computer are known in advance, it may be possible to extract some information on usual communication patterns, message sizes, network load, and so on. that information can be used for the optimization of some design parameters. when it is not possible to get information on expected workloads, network design should be robust; that is, design parameters should be selected in such a way that performance is good over a wide range of trafﬁc conditions.
10. cost constraints. finally, it is obvious that the “best” network may be too expensive. design decisions often are trade-offs between cost and other design factors. fortunately, cost is not always directly proportional to performance. using commodity components whenever possible may considerably reduce the overall cost.
among other criteria, interconnection networks have been traditionally classiﬁed according to the operating mode (synchronous or asynchronous) and network control (centralized, decentralized, or distributed). nowadays, multicomputers, multiprocessors, and nows dominate the parallel computing market. all of these architectures implement asynchronous networks with distributed control. therefore, we will focus on other criteria that are currently more signiﬁcant.
a classiﬁcation scheme is shown in figure 1.2, which categorizes the known interconnection networks into four major classes based primarily on network topology: shared-medium networks, direct networks, indirect networks, and hybrid networks. for each class, the ﬁgure shows a hierarchy of subclasses, also indicating some real implementations for most of them. this classiﬁcation scheme is based on the classiﬁcation proposed in [253], and it mainly focuses on networks that have been implemented. it is by no means complete, as other new and innovative interconnection networks may emerge as technology further advances, such as mobile communication and optical interconnections. in shared-medium networks, the transmission medium is shared by all communicating devices. an alternative to this approach consists of having point-to-point links directly connecting each communicating device to a (usually small) subset of other communicating devices in the network. in this case, any communication between nonneighboring devices requires transmitting the information through several intermediate devices. these networks are known as direct networks. instead of directly connecting the communicating devices between them, indirect networks connect those devices by means of one or more
switches. if several switches exist, they are connected between them by using point-topoint links. in this case, any communication between communicating devices requires transmitting the information through one or more switches. finally, hybrid approaches are possible. these network classes and the corresponding subclasses will be described in the following sections.
the least complex interconnect structure is one in which the transmission medium is shared by all communicating devices. in such shared-medium networks, only one device is allowed to use the network at a time. every device attached to the network has requester, driver, and receiver circuits to handle the passing of addresses and data. the network itself is usually passive, since the network itself does not generate messages.
an important issue here is the arbitration strategy that determines the mastership of the shared-medium network to resolve network access conﬂicts. a unique characteristic of a shared medium is its ability to support atomic broadcast, in which all devices on the medium can monitor network activities and receive the information transmitted on the shared medium. this property is important to efﬁciently support many applications requiring one-to-all or one-to-many communication services, such as barrier synchronization and snoopy cache coherence protocols. due to limited network bandwidth, a single shared medium can only support a limited number of devices before the medium becomes a bottleneck.
shared-medium networks constitute a well-established technology.additionally, their limited bandwidth restricts their use in multiprocessors. so, these networks will not be covered in this book, but we will present a short introduction in the following sections. there are two major classes of shared-medium networks: local area networks, mainly used to construct computer networks that span physical distances no longer than a few kilometers, and backplane buses, mainly used for internal communication in uniprocessors and multiprocessors.
high-speed lans can be used as a networking backbone to interconnect computers to provide an integrated parallel and distributed computing environment. physically, a sharedmedium lan uses copper wires or ﬁber optics in a bit-serial fashion as the transmission medium. the network topology is either a bus or a ring. depending on the arbitration mechanism used, different lans have been commercially available. for performance and implementation reasons, it is impractical to have a centralized control or to have some ﬁxed access assignment to determine the bus master who can access the bus. three major classes of lans based on distributed control are described below.
programming multicomputers is not an easy task. the programmer has to take care of distributing code and data among the processors in an efﬁcient way, invoking messagepassing calls whenever some data are needed by other processors. on the other hand, shared-memory multiprocessors provide a single memory space to all the processors, simplifying the task of exchanging data among processors. access to shared memory has been traditionally implemented by using an interconnection network between processors and memory (figure 1.1(b)). this architecture is referred to as uniform memory access (uma) architecture. it is not scalable because memory access time includes the latency of the interconnection network, and this latency increases with system size.
more recently, shared-memory multiprocessors followed some trends previously established for multicomputers. in particular, memory has been physically distributed among processors, therefore reducing the memory access time for local accesses and increasing scalability. these parallel computers are referred to as distributed sharedmemory multiprocessors (dsms). accesses to remote memory are performed through an interconnection network, very much like in multicomputers. the main difference between dsms and multicomputers is that messages are initiated by memory accesses rather than by calling a system function. in order to reduce memory latency, each processor has several levels of cache memory, thus matching the speed of processors and memories. this architecture provides nonuniform memory access (numa) time. indeed, most of the nonuniformity is due to the different access time between caches and main memories, rather than the different access time between local and remote memories. the main problem arising in dsms is cache coherence. several hardware and software cache coherence protocols have been proposed. these protocols produce additional trafﬁc through the interconnection network.
the use of custom interconnects makes multicomputers and dsms quite expensive. so, networks of workstations (nows) have been proposed as an inexpensive approach to build parallel computers. nows take advantage of recent developments in lans. in particular, the use of atm switches has been proposed to implement nows. however, atm switches are still expensive, which has motivated the development of high-performance
size of 1,024 nodes may contain many unused communication links when the network is implemented with a smaller size. interconnection networks should provide incremental expandability, allowing the addition of a small number of nodes while minimizing resource wasting.
4. partitionability. parallel computers are usually shared by several users at a time. in this case, it is desirable that the network trafﬁc produced by each user does not affect the performance of other applications. this can be ensured if the network can be partitioned into smaller functional subsystems. partitionability may also be required for security reasons.
5. simplicity. simple designs often lead to higher clock frequencies and may achieve higher performance. additionally, customers appreciate networks that are easy to understand because it is easier to exploit their performance.
6. distance span. this factor may lead to very different implementations. in multicomputers and dsms, the network is assembled inside a few cabinets. the maximum distance between nodes is small. as a consequence, signals are usually transmitted using copper wires. these wires can be arranged regularly, reducing the computer size and wire length. in nows, links have very different lengths and some links may be very long, producing problems such as coupling, electromagnetic noise, and heavy link cables. the use of optical links solves these problems, equalizing the bandwidth of short and long links up to a much greater distance than when copper wire is used. also, geographical constraints may impose the use of irregular connection patterns between nodes, making distributed control more difﬁcult to implement.
7. physical constraints. an interconnection network connects processors, memories, and/or i/o devices. it is desirable for a network to accommodate a large number of components while maintaining a low communication latency. as the number of components increases, the number of wires needed to interconnect them also increases. packaging these components together usually requires meeting certain physical constraints, such as operating temperature control, wiring length limitation, and space limitation. two major implementation problems in large networks are the arrangement of wires in a limited area and the number of pins per chip (or board) dedicated to communication channels. in other words, the complexity of the connection is limited by the maximum wire density possible and by the maximum pin count. the speed at which a machine can run is limited by the wire lengths, and the majority of the power consumed by the system is used to drive the wires. this is an important and challenging issue to be considered. different engineering technologies for packaging, wiring, and maintenance should be considered.
8. reliability and repairability. an interconnection network should be able to deliver information reliably. interconnection networks can be designed for continuous operation in the presence of a limited number of faults. these networks are able to send messages through alternative paths when some faults are detected. in addition
it must be noticed that starvation is prevented using a round-robin strategy when several message headers are waiting for the router, according to assumption 7. the selection function will only affect performance.
given an interconnection network i , a routing function r, and a pair of adjacent channels ci , cj ∈ c, there is a direct dependency from ci to cj iff ∃x ∈ n such that ci ∈ r(si , x) and cj ∈ r(di , x)
a channel dependency graph d for a given interconnection network i and routing function r is a directed graph, d = g(c, e). the vertices of d are the channels of i . the arcs of d are the pairs of channels (ci , cj ) such that there is a direct dependency from ci to cj .
a conﬁguration is an assignment of a set of ﬂits to each queue. all of the ﬂits in any one queue belong to the same message (assumption 5). the number of ﬂits in the queue for channel ci is denoted size(ci ). if the ﬁrst ﬂit in the queue for channel ci is destined for node nd, then head(ci ) = nd. if the ﬁrst ﬂit is not a header and the next channel reserved by its header is cj , then next(ci ) = cj . let ch ⊆ c be the set of channels containing a header ﬂit at their queue head. let cd ⊆ c be the set of channels containing a data or tail ﬂit at their queue head. a conﬁguration is legal iff ∀ci ∈ c
received status of ﬂow control information may not reﬂect the actual status at the time it is received. this is an issue for both half-duplex and full-duplex pipelined links.
for example, the transmission of ﬂow control information to indicate the lack of buffer space arrives too late at the sender: multiple phits are already in transit and will be dropped due to lack of space. however, full link utilization can be maintained without loss with a buffer on the receiver of sufﬁcient depth to store all the phits in transit across the channel. the relationship between the buffer depth, wire delay, bandwidth, and so on can be derived as follows [106]. assume that propagation delay is speciﬁed as p ns per unit length, and the length of the wire in the channel is l units. when the receiver requests the sender to stop transmitting, there may be lpb phits in transit on a channel running at b gphits/s. in addition, by the time the ﬂow control signal propagates to the sender, an additional lpb phits may be placed in the channel. finally, if the processing time at the receiver is f ns (e.g., ﬂow control operation latency), this delay permits the sender to place another fb phits on the channel. the available buffer space in the receiver must be large enough to receive all of these phits when a stop signal is transmitted to the sender. these slack buffers [315] serve to hide the wire latency and the ﬂow control latency. the size of the buffers can be estimated as follows.
in cases where long wire latencies dominate, the use of link pipelining favors the use of full-duplex channels, particularly under high loads. this is typically the case in the emerging class of low-latency interconnects for workstation/pc clusters. to reduce the wiring costs of long interconnects as well as maintain high link utilization, full-duplex channel techniques have been developed using slack buffers and pin-efﬁcient ﬂow control. an example is described in example 7.3.
the structure of an example full-duplex channel is shown in figure 7.12. this channel is used in the routing fabric of tandem’s servernet [155] and differs from the preceding example in several ways. physical channel ﬂow control is synchronous, and the clock is transmitted across the link. at the transmitting end of the channel, the clock is sent through similar delay paths as the data so
software messaging layer. a high percentage of the communication latency in multicomputers is produced by the overhead in the software messaging layer. reducing or hiding this overhead is likely to have a higher impact on performance than the remaining design parameters, especially when messages are relatively short. the use of vcc can eliminate or hide most of the software overhead by overlapping path setup with computation, and caching and retaining virtual circuits for use by multiple messages. vcc complements the use of techniques to reduce the software overhead, like active messages. it should be noted that software overhead is so high in some multicomputers that it makes no sense improving other design parameters. however, once this overhead has been removed or hidden, the remaining design parameters become much more important.
software support for collective communication. collective communication operations beneﬁt considerably from using speciﬁc algorithms. using separate addressing, latency increases linearly with the number of participating nodes. however, when algorithms for collective communication are implemented in software, latency is considerably reduced, increasing logarithmically with the number of participating nodes. this improvement is achieved with no additional hardware cost and no overhead for unicast messages.
number of ports. if the software overhead has been removed or hidden, the number of ports has a considerable inﬂuence on performance, especially when messages are sent locally. if the number of ports is too small, the network interface is likely to be a bottleneck for the network. the optimal number of ports heavily depends on the spatial locality of trafﬁc patterns.
switching technique. nowadays, most commercial and experimental parallel computers implement wormhole switching. although vct switching achieves a higher throughput, the performance improvement is small when virtual channels are used in wormhole switching. additionally, vct switching requires splitting messages into packets not exceeding buffer capacity. if messages are longer than buffer size, wormhole switching should be preferred. however, when messages are shorter than or equal to buffer size, vct switching performs slightly better than wormhole switching, also simplifying deadlock avoidance. this is the case for distributed, shared-memory multiprocessors. vct switching is also preferable when it is not easy to avoid deadlock in wormhole switching (multicast routing in multistage networks) or in some applications requiring real-time communication [294]. finally, a combination of wormhole switching and circuit switching with wave-pipelined switches and channels has the potential to increase performance, especially when messages are very long [101].
packet size. for pipelined switching techniques, ﬁlling the pipeline produces some overhead. also, routing a header usually takes longer than transmitting a data ﬂit
mttr << mtbf, the number of faulty components in a repair interval is small. in fact, the probability of the second or the third fault occurring before the ﬁrst fault is repaired is very low. in such environments, software-based rerouting can be a cost-effective and viable alternative [333].
the software-based approach is based on the observation that the majority of messages do not encounter faults and should be minimally impacted, while the relatively few messages that do encounter faults may experience substantially increased latency, although the network throughput may not be signiﬁcantly affected. the basic idea is simple. when a message encounters a faulty link, it is removed from the network or absorbed by the local router and delivered to the messaging layer of the local node’s operating system. the message-passing software either (1) modiﬁes the header so that the message may follow an alternative path or (2) computes an intermediate node address. in either case, the message is reinjected into the network. in the case that the message is transmitted to an intermediate node, it will be forwarded upon receipt to the ﬁnal destination. a message may encounter multiple faults and pass through multiple intermediate nodes. the issues are distinct from adaptive packet routing in networks using packet switching or vct switching. since messages are routed using wormhole switching, rerouting algorithms and the selection of intermediate nodes must consider dependencies across multiple routers caused by small buffers (< message size) and pipelined data ﬂow. since messages are reinjected into the network, dependencies are introduced between delivery channels at a router and the injection channels. these dependencies are introduced via local storage for absorbed packets. memory allocation must ensure that sufﬁcient buffer space can be allocated within the nodes or interfaces to avoid creating deadlock due to the introduction of these dependencies.
since messages are removed from the network and reinjected, the problems of routing in the presence of concave fault regions are simpliﬁed. consider the case of messages being routed obliviously using e-cube routing in a 2-d torus. figure 6.28 illustrates an example of software-based rerouting in the presence of concave fault regions. the resulting routing algorithm has been referred to as e-sft [333]. in step 1 a message from the source is routed through an e-cube path to the destination and encounters the fault region at node a. the message is absorbed at the node, and the header is modiﬁed by the messaging layer to reﬂect the reverse path in the same dimension, using the wraparound channels. the message again encounters the fault region at node b and is routed to an intermediate node in the vertical direction in an attempt to ﬁnd a path around the fault region. this procedure is repeated at intermediate nodes e and f before the message is successfully routed along the wraparound channels in the vertical dimension to the destination node d. while the path and the sequence of routing decisions to ﬁnd the path in the preceding example are easy to convey intuitively, the messaging layer must implement routing algorithms that can implement this decision process in a fully distributed manner with only local information about faults. these rerouting decisions must avoid deadlock and livelock. this is realized through the use of routing tables and by incorporating additional information in the routing header to capture the history of encounters with fault regions by this message. the important characteristic of this approach is that messages are still routed in dimension order between any pair of intermediate nodes.
algorithm: fault-tolerant origin-based routing in 2-d meshes procedure: 1. setup: source node s and destination node d. all fault-free nodes compute the distance to the nearest fault region and nearest safe node in each dimension. 2. phase 1: the message is adaptively routed in the in network from the source node until the message header arrives at a node in the outbox of d. if necessary, routing choices may be limited to shortest paths.
b. if a safe node is closer than the nearest faulty node in any direction, route the message to this node using only channels in out or only channels in in.
b. if the message is blocked by a fault, route the message along the faulty region until it can be turned back toward the nodes in the diagonal band. all nodes in the diagonal band are safe.
of fault regions in n-dimensional mesh networks where any 2-d cross section of the fault region produces a single rectangular fault region. such a fault model is referred to as a solid fault model [50]. figure 6.27 provides an example of a nonconvex fault region that follows the solid fault model, and a message being routed along a fault ring around the fault region. as in previous techniques, for nonoverlapping fault rings and nonfaulty boundary nodes, message types are distinguished by the relative position of the destination when the message is generated. for deterministic routing, we initially have ew and we messages. when the message eventually arrives at the destination column, the message type is changed to ns or sn depending on the relative location of the destination. when a message encounters a fault, the rules for routing the message along the fault ring are shown in table 6.2. there are four virtual channels over each physical channel: c0, c1, c2, and c3. as in preceding techniques, each set of channels implements a distinct virtual network.
if a message must travel along a fault ring before encountering a fault region (i.e., as shown in figure 6.27 at node a), then the message must continue to be routed in the same direction along the fault ring. otherwise the message follows the direction stated in table 6.2. each message type is routed in a distinct virtual network. from the routing rules we can observe that the channel dependency graph within a virtual network is acyclic.
the system [173]. this is unique for each multicast message. if each multicast constructs its tree based on the position of its corresponding source, then all the multicasts will end up with trees as unique as possible.
in the hl algorithm the multicast tree is generated using primarily the destination (distribution) information. it depends, to a very small degree, on the position of the source. so, it can be improved by choosing the leader sets for a multicast depending on the position of its source. in the source quadrant-based hierarchical leader (sqhl) scheme [173], groupings are done exactly as in the hl scheme. however, the leader nodes are chosen in a different way. consider a k-ary n-mesh. let si be the ith coordinate of the source node s of a multicast. consider the set of destinations or leaders of the previous level that only
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
message packets comprise a header and body. sample formats are shown in figure 7.19. the header is an integral number of 16-bit phits that contain routing information, control information for the remote pe, and possibly source information as well. the body makes use of check bits to detect errors. for example, a 64-bit word will be augmented with 14 check bits (hence the 5-phit body in figure 7.19) and four-word message bodies will have 56 check bits (necessitating 20-phit bodies). the messages are organized as sequences of ﬂits, with each ﬂit comprised of 8 phits. virtual channel buffers are 1 ﬂit deep.
the t3d utilizes source routing. header information identiﬁes the sequence of virtual channels that the message will traverse in each dimension. in order to understand the routing strategy, we must ﬁrst be familiar with the pe addressing scheme. while all pes have physical addresses, they also possess logical addresses and virtual addresses. logical addresses are based on the logical topology of the installation. thus, nodes can be addressed by the coordinate of their logical position in this topology and are in principle independent of their physical location. this approach permits spare nodes to be mapped into the network to replace failed nodes by assigning the spare node the same logical address. the virtual address is assigned to nodes within a partition allocated to a job. the virtual address is interpreted according to the shape of the allocated partition. for
differ in the ith coordinate. they will be reached by a single multidestination message. if si ≤ k/2, then the leader node will be the one in the set with the lowest coordinate in dimension i. otherwise, the leader node will be the one with the highest coordinate in dimension i. for example, when grouping is performed along a given row in a 2-d mesh, the leader will be the leftmost destination in that row if the source node is located in the left half of the network. otherwise, the leader will be the rightmost destination in that row. the remaining steps are the same as in the original hl scheme. the sqhl scheme allows multicast messages from source nodes in different quadrants of a mesh to proceed concurrently with minimal interference.
in the source-centered hierarchical leader (schl) scheme [173], each group g from the hl scheme is partitioned into at most two groups g1 and g2 based on the coordinates of the source node. let si be the ith coordinate of the source node s of a multicast. let us assume that destination nodes in g only differ in the coordinate corresponding to dimension i. all the nodes in that group with an ith coordinate lower than or equal to si will be in g1. the remaining nodes will be in g2. leader nodes are chosen so that they have an ith coordinate as close as possible to si. both the sqhl and schl schemes improve performance over the hl scheme, as will be seen in chapter 9.
path-based multicast routing functions avoid deadlock by transmitting messages in such a way that nodes are crossed either in increasing or decreasing label order. however, deadlock is still possible because messages transmitted on the high-channel and lowchannel subnetworks (see figure 5.24) use the same delivery channels at every destination node [216, 269].
figure 5.40 shows a deadlocked conﬁguration in which two multidestination messages m1 and m2 are destined for nodes a and b. m1 and m2 are traveling in the high-channel and low-channel subnetworks, respectively. assuming that ?(a) < ?(b), m1 ﬁrst reached node a, then node b. also, m2 ﬁrst reached node b, then node a. as there is a single delivery channel at each node, each message has reserved one delivery channel and is waiting for the other delivery channel to become free. note that m1 cannot be completely delivered to node a because wormhole switching is used.
deadlocks may arise because messages traveling in the high-channel and low-channel subnetworks share the same delivery channels, thus producing cyclic dependencies between them. effectively, message m1 in figure 5.40 has reserved the delivery channel
inspection of figures 5.55 and 5.56 shows that if the constituent unicast messages follow xy routing, then no contention is possible among them. two regions, low and high, are deﬁned on either side (with respect to <d) of the source node. by the construction of the u-mesh algorithm, any message sent by a node i in the high region will be destined for another node j , i <d j , in the high region. similarly, any message sent by a node i in the low region will be destined for another node j , j <d i, in the low region. stated in other terms, any reachable set includes nodes in either the low region or the high region, but not both. this property can be used to prove depth-contention-free message transmission within each region and, furthermore, that no channel contention can exist on the boundary between the two regions. this is stated by the following theorem:
however, contention may arise between concurrent multicast operations initiated by different nodes, especially when destination sets are identical. contention can be reduced by modifying the dimension-ordered chain according to the position of the source node in the chain. the source-partitioned u-mesh (spumesh) algorithm [173] performs a rotateleft operation on the dimension-ordered chain  for a source and set of destinations. the rotation produces a new chain 1 whose ﬁrst element is the source of the multicast. then, the u-mesh algorithm is performed on 1.
when designing a parallel computer, the designer faces many trade-offs. one of them concerns providing support for collective communication. depending on the architecture of the machine, different issues should be considered.
multicomputers usually rely on message passing to implement communication and synchronization between processes executing on different processors. as indicated in section 5.3.1, supporting collective communication operations may reduce communication latency even if those operations are not supported in hardware. the reason is that system calls and software overhead account for a large percentage of communication latency. therefore, replacing several unicast message-passing operations by a single collective communication operation usually reduces latency signiﬁcantly. for example, when a processor needs to send the same message to many different processors, a single multicast operation can replace many unicast message transmissions. even if multicast is not supported in hardware, some steps like system call, buffer reservation in kernel space, and message copy to the system buffer are performed only once. also, when multicast is not supported in hardware, performance can be considerably improved by using the techniques described in section 5.7 to organize the unicast messages as a multicast tree. using those techniques, communication latency increases logarithmically with the number of destinations. otherwise it would increase linearly. obviously, implementing some
or more outgoing links. a message may be replicated at intermediate nodes and forwarded along multiple outgoing links toward disjoint subsets of destinations. the destination nodes can be either leaf nodes or intermediate nodes in the tree. in this approach, each node in the network should be able to replicate messages by sending copies out through different output channels. message replication can be easily done by software in networks using saf switching. this has been the traditional approach to support collective communication. hardware replication of messages is much more complex, being suitable for networks using vct or wormhole switching.
in order to replicate messages, the routing hardware requires several changes with respect to the router model described in section 2.1. we assume that each destination address is encoded in a different ﬂit, and that each message contains a set of destination address ﬂits followed by data ﬂits. also, we assume that a single bit is used in each ﬂit to distinguish between destination addresses and data ﬂits. all the address ﬂits are routed at each intermediate node, either reserving a new output channel or being transmitted through a previously reserved channel. some status registers are required at the routing and arbitration unit to keep track of all the output channels reserved by a message arriving on a given input channel. once all the address ﬂits have been routed, incoming data ﬂits are simultaneously forwarded to all the output channels previously reserved by the address ﬂits. note that if the switch is a crossbar and it is implemented as a set of multiplexors (one for each switch output), it is possible to select the same input from several outputs. therefore, the switch can be easily conﬁgured so that ﬂits are simultaneously forwarded to several output channels from the same input channel. however, ﬂow control is more complex than in unicast routing. flit acknowledgment signals returned across all the output channels reserved by a message must be anded because ﬂits can only be forwarded when all the routers that should receive them have conﬁrmed buffer availability. finally, the delivery buffer at each router should be designed in such a way that ﬂits containing destination addresses are automatically discarded.
although this section focuses on hardware support of multicast communication, we ﬁrst describe a simple algorithm for broadcasting in hypercubes. this algorithm can be implemented in either hardware or software. it will serve to illustrate some drawbacks of tree-based multicast routing.
consider an n-cube topology. an original version of the following broadcast algorithm, which produces a spanning binomial tree based on the concept of recursive doubling, was proposed by sullivan and bashkow [334]. each node in the system will receive the broadcast message exactly once and in no later than n time steps. let s be the address of the source node and v be the node receiving a broadcast message. the broadcast algorithm is listed in figure 5.10. the function firstone(v) indicates the location of the least signiﬁcant 1 in an n-bit binary number v. if vk = 1 and vj = 0 for all 0 ≤ j ≤ k − 1, then firstone(v) = k. if v = 0, then k = n.
multiprocessor has 5 × 5 nonblocking crossbars at the lower level in the hierarchy, connecting four functional blocks and one i/o interface to form clusters or hypernodes. each functional block consists of two processors, two memory banks, and interfaces. these hypernodes are connected by a second-level coherent toroidal interconnect made out of multiple rings using scalable coherent interface (sci). each ring connects one functional block from all the hypernodes. at the lower level of the hierarchy, the crossbars allow all the processors within a hypernode to access the interleaved memory modules in that hypernode. at the higher level, the rings implement a cache coherence protocol.
many other hybrid topologies have been proposed [25, 337, 349]. among them, a particularly interesting class is the hypermesh [336]. a hypermesh is a regular topology consisting of a set of nodes arranged into several dimensions. instead of having direct connections to the neighbors in each dimension, each node is connected to all the nodes in each dimension through a bus. there are several ways to implement a hypermesh. the most straightforward way consists of connecting all the nodes in each dimension through a shared bus. figure 1.27 shows a 2-d hypermesh. in this network, multiple buses are arranged in two dimensions. each node is connected to one bus in each dimension. this topology was proposed by wittie [349] and was referred to as a spanning-bus hypercube. this topology has a very low diameter, and the average distance between nodes scales very well with network size. however, the overall network bandwidth does not scale well. additionally, the frequent changes of bus mastership incur signiﬁcant overheads.
in multidimensional direct networks, if the number of faulty components is less than the degree of a node, then the features of the topology can be exploited to ﬁnd a path between any two nodes. for example, consider an n-dimensional binary hypercube with f < n faulty components. in this topology there are n disjoint paths of length no greater than n + 2 between any pair of nodes [300]. as a result, n − 1 faulty components cannot physically disconnect the network, and we expect to be able to describe an (n − 1) faulttolerant routing algorithm. the behavior of these algorithms is sensitive to the extent of the fault information available at an intermediate node. if the fault status of larger regions around the intermediate node is known, more informed routing decisions can be made with consequent improved performance. if only local information is available, routing decisions are fast and adjacent faulty regions can be avoided. however, the lack of any lookahead prevents choosing globally efﬁcient routes, and path lengths may be longer than necessary. the following routing algorithms differ in the manner in which global information about the location and distribution of faulty components is acquired and used.
faulty components can be sidestepped by traversing a dimension orthogonal to the dimensions leading to the faulty components. chen and shin [53] describe such an algorithm for routing in binary hypercubes. unlike k-ary n-cubes, messages in binary hypercubes (k = 2) need only traverse one link in each dimension (refer to the description in chapter 1). if the source and destination node addresses differ in m bits, then the message will traverse m dimensions on a shortest path from the source to the destination. this sequence of dimensions is speciﬁed as a coordinate sequence and is represented as a list of dimensions. at an intermediate node, a dimension that is not on the shortest path to the destination is referred to as a spare dimension. when all of the shortest paths toward the destination from an intermediate node are blocked by faults, the message is transmitted across a spare dimension. the dimensions that were blocked by a fault, as well as those used as a spare dimension, are recorded in an n-bit tag maintained in the message header. note that the size of the tag accommodates at most n − 1 faulty components. the resulting message is comprised of several components: (m, [c1, c2, . . . , ck], message, d). the value of m represents the distance to the destination. when m = 0, the router can assert that the local pe is the destination. the tag d is initialized to zero at the source. when an intermediate node receives a message, it is forwarded along one of the dimensions in the coordinate sequence. the header is updated to decrement m and eliminate the dimension being traversed from the coordinate sequence. when blocked by faults along all of the shortest paths, the value of m is incremented, the spare dimension is added to the coordinate sequence, the tag d is updated, and the new message is misrouted along the spare dimension. this approach is (n − 1) fault tolerant. the routing algorithm is described in figure 6.5.
in [53], the algorithm is shown to be able to route messages between any two nodes if the number of faulty components is less than n. in this case there is at least one spare dimension into the node that is nonfaulty, and the algorithm will ﬁnd that spare
algorithm: fault-tolerant routing using safety levels input: message (message, destination), tag = current node ⊕ destination, and d is the distance to the destination procedure: 1. if tag = 0, then the destination has been reached. 2. if there is at least one preferred neighbor with a safety level ≥ d − 1, send 3. if there is at least one spare neighbhor with a safety level ≥ d + 1, update the
this example illustrates how the use of unsafe labels affects routing with the fault pattern shown in figure 6.8(a). figure 6.9 shows nodes that become labeled unsafe. the solid line illustrates the path that is now taken by a message that is routed from s1 to d1. note that the routing of messages through unsafe nodes is avoided. the ﬁgure also shows paths taken by messages from nodes s2 to d2 and from nodes s3 to d3. note that the former message is forwarded through an unsafe node. such routing decisions are preferred to misrouting. the message originating from node s3 must be misrouted since all shortest paths to the destination are blocked by faults.
the concept of a safe node captures the fault distribution within a neighborhood. a node is safe if all of the neighboring nodes and links are nonfaulty. this concept of safety can be extended to capture multiple distinct levels of safety in binary hypercubes [351]. a node has a safety level of k if any node at a distance of k can be reached by a path of length k. safety levels can be viewed as a form of nonlocal state information. the location and distribution of faults is encoded in the safety level of a node. the exact locations of the faulty nodes are not speciﬁed. however, in practice, knowledge of fault locations is for the purpose of ﬁnding (preferably) minimal-length paths. safety levels can be used to realize this goal by exploiting the following properties. if the safety level of a node is s, then for any message received by this node and destined for a node no more than distance s away, at least one neighbor on a shortest path to the destination will have a safety level of (s − 1). the neighbors on a shortest path to the destination will be referred to as preferred neighbors [351] and the remaining neighbors as spare neighbors. clearly, if the message cannot be delivered to the destination, the safety levels of all of the preferred neighbors will be < d − 1, where d is the distance to the destination, and that of the spare neighbors will be < d. once the safety levels have been computed for each node, a message may be routed using the sequence of steps shown in figure 6.11.
a faulty node has a safety level of 0. a safe node has a safety level of n (the diameter of the network). all nonfaulty nodes are initialized to a safety level of n. the levels of
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
several different bus arbitration algorithms have been proposed, which can be classiﬁed into centralized or distributed. a centralized method has a central bus arbiter. when a processor wants to become the bus master, it sends out a bus request to the bus arbiter, which then sends out a request grant signal to the requesting processor. a bus arbiter can be an encoder-decoder pair in hardware design. in a distributed method, such as the daisy chain method, there is no central bus arbiter. the bus request signals form a daisy chain. the mastership is released to the next device when data transfer is done.
most bus transactions involve request and response. this is the case for memory read operations. after a request is issued, it is desirable to have a fast response. if a fast response time is expected, the bus mastership is not released after sending the request, and data can be received soon. however, due to memory latency, the bus bandwidth is wasted while waiting for a response. in order to minimize the waste of bus bandwidth, the split transaction protocol has been used in many bus networks.
in this protocol, the bus mastership is released immediately after the request, and the memory has to gain mastership before it can send the data. split transaction protocol has a better bus utilization, but its control unit is much more complicated. buffering is needed in order to save messages before the device can gain the bus mastership.
to support shared-variable communication, some atomic read/modify/write operations to memories are needed. with the split transaction protocol, those atomic operations can no longer be indivisible. one approach to solve this problem is to disallow bus release for those atomic operations.
gigaplane used in sun ultra enterprise x000 server (ca. 1996): 2.68 gbyte/s peak, 256 bits data, 42 bits address, split transaction protocol, 83.8 mhz clock. dec alphaserver8x00, that is, 8200 and 8400 (ca. 1995): 2.1 gbyte/s, 256 bits data, 40 bits address, split transaction protocol, 100 mhz clock (1 foot length). sgi powerpath-2 (ca. 1993): 1.2 gbyte/s, 256 bits data, 40 bits address, 6 bits control, split transaction protocol, 47.5 mhz clock (1 foot length). hp9000 multiprocessor memory bus (ca. 1993): 1 gbyte/s, 128 bits data, 64 bits address, 13 inches, pipelined bus, 60 mhz clock.
scalability is an important issue in designing multiprocessor systems. bus-based systems are not scalable because the bus becomes the bottleneck when more processors are added.
we studied four particular cases of channel dependency (direct, direct cross-, indirect, and indirect cross-dependencies) in sections 3.1.4 and 3.1.5. for each of them we can deﬁne the corresponding multicast dependency, giving rise to direct multicast, direct cross-multicast, indirect multicast, and indirect cross-multicast dependencies. the only difference between these dependencies and the dependencies deﬁned in chapter 3 is that multicast dependencies are due to multicast messages reaching an intermediate destination. in other words, there is an intermediate destination in the path between the reserved channel and the requested channel.
the extended channel dependency graph deﬁned in section 3.1.3 can be extended by including multicast dependencies. the resulting graph is the extended multicast channel dependency graph [90, 96]. similarly to theorem 3.1, it is possible to deﬁne a condition for deadlock-free multicast routing based on that graph.
before proposing the condition, it is necessary to deﬁne a few additional concepts. the message preparation algorithm executed at the source node splits the destination set for a message into one or more destination subsets, possibly reordering the nodes. this algorithm has been referred to as a split-and-sort function ss [90, 96]. the destination subsets supplied by this function are referred to as valid.
a split-and-sort function ss and a connected routing function r form a compatible pair (ss, r) if and only if, when a given message destined for the destination set d is being routed, the destination subset containing the destinations that have not been reached yet is a valid destination set for the node containing the message header. this deﬁnition imposes restrictions on both ss and r because compatibility can be achieved either by deﬁning ss according to this deﬁnition and/or by restricting the paths supplied by the routing function. also, if (ss, r) is a compatible pair and r1 is a connected routing subfunction of r, then (ss, r1) is also a compatible pair.
the following theorem proposes a sufﬁcient condition for deadlock-free, path-based multicast routing [90, 96]. whether it is also a necessary condition for deadlock-free multicast routing remains as an open problem.
the most important drawback of trees as general-purpose interconnection networks is that the root node and the nodes close to it become a bottleneck. additionally, there are no alternative paths between any pair of nodes. the bottleneck can be removed by allocating a higher channel bandwidth to channels located close to the root node. the shorter the distance to the root node, the higher the channel bandwidth. however, using channels with different bandwidths is not practical, especially when message transmission is pipelined. a practical way to implement trees with higher channel bandwidth in the vicinity of the root node (fat trees) will be described in section 1.7.5.
one of the most interesting properties of trees is that, for any connected graph, it is possible to deﬁne a tree that spans the complete graph.as a consequence, for any connected network, it is possible to build an acyclic network connecting all the nodes by removing some links. this property can be used to deﬁne a routing algorithm for any irregular topology. however, that routing algorithm may be inefﬁcient due to the concentration of trafﬁc across the root node. a possible way to circumvent that limitation will be presented in section 4.9.
some topologies have been proposed with the purpose of reducing node degree while keeping the diameter small. most of these topologies can be viewed as a hierarchy of topologies. this is the case for the cube-connected cycles [284]. this topology can be considered as an n-dimensional hypercube of virtual nodes, where each virtual node is a ring with n nodes, for a total of n2n nodes. each node in the ring is connected to a single dimension of the hypercube. therefore, node degree is ﬁxed and equal to three: two links connecting to neighbors in the ring, and one link connecting to a node in another ring through one of the dimensions of the hypercube. however, the diameter is of the same order of magnitude as that of a hypercube of similar size. figure 1.7(a) shows a 24-node cube-connected cycles network. it is worth noting that cube-connected cycles are weakly orthogonal because the ring is a one-dimensional network, and displacement inside the ring does not change the position in the other dimensions. similarly, a displacement along a hypercube dimension does not affect the position in the ring. however, it is not possible to cross every dimension from each node.
many topologies have been proposed with the purpose of minimizing the network diameter for a given number of nodes and node degree. two well-known topologies proposed with this purpose are the de bruijn network and the star graphs. in the de bruijn network [301] there are d n nodes, and each node is represented by a set of n digits in base d. a node (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ d − 1 for 0 ≤ i ≤ n − 1, is connected to nodes (xn−2, . . . , x1, x0, p) and (p, xn−1, xn−2, . . . , x1), for all p such that 0 ≤ p ≤ d − 1. in other words, two nodes are connected if the representation of one node is a right or left shift of the representation of the other. figure 1.7(b) shows an eight-node de bruijn network. when networks are very large, this network topology achieves a very low diameter for a given number of nodes and node degree. however, routing is complex. additionally, the average distance between nodes is high, close to the network diameter. finally, some nodes have links connecting to themselves. all of these issues make the practical application of these networks very difﬁcult.
a star graph [6] can be informally described as follows. the vertices of the graph are labeled by permutations of n different symbols, usually denoted as 1 to n. a permutation
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
deadlocks, livelocks, and starvation arise because the number of resources is ﬁnite. additionally, some of these situations may produce the others. for instance, a deadlock permanently blocks some packets. as those packets are occupying some buffers, other packets may require them to reach their destination, being continuously misrouted around their destination node and producing livelock.
it is extremely important to remove deadlocks, livelocks, and starvation when implementing an interconnection network. otherwise, some packets may never reach their destination. as these situations arise because the storage resources are ﬁnite, the probability of reaching them increases with network trafﬁc and decreases with the amount of buffer storage. for instance, a network using wormhole switching is much more deadlock-prone than the same network using saf switching if the routing algorithm is not deadlock-free. a classiﬁcation of the situations that may prevent packet delivery and the techniques to solve these situations is shown in figure 3.1. starvation is relatively easy to solve. simply, a correct resource assignment scheme should be used. a simple demand-slotted round-robin scheme is enough to produce a fair use of resources. when some packets must have a higher priority, some bandwidth must be reserved for low-priority packets in order to prevent starvation. this can be done by limiting the number of high-priority packets or by reserving some virtual channels or buffers for low-priority packets.
livelock is also relatively easy to avoid. the simplest way consists of using only minimal paths. this restriction usually increases performance in networks using wormhole switching because packets do not occupy more channels than the ones strictly necessary to reach their destination. the main motivation for the use of nonminimal paths is fault tolerance. even when nonminimal paths are used, livelock can be prevented by limiting the number of misrouting operations. another motivation for using nonminimal paths is
this can be achieved in a fully distributed manner. the nodes are generally assumed to possess some ability for self-test as well as the ability to test neighboring nodes. we can envision an approach where, in one step, each node performs a self-test and interrogates the status of its neighbors. if neighbors or links in two or more dimensions are faulty, the node transitions to a faulty state, even though it may be nonfaulty. this diagnosis step is repeated. after a ﬁnite number of steps bounded by the diameter of the network, fault regions will have been created and will be rectangular in shape. in multidimensional meshes and tori under the block fault model, fault-free nodes are adjacent to at most one faulty node (i.e., along only one dimension). note that single component faults correspond to the block fault model with block sizes of 1.
the block fault model is particularly well suited to evolving trends in packaging technology and the use of low-dimensional networks. we will continue to see subnetworks implemented in chips, multichip modules (mcms), and boards. failures within these components will produce block faults at the chip, board, and mcm level. construction of block fault regions often naturally falls along chip, mcm, and board boundaries. for example, if submeshes are implemented on a chip, failure of two or more processors on a chip may result in marking the chip as faulty, leading to a block fault region comprised of all of the processors on the chip. the advantages in doing so include simpler solutions to deadlock- and livelock-free routing.
failures may be either static or dynamic. static failures are present in the network when the system is powered on. dynamic failures appear at random during the operation of the system. both types of faults are generally considered to be permanent; that is, they remain in the system until it is repaired.alternatively, faults may be transient.as integrated circuit feature sizes continue to decrease and speeds continue to increase, problems arise with soft errors that are transient or dynamic in nature. the difﬁculty of designing for such faults is that they often cannot be reproduced. for example, soft errors that occur in ﬂight tests of avionics hardware often cannot be reproduced on the ground. in addition,
a steiner tree, s(v , e), for a multicast set k is a subtree of g such that k ⊆ v (s). a minimal steiner tree (mst) is a steiner tree with a minimal total length.
in the steiner tree problem, it is not necessary to use a shortest path from the source to a destination. if the distance between two nodes is not a major factor in the communication time, such as in vct, wormhole, and circuit switching, the above optimization problem is appropriate. however, if the distance is a major factor in the communication time, such as in saf switching, then we may like to minimize time ﬁrst, then trafﬁc. the multicast communication problem is then modeled as an optimal multicast tree (omt). the omt problem was originally deﬁned in [195].
an omt, t (v , e), for k is a subtree of g such that (a) k ⊆ v (t ), (b) dt (u0, ui ) = dg(u0, ui ), for 1 ≤ i ≤ k, and (c) |e(t )| is as small as possible.
apparently, the complexity of each of the above optimization problems is directly dependent on the underlying host graph. the above graph optimization problems for the popular hypercube and 2-d mesh topologies were studied in [210, 212], showing that the omc and omp problems are np-complete for those topologies. also, it was shown that the mst and omt problems are np-complete for the hypercube topology [60, 135]. the mst problem for the 2-d mesh topology is equivalent to the rectilinear steiner tree problem, which is np-complete [120].
the np-completeness results indicate the necessity to develop heuristic multicast communication algorithms for popular interconnection topologies. multicast communication may be supported in hardware, software, or both. sections 5.5 and 5.7 will address hardware and software implementations, respectively, of multicast.
hardware support of multicast communication in multicomputers requires increased functionality within the routers. this functionality may include interpretation of multicast addresses (or group id) and forwarding of messages onto multiple outgoing channels (replication). the result is the capability for one local processor to efﬁciently send the
individually acknowledged. however, due to the length of the cable, up to 23 characters may be in transit in both directions at any given time. as in the sp2, these delays are accommodated with sufﬁcient buffering at the receiver. the stop and go control characters are generated at the receiver and multiplexed along the reverse channel (data may be ﬂowing along this channel) to implement ﬂow control. this slack buffer [315] is organized as shown in figure 7.50(c). as the buffer ﬁlls up from the bottom, the stop control character is transmitted to the source when the buffer crosses the stop line. there is enough remaining buffer space for all of the ﬂits in transit as well as the ﬂits that will be injected before the stop character arrives at the source. once the buffer has drained sufﬁciently (i.e., crosses the go line), a go character is transmitted and the sender resumes data ﬂow. the placement of the stop and go points within the buffer are designed to prevent constant oscillation between stop and go states, and reduce ﬂow control trafﬁc. a separate control character (gap) is used to signify the end-of-message. to enable detection of faulty links or deadlocked messages, non-idle characters must be periodically transmitted across each channel. longer timeout intervals are used to detect potentially deadlocked packets. for example, this may occur over a nonfaulty link due to bit errors in the header. in this case, after 50 ms the blocked part of the packet is dropped, and a forward reset signal is generated to the receiver and the link buffers reset.
myrinet employs wormhole switching with source routing. the message packets may be of variable length and are structured as shown in figure 7.50(c). the ﬁrst few ﬂits of the message header contain the address of the switch ports on intermediate switches. in a manner similar to the sp2 switches, each ﬂit addresses one of the output ports in a switch. each switch strips off the leading ﬂit as the message arrives and uses the contents to address an output port. conﬂicts for the same output port are resolved using a recirculating token arbitration mechanism to ensure fairness. the remaining ﬂits of the message are transmitted
delay through the entire circuit is negligible compared to clock cycle. hence, tdat a does not depend on that delay. the factor of 2 in the setup cost represents the time for the forward progress of the header and the return of the acknowledgment. the use of b hz as the channel speed represents the transmission across a hardwired path from source to destination.
in circuit switching, the complete message is transmitted after the circuit has been set up. alternatively, the message can be partitioned and transmitted as ﬁxed-length packets, for example, 128 bytes. the ﬁrst few bytes of a packet contain routing and control information and are referred to as the packet header. each packet is individually routed from source to destination. this technique is referred to as packet switching. a packet is completely buffered at each intermediate node before it is forwarded to the next node. this is the reason why this switching technique is also referred to as store-and-forward (saf) switching. the header information is extracted by the intermediate router and used to determine the output link over which the packet is to be forwarded. a time-space diagram of the progress of a packet across three links is shown in figure 2.8. from the ﬁgure we can see that the latency experienced by a packet is proportional to the distance between the source and destination nodes. note that the ﬁgure has omitted the packet latency, ts , through the router. packet switching is advantageous when messages are short and frequent. unlike circuit switching, where a segment of a reserved path may be idle for a signiﬁcant period of time, a communication link is fully utilized when there are data to be transmitted. many packets belonging to a message can be in the network simultaneously even if the ﬁrst packet has not yet arrived at the destination. however, splitting a message into packets produces some overhead. in addition to the time required at source and destination nodes, every packet must be routed at each intermediate node. an example of the format of a data packet header is shown in figure 2.9. this is the header format used in the jpl
is usually done taking into account the status of output channels at the current node. obviously, the selection is performed in such a way that a free channel (if any) is supplied. however, when several output channels are available, some policy is required to select one of them. policies may have different goals, like balancing the use of resources, reserving some bandwidth for high-priority packets, or even delaying the use of resources that are exclusively used for deadlock avoidance. regardless of the main goal of the policy, the selection function should give preference to channels belonging to minimal paths when the routing function is nonminimal. otherwise the selection function may produce livelock. in this section we present some selection functions proposed in the literature for several purposes.
in [73], three different selection functions were proposed for n-dimensional meshes using wormhole switching with the goal of maximizing performance: minimum congestion, maximum ﬂexibility, and straight lines. in minimum congestion, a virtual channel is selected in the dimension and direction with the most available virtual channels. this selection function tries to balance the use of virtual channels in different physical channels. the motivation for this selection function is that packet transmission is pipelined. hence, ﬂit transmission rate is limited by the slowest stage in the pipeline. balancing the use of virtual channels also balances the bandwidth allocated to different virtual channels. in maximum ﬂexibility, a virtual channel is selected in the dimension with the greatest distance to travel to the destination. this selection function tries to maximize the number of routing choices as a packet approaches its destination. in meshes, this selection function has the side effect of concentrating trafﬁc in the central part of the network bisection, therefore producing an uneven channel utilization and degrading performance. finally, in straight lines, a virtual channel is selected in the dimension closest to the current dimension. so, the packet will continue traveling in the same dimension whenever possible. this selection function tries to route packets in dimension order unless the requested channels in the corresponding dimension are busy. in meshes, this selection function achieves a good distribution of trafﬁc across the network bisection. these selection functions were evaluated in [73] for 2-d meshes, showing that minimum congestion achieves the lowest latency and highest throughput. straight lines achieves similar performance. however, maximum ﬂexibility achieves much worse results. these results may change for other topologies and routing functions, but in general minimum congestion is a good choice for the reason mentioned above.
for routing functions that allow cyclic dependencies between channels, the selection function should give preference to adaptive channels over channels used to escape from deadlocks [91]. by doing so, escape channels are only used when all the remaining channels are busy, therefore increasing the probability of escape channels being available when they are required to escape from deadlock. the selection among adaptive channels can be done by using any of the strategies described above. note that if escape channels are not free when requested, it does not mean that the routing function is not deadlock-free. escape channels are guaranteed to become free sooner or later. however, performance may degrade if packets take long to escape from deadlock. in order to reduce the utilization of escape channels and increase their availability to escape from deadlock, it is possible to delay the use of escape channels by using a timeout. in this case, the selection function can
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
mit reliable router: 2-d mesh, 23-bit links (16-bit data), 200 mhz, 400 mbytes/s per link per direction, bidirectional signaling, reliable transmission. chaos router: 2-d torus topology, bidirectional 8-bit links, 180 mhz, 360 mbytes/s in each direction. intel ipsc-2 hypercube: binary hypercube topology, bit-serial channels at 2.8 mbytes/s.
indirect or switch-based networks are another major class of interconnection networks. instead of providing a direct connection among some nodes, the communication between any two nodes has to be carried through some switches. each node has a network adapter that connects to a network switch. each switch can have a set of ports. each port consists of one input and one output link. a (possibly empty) set of ports in each switch is either connected to processors or left open, whereas the remaining ports are connected to ports of other switches to provide connectivity between the processors. the interconnection of those switches deﬁnes various network topologies.
switch-based networks have evolved considerably over time. a wide range of topologies has been proposed, ranging from regular topologies used in array processors and shared-memory uma multiprocessors to the irregular topologies currently used in nows. both network classes will be covered in this book. regular topologies have regular connection patterns between switches, while irregular topologies do not follow any predeﬁned pattern. figures 1.19 and 1.21 show several switch-based networks with regular topology. the corresponding connection patterns will be studied later. figure 1.8 shows a typical switch-based network with irregular topology. both network classes can be further classiﬁed according to the number of switches a message has to traverse before reaching its destination. although this classiﬁcation is not important in the case of irregular topologies, it makes a big difference in the case of regular networks because some speciﬁc properties can be derived for each network class.
indirect networks can also be modeled by a graph g(n, c), where n is the set of switches and c is the set of unidirectional or bidirectional links between the switches. for the analysis of most properties, it is not necessary to explicitly include processing nodes in the graph. although a similar model can be used for direct and indirect networks, a few differences exist between them. each switch in an indirect network may be connected to zero, one, or more processors. obviously, only the switches connected to some processor can be the source or the destination of a message. additionally, transmitting a message from a node to another node requires crossing the link between the source node and the switch connected to it, and the link between the last switch in the path and the destination
networks, for example, circuit switching and packet switching. however, as the application of multiprocessor systems spread into increasingly compute-intensive domains, the traditional layered communication designs borrowed from lans became a limiting performance bottleneck. new switching techniques and implementations evolved that were better suited to the low-latency demands of parallel programs. this chapter reviews these switching techniques and their accompanying ﬂow control and buffer management algorithms.
in comparing and contrasting alternative implementations of the switching layer, we are interested in evaluating their impact on the router implementations. the implementations in turn determine the cycle time of router operation and therefore the resulting message latency and network bandwidth. the architecture of a generic router is shown in figure 2.1 and is comprised of the following major components.
buffers. these are ﬁrst-in ﬁrst-out (fifo) buffers for storing messages in transit. in the model shown in figure 2.1, a buffer is associated with each input physical channel and each output physical channel. in alternative designs, buffers may be associated only with inputs (input buffering) or outputs (output buffering). the buffer size is an integral number of ﬂow control units.
switch. this component is responsible for connecting router input buffers to router output buffers. high-speed routers will utilize crossbar networks with full connectivity, while lower-speed implementations may utilize networks that do not provide full connectivity between input buffers and output buffers.
routing and arbitration unit. this component implements the routing algorithms, selects the output link for an incoming message, and accordingly sets the switch. if multiple messages simultaneously request the same output link, this component must provide for arbitration between them. if the requested link is busy, the incoming message remains in the input buffer. it will be routed again after the link is freed and if it successfully arbitrates for the link.
link controllers (lcs). the ﬂow of messages across the physical channel between adjacent routers is implemented by the link controller. the link controllers on either side of a channel coordinate to transfer units of ﬂow control.
processor interface. this component simply implements a physical channel interface to the processor rather than to an adjacent router. it consists of one or more injection channels from the processor and one or more ejection channels to the processor. ejection channels are also referred to as delivery channels or consumption channels.
consider the router model described in section 2.1. we assume that all the operations inside each router are synchronized by its local clock signal. to compute the clock frequency of each router, we will use the delay model proposed in [57]. it assumes 0.8 µm cmos gate array technology for the implementation.
routing control unit. routing a message involves the following operations: address decoding, routing decision, and header selection. the ﬁrst operation extracts the message header and generates requests of acceptable outputs based on the routing algorithm. in other words, the address decoder implements the routing function. according to [57], the address decoder delay is constant and equal to 2.7 ns. the routing decision logic takes as inputs the possible output channels generated by the address decoder and the status of the output channels. in other words, this logic implements the selection function. this circuit has a delay that grows logarithmically with the number of alternatives, or degrees of freedom, offered by the routing algorithm. representing the degrees of freedom by f , this circuit has a delay value given by 0.6 + 0.6 log f ns. finally, the routing control unit must compute the new header, depending on the output channel selected. while new headers can be computed in parallel with the routing decision, it is necessary to select the appropriate one when this decision is made. this operation has a delay that grows logarithmically with the degrees of freedom. thus, this delay will be 1.4 + 0.6 log f ns. the operations and the associated delays are shown in figure 9.31. the total routing time will be the sum of all delays, yielding tr = 2.7 + 0.6 + 0.6 log f + 1.4 + 0.6 log f = 4.7 + 1.2 log f ns
switch. the time required to transfer a ﬂit from one input channel to the corresponding output channel is the sum of the delay involved in the internal ﬂow control unit, the delay of the crossbar, and the setup time of the output channel latch. the ﬂow control unit manages the buffers, preventing overﬂow and underﬂow. it has a
mit reliable router: 2-d mesh, 23-bit links (16-bit data), 200 mhz, 400 mbytes/s per link per direction, bidirectional signaling, reliable transmission. chaos router: 2-d torus topology, bidirectional 8-bit links, 180 mhz, 360 mbytes/s in each direction. intel ipsc-2 hypercube: binary hypercube topology, bit-serial channels at 2.8 mbytes/s.
indirect or switch-based networks are another major class of interconnection networks. instead of providing a direct connection among some nodes, the communication between any two nodes has to be carried through some switches. each node has a network adapter that connects to a network switch. each switch can have a set of ports. each port consists of one input and one output link. a (possibly empty) set of ports in each switch is either connected to processors or left open, whereas the remaining ports are connected to ports of other switches to provide connectivity between the processors. the interconnection of those switches deﬁnes various network topologies.
switch-based networks have evolved considerably over time. a wide range of topologies has been proposed, ranging from regular topologies used in array processors and shared-memory uma multiprocessors to the irregular topologies currently used in nows. both network classes will be covered in this book. regular topologies have regular connection patterns between switches, while irregular topologies do not follow any predeﬁned pattern. figures 1.19 and 1.21 show several switch-based networks with regular topology. the corresponding connection patterns will be studied later. figure 1.8 shows a typical switch-based network with irregular topology. both network classes can be further classiﬁed according to the number of switches a message has to traverse before reaching its destination. although this classiﬁcation is not important in the case of irregular topologies, it makes a big difference in the case of regular networks because some speciﬁc properties can be derived for each network class.
indirect networks can also be modeled by a graph g(n, c), where n is the set of switches and c is the set of unidirectional or bidirectional links between the switches. for the analysis of most properties, it is not necessary to explicitly include processing nodes in the graph. although a similar model can be used for direct and indirect networks, a few differences exist between them. each switch in an indirect network may be connected to zero, one, or more processors. obviously, only the switches connected to some processor can be the source or the destination of a message. additionally, transmitting a message from a node to another node requires crossing the link between the source node and the switch connected to it, and the link between the last switch in the path and the destination
mit reliable router: 2-d mesh, 23-bit links (16-bit data), 200 mhz, 400 mbytes/s per link per direction, bidirectional signaling, reliable transmission. chaos router: 2-d torus topology, bidirectional 8-bit links, 180 mhz, 360 mbytes/s in each direction. intel ipsc-2 hypercube: binary hypercube topology, bit-serial channels at 2.8 mbytes/s.
indirect or switch-based networks are another major class of interconnection networks. instead of providing a direct connection among some nodes, the communication between any two nodes has to be carried through some switches. each node has a network adapter that connects to a network switch. each switch can have a set of ports. each port consists of one input and one output link. a (possibly empty) set of ports in each switch is either connected to processors or left open, whereas the remaining ports are connected to ports of other switches to provide connectivity between the processors. the interconnection of those switches deﬁnes various network topologies.
switch-based networks have evolved considerably over time. a wide range of topologies has been proposed, ranging from regular topologies used in array processors and shared-memory uma multiprocessors to the irregular topologies currently used in nows. both network classes will be covered in this book. regular topologies have regular connection patterns between switches, while irregular topologies do not follow any predeﬁned pattern. figures 1.19 and 1.21 show several switch-based networks with regular topology. the corresponding connection patterns will be studied later. figure 1.8 shows a typical switch-based network with irregular topology. both network classes can be further classiﬁed according to the number of switches a message has to traverse before reaching its destination. although this classiﬁcation is not important in the case of irregular topologies, it makes a big difference in the case of regular networks because some speciﬁc properties can be derived for each network class.
indirect networks can also be modeled by a graph g(n, c), where n is the set of switches and c is the set of unidirectional or bidirectional links between the switches. for the analysis of most properties, it is not necessary to explicitly include processing nodes in the graph. although a similar model can be used for direct and indirect networks, a few differences exist between them. each switch in an indirect network may be connected to zero, one, or more processors. obviously, only the switches connected to some processor can be the source or the destination of a message. additionally, transmitting a message from a node to another node requires crossing the link between the source node and the switch connected to it, and the link between the last switch in the path and the destination
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
interprocessor communication can be viewed as a hierarchy of services starting from the physical layer that synchronizes the transfer of bit streams to higher-level protocol layers that perform functions such as packetization, data encryption, data compression, and so on. such a layering of communication services is common in the local and wide area network communities. while there currently may not be a consensus on a standard set of layers for multiprocessor systems, we ﬁnd it useful to distinguish between three layers in the operation of the interconnection network: the routing layer, the switching layer, and the physical layer. the physical layer refers to link-level protocols for transferring messages and otherwise managing the physical channels between adjacent routers. the switching layer utilizes these physical layer protocols to implement mechanisms for forwarding messages through the network. finally, the routing layer makes routing decisions to determine candidate output channels at intermediate router nodes and thereby establish the path through the network. the design of routing protocols and their properties (e.g., deadlock and livelock freedom) are largely determined by the services provided by the switching layer.
this chapter focuses on the techniques that are implemented within the network routers to realize the switching layer. these techniques differ in several respects. the switching techniques determine when and how internal switches are set to connect router inputs to outputs and the time at which message components may be transferred along these paths. these techniques are coupled with ﬂow control mechanisms for the synchronized transfer of units of information between routers and through routers in forwarding messages through the network. flow control is tightly coupled with buffer management algorithms that determine how message buffers are requested and released, and as a result determine how messages are handled when blocked in the network. implementations of the switching layer differ in decisions made in each of these areas, and in their relative timing, that is, when one operation can be initiated relative to the occurrence of the other. the speciﬁc choices interact with the architecture of the routers and trafﬁc patterns imposed by parallel programs in determining the latency and throughput characteristics of the interconnection network.
as we might expect, the switching techniques employed in multiprocessor networks initially followed those techniques employed in local and wide area communication
software messaging layer. a high percentage of the communication latency in multicomputers is produced by the overhead in the software messaging layer. reducing or hiding this overhead is likely to have a higher impact on performance than the remaining design parameters, especially when messages are relatively short. the use of vcc can eliminate or hide most of the software overhead by overlapping path setup with computation, and caching and retaining virtual circuits for use by multiple messages. vcc complements the use of techniques to reduce the software overhead, like active messages. it should be noted that software overhead is so high in some multicomputers that it makes no sense improving other design parameters. however, once this overhead has been removed or hidden, the remaining design parameters become much more important.
software support for collective communication. collective communication operations beneﬁt considerably from using speciﬁc algorithms. using separate addressing, latency increases linearly with the number of participating nodes. however, when algorithms for collective communication are implemented in software, latency is considerably reduced, increasing logarithmically with the number of participating nodes. this improvement is achieved with no additional hardware cost and no overhead for unicast messages.
number of ports. if the software overhead has been removed or hidden, the number of ports has a considerable inﬂuence on performance, especially when messages are sent locally. if the number of ports is too small, the network interface is likely to be a bottleneck for the network. the optimal number of ports heavily depends on the spatial locality of trafﬁc patterns.
switching technique. nowadays, most commercial and experimental parallel computers implement wormhole switching. although vct switching achieves a higher throughput, the performance improvement is small when virtual channels are used in wormhole switching. additionally, vct switching requires splitting messages into packets not exceeding buffer capacity. if messages are longer than buffer size, wormhole switching should be preferred. however, when messages are shorter than or equal to buffer size, vct switching performs slightly better than wormhole switching, also simplifying deadlock avoidance. this is the case for distributed, shared-memory multiprocessors. vct switching is also preferable when it is not easy to avoid deadlock in wormhole switching (multicast routing in multistage networks) or in some applications requiring real-time communication [294]. finally, a combination of wormhole switching and circuit switching with wave-pipelined switches and channels has the potential to increase performance, especially when messages are very long [101].
packet size. for pipelined switching techniques, ﬁlling the pipeline produces some overhead. also, routing a header usually takes longer than transmitting a data ﬂit
injected messages to use some predetermined virtual channel(s) or waiting until the number of free output virtual channels at a node is higher than a threshold are enough. injection limitation mechanisms are especially recommended when using routing algorithms that allow cyclic dependencies between channels, and to limit the frequency of deadlock when using deadlock recovery mechanisms.
buffer size. for wormhole switching and short messages, increasing buffer size above a certain threshold does not improve performance signiﬁcantly. for long messages (or packets), increasing buffer size increases performance because blocked messages occupy fewer channels. however, when messages are very long, increasing buffer size only helps if buffers are deep enough to allow blocked messages to leave the source node and release some channels. it should be noted that communication locality may prevent most messages from leaving the source node before reaching their destination even when using deep buffers. therefore, in most cases small buffers are enough to achieve good performance. however, this analysis assumes that ﬂits are individually acknowledged. indeed, buffer size in wormhole switching is mainly determined by the requirements of the ﬂow control mechanism. optimizations like block acknowledgments require a certain buffer capacity to perform efﬁciently. moreover, when channels are pipelined, buffers must be deep enough to store all the ﬂits in transit across the physical link plus the ﬂits injected into the link during the propagation of the ﬂow control signals. some additional capacity is required for the ﬂow control mechanism to operate without introducing bubbles in the message pipeline. thus, when channels are pipelined, buffer size mainly depends on the degree of channel pipelining. for vct switching, throughput increases considerably when moving from one to two packet buffers. adding more buffers yields diminishing returns.
reliability/performance trade-offs. an interconnection network should be reliable. depending on the intended applications and the relative value of mtbf and mttr, different trade-offs are possible. when mttr << mtbf, the probability of the second or the third fault occurring before the ﬁrst fault is repaired is very low. in such environments, software-based rerouting is a cost-effective and viable alternative. this technique supports many fault patterns and requires minimum hardware support. however, performance degrades signiﬁcantly when faults occur, increasing latency for messages that meet faults by a factor of 2–4. when faults are more frequent or performance degradation is not acceptable, a more complex hardware support is required. the fault tolerance properties of the routing algorithm are constrained by the underlying switching technique, as indicated in the next item.
switching technique. if performance is more important than reliability, fault tolerance should be achieved without modifying the switching technique. in this case, additional resources (usually virtual channels) are required to implement
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
provided is dependent on the technology. the number of processors that can be put on a bus depends on many factors, such as processor speed, bus bandwidth, cache architecture, and program behavior.
both data and address information must be carried in the bus. in order to increase the bus bandwidth and provide a large address space, both data width and address bits have to be increased. such an increase implies another increase in the bus complexity and cost. some designs try to share address and data lines. for multiplexed transfer, addresses and data are sent alternatively. hence, they can share the same physical lines and require less power and fewer chips. for nonmultiplexed transfer, address and data lines are separated. thus, data transfer can be done faster.
in synchronous bus design, all devices are synchronized with a common clock. it requires less complicated logic and has been used in most existing buses. however, a synchronous bus is not easily upgradable. new faster processors are difﬁcult to ﬁt into a slow bus.
in asynchronous buses, all devices connected to the bus may have different speeds and their own clocks. they use a handshaking protocol to synchronize with each other. this provides independence for different technologies and allows slower and faster devices with different clock rates to operate together. this also implies buffering is needed, since slower devices cannot handle messages as quickly as faster devices.
in a single-bus network, several processors may attempt to use the bus simultaneously. to deal with this, a policy must be implemented that allocates the bus to the processors making such requests. for performance reasons, bus allocation must be carried out by hardware arbiters. thus, in order to perform a memory access request, the processor has to exclusively own the bus and become the bus master. to become the bus master, each processor implements a bus requester, which is a collection of logic to request control of the data transfer bus. on gaining control, the requester notiﬁes the requesting master.
advance toward its destination. such a pruning will release channels [9, 8] and [8, 4], which is blocking message b. then, message b will advance, eventually releasing channel [9, 5], which is requested by message a. as a result, deadlock has been recovered from. in the event that ﬂow control stopped the advancing of ﬂits at node 13, message b would also prune its branch destined for node 5. this pruning is redundant, but it shows that nodes performing a pruning do not need to synchronize.
note that this example of deadlock recovery only serves to illustrate the pruning mechanism. it would not produce pruning in a real network unless the network was larger and each message would be destined for several additional nodes. the reason is that pruning would only take place at node 9 if there were more destination address ﬂits in message a after the one destined for node 1. similarly, pruning could only take place at node 13 if message b contained enough destination address ﬂits to ﬁll the buffers at nodes 8 and 12.
the most natural way of implementing multicast/broadcast routing in multistage interconnection networks (mins) is by using tree-based routing. multicast can be implemented in a single pass through the network by simultaneously forwarding ﬂits to several outputs at some switches. broadcast can be implemented in a single pass by simultaneously forwarding ﬂits to all the outputs at all the switches traversed by a message.
the replication of messages at some switches can be synchronous or asynchronous. in synchronous replication, the branches of a multidestination message can only forward if all the requested output channels are available. hence, at a given time, all the message headers are at different switches of the same stage of the network. synchronous replication requires a complex hardware signaling mechanism, usually slowing down ﬂit propagation. in asynchronous replication, each branch can forward independently without coordinating with other branches. as a consequence, hardware design is much simpler. however, bubbles may arise when some requested output channels are not available. the reason is that branches ﬁnding a free output channel are able to advance while branches ﬁnding a busy output channel are blocked. a similar situation for direct networks was shown in figure 5.12.
the propagation of multidestination messages may easily lead to deadlock, regardless of whether synchronous or asynchronous message replication is performed, as shown in the following example.
figure 5.21(a) shows a deadlocked conﬁguration in a 16-node butterﬂy min using asynchronous message replication. figure 5.21(b) shows the same deadlocked conﬁguration when messages are replicated synchronously. in each part of the ﬁgure, two multidestination messages a and b, destined for nodes 1000 and 1111, are sent by nodes 1010 and 1110, respectively (see example 4.7 to see how these messages are routed). in figure 5.21(a), the upper branch of message a successfully reserved the required output channel at stage g2, proceeding
path between them is established in a distributed manner. the packet may be delivered to all the computed destination nodes (multicast routing) or only to the last destination node (unicast routing). in this case, intermediate nodes are used to avoid congestion or faults. routing algorithms can be implemented in different ways. the most interesting ways proposed up to now consist of either looking at a routing table (table lookup) or executing a routing algorithm in software or hardware according to a ﬁnite-state machine. in both cases, the routing algorithm can be either deterministic or adaptive. deterministic routing algorithms always supply the same path between a given source/destination pair. adaptive routing algorithms use information about network trafﬁc and/or channel status to avoid congested or faulty regions of the network. routing algorithms designed to tolerate faults will be studied in depth in chapter 6.
adaptive routing algorithms can be classiﬁed according to their progressiveness as progressive or backtracking. progressive routing algorithms move the header forward, reserving a new channel at each routing operation. backtracking algorithms also allow the header to backtrack, releasing previously reserved channels. backtracking algorithms are mainly used for fault-tolerant routing.
at a lower level, routing algorithms can be classiﬁed according to their minimality as proﬁtable or misrouting. proﬁtable routing algorithms only supply channels that bring the packet closer to its destination. they are also referred to as minimal. misrouting algorithms may also supply channels that send the packet away from its destination. they are also referred to as nonminimal. at the lowest level, routing algorithms can be classiﬁed according to the number of alternative paths as completely adaptive (also known as fully adaptive) or partially adaptive.
in this chapter we focus on unicast routing algorithms for multiprocessors and multicomputers. centralized routing requires a central control unit. this is the kind of routing algorithm used in single-instruction multiple-data (simd) machines [162]. in source routing, the source node speciﬁes the routing path on the basis of a deadlockfree routing algorithm (either using table lookup or not). the computed path is stored in the packet header, being used at intermediate nodes to reserve channels. the routing algorithm may use only the addresses of current and destination nodes to compute the path (deterministic routing) or may also use information collected from other nodes about trafﬁc conditions in the network (adaptive routing). note that collecting information from other nodes may produce a considerable overhead. additionally, that information may be obsolete. thus, adaptive source routing is only interesting if trafﬁc conditions change very slowly. source routing has been mainly used in computer networks with irregular topologies [338]. myrinet [30], a high-performance lan supporting irregular topologies, also uses source routing. the ﬁrst few ﬂits of the packet header contain the address of the switch ports on intermediate switches. see section 7.2.8 for a description of myrinet.
source routing has also been proposed for multicomputer interconnection networks. as trafﬁc conditions may change very quickly in these networks, adaptive source routing is not interesting. since the header itself must be transmitted through the network, thereby consuming network bandwidth, it is important to minimize header length. one source routing method that achieves this goal is called street-sign routing. the header is analogous to a set of directions given to a driver in a city. only the names of the streets that the driver
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
although there are many similarities between interconnection networks for multicomputers and dsms, it is important to keep in mind that performance requirements may be very different. messages are usually very short when dsms are used. additionally, network latency is important because memory access time depends on that latency. however, messages are typically longer and less frequent when using multicomputers. usually the programmer is able to adjust the granularity of message communication in a multicomputer. on the other hand, interconnection networks for multicomputers and nows are mainly used for message passing. however, the geographical distribution of workstations usually imposes constraints on the way processors are connected. also, individual processors may be connected to or disconnected from the network at any time, thus imposing additional design constraints.
interconnection networks play a major role in the performance of modern parallel computers. there are many factors that may affect the choice of an appropriate interconnection network for the underlying parallel computer. these factors include the following:
1. performance requirements. processes executing in different processors synchronize and communicate through the interconnection network. these operations are usually performed by explicit message passing or by accessing shared variables. message latency is the time elapsed between the time a message is generated at its source node and the time the message is delivered at its destination node. message latency directly affects processor idle time and memory access time to remote memory locations. also, the network may saturate—it may be unable to deliver the ﬂow of messages injected by the nodes, limiting the effective computing power of a parallel computer. the maximum amount of information delivered by the network per time unit deﬁnes the throughput of that network.
2. scalability. a scalable architecture implies that as more processors are added, their memory bandwidth, i/o bandwidth, and network bandwidth should increase proportionally. otherwise the components whose bandwidth does not scale may become a bottleneck for the rest of the system, decreasing the overall efﬁciency accordingly.
3. incremental expandability. customers are unlikely to purchase a parallel computer with a full set of processors and memories. as the budget permits, more processors and memories may be added until a system’s maximum conﬁguration is reached. in some interconnection networks, the number of processors must be a power of 2, which makes them difﬁcult to expand. in other cases, expandability is provided at the cost of wasting resources. for example, a network designed for a maximum
higher latency than the average value. a high value of the standard deviation usually indicates that some messages are blocked for a long time in the network. the peak value of the latency can also help in identifying these situations.
latency is measured in time units. however, when comparing several design choices, the absolute value is not important. as many comparisons are performed by using network simulators, latency can be measured in simulator clock cycles. unless otherwise stated, the latency plots presented in this chapter for unicast messages measure the average value of the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. in most cases, the simulator clock cycle is the unit of measurement. however, in section 9.10, latency is measured in nanoseconds.
throughput is the maximum amount of information delivered per time unit. it can also be deﬁned as the maximum trafﬁc accepted by the network, where trafﬁc, or accepted trafﬁc is the amount of information delivered per time unit. throughput could be measured in messages per second or messages per clock cycle, depending on whether absolute or relative timing is used. however, throughput would depend on message and network size. so, throughput is usually normalized, dividing it by message size and network size. as a result, throughput can be measured in bits per node and microsecond, or in bits per node and clock cycle. again, when comparing different design choices by simulation, and assuming that channel width is equal to ﬂit size, throughput can be measured in ﬂits per node and clock cycle. alternatively, accepted trafﬁc and throughput can be measured as a fraction of network capacity. a uniformly loaded network is operating at capacity if the most heavily loaded channel is used 100% of the time [72]. again, network capacity depends on the communication pattern.
a standard way to measure accepted trafﬁc and throughput was proposed at the workshop on parallel computer routing and communication (pcrcw’94). it consists of representing them as a fraction of the network capacity for a uniform distribution of destinations, assuming that the most heavily loaded channels are located in the network bisection. this network capacity is referred to as normalized bandwidth. so, regardless of the communication pattern used, it is recommended to measure applied load, accepted trafﬁc, and throughput as a fraction of normalized bandwidth. normalized bandwidth can be easily derived by considering that 50% of uniform random trafﬁc crosses the bisection of the network. thus, if a network has bisection bandwidth b bits/s, each node in an n-node network can inject 2b/n bits/s at the maximum load. unless otherwise stated, accepted trafﬁc and throughput are measured as a fraction of normalized bandwidth. while this is acceptable when comparing different design choices in the same network, it should be taken into account that those choices may lead to different clock cycles. in this case, each set of design parameters may produce a different bisection bandwidth, therefore invalidating the normalized bandwidth as a trafﬁc unit. in that case, accepted trafﬁc and throughput can be measured in bits (ﬂits) per node and microsecond. we use this unit in section 9.10.
a common misconception consists of using throughput instead of trafﬁc. as mentioned above, throughput is the maximum accepted trafﬁc. another misconception consists of considering throughput or trafﬁc as input parameters instead of measurements,
only select an escape channel if the packet header is waiting for longer than the timeout. the motivation for this kind of selection function is that there is a high probability of an adaptive channel becoming available before the timeout expires. this kind of selection function is referred to as a time-dependent selection function [88, 94]. in particular, the behavior of progressive deadlock recovery mechanisms (see section 3.6) can be modeled by using a time-dependent selection function.
the selection function also plays a major role when real-time communication is required. in this case, best-effort packets and guaranteed packets compete for network resources. if the number of priority classes for guaranteed packets is small, each physical channel may be split into as many virtual channels as priority classes. in this case, the selection function will select the appropriate virtual channel for each packet according to its priority class. when the number of priority classes is high, the set of virtual channels may be split into two separate virtual networks, assigning best-effort packets and guaranteed packets to different virtual networks. in this case, scheduling of guaranteed packets corresponding to different priority classes can be achieved by using packet switching [294]. in this switching technique, packets are completely buffered before being routed in intermediate nodes, therefore allowing the scheduling of packets with the earliest deadline ﬁrst. wormhole switching is used for best-effort packets. latency of guaranteed packets can be made even more predictable by using an appropriate policy for channel bandwidth allocation, as indicated in the next section.
there exist some situations where several packets contend for the use of resources, therefore requiring some arbitration and allocation policy. these situations are not related to the routing algorithm but are described here for completeness. the most interesting cases of conﬂict in the use of resources arise when several virtual channels belonging to the same physical channel are ready to transfer a ﬂit, and when several packet headers arrive at a node and need to be routed. in the former case, a virtual channel allocation policy is required, while the second case requires a routing control unit allocation policy. flit-level ﬂow control across a physical channel involves allocating channel bandwidth among virtual channels that have a ﬂit ready to transmit and have space for this ﬂit at the receiving end. any arbitration algorithm can be used to allocate channel bandwidth including random, round-robin, or priority schemes. for random selection, an arbitrary virtual channel satisfying the above-mentioned conditions is selected. for round-robin selection, virtual channels are arranged in a circular list. when a virtual channel transfers a ﬂit, the next virtual channel in the list satisfying the above-mentioned conditions is selected for the next ﬂit transmission. this policy is usually referred to as demand-slotted round-robin and is the most frequently used allocation policy for virtual channels.
priority schemes require some information to be carried in the packet header. this information should be stored in a status register associated with each virtual channel reserved by the packet. deadline scheduling can be implemented by allocating channel bandwidth based on a packet’s deadline or age (earliest deadline or oldest age
node has a ﬁxed number of input and output channels, and every input channel is paired with a corresponding output channel. through the connections among these channels, there are many ways to interconnect these nodes. obviously, every node in the network should be able to reach every other node.
direct networks have been traditionally modeled by a graph g(n, c), where the vertices of the graph n represent the set of processing nodes and the edges of the graph c represent the set of communication channels. this is a very simple model that does not consider implementation issues. however, it allows the study of many interesting network properties. depending on the properties under study, a bidirectional channel may be modeled either as an edge or as two arcs in opposite directions (two unidirectional channels). the latter is the case for deadlock avoidance in chapter 3. let us assume that a bidirectional channel is modeled as an edge. some basic network properties can be deﬁned from the graph representation:
node degree: number of channels connecting that node to its neighbors. diameter: the maximum distance between two nodes in the network. regularity: a network is regular when all the nodes have the same degree. symmetry: a network is symmetric when it looks alike from every node.
a direct network is mainly characterized by three factors: topology, routing, and switching. the topology deﬁnes how the nodes are interconnected by channels and is usually modeled by a graph as indicated above. for direct networks, the ideal topology would connect every node to every other node. no message would even have to pass through an intermediate node before reaching its destination. this fully connected topology requires a router with n links (including the internal one) at each node for a network with n nodes. therefore, the cost is prohibitive for networks of moderate to large size. additionally, the number of physical connections of a node is limited by hardware constraints such as the number of available pins and the available wiring area. these engineering and scaling difﬁculties preclude the use of such fully connected networks even for small network sizes. as a consequence, many topologies have been proposed, trying to balance performance and some cost parameters. in these topologies, messages may have to traverse some intermediate nodes before reaching the destination node.
from the programmer’s perspective, the unit of information exchange is the message. the size of messages may vary depending on the application. for efﬁcient and fair use of network resources, a message is often divided into packets prior to transmission. a packet is the smallest unit of communication that contains the destination address and sequencing information, which are carried in the packet header. for topologies in which packets may have to traverse some intermediate nodes, the routing algorithm determines the path selected by a packet to reach its destination. at each intermediate node, the
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
routing algorithm indicates the next channel to be used. that channel may be selected among a set of possible choices. if all the candidate channels are busy, the packet is blocked and cannot advance. obviously, efﬁcient routing is critical to the performance of interconnection networks.
when a message or packet header reaches an intermediate node, a switching mechanism determines how and when the router switch is set; that is, the input channel is connected to the output channel selected by the routing algorithm. in other words, the switching mechanism determines how network resources are allocated for message transmission. for example, in circuit switching, all the channels required by a message are reserved before starting message transmission. in packet switching, however, a packet is transmitted through a channel as soon as that channel is reserved, but the next channel is not reserved (assuming that it is available) until the packet releases the channel it is currently using. obviously, some buffer space is required to store the packet until the next channel is reserved. that buffer should be allocated before starting packet transmission. so, buffer allocation is closely related to the switching mechanism. flow control is also closely related to the switching and buffer allocation mechanisms. the ﬂow control mechanism establishes a dialog between sender and receiver nodes, allowing and stopping the advance of information. if a packet is blocked, it requires some buffer space to be stored. when there is no more available buffer space, the ﬂow control mechanism stops information transmission. when the packet advances and buffer space is available, transmission is started again. if there is no ﬂow control and no more buffer space is available, the packet may be dropped or derouted through another channel.
the above factors affect the network performance. they are not independent of each other but are closely related. for example, if a switching mechanism reserves resources in an aggressive way (as soon as a packet header is received), packet latency can be reduced. however, each packet may be holding several channels at the same time. so, such a switching mechanism may cause severe network congestion and, consequently, make the design of efﬁcient routing and ﬂow control policies difﬁcult. the network topology also affects performance, as well as how the network trafﬁc can be distributed over available channels. in most cases, the choice of a suitable network topology is restricted by wiring and packaging constraints.
many network topologies have been proposed in terms of their graph-theoretical properties. however, very few of them have ever been implemented. most of the implemented networks have an orthogonal topology. a network topology is orthogonal if and only if nodes can be arranged in an orthogonal n-dimensional space, and every link can be arranged in such a way that it produces a displacement in a single dimension. orthogonal topologies can be further classiﬁed as strictly orthogonal and weakly orthogonal. in a strictly orthogonal topology, every node has at least one link crossing each dimension. in a weakly orthogonal topology, some nodes may not have any link in some dimensions.
hence, it is not possible to cross every dimension from every node. crossing a given dimension from a given node may require moving in another dimension ﬁrst.
the most interesting property of strictly orthogonal topologies is that routing is very simple. thus, the routing algorithm can be efﬁciently implemented in hardware. effectively, in a strictly orthogonal topology, nodes can be numbered by using their coordinates in the n-dimensional space. since each link traverses a single dimension and every node has at least one link crossing each dimension, the distance between two nodes can be computed as the sum of dimension offsets. also, the displacement along a given link only modiﬁes the offset in the corresponding dimension. taking into account that it is possible to cross any dimension from any node in the network, routing can be easily implemented by selecting a link that decrements the absolute value of the offset in some dimension. the set of dimension offsets can be stored in the packet header and updated (by adding or subtracting one unit) every time the packet is successfully routed at some intermediate node. if the topology is not strictly orthogonal, however, routing may become much more complex.
the most popular direct networks are the n-dimensional mesh, the k-ary n-cube or torus, and the hypercube. all of them are strictly orthogonal. formally, an n-dimensional mesh has k0 × k1 × · · · × kn−2 × kn−1 nodes, ki nodes along each dimension i, where ki ≥ 2 and 0 ≤ i ≤ n − 1. each node x is identiﬁed by n coordinates, (xn−1, xn−2, . . . , x1, x0), where 0 ≤ xi ≤ ki − 1 for 0 ≤ i ≤ n − 1. two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = xj ± 1. thus, nodes have from n to 2n neighbors, depending on their location in the mesh. therefore, this topology is not regular.
in a bidirectional k-ary n-cube [70], all nodes have the same number of neighbors. the deﬁnition of a k-ary n-cube differs from that of an n-dimensional mesh in that all of the ki are equal to k and two nodes x and y are neighbors if and only if yi = xi for all i, 0 ≤ i ≤ n − 1, except one, j , where yj = (xj ± 1) mod k. the change to modular arithmetic in the deﬁnition adds wraparound channels to the k-ary n-cube, giving it regularity and symmetry. every node has n neighbors if k = 2, and 2n neighbors if k > 2. when n = 1, the k-ary n-cube collapses to a bidirectional ring with k nodes.
another topology with regularity and symmetry is the hypercube, which is a special case of both n-dimensional meshes and k-ary n-cubes. a hypercube is an n-dimensional mesh in which ki = 2 for 0 ≤ i ≤ n − 1, or a 2-ary n-cube, also referred to as a binary n-cube.
figure 1.5(a) depicts a binary 4-cube or 16-node hypercube. figure 1.5(b) illustrates a 3-ary 2-cube or two-dimensional (2-d) torus. figure 1.5(c) shows a 3-ary threedimensional (3-d) mesh, resulting by removing the wraparound channels from a 3-ary 3-cube.
two conﬂicting requirements of a direct network are that it must accommodate a large number of nodes while maintaining a low network latency. this issue will be addressed in chapter 7.
in all-to-all communication, all processes in a process group perform their own one-to-all communication. thus, each process will receive n messages from n different senders in the process group. again, there are two distinct services:
all-broadcast. all processes perform their own broadcast. usually, the received n messages are concatenated together based on the id of the senders. thus, all processes have the same set of received messages. this service is also referred to as gossiping or total exchange.
all-scatter. all processes perform their own scatter. the n concatenated messages are different for different processes. this service is also referred to as personalized all-to-all broadcast, index, or complete exchange.
higher latency than the average value. a high value of the standard deviation usually indicates that some messages are blocked for a long time in the network. the peak value of the latency can also help in identifying these situations.
latency is measured in time units. however, when comparing several design choices, the absolute value is not important. as many comparisons are performed by using network simulators, latency can be measured in simulator clock cycles. unless otherwise stated, the latency plots presented in this chapter for unicast messages measure the average value of the time elapsed from when the message header is injected into the network at the source node until the last unit of information is received at the destination node. in most cases, the simulator clock cycle is the unit of measurement. however, in section 9.10, latency is measured in nanoseconds.
throughput is the maximum amount of information delivered per time unit. it can also be deﬁned as the maximum trafﬁc accepted by the network, where trafﬁc, or accepted trafﬁc is the amount of information delivered per time unit. throughput could be measured in messages per second or messages per clock cycle, depending on whether absolute or relative timing is used. however, throughput would depend on message and network size. so, throughput is usually normalized, dividing it by message size and network size. as a result, throughput can be measured in bits per node and microsecond, or in bits per node and clock cycle. again, when comparing different design choices by simulation, and assuming that channel width is equal to ﬂit size, throughput can be measured in ﬂits per node and clock cycle. alternatively, accepted trafﬁc and throughput can be measured as a fraction of network capacity. a uniformly loaded network is operating at capacity if the most heavily loaded channel is used 100% of the time [72]. again, network capacity depends on the communication pattern.
a standard way to measure accepted trafﬁc and throughput was proposed at the workshop on parallel computer routing and communication (pcrcw’94). it consists of representing them as a fraction of the network capacity for a uniform distribution of destinations, assuming that the most heavily loaded channels are located in the network bisection. this network capacity is referred to as normalized bandwidth. so, regardless of the communication pattern used, it is recommended to measure applied load, accepted trafﬁc, and throughput as a fraction of normalized bandwidth. normalized bandwidth can be easily derived by considering that 50% of uniform random trafﬁc crosses the bisection of the network. thus, if a network has bisection bandwidth b bits/s, each node in an n-node network can inject 2b/n bits/s at the maximum load. unless otherwise stated, accepted trafﬁc and throughput are measured as a fraction of normalized bandwidth. while this is acceptable when comparing different design choices in the same network, it should be taken into account that those choices may lead to different clock cycles. in this case, each set of design parameters may produce a different bisection bandwidth, therefore invalidating the normalized bandwidth as a trafﬁc unit. in that case, accepted trafﬁc and throughput can be measured in bits (ﬂits) per node and microsecond. we use this unit in section 9.10.
a common misconception consists of using throughput instead of trafﬁc. as mentioned above, throughput is the maximum accepted trafﬁc. another misconception consists of considering throughput or trafﬁc as input parameters instead of measurements,
this can be achieved in a fully distributed manner. the nodes are generally assumed to possess some ability for self-test as well as the ability to test neighboring nodes. we can envision an approach where, in one step, each node performs a self-test and interrogates the status of its neighbors. if neighbors or links in two or more dimensions are faulty, the node transitions to a faulty state, even though it may be nonfaulty. this diagnosis step is repeated. after a ﬁnite number of steps bounded by the diameter of the network, fault regions will have been created and will be rectangular in shape. in multidimensional meshes and tori under the block fault model, fault-free nodes are adjacent to at most one faulty node (i.e., along only one dimension). note that single component faults correspond to the block fault model with block sizes of 1.
the block fault model is particularly well suited to evolving trends in packaging technology and the use of low-dimensional networks. we will continue to see subnetworks implemented in chips, multichip modules (mcms), and boards. failures within these components will produce block faults at the chip, board, and mcm level. construction of block fault regions often naturally falls along chip, mcm, and board boundaries. for example, if submeshes are implemented on a chip, failure of two or more processors on a chip may result in marking the chip as faulty, leading to a block fault region comprised of all of the processors on the chip. the advantages in doing so include simpler solutions to deadlock- and livelock-free routing.
failures may be either static or dynamic. static failures are present in the network when the system is powered on. dynamic failures appear at random during the operation of the system. both types of faults are generally considered to be permanent; that is, they remain in the system until it is repaired.alternatively, faults may be transient.as integrated circuit feature sizes continue to decrease and speeds continue to increase, problems arise with soft errors that are transient or dynamic in nature. the difﬁculty of designing for such faults is that they often cannot be reproduced. for example, soft errors that occur in ﬂight tests of avionics hardware often cannot be reproduced on the ground. in addition,
in addition to the topologies deﬁned above, many other topologies have been proposed in the literature. most of them were proposed with the goal of minimizing the network diameter for a given number of nodes and node degree. as will be seen in chapter 2, for pipelined switching techniques, network latency is almost insensitive to network diameter, especially when messages are long. so it is unlikely that those topologies are implemented. in the following paragraphs, we present an informal description of some relevant direct network topologies.
a popular topology is the tree. this topology has a root node connected to a certain number of descendant nodes. each of these nodes is in turn connected to a disjoint set (possibly empty) of descendants.a node with no descendants is a leaf node.a characteristic property of trees is that every node but the root has a single parent node. therefore, trees contain no cycles. a tree in which every node but the leaves has a ﬁxed number k of descendants is a k-ary tree. when the distance from every leaf node to the root is the same (i.e., all the branches of the tree have the same length), the tree is balanced. figure 1.6(a) and (b) shows an unbalanced and a balanced binary tree, respectively.
time. the start-up latency, ts, is the time required for message framing/unframing, memory/buffer copying, validation, and so on, at both source and destination nodes. the start-up latency is mainly dependent on the design of system software within the nodes and the interface between nodes and routers. the network latency equals the elapsed time after the head of a message has entered the network at the source until the tail of the message emerges from the network at the destination. given a source and destination node, the startup and network latencies are static values, frequently used to characterize contention-free networks. the blocking time includes all possible delays encountered during the lifetime of a message. these delays are mainly due to conﬂicts over the use of shared resources, for example, a message encountering a busy channel or a full buffer. blocking time reﬂects the dynamic behavior of the network due to the passing of multiple messages and may be high if the network trafﬁc is heavy or unevenly distributed.
multicast latency refers to the elapsed time from when the source sends out its ﬁrst copy of the message until the last destination has received its copy of the message. multicast latency can be critical to program speedup because, as in the case of barrier synchronization and data replication, the multicast operation may be performed in the serial component of the parallel algorithm.
we shall use graphs to model the underlying topology of multicomputers. let graph g(v , e) denote a graph with node set v and edge set e. when g is known from context, the sets v (g) and e(g) will be referred to as v and e, respectively. a path with length n is a sequence of edges e1, e2, . . . , en such that
1. ei (cid:16)= ej if i (cid:16)= j . 2. ei and ei+1 have a common end node. 3. if ei is not the ﬁrst or last edge, then it shares one of its end nodes with ei−1 and
suppose ei = (vi , vi+1) for 1 ≤ i ≤ n. in the following discussion, a path with length n will be represented by its node-visiting sequence (v1, v2, . . . , vn, vn+1). a cycle is a path whose starting and ending nodes are the same (i.e., v1 = vn+1). furthermore, we assume that every pair of nodes in the path, except v1 and vn+1, are different. a graph is said to be connected if every pair of its nodes are joined by a path. a tree is a connected graph that contains no cycles. a graph f (v , e) is a subgraph of another graph g(v , e) if v (f ) ⊆ v (g) and e(f ) ⊆ e(g). a subgraph that is a tree is referred to as a subtree. for a pair of nodes u, v in v (g), dg(u, v) denotes the length (the number of edges) of a shortest path from u to v in g.
the interconnection topology of a multicomputer is denoted by a host graph g(v , e), where each vertex in v corresponds to a node and each edge in e corresponds to a communication channel (link). for a multicast communication, let u0 denote the source node and
in addition to the topologies deﬁned above, many other topologies have been proposed in the literature. most of them were proposed with the goal of minimizing the network diameter for a given number of nodes and node degree. as will be seen in chapter 2, for pipelined switching techniques, network latency is almost insensitive to network diameter, especially when messages are long. so it is unlikely that those topologies are implemented. in the following paragraphs, we present an informal description of some relevant direct network topologies.
a popular topology is the tree. this topology has a root node connected to a certain number of descendant nodes. each of these nodes is in turn connected to a disjoint set (possibly empty) of descendants.a node with no descendants is a leaf node.a characteristic property of trees is that every node but the root has a single parent node. therefore, trees contain no cycles. a tree in which every node but the leaves has a ﬁxed number k of descendants is a k-ary tree. when the distance from every leaf node to the root is the same (i.e., all the branches of the tree have the same length), the tree is balanced. figure 1.6(a) and (b) shows an unbalanced and a balanced binary tree, respectively.
in addition to the topologies deﬁned above, many other topologies have been proposed in the literature. most of them were proposed with the goal of minimizing the network diameter for a given number of nodes and node degree. as will be seen in chapter 2, for pipelined switching techniques, network latency is almost insensitive to network diameter, especially when messages are long. so it is unlikely that those topologies are implemented. in the following paragraphs, we present an informal description of some relevant direct network topologies.
a popular topology is the tree. this topology has a root node connected to a certain number of descendant nodes. each of these nodes is in turn connected to a disjoint set (possibly empty) of descendants.a node with no descendants is a leaf node.a characteristic property of trees is that every node but the root has a single parent node. therefore, trees contain no cycles. a tree in which every node but the leaves has a ﬁxed number k of descendants is a k-ary tree. when the distance from every leaf node to the root is the same (i.e., all the branches of the tree have the same length), the tree is balanced. figure 1.6(a) and (b) shows an unbalanced and a balanced binary tree, respectively.
in addition to the topologies deﬁned above, many other topologies have been proposed in the literature. most of them were proposed with the goal of minimizing the network diameter for a given number of nodes and node degree. as will be seen in chapter 2, for pipelined switching techniques, network latency is almost insensitive to network diameter, especially when messages are long. so it is unlikely that those topologies are implemented. in the following paragraphs, we present an informal description of some relevant direct network topologies.
a popular topology is the tree. this topology has a root node connected to a certain number of descendant nodes. each of these nodes is in turn connected to a disjoint set (possibly empty) of descendants.a node with no descendants is a leaf node.a characteristic property of trees is that every node but the root has a single parent node. therefore, trees contain no cycles. a tree in which every node but the leaves has a ﬁxed number k of descendants is a k-ary tree. when the distance from every leaf node to the root is the same (i.e., all the branches of the tree have the same length), the tree is balanced. figure 1.6(a) and (b) shows an unbalanced and a balanced binary tree, respectively.
figure 5.9 multiaddress encoding schemes: (a) all-destination encoding, (b) bit string encoding, (c) multiple-region broadcast encoding, (d) multiple-region stride encoding, and (e) multiple-region bit string encoding.
the remaining encoding schemes try to optimize the header length by considering ranges of addresses or regions. in figure 5.9(c), each region is speciﬁed by two ﬁelds: the beginning and ending addresses of the region. within each region, the message is broadcast to all the addresses in the range. in some applications, a node may send a message to a set of destination addresses that have a constant distance between two adjacent addresses. a suitable encoding scheme for those applications consists of adding a stride to the deﬁnition of each region, as shown in figure 5.9(d). finally, if the destination addresses are irregularly distributed but can be grouped into regions, each region can be speciﬁed by a bit string, in addition to the beginning and ending addresses (see figure 5.9(e)). the main drawback of encoding schemes based on regions is that the routing hardware required to decode addresses is complex. also, several header ﬂits may be required to encode each region. those ﬂits should reach the router before starting a routing operation.
one approach to multicast routing is to deliver the message along a common path as far as possible, then replicate the message and forward each copy on a different channel bound for a unique set of destination nodes. the path followed by each copy may further branch in this manner until the message is delivered to every destination node. in such tree-based routing, the destination set is partitioned at the source, and separate copies are sent on one
resources in a cyclic way. if we prevent packets from waiting on some resources when blocked, we can prevent deadlocks. however, the additional routing ﬂexibility offered by this model is usually small.
models based on waiting channels are more general than the ones based on channel dependencies. in fact, the latter are particular cases of the former when a packet is allowed to wait on all the channels supplied by the routing function. another particular case of interest arises when a blocked packet always waits on a single channel. in this case, this model matches the behavior of routers that buffer blocked packets in queues associated with output channels. note that those routers do not use wormhole switching because they buffer whole packets.
although models based on waiting channels are more general than the ones based on channel dependencies, note that adaptive routing is useful because it offers alternative paths when the network is congested. in order to maximize performance, a blocked packet should be able to reserve the ﬁrst valid channel that becomes available. thus, restricting the set of routing options when a packet is blocked does not seem to be an attractive choice from the performance point of view.
a model that increases routing ﬂexibility in the absence of contention is based on the wait-for graph [73]. this graph indicates the resources that blocked packets are waiting for. packets are even allowed to use nonminimal paths as long as they are not blocked. however, blocked packets are not allowed to wait for channels held by other packets in a cyclic way. thus, some blocked packets are obliged to use deterministic routing until delivered if they produce a cycle in the wait-for graph.
an interesting model based on waiting channels is the message ﬂow model [207, 209]. in this model, a routing function is deadlock-free if and only if all the channels are deadlock-immune. a channel is deadlock-immune if every packet that reserves that channel is guaranteed to be delivered. the model starts by analyzing the channels for which a packet reserving them is immediately delivered. those channels are deadlockimmune. then the model analyzes the remaining channels step by step. in each step, the channels adjacent to the ones considered in the previous step are analyzed. a channel is deadlock-immune if for all the alternative paths a packet can follow, the next channel to be reserved is also deadlock-immune. waiting channels play a role similar to routing subfunctions, serving as escape paths when a packet is blocked.
another model uses a channel waiting graph [309]. this graph captures the relationship between channels in the same way as the channel dependency graph. however, this model does not distinguish between routing and selection functions. thus, it considers the dynamic evolution of the network because the selection function takes into account channel status. two theorems are proposed. the ﬁrst one considers that every packet has a single waiting channel. it states that a routing algorithm is deadlock-free if it is waitconnected and there are no cycles in the channel waiting graph. the routing algorithm is wait-connected if it is connected by using only waiting channels.
but the most important result is a necessary and sufﬁcient condition for deadlock-free routing. this condition assumes that a packet can wait on any channel supplied by the routing algorithm. it uses the concept of true cycles. a cycle is a true cycle if it is reachable, starting from an empty network. the theorem states that a routing algorithm is deadlock-
channels that will allow further progress to the destination.although misrouting algorithms are more ﬂexible, they usually consume more network resources. as a consequence, misrouting algorithms usually exhibit a lower performance when combined with pipelined switching techniques. also, misrouting algorithms may suffer from livelock, as seen in chapter 3. misrouting algorithms are usually proposed for fault-tolerant routing because they are able to ﬁnd alternative paths when all the minimal paths are faulty. these algorithms will also be studied in chapter 6.
at the lowest level, routing algorithms can be completely adaptive (also known as fully adaptive) or partially adaptive. a fully adaptive algorithm can use all the physical paths in its class. for example, a proﬁtable algorithm that is fully adaptive is able to choose among all the minimal paths available in the network. these algorithms are also called fully adaptive minimal routing algorithms. it should be noted that although all the physical paths are available, a given routing algorithm may restrict the use of virtual channels in order to avoid deadlock. a routing algorithm that maximizes the number of routing options while avoiding deadlock is referred to as maximally adaptive. an even higher ﬂexibility in the use of virtual channels can be achieved by using deadlock recovery techniques. in this case, there is no restriction on the use of virtual channels, and the corresponding routing algorithm is referred to as true fully adaptive. a completely adaptive backtracking algorithm is also called exhaustive. partially adaptive algorithms are only able to use a subset of the paths in their class.
note that deterministic routing algorithms should be progressive and proﬁtable. backtracking makes no sense because the same path will be reserved again. also, misrouting is not interesting because some bandwidth is wasted without any beneﬁt.
this chapter is organized as follows. section 4.2 studies some deterministic routing algorithms as well as a basic design methodology. section 4.3 presents some partially adaptive routing algorithms and a design methodology. section 4.4 analyzes fully adaptive routing algorithms and their evolution, also presenting design methodologies. section 4.5 describes some routing algorithms that maximize adaptivity or minimize the routing resources required for fully adaptive routing. section 4.6 presents some nonminimal routing algorithms. section 4.7 describes some backtracking algorithms. as backtracking algorithms have interesting properties for fault-tolerant routing, these algorithms will also be analyzed in chapter 6. sections 4.8 and 4.9 study some routing algorithms for switch-based networks, focusing on multistage interconnection networks and irregular topologies, respectively. finally, section 4.10 presents several selection functions as well as some resource allocation policies. the chapter ends with some engineering issues and commented references.
deterministic routing algorithms establish the path as a function of the destination address, always supplying the same path between every pair of nodes. deterministic routing is distinguished from oblivious routing. although both concepts are sometimes considered
assume that p moves to the a queue in node (3, 1). as p is still allowed to move to the inside, it moves to the b queue in node (3, 2), reaching its destination node, and moving to the delivery queue in node (3, 2).
all the routing algorithms described in previous sections use avoidance techniques to handle deadlocks, therefore restricting routing. deadlock recovery techniques do not restrict routing to avoid deadlock. hence, routing strategies based on deadlock recovery allow maximum routing adaptivity (even beyond that proposed in [306, 308]) as well as minimum resource requirements. in particular, progressive deadlock recovery techniques, like disha (see section 3.6), decouple deadlock-handling resources from normal routing resources by dedicating minimum hardware to efﬁcient deadlock recovery in order to make the common case (i.e., no deadlocks) fast.as proposed, sequential recovery from deadlocks requires only one central ﬂit-sized buffer applicable to arbitrary network topologies [8, 9], and concurrent recovery requires at most two central buffers for any topology on which a hamiltonian path or a spanning tree can be deﬁned [10].
when routing is not restricted, no virtual channels are dedicated to avoid deadlocks. instead, virtual channels are used for the sole purpose of improving channel utilization and adaptivity. hence, true fully adaptive routing is permitted on all virtual channels within each physical channel, regardless of network topology. true fully adaptive routing can be minimal or nonminimal, depending on whether routing is restricted to minimal paths or not. note that fully adaptive routing used in the context of avoidance-based algorithms connotes full adaptivity across all physical channels but only partial adaptivity across virtual channels within a given physical channel. on the other hand, true fully adaptive routing used in the context of recovery-based algorithms connotes full adaptivity across all physical channel dimensions as well as across all virtual channels within a given physical channel. routing restrictions on virtual channels are therefore completely relaxed so that no ordering among these resources is enforced.
as an example, figure 4.28 shows a true fully adaptive minimal routing algorithm for 2-d meshes. each physical channel is assumed to be split into two virtual channels a and b. in this ﬁgure, xa+ and xb+ denote the a and b channels, respectively, in the positive direction of the x dimension. a similar notation is used for the other direction and dimension. as can be seen, no routing restrictions are enforced, except for paths to be minimal.
figure 4.28 only shows the routing algorithm. it does not include deadlock handling. this issue was covered in section 3.6. for the sake of completeness, figure 4.29 shows a ﬂow diagram of the true fully adaptive nonminimal routing algorithm implemented by disha. the shaded box corresponds to the routing algorithm described in figure 4.28 (extended to handle nonminimal routing). if after a number of tries a packet cannot access any virtual channel along any minimal path to its destination, it is allowed to access any misrouting channel except those resulting in 180-degree turns. if all minimal and misrouting channels remain busy for longer than the timeout for deadlock detection, the
as presented above, the chain algorithm is only applicable to those cases in which the source address is less than or greater than (according to <d) all the destination addresses. clearly, this situation is not true in general. for a hypercube network in which e-cube routing is used, it is straightforward to construct a depth-contention-free multicast algorithm using the chain algorithm. speciﬁcally, the symmetry of the hypercube effectively allows the source node to play the role of the ﬁrst node in a dimension-ordered chain. the exclusive-or operation, denoted ⊕, is used to carry out this task.
a sequence d1, d2, . . . , dm−1 of hypercube addresses is called a d0-relative dimension-ordered chain if and only if d0 ⊕ d1, d0 ⊕ d2, . . . , d0 ⊕ dm−1 is a dimension-ordered chain. let d0 be the address of the source of a multicast with m − 1 destinations. the source can easily sort the m − 1 destinations into a d0-relative dimension-ordered chain, = d1, d2, . . . , dm−1. the source may then execute the chain algorithm using  instead of the original addresses. the multicast tree resulting from this method is called a unicastcube, or u-cube, tree. an interesting and useful property of the u-cube tree involves broadcast: the well-known binomial tree [334] is a special case of the u-cube tree when the source node and all destinations form a subcube. also, the implementation constituting a u-cube tree is a depth-contention-free, minimum-time implementation.
unlike hypercubes, n-dimensional meshes are not symmetric. the source address may lie in the middle of a dimension-ordered chain of destination addresses, but the exclusive-or operation is not applicable in the implementation of depth-contention-free communication. however, another relatively simple method may be used, again based on the chain algorithm, to address this problem.
the u-mesh algorithm is given in figure 5.54. the source and destination addresses are sorted into a dimension-ordered chain, denoted , at the time when multicast is initiated by calling the u-mesh algorithm. the source node successively divides  in half. if the source is in the lower half, then it sends a copy of the message to the smallest node (with respect to <d) in the upper half. that node will be responsible for delivering the message to the other nodes in the upper half, using the same u-mesh algorithm. if the source is in the upper half, then it sends a copy of the message to the largest node in the lower half. in addition to the data, each message carries the addresses of the destinations for which the receiving node is responsible. at each step, the source deletes from  the receiving node and those nodes in the half not containing the source. the source continues this procedure until  contains only its own address. note that if the source happens to lie at the beginning or end of , then the u-mesh algorithm degenerates to the chain algorithm.
as presented above, the chain algorithm is only applicable to those cases in which the source address is less than or greater than (according to <d) all the destination addresses. clearly, this situation is not true in general. for a hypercube network in which e-cube routing is used, it is straightforward to construct a depth-contention-free multicast algorithm using the chain algorithm. speciﬁcally, the symmetry of the hypercube effectively allows the source node to play the role of the ﬁrst node in a dimension-ordered chain. the exclusive-or operation, denoted ⊕, is used to carry out this task.
a sequence d1, d2, . . . , dm−1 of hypercube addresses is called a d0-relative dimension-ordered chain if and only if d0 ⊕ d1, d0 ⊕ d2, . . . , d0 ⊕ dm−1 is a dimension-ordered chain. let d0 be the address of the source of a multicast with m − 1 destinations. the source can easily sort the m − 1 destinations into a d0-relative dimension-ordered chain, = d1, d2, . . . , dm−1. the source may then execute the chain algorithm using  instead of the original addresses. the multicast tree resulting from this method is called a unicastcube, or u-cube, tree. an interesting and useful property of the u-cube tree involves broadcast: the well-known binomial tree [334] is a special case of the u-cube tree when the source node and all destinations form a subcube. also, the implementation constituting a u-cube tree is a depth-contention-free, minimum-time implementation.
unlike hypercubes, n-dimensional meshes are not symmetric. the source address may lie in the middle of a dimension-ordered chain of destination addresses, but the exclusive-or operation is not applicable in the implementation of depth-contention-free communication. however, another relatively simple method may be used, again based on the chain algorithm, to address this problem.
the u-mesh algorithm is given in figure 5.54. the source and destination addresses are sorted into a dimension-ordered chain, denoted , at the time when multicast is initiated by calling the u-mesh algorithm. the source node successively divides  in half. if the source is in the lower half, then it sends a copy of the message to the smallest node (with respect to <d) in the upper half. that node will be responsible for delivering the message to the other nodes in the upper half, using the same u-mesh algorithm. if the source is in the upper half, then it sends a copy of the message to the largest node in the lower half. in addition to the data, each message carries the addresses of the destinations for which the receiving node is responsible. at each step, the source deletes from  the receiving node and those nodes in the half not containing the source. the source continues this procedure until  contains only its own address. note that if the source happens to lie at the beginning or end of , then the u-mesh algorithm degenerates to the chain algorithm.
figure 4.1 presents a taxonomy of routing algorithms that extends an earlier classiﬁcation scheme [125]. routing algorithms can be classiﬁed according to several criteria. those criteria are indicated in the left column in italics. each row contains the alternative approaches that can be followed for each criterion. arrows indicate the relations between different approaches. an overview of the taxonomy is presented ﬁrst, developing it in greater detail later. routing algorithms can be ﬁrst classiﬁed according to the number of destinations. packets may have a single destination (unicast routing) or multiple destinations (multicast routing). multicast routing will be studied in depth in chapter 5 and is included here for completeness.
routing algorithms can also be classiﬁed according to the place where routing decisions are taken. basically, the path can be either established by a centralized controller (centralized routing) at the source node prior to packet injection (source routing) or determined in a distributed manner while the packet travels across the network (distributed routing). hybrid schemes are also possible. we call these hybrid schemes multiphase routing. in multiphase routing, the source node computes some destination nodes. the
as presented above, the chain algorithm is only applicable to those cases in which the source address is less than or greater than (according to <d) all the destination addresses. clearly, this situation is not true in general. for a hypercube network in which e-cube routing is used, it is straightforward to construct a depth-contention-free multicast algorithm using the chain algorithm. speciﬁcally, the symmetry of the hypercube effectively allows the source node to play the role of the ﬁrst node in a dimension-ordered chain. the exclusive-or operation, denoted ⊕, is used to carry out this task.
a sequence d1, d2, . . . , dm−1 of hypercube addresses is called a d0-relative dimension-ordered chain if and only if d0 ⊕ d1, d0 ⊕ d2, . . . , d0 ⊕ dm−1 is a dimension-ordered chain. let d0 be the address of the source of a multicast with m − 1 destinations. the source can easily sort the m − 1 destinations into a d0-relative dimension-ordered chain, = d1, d2, . . . , dm−1. the source may then execute the chain algorithm using  instead of the original addresses. the multicast tree resulting from this method is called a unicastcube, or u-cube, tree. an interesting and useful property of the u-cube tree involves broadcast: the well-known binomial tree [334] is a special case of the u-cube tree when the source node and all destinations form a subcube. also, the implementation constituting a u-cube tree is a depth-contention-free, minimum-time implementation.
unlike hypercubes, n-dimensional meshes are not symmetric. the source address may lie in the middle of a dimension-ordered chain of destination addresses, but the exclusive-or operation is not applicable in the implementation of depth-contention-free communication. however, another relatively simple method may be used, again based on the chain algorithm, to address this problem.
the u-mesh algorithm is given in figure 5.54. the source and destination addresses are sorted into a dimension-ordered chain, denoted , at the time when multicast is initiated by calling the u-mesh algorithm. the source node successively divides  in half. if the source is in the lower half, then it sends a copy of the message to the smallest node (with respect to <d) in the upper half. that node will be responsible for delivering the message to the other nodes in the upper half, using the same u-mesh algorithm. if the source is in the upper half, then it sends a copy of the message to the largest node in the lower half. in addition to the data, each message carries the addresses of the destinations for which the receiving node is responsible. at each step, the source deletes from  the receiving node and those nodes in the half not containing the source. the source continues this procedure until  contains only its own address. note that if the source happens to lie at the beginning or end of , then the u-mesh algorithm degenerates to the chain algorithm.
path between every input/output pair, thus minimizing the number of switches and stages. however, it is also possible to provide multiple paths to reduce conﬂicts and increase fault tolerance. these blocking networks are also known as multipath networks.
2. nonblocking. any input port can be connected to any free output port without affecting the existing connections. nonblocking networks have the same functionality as a crossbar. they require multiple paths between every input and output, which in turn leads to extra stages.
3. rearrangeable. any input port can be connected to any free output port. however, the existing connections may require rearrangement of paths. these networks also require multiple paths between every input and output, but the number of paths and the cost is smaller than in the case of nonblocking networks.
nonblocking networks are expensive. although they are cheaper than a crossbar of the same size, their cost is prohibitive for large sizes. the best-known example of a nonblocking multistage network is the clos network, initially proposed for telephone networks. rearrangeable networks require less stages or simpler switches than a nonblocking network. the best-known example of a rearrangeable network is the bene˘s network. figure 1.17 shows an 8 × 8 bene˘s network. for 2n inputs, this network requires 2n − 1 stages and provides 2n−1 alternative paths. rearrangeable networks require a central controller to rearrange connections and were proposed for array processors. however, connections cannot be easily rearranged on multiprocessors because processors access the network asynchronously. so, rearrangeable networks behave like blocking networks when accesses are asynchronous. thus, this class has not been included in figure 1.2. we will mainly focus on blocking networks.
up to now, there has been no agreement on a set of standard traces that could be used for network evaluation. most performance analysis used synthetic workloads with different characteristics. in what follows, we describe the most frequently used workload models. these models can be used in the absence of more detailed information about the applications.
the workload model is basically deﬁned by three parameters: distribution of destinations, injection rate, and message length. the distribution of destinations indicates the destination for the next message at each node. the most frequently used distribution is the uniform one. in this distribution, the probability of node i sending a message to node j is the same for all i and j , i (cid:16)= j [288]. the case of nodes sending messages to themselves is excluded because we are interested in message transfers that use the network. the uniform distribution makes no assumptions about the type of computation generating the messages. in the study of interconnection networks, it is the most frequently used distribution. the uniform distribution provides what is likely to be an upper bound on the mean internode distance because most computations exhibit some degree of communication locality.
communication locality can be classiﬁed as spatial or temporal [288]. an application exhibits spatial locality when the mean internode distance is smaller than in the uniform distribution. as a result, each message consumes less resources, also reducing contention. an application has temporal locality when it exhibits communication afﬁnity among a subset of nodes. as a consequence, the probability of sending messages to nodes that were recently used as destinations for other messages is higher than for other nodes. it should be noted that nodes exhibiting communication afﬁnity need not be near one another in the network.
when network trafﬁc is not uniform, we would expect any reasonable mapping of a parallel computation to place those tasks that exchange messages with high frequency in close physical locations. two simple distributions to model spatial locality are the sphere of locality and the decreasing probability distribution [288]. in the former, a node sends messages to nodes inside a sphere centered on the source node with some usually high probability φ, and to nodes outside the sphere with probability 1 − φ. all the nodes inside the sphere have the same probability of being reached. the same occurs for the nodes outside the sphere. it should be noted that when the network size varies, the ratio between the number of nodes inside and outside the sphere is not constant. this distribution models the communication locality typical of programs solving structured problems (e.g., the nearest-neighbor communication typical of iterative partial differential equation solvers coupled with global communication for convergence checking). in practice, the sphere can be replaced by other geometric ﬁgures depending on the topology. for example, it could become a square or a cube in 2-d and 3-d meshes, respectively.
in the decreasing probability distribution, the probability of sending a message to a node decreases as the distance between the source and destination nodes increases. reed and grunwald [288] proposed the distribution function (d) = decay(l, dmax) × ld , 0 < l < 1, where d is the distance between the source and destination nodes, dmax is the network diameter, and l is a locality parameter. decay(l, dmax) is a normalizing constant for the probability , chosen such that the sum of the probabilities is equal to one. small values of the locality parameter l mean a high degree of locality; larger values of l mean
programming multicomputers is not an easy task. the programmer has to take care of distributing code and data among the processors in an efﬁcient way, invoking messagepassing calls whenever some data are needed by other processors. on the other hand, shared-memory multiprocessors provide a single memory space to all the processors, simplifying the task of exchanging data among processors. access to shared memory has been traditionally implemented by using an interconnection network between processors and memory (figure 1.1(b)). this architecture is referred to as uniform memory access (uma) architecture. it is not scalable because memory access time includes the latency of the interconnection network, and this latency increases with system size.
more recently, shared-memory multiprocessors followed some trends previously established for multicomputers. in particular, memory has been physically distributed among processors, therefore reducing the memory access time for local accesses and increasing scalability. these parallel computers are referred to as distributed sharedmemory multiprocessors (dsms). accesses to remote memory are performed through an interconnection network, very much like in multicomputers. the main difference between dsms and multicomputers is that messages are initiated by memory accesses rather than by calling a system function. in order to reduce memory latency, each processor has several levels of cache memory, thus matching the speed of processors and memories. this architecture provides nonuniform memory access (numa) time. indeed, most of the nonuniformity is due to the different access time between caches and main memories, rather than the different access time between local and remote memories. the main problem arising in dsms is cache coherence. several hardware and software cache coherence protocols have been proposed. these protocols produce additional trafﬁc through the interconnection network.
the use of custom interconnects makes multicomputers and dsms quite expensive. so, networks of workstations (nows) have been proposed as an inexpensive approach to build parallel computers. nows take advantage of recent developments in lans. in particular, the use of atm switches has been proposed to implement nows. however, atm switches are still expensive, which has motivated the development of high-performance
flow control is a synchronization protocol for transmitting and receiving a unit of information. the unit of ﬂow control refers to that portion of the message whose transfer must be synchronized. this unit is deﬁned as the smallest unit of information whose transfer is requested by the sender and acknowledged by the receiver. the request/acknowledgment signaling is used to ensure successful transfer and the availability of buffer space at the receiver. note that there is no restriction on when requests or acknowledgments are actually sent or received. implementation efﬁciency governs the actual exchange of these control signals (e.g., the use of block acknowledgments). for example, it is easy to think of messages in terms of ﬁxed-length packets. a packet is forwarded across a physical channel or from the input buffers of a router to the output buffers. note that these transfers are atomic in the sense that sufﬁcient buffering must be provided so that either a packet is transferred in its entirety or transmission is delayed until sufﬁcient buffer space becomes available. in this example, the ﬂow of information is managed and controlled at the level of an entire packet.
flow control occurs at two levels. in the preceding example, message ﬂow control occurs at the level of a packet. however, the transfer of a packet across a physical channel between two routers may take several steps or cycles, for example, the transfer of a 128byte packet across a 16-bit data channel. the resulting multicycle transfers use physical channel ﬂow control to forward a message ﬂow control unit across the physical link connecting routers.
switching techniques differ in the relationship between the sizes of the physical and message ﬂow control units. in general, each message may be partitioned into ﬁxed-length packets. packets in turn may be broken into message ﬂow control units or ﬂits [77]. due to channel width constraints, multiple physical channel cycles may be used to transfer a single ﬂit. a phit is the unit of information that can be transferred across a physical channel in a single step or cycle. flits represent logical units of information, as opposed to phits, which correspond to physical quantities, that is, the number of bits that can be transferred in parallel in a single cycle. an example of a message comprised of n packets, 6 ﬂits/packet, and 2 phits/ﬂit is shown in figure 2.2.
the relationships between the sizes of phits, ﬂits, and packets differ across machines. many machines have the phit size equivalent to the ﬂit size. in the ibm sp2 switch [328], a ﬂit is 1 byte and is equivalent to a phit. alternatively, the cray t3d [312] utilizes ﬂitlevel message ﬂow control where each ﬂit is comprised of eight 16-bit phits. the speciﬁc choices reﬂect trade-offs in performance, reliability, and implementation complexity.
that each node has a list of faulty links and nodes within a k-neighborhood [198], that is, all faulty components within a distance of k links. the preceding routing algorithm might be modiﬁed as follows. consider an intermediate node x = (xn−1, xn−2, . . . , x1, x0) that receives a message with a coordinate sequence (c1, c2, . . . , cr ) that describes the remaining sequence of dimensions to be traversed to the destination. from the coordinate sequence we can construct the addresses of all nodes on a minimum-length path to the destination. for example, from intermediate node 0110, the coordinate sequence (0, 2, 3) describes a path through nodes 0111, 0011, and 1011. by examining all possible r! orderings of the coordinate sequence, we can generate all paths to the destination. if the ﬁrst k elements of any such path are nonfaulty, the message can be forwarded along this path. the complexity of this operation can be reduced by examining only the r disjoint paths to the destination. these disjoint paths correspond to the following coordinate sequences: (c1, c2, . . . , cr ), (c2, c3, . . . , cr , c1), (c3, c4, . . . , cr , c1, c2), and so on.
rather than explicitly maintaining a list of faulty components in a k-neighborhood, an alternative method for expanding the extent of fault information is by encoding the state of a node. lee and hayes [198] introduced the notion of unsafe node. a nonfaulty node is unsafe if it is adjacent to two or more faulty or unsafe nodes. each node now maintains two lists: one of faulty adjacent nodes and one for unsafe adjacent nodes. with a ﬁxed set of faulty components, the status of a node may be faulty, nonfaulty, or unsafe. the status can be computed for all nodes in parallel in o(log3 n ) steps for n nodes. each node examines the state of its neighbors and changes its state if two or more are unsafe/faulty. a node need only transmit state information to the neighboring nodes if the local state has changed. this labeling algorithm will produce rectangular regions of nodes marked as faulty or unsafe. figure 6.8 shows an example of the result of the labeling process in a 2-d mesh.
with fault distributions encoded in the state of the nodes, the approach toward routing now can be further modiﬁed. the unsafe status of nodes serves as a warning that messages may become trapped or delayed if routed via these nodes. therefore, the routing algorithm
however, if fault rings in tori overlap, then messages that use the wraparound links share virtual channels with messages that do not use the wraparound links. this sharing occurs over the physical channels corresponding to the overlapping region of the fault rings. the virtual networks are no longer independent, and cyclic dependencies can be created between the virtual networks and, as a result, among messages. it follows from the preceding discussion that four more virtual channels [37] can be introduced across each physical link, creating four additional networks to further separate the message trafﬁc over shared links. this is a rather expensive solution. the alternative is to have the network functioning as a mesh. in this case the beneﬁts of a toroidal connection are lost.
a different labeling procedure is used in [43] to permit misrouting around rectangular fault regions in mesh networks. initially fault regions are grown in a manner similar to the preceding schemes, and all nonfaulty nodes that are marked as faulty are labeled as deactivated. nonfaulty nodes on the boundary of the fault region are now labeled as unsafe. thus, all unsafe nodes are adjacent to at least one nonfaulty node. an example of faulty, unsafe, and deactivated nodes is shown in figure 6.23. there are three virtual channels traversing each physical channel. these virtual channels are partitioned into classes. nodes adjacent to only nonfaulty nodes have the virtual channels labeled as two class 1 channels and one class 2 channel. nodes adjacent to any other node type have the channels partitioned into class 2, class 3, and class 4 channels. in nonfaulty regions of the network, a message may traverse a class 1 channel along any shortest path to the destination. if no class 1 channel is available, class 2 channels are traversed in two phases. the ﬁrst phase permits dimension-order traversal of positive direction channels, and in the second phase negative direction class 2 channels can be traversed in any order (i.e., not in dimension order). the only time the fully adaptive variant of this algorithm must consider a fault region is when the last dimension to be traversed is blocked by a fault region necessitating nonminimal routing. dimension (i + 1) and dimension i channels are used to route the message around the fault region. messages ﬁrst attempt routing along a path using class 3 channels and class 2 channels in the positive direction. subsequently messages utilize class 4 channels and class 2 channels in the negative direction. note that class 3 and class 4 channels only exist across physical channels in the vicinity of faults. the routing restrictions prevent the occurrence of cyclic dependencies between the channels, avoiding deadlock.
information, 8 bits of virtual channel ﬂow control information, and 16 crc bits to protect against errors. the links implement automatic retransmission on errors until the sequence number of the transmitted ﬂit is acknowledged. the micropacket format is shown in figure 7.28. the number on top of each ﬁeld indicates the ﬁeld size in bits. virtual channel buffer management utilizes credit-based ﬂow control. on initialization, all channels credit their counterparts with buffer space corresponding to their size. the 8 bits of virtual channel ﬂow control information are used to identify the virtual channel number of the transmitted micropacket, the address of the virtual channel being credited with additional buffer space, and the amount of credit. an interesting feature of spider links is the transparent negotiation across the link to determine the width of the channel. for example, the router can negotiate to use only the lower 10 bits of the channel. the interface to the chip core remains the same, while the data rate is a function of the negotiated port width. the spider chip supports two styles of routing. the ﬁrst is source routing, which is referred to as vector routing. this mechanism is utilized for network conﬁguration information and administration. the second style is table-driven routing. the organization of the tables is based on a structured view of the network as a set of metadomains and local networks within each domain. the topology of the interdomain network and the local network can be different. messages are ﬁrst routed to the correct destination domain and then to the correct node within a domain. routing is organized as shown in figure 7.29. the destination address is a 9-bit ﬁeld, comprised of two subﬁelds: a 5-bit metadomain ﬁeld and a 4-bit local ﬁeld. the tables map these indices into 4-bit router port addresses, thereby supporting routers with up to 16 ports. access to the metatable provides a port address to route the message toward the correct domain. if the message is already in the correct domain as determined by comparison with the local meta id, the local address table is used to determine the output port that will take the message toward the destination node. not shown in the ﬁgure is a mode to force the message to use the local address. table-driven routing is oblivious and forces messages to use ﬁxed paths. the table entries are computed to avoid deadlock and can be reinitialized in response to faults. messages using vector routing can be used to reload the tables.
normally arbitration for the output of the crossbar is not possible until the routing operation has been completed and has returned a crossbar output port address. in order to overlap arbitration and table lookup, the routing operation described above produces the output port address at the next router. this 4-bit port address (dir) is passed along with the message. at the adjacent router, the port address is extracted from the header and can be immediately submitted for crossbar output arbitration while table lookup progresses in parallel. this overlap saves one cycle. the header is encoded into the 128 data bits of a
commonly operated as fifo queues. therefore, once a message occupies a buffer for a channel, no other message can access the physical channel, even if the message is blocked. alternatively, a physical channel may support several logical or virtual channels multiplexed across the physical channel. each unidirectional virtual channel is realized by an independently managed pair of message buffers as illustrated in figure 2.17. this ﬁgure shows two unidirectional virtual channels in each direction across the physical channel. consider wormhole switching with a message in each virtual channel. each message can share the physical channel on a ﬂit-by-ﬂit basis. the physical channel protocol must be able to distinguish between the virtual channels using the physical channel. logically, each virtual channel operates as if each were using a distinct physical channel operating at half the speed. virtual channels were originally introduced to solve the problem of deadlock in wormhole-switched networks. deadlock is a network state where no messages can advance because each message requires a channel occupied by another message. this issue is discussed in detail in chapter 3.
virtual channels can also be used to improve message latency and network throughput. by allowing messages to share a physical channel, messages can make progress rather than remain blocked. for example, figure 2.18 shows two messages crossing the physical channel between routers r1 and r2. with no virtual channels, message a will prevent message b from advancing until the transmission of message a has been completed. however, in the ﬁgure, there are two single-ﬂit virtual channels multiplexed over each physical channel. by multiplexing the two messages on a ﬂit-by-ﬂit basis, both messages continue to make progress. the rate at which each message is forwarded is nominally onehalf the rate achievable when the channel is not shared. in effect, the use of virtual channels decouples the physical channels from message buffers, allowing multiple messages to share a physical channel in the same manner that multiple programs may share a cpu. the overall time a message spends blocked at a router waiting for a free channel is reduced, leading to an overall reduction in individual message latency. there are two speciﬁc cases where such sharing of the physical link bandwidth is particularly beneﬁcial. consider the case where message a is temporarily blocked downstream from the current node. with an appropriate physical channel ﬂow control protocol, message b can make use of the
tiprocessors implement latency-hiding mechanisms and application performance mainly depends on the throughput achievable by the interconnection network, then adaptive routing is expected to achieve higher performance than deterministic routing. in the case of using adaptive routing, the additional cost of implementing fully adaptive routing should be kept small. therefore, routing algorithms that require few resources to avoid deadlock or to recover from it, like the ones evaluated in this chapter, should be preferred. for these routing algorithms, the additional complexity of fully adaptive routing usually produces a small reduction in clock frequency. number of virtual channels. in wormhole switching, when no virtual channels are used, blocked messages do not allow other messages to use the bandwidth of the physical channels they are occupying. adding the ﬁrst additional virtual channel usually increases throughput considerably at the expense of a small increase in latency. on the other hand, adding more virtual channels produces a much smaller increment in throughput while increasing hardware delays considerably. for deterministic routing in meshes, two virtual channels provide a good trade-off. for tori, the partially adaptive algorithm evaluated in this chapter with two virtual channels also provides a good trade-off, achieving the advantages of channel multiplexing without increasing the number of virtual channels with respect to the deterministic algorithm. if fully adaptive routing is preferred, the minimum number of virtual channels should be used. fully adaptive routing requires a minimum of two (three) virtual channels to avoid deadlock in meshes (tori). again, for applications that require low latency and produce a relatively small amount of trafﬁc, adding virtual channels does not help. virtual channels only increase performance when applications beneﬁt from a higher network throughput. hardware support for collective communication. adding hardware support for multidestination message passing usually reduces the latency of collective communication operations with respect to software algorithms. however, this reduction is very small (if any) when the number of participating nodes is small. when many nodes participate and trafﬁc is only composed of multidestination messages, latency reduction ranges from 2 to 7, depending on several parameters. in real applications, trafﬁc for collective communication operations usually represents a much smaller fraction of network trafﬁc. also, the number of participating nodes may vary considerably from one application to another. in general, this number is small except for broadcast and barrier synchronization. in summary, whether adding hardware support for collective communication is worth its cost depends on the application requirements. injection limitation mechanism. when fully adaptive routing is used, network interfaces should include some mechanism to limit the injection of new messages when the network is heavily loaded. otherwise, increasing applied load above the saturation point may degrade performance severely. in some cases, the start-up latency is so high that it effectively limits the injection rate. when the start-up latency does not prevent network saturation, simple mechanisms like restricting
link controller (lc). flow control across the physical channel between adjacent routers is implemented by this unit. the link controllers on either side of a channel coordinate to transfer ﬂow control units. sufﬁcient buffering must be provided on the receiving side to account for delays in propagation of data and ﬂow control signals. when a ﬂow control event signaling a full buffer is transmitted to the sending controller, there must still be sufﬁcient buffering at the receiver to store all of the phits in transit, as well as all of the phits that will be injected during the time it takes for the ﬂow control signal to propagate back to the sender. if virtual channels are present, the controller is also responsible for decoding the destination channel of the received phit.
virtual channel controller (vc). this component is responsible for multiplexing the contents of the virtual channels onto the physical channel. with tree-based arbitration, the delay can be expected to be logarithmic in the number of channels.
routing and arbitration unit. this logic implements the routing function. for adaptive routing protocols, the message headers are processed to compute the set of candidate output channels and generate requests for these channels. if relative addressing is being used in the network, the new headers, one for each candidate output channel, must be generated. for oblivious routing protocols, header update is a very simple operation. alternatively, if absolute addressing is used, header processing is reduced since new headers do not need to be generated. this unit also implements the selection function component of the routing algorithm: selecting the output link for an incoming message. output channel status is combined
∗ complementary channel (vd i ) and is referred to as a virtual channel trio. the router header will traverse vc i . the complementary ∗ i is reserved for use by acknowledgment ﬂits and backtracking header ﬂits. the channel v complementary channel of a trio traverses the physical channel in the direction opposite to that of its associated data channel. the channel model is illustrated in figure 2.22. there are two virtual channels vi(vr) and vj (vs) from r1 (r2) to r2 (r1). only one message can be in progress over a given data channel. therefore, compared to existing channel models, this model requires exactly two extra ﬂit buffers for each data channel—one each for the corresponding channel and the complementary channel, respectively. since control ﬂit trafﬁc is a small percentage of the overall ﬂit trafﬁc, in practice all control channels across a physical link are multiplexed through a single virtual control channel as shown in figure 2.22(b). thus, compared to the more common use of virtual channels, this model requires one extra virtual channel in each direction between a pair of adjacent routers. ∗ for example, channel c1 in figure 2.22(b) corresponds to ﬂit buffers vc s in figure 2.22(a). the implementation of pcs in the ariadne router [7] utilized two data channels and one virtual control channel over each physical link.
this separation of control trafﬁc and data trafﬁc is useful in developing fault-tolerant routing and distributed fault recovery mechanisms. such mechanisms are discussed in greater detail in chapter 6. the ariadne router [7] is a single-chip pcs router with two virtual channel trios per physical channel. the prototype router had byte-wide physical channels and 8-bit ﬂits. the format of the header ﬂit is shown in figure 2.23. in this design a single bit distinguished a control ﬂit from a data ﬂit (this only left 7-bit data ﬂits!). a single bit distinguishes between backtracking ﬂits and ﬂits making forward progress. the misroute ﬁeld keeps track of the number of misrouting steps the header has taken. the maximum number of misroutes that the header can take in this design is three. finally, two ﬁelds provide x and y offsets for routing in a 2-d mesh.
∗ complementary channel (vd i ) and is referred to as a virtual channel trio. the router header will traverse vc i . the complementary ∗ i is reserved for use by acknowledgment ﬂits and backtracking header ﬂits. the channel v complementary channel of a trio traverses the physical channel in the direction opposite to that of its associated data channel. the channel model is illustrated in figure 2.22. there are two virtual channels vi(vr) and vj (vs) from r1 (r2) to r2 (r1). only one message can be in progress over a given data channel. therefore, compared to existing channel models, this model requires exactly two extra ﬂit buffers for each data channel—one each for the corresponding channel and the complementary channel, respectively. since control ﬂit trafﬁc is a small percentage of the overall ﬂit trafﬁc, in practice all control channels across a physical link are multiplexed through a single virtual control channel as shown in figure 2.22(b). thus, compared to the more common use of virtual channels, this model requires one extra virtual channel in each direction between a pair of adjacent routers. ∗ for example, channel c1 in figure 2.22(b) corresponds to ﬂit buffers vc s in figure 2.22(a). the implementation of pcs in the ariadne router [7] utilized two data channels and one virtual control channel over each physical link.
this separation of control trafﬁc and data trafﬁc is useful in developing fault-tolerant routing and distributed fault recovery mechanisms. such mechanisms are discussed in greater detail in chapter 6. the ariadne router [7] is a single-chip pcs router with two virtual channel trios per physical channel. the prototype router had byte-wide physical channels and 8-bit ﬂits. the format of the header ﬂit is shown in figure 2.23. in this design a single bit distinguished a control ﬂit from a data ﬂit (this only left 7-bit data ﬂits!). a single bit distinguishes between backtracking ﬂits and ﬂits making forward progress. the misroute ﬁeld keeps track of the number of misrouting steps the header has taken. the maximum number of misroutes that the header can take in this design is three. finally, two ﬁelds provide x and y offsets for routing in a 2-d mesh.
in many environments, rather than minimizing message latency or maximizing network throughput, the overriding issue is the ability to tolerate the failure of network components such as routers and links. in wormhole switching, header ﬂits containing routing information establish a path through the network from source to destination. data ﬂits are pipelined through the path immediately following the header ﬂits. if the header cannot progress due to a faulty component, the message is blocked in place indeﬁnitely, holding buffer resources and blocking other messages. this situation can eventually result in a deadlocked conﬁguration of messages. while techniques such as adaptive routing can alleviate the problem, it cannot by itself solve the problem. this has motivated the development of different switching techniques.
pipelined circuit switching (pcs) combines aspects of circuit switching and wormhole switching. pcs sets up a path before starting data transmission as in circuit switching. basically, pcs differs from circuit switching in that paths are formed by virtual channels instead of physical channels. in pipelined circuit switching, data ﬂits do not immediately follow the header ﬂits into the network as in wormhole switching. consequently, increased ﬂexibility is available in routing the header ﬂit. for example, rather than blocking on a faulty output channel at an intermediate router, the header may backtrack to the preceding router and release the previously reserved channel. a new output channel may now be attempted at the preceding router in ﬁnding an alternative path to the destination. when the header ﬁnally reaches the destination node, an acknowledgment ﬂit is transmitted back to the source node. now data ﬂits can be pipelined over the path just as in wormhole switching. the resilience to component failures is obtained at the expense of larger path setup times. this approach is ﬂexible in that headers can perform a backtracking search of the network, reserving and releasing virtual channels in an attempt to establish a fault-free path to the destination. this technique combines message pipelining from wormhole switching with a more conservative path setup algorithm based on circuit-switching techniques. a timespace diagram of a pcs message transmission over three links in the absence of any trafﬁc or failures is shown in figure 2.21.
since headers do not block holding channel or buffer resources, routing restrictions are not necessary to avoid deadlock. this increases the probability of ﬁnding a path while still avoiding deadlocked conﬁgurations of messages. moreover, reservation of virtual channels by the header does not by itself lead to use of physical channel bandwidth. therefore, unlike circuit switching, path setup does not lead to excessive blocking of other messages. as a result, multipath networks in conjunction with the ﬂexibility of pcs are good candidates for providing low-latency, fault-tolerant performance. for purely performance-driven applications where fault tolerance is not a primary concern, the added overhead of pcs makes wormhole switching the mechanism of choice.
in pcs, we distinguish between ﬂits that carry control information (e.g., header ﬂits and acknowledgment ﬂits) and those that carry data. this distinction is supported in the virtual channel model that separates control ﬂit trafﬁc and data ﬂit trafﬁc. a unidirectional is composed of a data channel, a corresponding channel, and a virtual channel vi
in many environments, rather than minimizing message latency or maximizing network throughput, the overriding issue is the ability to tolerate the failure of network components such as routers and links. in wormhole switching, header ﬂits containing routing information establish a path through the network from source to destination. data ﬂits are pipelined through the path immediately following the header ﬂits. if the header cannot progress due to a faulty component, the message is blocked in place indeﬁnitely, holding buffer resources and blocking other messages. this situation can eventually result in a deadlocked conﬁguration of messages. while techniques such as adaptive routing can alleviate the problem, it cannot by itself solve the problem. this has motivated the development of different switching techniques.
pipelined circuit switching (pcs) combines aspects of circuit switching and wormhole switching. pcs sets up a path before starting data transmission as in circuit switching. basically, pcs differs from circuit switching in that paths are formed by virtual channels instead of physical channels. in pipelined circuit switching, data ﬂits do not immediately follow the header ﬂits into the network as in wormhole switching. consequently, increased ﬂexibility is available in routing the header ﬂit. for example, rather than blocking on a faulty output channel at an intermediate router, the header may backtrack to the preceding router and release the previously reserved channel. a new output channel may now be attempted at the preceding router in ﬁnding an alternative path to the destination. when the header ﬁnally reaches the destination node, an acknowledgment ﬂit is transmitted back to the source node. now data ﬂits can be pipelined over the path just as in wormhole switching. the resilience to component failures is obtained at the expense of larger path setup times. this approach is ﬂexible in that headers can perform a backtracking search of the network, reserving and releasing virtual channels in an attempt to establish a fault-free path to the destination. this technique combines message pipelining from wormhole switching with a more conservative path setup algorithm based on circuit-switching techniques. a timespace diagram of a pcs message transmission over three links in the absence of any trafﬁc or failures is shown in figure 2.21.
since headers do not block holding channel or buffer resources, routing restrictions are not necessary to avoid deadlock. this increases the probability of ﬁnding a path while still avoiding deadlocked conﬁgurations of messages. moreover, reservation of virtual channels by the header does not by itself lead to use of physical channel bandwidth. therefore, unlike circuit switching, path setup does not lead to excessive blocking of other messages. as a result, multipath networks in conjunction with the ﬂexibility of pcs are good candidates for providing low-latency, fault-tolerant performance. for purely performance-driven applications where fault tolerance is not a primary concern, the added overhead of pcs makes wormhole switching the mechanism of choice.
in pcs, we distinguish between ﬂits that carry control information (e.g., header ﬂits and acknowledgment ﬂits) and those that carry data. this distinction is supported in the virtual channel model that separates control ﬂit trafﬁc and data ﬂit trafﬁc. a unidirectional is composed of a data channel, a corresponding channel, and a virtual channel vi
in this section we present some performance evaluation results showing the advantages of establishing virtual circuits between nodes, and caching and reusing those circuits. this technique is referred to as virtual circuit caching (vcc). vcc is implemented by allocating buffers for message transmission at source and destination nodes before data are available. those buffers are reused for all the subsequent transmissions between the same nodes, therefore reducing software overhead considerably. a directory of cached virtual circuits is kept at each node. the evaluation results presented in this section are based on [81].
the simulation study has been performed using a time-step simulator written in c. the simulated topology is a k-ary n-cube. a simulator cycle is normalized to the time to transmit one ﬂit across a physical link. in addition, the size of one ﬂit is 2 bytes. since the vcc mechanism exploits communication locality, performance evaluation was performed using traces of parallel programs. traces were gathered for several parallel kernels executing on a 256-processor system.
the communication traces were collected using an execution-driven simulator from the spasm toolset [323]. the parallel kernels are the (1) broadcast-and-roll parallel matrix multiply (mm), (2) a nas kernel (ep), (3) a fast fourier transform (fft), (4) a kalman ﬁlter (kalman), and (5) a multigrid solver for computing a 3-d ﬁeld (mg). the algorithm kernels display different communication trafﬁc patterns.
the following metric attempts to capture communication locality by computing the average number of messages that are transmitted between a pair of processors:
expression would be modiﬁed accordingly. the important point to note is that the latency is directly proportional to the distance between the source and destination nodes.
packet switching is based on the assumption that a packet must be received in its entirety before any routing decision can be made and the packet forwarded to the destination. this is not generally true. consider a 128-byte packet and the router model shown in figure 2.1. in the absence of 128-byte-wide physical channels, the transfer of the packet across the physical channel will take multiple cycles. however, the ﬁrst few bytes will contain routing information that is typically available after the ﬁrst few cycles. rather than waiting for the entire packet to be received, the packet header can be examined as soon as it is received. the router can start forwarding the header and following data bytes as soon as routing decisions have been made and the output buffer is free. in fact, the message does not even have to be buffered at the output and can cut through to the input of the next router before the complete packet has been received at the current router. this switching technique is referred to as virtual cut-through switching (vct). in the absence of blocking, the latency experienced by the header at each node is the routing latency and propagation delay through the router and along the physical channels. the message is effectively pipelined through successive switches. if the header is blocked on a busy output channel, the complete message is buffered at the node. thus, at high network loads, vct switching behaves like packet switching.
figure 2.10 illustrates a time-space diagram of a message transferred using vct switching where the message is blocked after the ﬁrst link waiting for an output channel to become free. in this case we see that the complete packet has to be transferred to the ﬁrst router where it remains blocked waiting for a free output port. however, from the ﬁgure we can see that the message is successful in cutting through the second router and across the third link.
cut-through routing is assumed to occur at the ﬂit level with the routing information contained in 1 ﬂit. this model assumes that there is no time penalty for cutting through a router if the output buffer and output channel are free. depending on the speed of operation of the routers, this may not be realistic. note that only the header experiences routing delay, as well as the switching delay and wire delay at each router. this is because the transmission is pipelined and the switch is buffered at the input and output. once the header ﬂit reaches the destination, the cycle time of this message pipeline is determined by the maximum of the switch delay and wire delay between routers. if the switch had been buffered only at the input, then in one cycle of operation, a ﬂit traverses the switch
full bandwidth of the physical channel between the routers. without virtual channels, both messages would be blocked. alternatively, consider the case where message a is a very long message relative to message b. message b can still make progress at half the link speed, and then message a can resume transmission at the full link speed. studies have shown that message trafﬁc in parallel programs is often bimodal, comprised of short (cache lines, control messages) and long messages (data structures) [176].
the approach described in the preceding paragraph does not place any restrictions on the use of the virtual channels. therefore, when used in this manner these buffers are referred to as virtual lanes. virtual channels were originally introduced as a mechanism for deadlock avoidance in networks with physical cycles, and as such routing restrictions are placed on their use. for example, packets may be prohibited from being transferred between certain classes of virtual channels to prevent cyclic waiting dependencies for buffer space. thus, in general we have virtual channels that may in turn be comprised of multiple lanes. while the choice of virtual channels at a router may be restricted, it does not matter which lane within a virtual channel is used by a message, although all of the ﬂits within a message will use the same lane within a channel.
we have seen from section 2.2 that acknowledgment trafﬁc is necessary to regulate the ﬂow of data and to ensure the availability of buffer space on the receiver.acknowledgments are necessary for each virtual channel or lane, increasing the volume of such trafﬁc across the physical channel. furthermore, for a ﬁxed amount of buffer space within a router, the size of each virtual channel or lane buffer is now smaller. therefore, the effect of optimizations such as the use of acknowledgments for a block of ﬂits or phits is limited. if physical channel bandwidth is allocated in a demand-driven fashion, the operation of the physical channel now includes the transmission of the virtual channel address to correctly identify the receiving virtual channel, or to indicate which virtual channel has available message buffers.
we can envision continuing to add virtual channels to further reduce the blocking experienced by each message. the result is increased network throughput measured in ﬂits/s, due to increased physical channel utilization. however, each additional virtual channel improves performance by a smaller amount, and the increased channel multiplexing
this scheme produces an unbalanced use of virtual channels because all the packets start using virtual channel 0 of some physical channel. however, very few packets take the maximum number of hops. this scheme can be improved by giving each packet a number of bonus cards equal to the maximum number of hops minus the number of hops it is going to take [36]. at each node, the packet has some ﬂexibility in the selection of virtual channels. the range of virtual channels that can be selected for each physical channel is equal to the number of bonus cards available plus one. thus, when no bonus cards are available, a single virtual channel per physical channel can be selected. if the packet uses virtual channel j after using virtual channel i, it consumes j − i − 1 bonus cards.
using this methodology, the hop algorithms presented in section 4.4.1 can be redeﬁned for wormhole switching. the routing algorithms resulting from the application of this methodology have the same advantages and disadvantages as the original hop algorithms for saf networks: they provide fully adaptive minimal routing for any topology at the expense of a high number of virtual channels. additionally, the number of virtual channels depends on network size, thus limiting scalability.
a useful concept to design routing algorithms consists of splitting the network into several virtual networks. a virtual network is a subset of channels that are used to route packets toward a particular set of destinations. the channel sets corresponding to different virtual networks are disjoint. depending on the destination, each packet is injected into a particular virtual network, where it is routed until it arrives at its destination. in some proposals, packets traveling in a given virtual network have some freedom to move to another virtual network. virtual networks can be implemented by using disjoint sets of virtual channels for each virtual network and mapping those channels over the same set of physical channels. of course it is also possible to implement virtual networks by using separate sets of physical channels.
within a group can be thought of as possessing a communicator for the communication domain within which it is a member. the communicator is logically a set of links to other processes within the same group (i.e., communication domain) and is referred to as an intracommunicator. the relationship between intracommunicators in a communication domain is illustrated in figure 8.8.
the notion of a communicator provides a very simple concept with which to structure groups of processes. often it is desirable to allocate one subgroup of processes to a speciﬁc subtask while another subgroup of processes is tasked with a distinct computation. processes within subgroups will communicate among themselves. there will also usually be communication between processes in distinct subgroups. in mpi terminology, each subgroup will have intracommunicators for communication within the subgroup. each process must also possess intercommunicators for communication between subgroups. logically the intercommunicator used by processes within one group can be thought of as links to processes within the other group and vice versa. this information captured within intercommunicators is illustrated in figure 8.9.
the linear rank of a process within a group provides no information about the structure of the problem, and a great deal of mental bookkeeping is often required to orchestrate interprocessor communication to follow the structure of the problem. we often keep a mental map or even a physical drawing of how processes must communicate. to facilitate such thinking, mpi provides for the speciﬁcation and use of virtual topologies of processes within a group. this virtual topology can be used for realizing efﬁcient assignments of processes to processors or for writing scalable programs. a substantial body of work exists on algorithms for mapping parallel programs onto parallel architectures. optimal mapping
it might initially appear that fully adaptive routing algorithms would enable messages to be routed around faulty regions. in reality, even the failure of a single link can destroy the deadlock freedom properties of adaptive routing algorithms. in this section, examples are presented to demonstrate how the occurrence of distinct classes of failures can lead to deadlock even for fully adaptive routing algorithms. deadlocked conﬁgurations of messages are characterized, and key issues that must be resolved are identiﬁed. in the remainder of this chapter, fault-tolerant routing techniques are then presented and discussed according to how they have chosen to address these issues.
consider duato’s protocol (dp), a fully adaptive, minimal-distance routing algorithm using wormhole switching in a 2-d mesh. this algorithm was shown in figure 4.22. recall that dp guarantees deadlock freedom by splitting each physical channel into two virtual channels: an adaptive channel and an escape channel. fully adaptive routing is permitted using the adaptive channels, while deadlock is avoided by only permitting dimension-order routing on the escape channels.
consider what happens when the router shown in figure 6.1 fails. the ﬁgure shows three messages, a, b, and c. the destination nodes of these messages are also shown, although the source nodes are not. note that the state shown in the ﬁgure is not dependent on the location of the source nodes. the ﬁgure illustrates the network state at the point in time where all messages have only the last dimension to traverse. since dp is a minimaldistance routing algorithm, in this last dimension a message has only two candidate virtual channels, and both of them traverse the same physical channel. suppose that message a reserved adaptive channels from node i to node j , and that message b reserved the escape channel at node i and then two vertical adaptive channels and two horizontal escape channels, reaching node j . it is apparent from the ﬁgure that message a can never make progress since the physical channel connected to the failed router is labeled faulty. similarly, message b cannot make progress. in addition to blocking the messages requesting a faulty link, a faulty component can also block other messages. as can be seen in figure 6.1, message c cannot make progress because the two virtual channels requested by it are occupied by messages a and b, respectively. thus, the messages cannot advance and remain in an effectively deadlocked conﬁguration even though cyclic dependencies between resources do not exist.
an equivalent situation can be constructed even if routers have multiple virtual channels and multiple virtual lanes in each channel. all lanes may be occupied by other messages. in general, if the fault is permanent, then any message waiting for the faulty component waits indeﬁnitely, holding buffer and channel resources. the effect of the fault is propagated through permissible dependencies to affect messages that may not have to traverse the faulty component. these messages are said to form a wait chain [123]. misrouting can avoid such deadlock, but must be controlled so that livelock is avoided and newly introduced dependencies do not produce deadlock. fault recovery mechanisms must (1) recover messages indeﬁnitely blocked on faulty components and (2) be propagated along wait chains to recover messages indirectly blocked on faulty components.
resources in a cyclic way. if we prevent packets from waiting on some resources when blocked, we can prevent deadlocks. however, the additional routing ﬂexibility offered by this model is usually small.
models based on waiting channels are more general than the ones based on channel dependencies. in fact, the latter are particular cases of the former when a packet is allowed to wait on all the channels supplied by the routing function. another particular case of interest arises when a blocked packet always waits on a single channel. in this case, this model matches the behavior of routers that buffer blocked packets in queues associated with output channels. note that those routers do not use wormhole switching because they buffer whole packets.
although models based on waiting channels are more general than the ones based on channel dependencies, note that adaptive routing is useful because it offers alternative paths when the network is congested. in order to maximize performance, a blocked packet should be able to reserve the ﬁrst valid channel that becomes available. thus, restricting the set of routing options when a packet is blocked does not seem to be an attractive choice from the performance point of view.
a model that increases routing ﬂexibility in the absence of contention is based on the wait-for graph [73]. this graph indicates the resources that blocked packets are waiting for. packets are even allowed to use nonminimal paths as long as they are not blocked. however, blocked packets are not allowed to wait for channels held by other packets in a cyclic way. thus, some blocked packets are obliged to use deterministic routing until delivered if they produce a cycle in the wait-for graph.
an interesting model based on waiting channels is the message ﬂow model [207, 209]. in this model, a routing function is deadlock-free if and only if all the channels are deadlock-immune. a channel is deadlock-immune if every packet that reserves that channel is guaranteed to be delivered. the model starts by analyzing the channels for which a packet reserving them is immediately delivered. those channels are deadlockimmune. then the model analyzes the remaining channels step by step. in each step, the channels adjacent to the ones considered in the previous step are analyzed. a channel is deadlock-immune if for all the alternative paths a packet can follow, the next channel to be reserved is also deadlock-immune. waiting channels play a role similar to routing subfunctions, serving as escape paths when a packet is blocked.
another model uses a channel waiting graph [309]. this graph captures the relationship between channels in the same way as the channel dependency graph. however, this model does not distinguish between routing and selection functions. thus, it considers the dynamic evolution of the network because the selection function takes into account channel status. two theorems are proposed. the ﬁrst one considers that every packet has a single waiting channel. it states that a routing algorithm is deadlock-free if it is waitconnected and there are no cycles in the channel waiting graph. the routing algorithm is wait-connected if it is connected by using only waiting channels.
but the most important result is a necessary and sufﬁcient condition for deadlock-free routing. this condition assumes that a packet can wait on any channel supplied by the routing algorithm. it uses the concept of true cycles. a cycle is a true cycle if it is reachable, starting from an empty network. the theorem states that a routing algorithm is deadlock-
we can generalize the results for edge buffers and central queues by mixing both kinds of resources [10]. it will be useful in section 3.6. a resource is either a channel or a central queue. note that we only consider the resources that can be held by a packet when it is blocked. thus, there is a resource dependency between two resources when a packet is holding one of them and it requests the other one. once again, the deﬁnitions and theoretical results proposed for edge buffers have their counterpart for resources. all the issues, including coherency and reachability, can be considered in the same way as for edge buffers. in particular, theorem 3.1 can be restated simply by replacing the extended channel dependency graph by the extended resource dependency graph. note that the edge buffer or central queue containing the packet header is usually required in the domain of the routing function to determine the kind of resource being used by the packet. so, strictly speaking, the resulting theorem would only be a sufﬁcient condition for deadlock freedom, as mentioned in section 3.2.2. note that this generalized result is valid for all the switching techniques considered in section 3.1.
there are alternative approaches to avoid deadlocks. some of them are theoretical approaches based on other models. other approaches are simple and effective tricks to avoid deadlocks. these tricks do not work for all switching techniques.
the theory presented in section 3.1 relies on the concept of channel dependency or resource dependency. however, some authors used different tools to analyze deadlocks. the most interesting one is the concept of waiting channel [207]. the basic idea is the same as for channel dependency: a packet is holding some channel(s) while waiting for other channel(s). the main difference with respect to channel dependency is that waiting channels are not necessarily the same channels offered by the routing function to that packet. more precisely, if the routing function supplies a set of channels ci to route a packet in the absence of contention, the packet only waits for a channel belonging to cj ⊆ ci when it is blocked. the subset cj may contain a single channel. in other words, if a packet is blocked, the number of routing options available to that packet may be reduced. this is equivalent to dynamically changing the routing function depending on packet status.
by reducing the number of routing options when a packet is blocked, it is possible to allow more routing ﬂexibility in the absence of contention [73]. in fact, some routing options that may produce deadlock are forbidden when the packet is blocked. remember that deadlocks arise because some packets are holding resources while waiting for other
and another multidestination message arrives on another virtual lane in the same set, it must wait. this waiting cannot produce deadlock because both messages follow the same direction. also, it should be noted that adaptive routing algorithms that allow cyclic dependencies between channels only require the escape channels to avoid deadlock. since the brcp model restricts routing for multidestination messages according to the paths deﬁned by escape channels, only the escape channels need to be considered when computing the number of delivery channels required for those algorithms.
the high number of delivery channels required to implement deadlock-free multidestination communication under the brcp model may restrict the applicability of this model. fortunately, current trends in network topologies recommend the use of low-dimensional meshes or tori (two or three dimensions at most). also, as shown in sections 4.2 and 4.4.4, it is possible to design deterministic and fully adaptive routing algorithms with a very small number of virtual channels. one and two virtual channels per physical channel are enough for deterministic routing in n-dimensional meshes and k-ary n-cubes, respectively. for fully adaptive routing, the requirements for escape channels are identical to those for deterministic routing. as indicated in [268], four delivery channels are enough to support deadlock-free multidestination communication under the brcp model in 2-d meshes when the base routing is either xy routing, nonminimal west-ﬁrst, or fully adaptive routing based on escape channels (described in section 4.4.4). moreover, there is very little blocking probability with all delivery channels being accessed simultaneously. hence, delivery channels can be implemented as virtual channels by multiplexing the available bandwidth at the network interface.
in this section we describe algorithms and architectural support to perform barrier synchronization, reduction, and global combining. these algorithms have been proposed by panda in [265, 266] and are based on the brcp model described in section 5.5.3.
barrier synchronization can be performed in two phases by using multidestination messages. the ﬁrst phase implements reporting by using gather messages. the second phase implements wake-up by using broadcasting messages.
consider a linear array of six processors as shown in figure 5.42. assume that four processors (p 0, p 1, p 2, and p 4) participate in a barrier. the rightmost processor p 4, after reaching its barrier point, can send a multidestination gather message. the header of this message consists of an ordered destination list (p 2, p 1, and p 0) with p 0 as the ﬁnal destination. as this message propagates toward p 0, it can gather information from processors p 2 and p 1 regarding whether they have reached the barrier or not. if this
buffering logic within the router. in contrast, the format shown in figure 2.7 enables a fast lookup using the header and simple processing within the router.
circuit switching is generally advantageous when messages are infrequent and long; that is, the message transmission time is long compared to the path setup time. the disadvantage is that the physical path is reserved for the duration of the message and may block other messages. for example, consider the case where the probe is blocked waiting for a physical link to become free. all of the links reserved by the probe up to that point remain reserved, cannot be used by other circuits, and may be blocking other circuits, preventing them from being set up. thus, if the size of the message is not that much greater than the size of the probe, it would be advantageous to transmit the message along with the header and buffer the message within the routers while waiting for a free link. this alternative technique is referred to as packet switching, and will be studied in section 2.3.2.
the base latency of a circuit-switched message is determined by the time to set up a path and the subsequent time the path is busy transmitting data. the router operation differs a bit from that shown in figure 2.1. while the routing probe is buffered at each router, data bits are not. there are no intervening data buffers in the circuit, which operates effectively as a single wire from source to destination. this physical circuit may use asynchronous or synchronous ﬂow control, as shown in figures 2.3 or 2.4. in this case the time for the transfer of each ﬂit from source to destination is determined by the clock speed of the synchronous circuit or signaling speed of the asynchronous handshake lines. the signaling period or clock period must be greater than the propagation delay through this circuit. this places a practical limit on the speed of circuit switching as a function of system size. more recent techniques have begun to investigate the use of this delay as a form of storage. at very high signal speeds, multiple bits may be present on a wire concurrently, proceeding as waves of data. such techniques have been referred to as wave pipelining [111]. using such techniques, the technological limits of router and network designs have been reexamined [101, 311], and it has been found that substantial improvements in wire bandwidth are possible. the challenges to widespread use remain the design of circuits that can employ wave pipelining with stable and predictable delays, while in large designs the signal skew remains particularly challenging.
actual latencies clearly depend on a myriad of implementation details. figure 2.6 represents some simplifying assumptions about the time necessary for various events, such as processing an acknowledgment or initiating the transmission of the ﬁrst data ﬂit. in particular, it is assumed that, once the circuit has been established, propagation
behind preestablished circuits is similar to the use of cache memory in a processor: a set of channels is reserved once and used several times to transmit messages.
in structured message-passing programs, it may be feasible to identify and set up circuits prior to actual use. this setup may be overlapped with useful computation. when data are available, the circuit would have already been established and would permit lowerlatency message delivery. however, setting up a circuit in advance only eliminates some source software overheads such as buffer allocation, as well as routing time and contention. this may be a relatively small advantage compared to the bandwidth wasted by channels that have been reserved but are not currently used. circuits could be really useful if they performed like caches; that is, preestablished circuits actually operate faster. link-level pipelining can be employed in a rather unique way to ensure this cache-like behavior. we use the term wave pipelining [101] to represent data transmission along such high-speed circuits.
it is possible to use wave pipelining across switches and physical channels, enabling very high clock rates. with a preestablished circuit, careful design is required to minimize the skew between wires in a wave-pipelined parallel data path. synchronizers are required at each delivery channel. synchronizers may also be required at each switch input to reduce the skew. clock frequency is limited by delivery bandwidth, by signal skew, and by latch setup time. with a proper router and memory design, this frequency can potentially be higher than that employed in current routers, increasing channel bandwidth and network throughput accordingly. spice simulations [101] show that up to a factor of 4 increase in clock speed is feasible. implementations would be necessary to validate the models. the use of pipelined channels allows the designer to compute clock frequency independently of wire delay, limited by routing delay and switch delay. an example of a router organization that permits the use of higher clock frequencies based on the use of wave pipelining across both switches and channels is shown in figure 7.41. each physical channel in switch s0 is split into k + w virtual channels. among them, k channels are the control channels associated with the corresponding physical channels in switches s1, . . . , sk. control channels are only used to set up and tear down physical circuits. these channels have capacity for a single ﬂit and only transmit control ﬂits. control channels are set up using pipelined circuit switching and handled by the pcs routing control unit. the remaining w virtual channels are used to transmit messages using wormhole switching and require deeper buffers. they are handled by the wormhole routing control unit. the hybrid switching technique implemented by this router architecture is referred to as wave switching [101]. with such a router architecture, it is possible to conceive of approaches to optimizing the allocation of link bandwidth in support of realtime communication, priority trafﬁc, or high throughput.
the implementation of routing protocols based on pipelined circuit switching presents different challenges from those presented by wormhole-switched routers, notably in the need to provide support for backtracking and the distinction between control ﬂit trafﬁc and
behind preestablished circuits is similar to the use of cache memory in a processor: a set of channels is reserved once and used several times to transmit messages.
in structured message-passing programs, it may be feasible to identify and set up circuits prior to actual use. this setup may be overlapped with useful computation. when data are available, the circuit would have already been established and would permit lowerlatency message delivery. however, setting up a circuit in advance only eliminates some source software overheads such as buffer allocation, as well as routing time and contention. this may be a relatively small advantage compared to the bandwidth wasted by channels that have been reserved but are not currently used. circuits could be really useful if they performed like caches; that is, preestablished circuits actually operate faster. link-level pipelining can be employed in a rather unique way to ensure this cache-like behavior. we use the term wave pipelining [101] to represent data transmission along such high-speed circuits.
it is possible to use wave pipelining across switches and physical channels, enabling very high clock rates. with a preestablished circuit, careful design is required to minimize the skew between wires in a wave-pipelined parallel data path. synchronizers are required at each delivery channel. synchronizers may also be required at each switch input to reduce the skew. clock frequency is limited by delivery bandwidth, by signal skew, and by latch setup time. with a proper router and memory design, this frequency can potentially be higher than that employed in current routers, increasing channel bandwidth and network throughput accordingly. spice simulations [101] show that up to a factor of 4 increase in clock speed is feasible. implementations would be necessary to validate the models. the use of pipelined channels allows the designer to compute clock frequency independently of wire delay, limited by routing delay and switch delay. an example of a router organization that permits the use of higher clock frequencies based on the use of wave pipelining across both switches and channels is shown in figure 7.41. each physical channel in switch s0 is split into k + w virtual channels. among them, k channels are the control channels associated with the corresponding physical channels in switches s1, . . . , sk. control channels are only used to set up and tear down physical circuits. these channels have capacity for a single ﬂit and only transmit control ﬂits. control channels are set up using pipelined circuit switching and handled by the pcs routing control unit. the remaining w virtual channels are used to transmit messages using wormhole switching and require deeper buffers. they are handled by the wormhole routing control unit. the hybrid switching technique implemented by this router architecture is referred to as wave switching [101]. with such a router architecture, it is possible to conceive of approaches to optimizing the allocation of link bandwidth in support of realtime communication, priority trafﬁc, or high throughput.
the implementation of routing protocols based on pipelined circuit switching presents different challenges from those presented by wormhole-switched routers, notably in the need to provide support for backtracking and the distinction between control ﬂit trafﬁc and
suggesting the corresponding west-ﬁrst routing algorithm: route a packet ﬁrst west, if necessary, and then adaptively south, east, and north. the two turns prohibited in figure 4.9(c) are the two turns to the west. therefore, in order to travel west, a packet must begin in that direction. figure 4.10 shows the minimal west-ﬁrst routing algorithm for 2-d meshes, where select() is the selection function deﬁned in section 3.1.2. this function returns a free channel (if any) from the set of channels passed as parameters. see exercise 4.5 for a nonminimal version of this algorithm. three example paths for the west-ﬁrst algorithm are shown in figure 4.11. the channels marked as unavailable are either faulty or being used by other packets. one of the paths shown is minimal, while the other two paths are nonminimal, resulting from routing around unavailable channels. because cycles are avoided, west-ﬁrst routing is deadlock-free. for minimal routing, the algorithm is fully adaptive if the destination is on the right-hand side (east) of the source; otherwise, it is deterministic. if nonminimal routing is allowed, the algorithm is adaptive in either case. however, it is not fully adaptive.
there are other ways to select six turns so as to prohibit cycles. however, the selection of the two prohibited turns may not be arbitrary [129]. if turns are prohibited as in figure 4.12, deadlock is still possible. figure 4.12(a) shows that the three remaining left turns are equivalent to the prohibited right turn, and figure 4.12(b) shows that the three remaining right turns are equivalent to the prohibited left turn. figure 4.12(c) illustrates how cycles may still occur. of the 16 different ways to prohibit two turns, 12 prevent deadlock and only 3 are unique if symmetry is taken into account. these three combinations correspond to the west-ﬁrst, north-last, and negative-ﬁrst routing algorithms. the northlast routing algorithm does not allow turns from north to east or from north to west. the negative-ﬁrst routing algorithm does not allow turns from north to west or from east to south.
in addition to 2-d mesh networks, the turn model can be used to develop partially adaptive routing algorithms for n-dimensional meshes, for k-ary n-cubes, and for hypercubes [129]. by applying the turn model to the hypercube, an adaptive routing algorithm, namely, p-cube routing, can be developed. let s = sn−1sn−2 . . . s0 and d = dn−1dn−2 . . . d0 be the source and destination nodes, respectively, in a binary n-cube. the set e consists of all the dimension numbers in which s and d differ. the size of e is the hamming distance between s and d. thus, i ∈ e if si (cid:16)= di. e is divided into two disjoint subsets, e0 and e1, where i ∈ e0 if si = 0 and di = 1, and j ∈ e1 if sj = 1 and
any of the edges; (b) destination is in the upper edge and the message encounters one fault; (c) destination is in the upper edge and the message encounters two faults; (d) destination is in the upper-left corner; (e) destination is in the right edge; and (f) destination is in the upper-right corner.
as the eastward virtual network. the second virtual network also consists of one virtual channel in each direction. it is used to send packets toward the west (x-negative), and it will be referred to as the westward virtual network. when the x coordinates of the source and destination nodes are equal, packets can be introduced in either virtual network. the routing algorithms for eastward and westward virtual networks are based on nonminimal west-last and east-last, respectively. once a packet has used a west (east) channel in the eastward (westward) virtual network, it cannot turn again. however, 180-degree turns are allowed in y channels except in the east (west) edge of the mesh. this routing algorithm is fully adaptive nonminimal. it is deadlock-free, as shown in exercise 3.3 for 2-d meshes. while routing in the eastward (westward) virtual network, it tolerates faults except when routing on the east (west) edge. in order to tolerate faults in the edges, two extra virtual networks are used. the extra eastward (westward) virtual network is used when a fault is met while routing on the west (east) edge of the mesh. the resulting routing algorithm is very resilient to faults under both the node failure and the link failure models [93, 95]. figure 6.33 shows the paths followed when some channels fail. figure 6.34 shows the paths followed when some nodes fail. this approach also supports rectangular fault regions.
and channel between the routers. in this case the coefﬁcient of the second term and the pipeline cycle time would be (ts + tw). note that the unit of message ﬂow control is a packet. therefore, even though the message may cut through the router, sufﬁcient buffer space must be allocated for a complete packet in case the header is blocked.
the need to buffer complete packets within a router can make it difﬁcult to construct small, compact, and fast routers. in wormhole switching, message packets are also pipelined through the network. however, the buffer requirements within the routers are substantially reduced over the requirements for vct switching. a message packet is broken up into ﬂits. the ﬂit is the unit of message ﬂow control, and input and output buffers at a router are typically large enough to store a few ﬂits. for example, the message buffers in the cray t3d are 1 ﬂit deep, and each ﬂit is comprised of eight 16-bit phits. the message is pipelined through the network at the ﬂit level and is typically too large to be completely buffered within a router. thus, at any instant in time a blocked message occupies buffers in several routers. the time-space diagram of a wormhole-switched message is shown in figure 2.11. the clear rectangles illustrate the propagation of single ﬂits across the physical channel. the shaded rectangles illustrate the propagation of header ﬂits across the physical channel. routing delays and intrarouter propagation of the header ﬂits are also captured in this ﬁgure. the primary difference between wormhole switching and vct switching is that, in the former, the unit of message ﬂow control is a single ﬂit and, as a consequence, small buffers can be used. just a few ﬂits need to be buffered at a router.
in the absence of blocking, the message packet is pipelined through the network. however, the blocking characteristics are very different from that of vct. if the required
the paradigm adopted throughout the preceding examples has been to characterize messages by the direction of traversal in the network and therefore the direction and virtual channels occupied by these messages when misrouted around a fault ring. the addition of virtual channels for each message type ensures that these messages occupy disjoint virtual networks and therefore channel resources. since the usage of resources within a network is orchestrated to be acyclic and transitions made by messages between networks remain acyclic, routing can be guaranteed to be deadlock-free. however, the addition of virtual channels affects the speed and complexity of the routers.arbitration between virtual channels and the multiplexing of virtual channels across the physical channel can have a substantial impact on the ﬂow control latency through the router [57]. this motivated investigations of solutions that did not rely on many (or any) virtual channels.
origin-based fault-tolerant routing is a paradigm that enables fault-tolerant routing in mesh networks under a similar fault model, but without the addition of virtual channels [145]. the basic fault-free form of origin-based routing follows the paradigm of several adaptive routing algorithms proposed for binary hypercubes and the turn model proposed for more general direct network topologies. each message progresses through two phases. in the ﬁrst phase, the message is adaptively routed toward a special node. on reaching this node, the message is adaptively routed to the destination in the second phase. in previous applications of this paradigm to binary hypercubes, this special node could be the zenith node whose address is given by the logical or of the source and destination addresses. messages are ﬁrst routed adaptively to their zenith and then adaptively toward the destination [184]. this phase ordering prevents the formation of cycles in the channel dependency graphs. variants of this approach have also been proposed, including the relaxation of ordering restrictions on the phases [59]. in origin-based routing in mesh networks this special node is the node that is designated as the origin according to the mesh coordinates. while the approach is directly extensible to multidimensional networks, the following description deals with 2-d meshes.
all of the physical channels are partitioned into two disjoint networks. the in network consists of all of the unidirectional channels that are directed toward the origin, while the out network consists of all of the unidirectional channels directed away from the origin. the orientation of a channel can be determined by the node at the receiving end of the channel. if this node is closer to the origin than the sending end of the channel, the channel is in the in network. otherwise it is in the out network. the outbox for a node in the mesh is the submesh comprised of all nodes on a shortest path to the origin. an example of an outbox for a destination node d is shown in figure 6.24. messages are routed in two phases. in the ﬁrst phase messages are routed adaptively toward the destination/origin, over any shortest path using channels in the in network. when the header ﬂit arrives at any node in the outbox for the destination, the message is now routed adaptively toward the destination using only channels in the out network. as shown in the example in figure 6.24, the resulting complete path may not be a minimal path. the choice of minimal paths is easily enforced by restricting messages traversing the in network to use channels that take the message closer to the destination. the result of enforcing such a restriction on the same (source, destination) pair is also shown in figure 6.24. the choice of the origin node is important. since all messages are ﬁrst routed toward the origin, hot spots can
