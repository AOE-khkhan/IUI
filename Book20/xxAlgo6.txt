2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
in this example, each element of the list is stored in it, and passed in turn to the dosomething function. the loop terminates when the current position reaches the position equal to the length of the list.
the list class declaration presented here is just one of many possible interpretations for lists. figure 4.1 provides most of the operations that one naturally expects to perform on lists and serves to illustrate the issues relevant to implementing the list data structure. as an example of using the list adt, we can create a function to return true if there is an occurrence of a given integer in the list, and false otherwise. the find method needs no knowledge about the speciﬁc list implementation, just the list adt.
while this implementation for find is generic with respect to the list implementation, it is limited in its ability to handle different data types stored on the list. in particular, it only works when the description for the object being searched for (k in the function) is of the same type as the objects themselves. a more typical situation is that we are searching for a record that contains a key ﬁeld who’s value matches k. similar functions to ﬁnd and return a composite element based on a key value can be created using the list implementation, but to do so requires some agreement between the list adt and the find function on the concept of a key, and on how keys may be compared. this topic will be discussed in section 4.4.
4.1.1 array-based list implementation there are two standard approaches to implementing lists, the array-based list, and the linked list. this section discusses the array-based approach. the linked list is presented in section 4.1.2. time and space efﬁciency comparisons for the two are discussed in section 4.1.3.
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
for more discussion on choice of functions used to deﬁne the list adt, see the work of the reusable software research group from ohio state. their deﬁnition for the list adt can be found in [swh93]. more information about designing such classes can be found in [sw94].
4.2 show the list conﬁguration resulting from each series of list operations using the list adt of figure 4.1. assume that lists l1 and l2 are empty at the beginning of each series. show where the current position is in the list.
4.3 write a series of java statements that uses the list adt of figure 4.1 to create a list capable of holding twenty elements and which actually stores the list with the following conﬁguration:
this section presents a simple, compact implementation for complete binary trees. recall that complete binary trees have all levels except the bottom ﬁlled out completely, and the bottom level has all of its nodes ﬁlled in from left to right. thus, a complete binary tree of n nodes has only one possible shape. you might think that a complete binary tree is such an unusual occurrence that there is no reason to develop a special implementation for it. however, the complete binary tree has practical uses, the most important being the heap data structure discussed in section 5.5. heaps are often used to implement priority queues (section 5.5) and for external sorting algorithms (section 8.5.2).
we begin by assigning numbers to the node positions in the complete binary tree, level by level, from left to right as shown in figure 5.12(a). an array can store the tree’s data values efﬁciently, placing each data value in the array position corresponding to that node’s position within the tree. figure 5.12(b) lists the array indices for the children, parent, and siblings of each node in figure 5.12(a). from figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives within the array. simple formulae can be derived for calculating the array index for each relative of a node r from r’s index. no explicit pointers are necessary to reach a node’s left or right child. this means there is no overhead to the array implementation if the array is selected to be of size n for a tree of n nodes.
the formulae for calculating the array indices of the various relatives of a node are as follows. the total number of nodes in the tree is n. the index of the node in question is r, which must fall in the range 0 to n − 1.
• parent(r) = b(r − 1)/2c if r 6= 0. • left child(r) = 2r + 1 if 2r + 1 < n. • right child(r) = 2r + 2 if 2r + 2 < n. • left sibling(r) = r − 1 if r is even. • right sibling(r) = r + 1 if r is odd and r + 1 < n.
section 4.4 presented the dictionary adt, along with dictionary implementations based on sorted and unsorted lists. when implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. however, searching an unsorted list for a particular record requires Θ(n) time in the average case. for a large database, this is probably much too slow. alternatively, the records can be stored in a sorted list. if the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. on the other hand, if we use a sorted
figure 5.13 two binary search trees for a collection of values. tree (a) results if values are inserted in the order 37, 24, 42, 7, 2, 40, 42, 32, 120. tree (b) results if the same values are inserted in the order 120, 42, 42, 7, 2, 32, 37, 24, 40.
figure 5.14 shows a class declaration for the bst that implements the dictionary adt. the public member functions include those required by the dictionary adt, along with a constructor and destructor.
to ﬁnd a record with key value k in a bst, begin at the root. if the root stores a record with key value k, then the search is over. if not, then we must search deeper in the tree. what makes the bst efﬁcient during search is that we need search only one of the node’s two subtrees. if k is less than the root node’s key value, we search only the left subtree. if k is greater than the root node’s key value, we search only the right subtree. this process continues until a record with key value k is found, or we reach a leaf node. if we reach a leaf node without encountering k, then no record exists in the bst whose key value is k.
example 5.5 consider searching for the node with key value 32 in the tree of figure 5.13(a). because 32 is less than the root value of 37, the search proceeds to the left subtree. because 32 is greater than 24, we search in 24’s right subtree. at this point the node containing 32 is found. if the search value were 35, the same path would be followed to the node containing 32. because this node has no children, we know that 35 is not in the bst.
notice that in figure 5.14, public member function find calls a private member function named findhelp. method find takes the search key as an explicit parameter and its bst as an implicit parameter, along with space to place a copy of
figure 6.1 notation for general trees. node p is the parent of nodes v, s1, and s2. thus, v, s1, and s2 are children of p. nodes r and p are ancestors of v. nodes v, s1, and s2 are called siblings. the oval surrounds the subtree having v as its root.
into n ≥ 0 disjoint subsets t0, t1, ..., tn−1, each of which is a tree, and whose roots r1, r2, ..., rn, respectively, are children of r. the subsets ti (0 ≤ i < n) are said to be subtrees of t. these subtrees are ordered in that ti is said to come before tj if i < j. by convention, the subtrees are arranged from left to right with subtree t0 called the leftmost child of r. a node’s out degree is the number of children for that node. a forest is a collection of one or more trees. figure 6.1 presents further tree notation generalized from the notation for binary trees presented in chapter 5. each node in a tree has precisely one parent, except for the root, which has no parent. from this observation, it immediately follows that a tree with n nodes must have n − 1 edges because each node, aside from the root, has one edge connecting that node to its parent.
6.1.1 an adt for general tree nodes before discussing general tree implementations, we should ﬁrst make precise what operations such implementations must support. any implementation must be able to initialize a tree. given a tree, we need access to the root of that tree. there must be some way to access the children of a node. in the case of the adt for binary tree nodes, this was done by providing member functions that give explicit
to perform a preorder traversal, it is necessary to visit each of the children for a given node (say r) from left to right. this is accomplished by starting at r’s leftmost child (call it t). from t, we can move to t’s right sibling, and then to that node’s right sibling, and so on.
using the adt of figure 6.2, here is an implementation to print the nodes of a general tree in preorder note the for loop at the end, which processes the list of children by beginning with the leftmost child, then repeatedly moving to the next child until calling next returns null.
perhaps the simplest general tree implementation is to store for each node only a pointer to that node’s parent. we will call this the parent pointer implementation.
union rule for joining sets) is Θ(n log∗ n). the notation “log∗ n” means the number of times that the log of n must be taken before n ≤ 1. for example, log∗ 65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and ﬁnally log 2 = 1. thus, log∗ n grows very slowly, so the cost for a series of n find operations is very close to n.
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth Θ(log∗ n). one can devise a series of equivalence operations that yields Θ(log n) depth for the resulting tree. however, many of the equivalences in such a series will look only at the roots of the trees being merged, requiring little processing time. the total amount of processing time required for n operations will be Θ(n log∗ n), yielding nearly constant time for each equivalence operation. this is an example of the technique of amortized analysis, discussed further in section 14.3.
we now tackle the problem of devising an implementation for general trees that allows efﬁcient processing of all member functions of the adt shown in figure 6.2. this section presents several approaches to implementing general trees. each implementation yields advantages and disadvantages in the amount of space required to store a node and the relative ease with which key operations can be performed. general tree implementations should place no restriction on how many children a node may have. in some applications, once a node is created the number of children never changes. in such cases, a ﬁxed amount of space can be allocated for the node when it is created, based on the number of children for the node. matters become more complicated if children can be added to or deleted from a node, requiring that the node’s space allocation be adjusted accordingly.
figure 6.14 converting from a forest of general trees to a single binary tree. each node stores pointers to its left child and right sibling. the tree roots are assumed to be siblings for the purpose of converting.
compared with the implementation illustrated by figure 6.13 which requires overhead of three pointers/node, the implementation of figure 6.15 only requires two pointers per node.
because each node of the general tree now contains a ﬁxed number of pointers, and because each function of the general tree adt can now be implemented efﬁciently, the dynamic “left-child/right-sibling” implementation is preferred to the other general tree implementations described in sections 6.3.1 to 6.3.3.
k-ary trees are trees whose internal nodes all have exactly k children. thus, a full binary tree is a 2-ary tree. the pr quadtree discussed in section 13.3 is an
6.2 write an algorithm to determine if two binary trees are identical when the ordering of the subtrees for a node is ignored. for example, if a tree has root node with value r, left child with value a and right child with value b, this would be considered identical to another tree with root node value r, left child value b, and right child value a. make the algorithm as efﬁcient as you can. analyze your algorithm’s running time. how much harder would it be to make this algorthm work on a general tree?
6.4 write a function that takes as input a general tree and returns the number of nodes in that tree. write your function to use the gentree and gtnode adts of figure 6.2.
6.5 describe how to implement the weighted union rule efﬁciently. in particular, describe what information must be stored with each node and how this information is updated when two trees are merged. modify the implementation of figure 6.4 to support the weighted union rule.
6.6 a potential alternatative to the weighted union rule for combining two trees is the height union rule. the height union rule requires that the root of the tree with greater height become the root of the union. explain why the height union rule can lead to worse average time behavior than the weighted union rule.
6.7 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (0, 2) (1, 2) (3, 4) (3, 1) (3, 5) (9, 11) (12, 14) (3, 9) (4, 14) (6, 7) (8, 10) (8, 7) (7, 0) (10, 15) (10, 13)
6.8 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (2, 3) (4, 5) (6, 5) (3, 5) (1, 0) (7, 8) (1, 8) (3, 8) (9, 10) (11, 14) (11, 10) (12, 13) (11, 13) (14, 1)
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
reading blocks stored on slower, secondary memory (such as on the disk drive). the disk stores the complete contents of the virtual memory. blocks are read into main memory as demanded by memory accesses. naturally, programs using virtual memory techniques are slower than programs whose data are stored completely in main memory. the advantage is reduced programmer effort because a good virtual memory system provides the appearance of larger main memory without modifying the program.
example 8.2 consider a virtual memory whose size is ten sectors, and which has a buffer pool of ﬁve buffers associated with it. we will use a lru replacement scheme. the following series of memory requests occurs.
after the ﬁrst ﬁve requests, the buffer pool will store the sectors in the order 6, 7, 1, 0, 9. because sector 6 is already at the front, the next request can be answered without reading new data from disk or reordering the buffers. the request to sector 8 requires emptying the contents of the least recently used buffer, which contains sector 9. the request to sector 1 brings the buffer holding sector 1’s contents back to the front. processing the remaining requests results in the buffer pool as shown in figure 8.5.
example 8.3 figure 8.5 illustrates a buffer pool of ﬁve blocks mediating a virtual memory of ten blocks. at any given moment, up to ﬁve sectors of information can be in main memory. assume that sectors 1, 7, 5, 3, and 8 are curently in the buffer pool, stored in this order, and that we use the lru buffer replacement strategy. if a request for sector 9 is then received, then one sector currently in the buffer pool must be replaced. because the buffer containing sector 8 is the least recently used buffer, its contents will be copied back to disk at sector 8. the contents of sector 9 are then copied into this buffer, and it is moved to the front of the buffer pool (leaving the buffer containing sector 3 as the new least-recently used buffer). if the next memory request were to sector 5, no data would need to be read from disk. instead, the buffer containing sector 5 would be moved to the front of the buffer pool.
when implementing buffer pools, there are two basic approaches that can be taken regarding the transfer of information between the user of the buffer pool and
where disk i/o is the bottleneck for the program, even the time to copy lots of information between the buffer pool user and the buffer might be inconsequential. another advantage to buffer passing is the reduction in unnecessary read operations for data that will be overwritten anyway.
you should note that the implementations for class bufferpool above are not generic. instead, the space parameter and the buffer pointer are declared to be byte[]. when a class is generic, that means that the record type is arbitrary, in contrast, using a byte[] but that the class knows what the record type is. pointer for the space means that not only is the record type arbitrary, but also the buffer pool does not even know what the user’s record type is. in fact, a given buffer pool might have many users who store many types of records.
in a buffer pool, the user decides where a given record will be stored but has no control over the precise mechanism by which data are transfered to the backing storage. this is in contrast to the memory manager described in section 12.3 in which the user passes a record to the manager and has no control at all over where the record is stored.
the java programmer’s logical view of a random access ﬁle is a single stream of bytes. interaction with a ﬁle can be viewed as a communications channel for issuing one of three instructions: read bytes from the current position in the ﬁle, write bytes to the current position in the ﬁle, and move the current position within the ﬁle. you do not normally see how the bytes are stored in sectors, clusters, and so forth. the mapping from logical to physical addresses is done by the ﬁle system, and sector-level buffering is done automatically by the disk controller.
when processing records in a disk ﬁle, the order of access can have a great effect on i/o time. a random access procedure processes records in an order independent of their logical order within the ﬁle. sequential access processes records in order of their logical appearance within the ﬁle. sequential processing requires less seek time if the physical layout of the disk ﬁle matches its logical layout, as would be expected if the ﬁle were created on a disk with a high percentage of free space.
public void init(int n); public int n(); public int e(); public int first(int v); public int next(int v, int w); // get v’s next neighbor public void setedge(int i, int j, int wght); // set weight public void deledge(int i, int j); // delete edge (i, j) public boolean isedge(int i, int j); // if (i,j) an edge? public int weight(int i, int j); public void setmark(int v, int val); // set mark for v public int getmark(int v); // get mark for v
figure 11.5 a graph adt. this adt assumes that the number of vertices is ﬁxed when the graph is created, but that edges can be added and removed. it also supports a mark array to aid graph traversal algorithms.
the adjacency matrix often requires a higher asymptotic cost for an algorithm than would result if the adjacency list were used. the reason is that it is common for a graph algorithm to visit each neighbor of each vertex. using the adjacency list, only the actual edges connecting a vertex to its neighbors are examined. however, the adjacency matrix must look at each of its |v| potential edges, yielding a total cost of Θ(|v2|) time when the algorithm might otherwise require only Θ(|v|+|e|) time. this is a considerable disadvantage when the graph is sparse, but not when the graph is closer to full.
we next turn to the problem of implementing a graph class. figure 11.5 shows an abstract class deﬁning an adt for graphs. vertices are deﬁned by an integer index value. in other words, there is a vertex 0, vertex 1, and so on. we can assume that a graph application stores any additional information of interest about a given vertex elsewhere, such as a name or application-dependent value. note that this adt is not implemented using a template, because it is the graph class users’ responsibility to maintain information related to the vertices themselves. the graph class has no knowledge of the type or content of the information associated with a vertex, only the index number for that vertex.
abstract class graph has methods to return the number of vertices and edges (methods n and e, respectively). function weight returns the weight of a given edge, with that edge identiﬁed by its two incident vertices. for example, calling weight(0, 4) on the graph of figure 11.1 (c) would return 4. if no such edge
functions setedge and deledge set the weight of an edge and remove an edge from the graph, respectively. again, an edge is identiﬁed by its two incident vertices. setedge does not permit the user to set the weight to be 0, because this value is used to indicate a non-existent edge, nor are negative edge weights permitted. functions getmark and setmark get and set, respectively, a requested value in the mark array (described below) for vertex v.
nearly every graph algorithm presented in this chapter will require visits to all neighbors of a given vertex. two methods are provided to support this. they work in a manner similar to linked list access functions. function first takes as input a vertex v, and returns the edge to the ﬁrst neighbor for v (we assume the neighbor list is sorted by vertex number). function next takes as input vertices v1 and v2 and returns the index for the vertex forming the next edge with v1 after v2 on v1’s edge list. function next will return a value of n = |v| once the end of the edge list for v1 has been reached. the following line appears in many graph algorithms:
this for loop gets the ﬁrst neighbor of v, then works through the remaining neighbors of v until a value equal to g->n() is returned, signaling that all neighbors of v have been visited. for example, first(1) in figure 11.4 would return 0. next(1, 0) would return 3. next(0, 3) would return 4. next(1, 4) would return 5, which is not a vertex in the graph.
it is reasonably straightforward to implement our graph and edge adts using either the adjacency list or adjacency matrix. the sample implementations presented here do not address the issue of how the graph is actually created. the user of these implementations must add functionality for this purpose, perhaps reading the graph description from a ﬁle. the graph can be built up by using the setedge function provided by the adt.
figure 11.6 shows an implementation for the adjacency matrix. array mark stores the information manipulated by the setmark and getmark functions. the edge matrix is implemented as an integer array of size n × n for a graph of n vertices. position (i, j) in the matrix stores the weight for edge (i, j) if it exists. a weight of zero for edge (i, j) is used to indicate that no edge connects vertices i and j.
given a vertex v, function first locates the position in matrix of the ﬁrst edge (if any) of v by beginning with edge (v, 0) and scanning through row v until an edge is found. if no edge is incident on v, then first returns n.
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
12.2 implement the memmanager adt shown at the beginning of section 12.3. use a separate linked list to implement the freelist. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.3 implement the memmanager adt shown at the beginning of section 12.3. do not use separate memory for the free list, but instead embed the free list into the memory pool as shown in figure 12.12. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.4 implement the memmanager adt shown at the beginning of section 12.3 using the buddy method of section 12.3.1. your system should support requests for blocks of a speciﬁed size and release of previously requested blocks.
(a) show the result of building a bintree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (45, 25), h (45, 30), i (50, 30).
(b) show the result of deleting point c from the tree you built in part (a). (c) show the result of deleting point f from the resulting tree in part (b).
13.16 compare the trees constructed for exercises 12 and 15 in terms of the number of internal nodes, full leaf nodes, empty leaf nodes, and total depths of the two trees.
13.17 show the result of building a point quadtree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (31, 35), g (45, 26), h (44, 30), i (50, 30).
13.1 use the trie data structure to devise a program to sort variable-length strings. the program’s running time should be proportional to the total number of letters in all of the strings. note that some strings might be very long while most are short.
13.2 deﬁne the set of sufﬁx strings for a string s to be s, s without its ﬁrst character, s without its ﬁrst two characters, and so on. for example, the complete set of sufﬁx strings for “hello” would be
a sufﬁx tree is a pat trie that contains all of the sufﬁx strings for a given string, and associates each sufﬁx with the complete string. the advantage of a sufﬁx tree is that it allows a search for strings using “wildcards.” for example, the search key “th*” means to ﬁnd all strings with “th” as the ﬁrst two characters. this can easily be done with a regular trie. searching for “*th” is not efﬁcient in a regular trie, but it is efﬁcient in a sufﬁx tree. implement the sufﬁx tree for a dictionary of words or phrases, with support for wildcard search.
13.3 revise the bst class of section 5.4 to use the avl tree rotations. your new implementation should not modify the original bst class adt. compare your avl tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
example 1.6 when operating a car, the primary activities are steering, accelerating, and braking. on nearly all passenger cars, you steer by turning the steering wheel, accelerate by pushing the gas pedal, and brake by pushing the brake pedal. this design for cars can be viewed as an adt with operations “steer,” “accelerate,” and “brake.” two cars might implement these operations in radically different ways, say with different types of engine, or front- versus rear-wheel drive. yet, most drivers can operate many different cars because the adt presents a uniform method of operation that does not require the driver to understand the speciﬁcs of any particular engine or drive design. these differences are deliberately hidden.
the concept of an adt is one instance of an important principle that must be understood by any successful computer scientist: managing complexity through abstraction. a central theme of computer science is complexity and techniques for handling it. humans deal with complexity by assigning a label to an assembly of objects or concepts and then manipulating the label in place of the assembly. cognitive psychologists call such a label a metaphor. a particular label might be related to other pieces of information or other labels. this collection can in turn be given a label, forming a hierarchy of concepts and labels. this hierarchy of labels allows us to focus on important issues while ignoring unnecessary details.
example 1.7 we apply the label “hard drive” to a collection of hardware that manipulates data on a particular type of storage device, and we apply the label “cpu” to the hardware that controls execution of computer instructions. these and other labels are gathered together under the label “computer.” because even small home computers have millions of components, some form of abstraction is necessary to comprehend how a computer operates.
consider how you might go about the process of designing a complex computer program that implements and manipulates an adt. the adt is implemented in one part of the program by a particular data structure. while designing those parts of the program that use the adt, you can think in terms of operations on the data type without concern for the data structure’s implementation. without this ability to simplify your thinking about a complex program, you would have no hope of understanding or implementing it.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
operation) and removed from the front (called a dequeue operation). queues operate like standing in line at a movie theater ticket counter.1 if nobody cheats, then newcomers go to the back of the line. the person at the front of the line is the next to be served. thus, queues release their elements in order of arrival. accountants have used queues since long before the existence of computers. they call a queue a “fifo” list, which stands for “first-in, first-out.” figure 4.22 shows a sample queue adt. this section presents two implementations for queues: the array-based queue and the linked queue.
assume that there are n elements in the queue. by analogy to the array-based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only Θ(1) time because the front element of the queue (the one being removed) is the last element in the array. however,
will use the “)” symbol) to indicate the end of a child list. all leaf nodes are followed by a “)” symbol because they have no children. a leaf node that is also the last child for its parent would indicate this by two or more successive “)” symbols.
note that f is followed by three “)” marks, because it is a leaf, the last node of b’s rightmost subtree, and the last node of r’s rightmost subtree.
note that this representation for serializing general trees cannot be used for binary trees. this is because a binary tree is not merely a restricted form of general tree with at most two children. every binary tree node has a left and a right child, though either or both might be empty. for example, the representation of example 6.8 cannot let us distinguish whether node d in figure 6.17 is the left or right child of node b.
6.6 further reading the expression log∗ n cited in section 6.2 is closely related to the inverse of ackermann’s function. for more information about ackermann’s function and the cost of path compression, see robert e. tarjan’s paper “on the efﬁciency of a good but not linear set merging algorithm” [tar75]. the article “data structures and algorithms for disjoint set union problems” by galil and italiano [gi91] covers many aspects of the equivalence class problem.
foundations of multidimensional and metric data structures by hanan samet [sam06] treats various implementations of tree structures in detail within the context of k-ary trees. samet covers sequential implementations as well as the linked and array implementations such as those described in this chapter and chapter 5. while these books are ostensibly concerned with spatial data structures, many of the concepts treated are relevant to anyone who must implement tree structures.
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
algorithms: an algorithm is a method or a process followed to solve a problem. if the problem is viewed as a function, then an algorithm is an implementation for the function that transforms an input to the corresponding output. a problem can be solved by many different algorithms. a given algorithm solves only one problem (i.e., computes a particular function). this book covers many problems, and for several of these problems i present more than one algorithm. for the important problem of sorting i present nearly a dozen algorithms!
the advantage of knowing several solutions to a problem is that solution a might be more efﬁcient than solution b for a speciﬁc variation of the problem, or for a speciﬁc class of inputs to the problem, while solution b might be more efﬁcient than a for another variation or class of inputs. for example, one sorting algorithm might be the best for sorting a small collection of integers, another might be the best for sorting a large collection of integers, and a third might be the best for sorting a collection of variable-length strings.
by deﬁnition, an algorithm possesses several properties. something can only be called an algorithm to solve a particular problem if it has all of the following properties.
1. it must be correct. in other words, it must compute the desired function, converting each input to the correct output. note that every algorithm implements some function because every algorithm maps every input to some output (even if that output is a system crash). at issue here is whether a given algorithm implements the intended function.
2. it is composed of a series of concrete steps. concrete means that the action described by that step is completely understood — and doable — by the person or machine that must perform the algorithm. each step must also be doable in a ﬁnite amount of time. thus, the algorithm gives us a “recipe” for solving the problem by performing a series of steps, where each such step is within our capacity to perform. the ability to perform a step can depend on who or what is intended to execute the recipe. for example, the steps of a cookie recipe in a cookbook might be considered sufﬁciently concrete for instructing a human cook, but not for programming an automated cookiemaking factory.
3. there can be no ambiguity as to which step will be performed next. often it is the next step of the algorithm description. selection (e.g., the if statements in java) is normally a part of any language for describing algorithms. selection allows a choice for which step will be performed next, but the selection process is unambiguous at the time when the choice is made.
4. it must be composed of a ﬁnite number of steps. if the description for the algorithm were made up of an inﬁnite number of steps, we could never hope to write it down, nor implement it as a computer program. most languages for describing algorithms (including english and “pseudocode”) provide some way to perform repeated actions, known as iteration. examples of iteration in programming languages include the while and for loop constructs of java. iteration allows for short descriptions, with the number of steps actually performed controlled by the input.
programs: we often think of a computer program as an instance, or concrete representation, of an algorithm in some programming language. in this book, nearly all of the algorithms are presented in terms of programs, or parts of programs. naturally, there are many programs that are instances of the same algorithm, because any modern computer programming language can be used to implement the same collection of algorithms (although some programming languages can make life easier for the programmer). to simplify presentation throughout the remainder of the text, i often use the terms “algorithm” and “program” interchangeably, despite the fact that they are really separate concepts. by deﬁnition, an algorithm must provide sufﬁcient detail that it can be converted into a program when needed.
the requirement that an algorithm must terminate means that not all computer programs meet the technical deﬁnition of an algorithm. your operating system is one such program. however, you can think of the various tasks for an operating system (each with associated inputs and outputs) as individual problems, each solved by speciﬁc algorithms implemented by a part of the operating system program, and each one of which terminates once its output is produced.
to summarize: a problem is a function or a mapping of inputs to outputs. an algorithm is a recipe for solving a problem whose steps are concrete and unambiguous. the algorithm must be correct, of ﬁnite length, and must terminate for all inputs. a program is an instantiation of an algorithm in a computer programming language.
the ﬁrst authoritative work on data structures and algorithms was the series of books the art of computer programming by donald e. knuth, with volumes 1 and 3 being most relevant to the study of data structures [knu97, knu98]. a modern encyclopedic approach to data structures and algorithms that should be easy
1. to design an algorithm that is easy to understand, code, and debug. 2. to design an algorithm that makes efﬁcient use of the computer’s resources.
ideally, the resulting program is true to both of these goals. we might say that such a program is “elegant.” while the algorithms and program code examples presented here attempt to be elegant in this sense, it is not the purpose of this book to explicitly treat issues related to goal (1). these are primarily concerns of the discipline of software engineering. rather, this book is mostly about issues relating to goal (2).
how do we measure efﬁciency? chapter 3 describes a method for evaluating the efﬁciency of an algorithm or computer program, called asymptotic analysis. asymptotic analysis also allows you to measure the inherent difﬁculty of a problem. the remaining chapters use asymptotic analysis techniques for every algorithm presented. this allows you to see how each algorithm compares to other algorithms for solving the same problem in terms of its efﬁciency.
this ﬁrst chapter sets the stage for what is to follow, by presenting some higherorder issues related to the selection and use of data structures. we ﬁrst examine the process by which a designer selects a data structure appropriate to the task at hand. we then consider the role of abstraction in program design. we brieﬂy consider the concept of a design pattern and see some examples. the chapter ends with an exploration of the relationship between problems, algorithms, and programs.
you might think that with ever more powerful computers, program efﬁciency is becoming less important. after all, processor speed and memory size still seem to double every couple of years. won’t any efﬁciency problem we might have today be solved by tomorrow’s hardware?
as we develop more powerful computers, our history so far has always been to use additional computing power to tackle more complex problems, be it in the form of more sophisticated user interfaces, bigger problem sizes, or new problems previously deemed computationally infeasible. more complex problems demand more computation, making the need for efﬁcient programs even greater. worse yet, as tasks become more complex, they become less like our everyday experience. today’s computer scientists must be trained to have a thorough understanding of the principles behind efﬁcient program design, because their ordinary life experiences often do not apply when designing computer programs.
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. if you had a second program whose growth rate is 2n and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! thus, an exponential growth rate is radically different than the other growth rates shown in figure 3.3. the signiﬁcance of this difference is explored in chapter 17.
instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to n2 with a new algorithm whose running time is proportional to n log n. in the graph of figure 3.1, a ﬁxed amount of time would appear as a horizontal line. if the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. an algorithm with running time t(n) = n2 requires 1024 × 1024 = 1, 048, 576 time steps for an input of size n = 1024. an algorithm with running time t(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of size n = 1024, which is an improvement of much more than a factor of ten when compared to the algorithm with running time t(n) = n2. because n2 > 10n log n whenever n > 58, if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater beneﬁt in terms of larger problem size that can run in a certain time on the new computer.
despite the larger constant for the curve labeled 10n in figure 3.1, the curve labeled 2n2 crosses it at the relatively small value of n = 5. what if we double the value of the constant in front of the linear equation? as shown in the graph, the curve labeled 20n is surpassed by the curve labeled 2n2 once n = 10. the additional factor of two for the linear growth rate does not much matter; it only doubles the x-coordinate for the intersection point. in general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross.
when you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. the time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. for these reasons, we usually ignore the constants
rule (3) says that given two parts of a program run in sequence (whether two statements or two sections of code), you need consider only the more expensive part. this rule applies to Ω and Θ notations as well: for both, you need consider only the more expensive part.
rule (4) is used to analyze simple loops in programs. if some action is repeated some number of times, and each repetition has the same cost, then the total cost is the cost of the action multiplied by the number of times that the action takes place. this rule applies to Ω and Θ notations as well.
taking the ﬁrst three rules collectively, you can ignore all constants and all lower-order terms to determine the asymptotic growth rate for any cost function. the advantages and dangers of ignoring constants were discussed near the beginning of this section. ignoring lower-order terms is reasonable when performing an asymptotic analysis. the higher-order terms soon swamp the lower-order terms in their contribution to the total cost as n becomes larger. thus, if t(n) = 3n4 + 5n2, then t(n) is in o(n4). the n2 term contributes relatively little to the total cost.
3.4.5 classifying functions given functions f(n) and g(n) whose growth rates are expressed as algebraic equations, we might like to determine if one grows faster than the other. the best way to do this is to take the limit of the two functions as n grows towards inﬁnity,
if the limit goes to ∞, then f(n) is in Ω(g(n)) because f(n) grows faster. if the limit goes to zero, then f(n) is in o(g(n)) because g(n) grows faster. if the limit goes to some constant other than zero, then f(n) = Θ(g(n)) because both grow at the same rate.
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
bilities of modern compilers to make extremely good optimizations of expressions. “optimization of expressions” here means a rearrangement of arithmetic or logical expressions to run more efﬁciently. be careful not to damage the compiler’s ability to do such optimizations for you in an effort to optimize the expression yourself. always check that your “optimizations” really do improve the program by running the program before and after the change on a suitable benchmark set of input. many times i have been wrong about the positive effects of code tuning in my own programs. most often i am wrong when i try to optimize an expression. it is hard to do better than the compiler.
this chapter has focused on asymptotic analysis. this is an analytic tool, whereby we model the key aspects of an algorithm to determine the growth rate of the algorithm as the input size grows. as pointed out previously, there are many limitations to this approach. these include the effects at small problem size, determining the ﬁner distinctions between algorithms with the same growth rate, and the inherent difﬁculty of doing mathematical modeling for more complex problems.
an alternative to analytical approaches are empirical approaches. the most obvious empirical approach is simply to run two competitors and see which performs better. in this way we might overcome the deﬁciencies of analytical approaches.
be warned that comparative timing of programs is a difﬁcult business, often subject to experimental errors arising from uncontrolled factors (system load, the language or compiler used, etc.). the most important point is not to be biased in favor of one of the programs. if you are biased, this is certain to be reﬂected in the timings. one look at competing software or hardware vendors’ advertisements should convince you of this. the most common pitfall when writing two programs to compare their performance is that one receives more code-tuning effort than the other. as mentioned in section 3.10, code tuning can often reduce running time by a factor of ten. if the running times for two programs differ by a constant factor regardless of input size (i.e., their growth rates are the same), then differences in code tuning might account for any difference in running time. be suspicious of empirical comparisons in this situation.
sum += n; the ﬁrst line is Θ(1). the for loop is repeated n times. the third line takes constant time so, by simplifying rule (4) of section 3.4.4, the total cost for executing the two lines making up the for loop is Θ(n). by rule (3), the cost of the entire code fragment is also Θ(n).
a[k] = k; this code fragment has three separate statements: the ﬁrst assignment statement and the two for loops. again the assignment statement takes constant time; call it c1. the second for loop is just like the one in example 3.10 and takes c2n = Θ(n) time.
the ﬁrst for loop is a double loop and requires a special technique. we work from the inside of the loop outward. the expression sum++ requires constant time; call it c3. because the inner for loop is executed i times, by simplifying rule (4) it has cost c3i. the outer for loop is executed n times,
comparing sequential search to binary search, we see that as n grows, the Θ(n) running time for sequential search in the average and worst cases quickly becomes much greater than the Θ(log n) running time for binary search. taken in isolation, binary search appears to be much more efﬁcient than sequential search. this is despite the fact that the constant factor for binary search is greater than that for sequential search, because the calculation for the next search position in binary search is more expensive than just incrementing the current position, as sequential search does.
note however that the running time for sequential search will be roughly the same regardless of whether or not the array values are stored in order. in contrast, binary search requires that the array values be ordered from lowest to highest. depending on the context in which binary search is to be used, this requirement for a sorted array could be detrimental to the running time of a complete program, because maintaining the values in sorted order requires to greater cost when inserting new elements into the array. this is an example of a tradeoff between the advantage of binary search during search and the disadvantage related to maintaining a sorted array. only in the context of the complete problem to be solved can we know whether the advantage outweighs the disadvantage.
you most often use the techniques of “algorithm” analysis to analyze an algorithm, or the instantiation of an algorithm as a program. you can also use these same techniques to analyze the cost of a problem. it should make sense to you to say that the upper bound for a problem cannot be worse than the upper bound for the best algorithm that we know for that problem. but what does it mean to give a lower bound for a problem?
consider a graph of cost over all inputs of a given size n for some algorithm for a given problem. deﬁne a to be the collection of all algorithms that solve the problem (theoretically, there an inﬁnite number of such algorithms). now, consider the collection of all the graphs for all of the (inﬁnitely many) algorithms in a. the worst case lower bound is the least of all the highest points on all the graphs.
it is much easier to show that an algorithm (or program) is in Ω(f(n)) than it is to show that a problem is in Ω(f(n)). for a problem to be in Ω(f(n)) means that every algorithm that solves the problem is in Ω(f(n)), even algorithms that we have not thought of!
so far all of our examples of algorithm analysis give “obvious” results, with big-oh always matching Ω. to understand how big-oh, Ω, and Θ notations are
position i along the x axis). this is why we must always say that function f(n) is in o(g(n)) in the best, average, or worst case! if we leave off which class of inputs we are discussing, we cannot know which cost measure we are referring to for most algorithms.
sometimes the proper analysis for an algorithm requires multiple parameters to describe the cost. to illustrate the concept, consider an algorithm to compute the rank ordering for counts of all pixel values in a picture. pictures are often represented by a two-dimensional array, and a pixel is one cell in the array. the value of a pixel is either the code value for the color, or a value for the intensity of the picture at that pixel. assume that each pixel can take any integer value in the range 0 to c − 1. the problem is to ﬁnd the number of pixels of each color value and then sort the color values with respect to the number of times each value appears in the picture. assume that the picture is a rectangle with p pixels. a pseudocode algorithm to solve the problem follows.
in this example, count is an array of size c that stores the number of pixels for each color value. function value(i) returns the color value for pixel i.
the time for the ﬁrst for loop (which initializes count) is based on the number of colors, c. the time for the second loop (which determines the number of pixels with each color) is Θ(p ). the time for the ﬁnal line, the call to sort, depends on the cost of the sorting algorithm used. from the discussion of section 3.6, we can assume that the sorting algorithm has cost Θ(p log p ) if p items are sorted, thus yielding Θ(p log p ) as the total algorithm cost.
is this a good representation for the cost of this algorithm? what is actually being sorted? it is not the pixels, but rather the colors. what if c is much smaller than p ? then the estimate of Θ(p log p ) is pessimistic, because much fewer than p items are being sorted. instead, we should use p as our analysis variable for steps that look at each pixel, and c as our analysis variable for steps that look at colors. then we get Θ(c) for the initialization loop, Θ(p ) for the pixel count loop, and Θ(c log c) for the sorting operation. this yields a total cost of Θ(p + c log c).
why can we not simply use the value of c for input size and say that the cost of the algorithm is Θ(c log c)? because, c is typically much less than p . for example, a picture might have 1000 × 1000 pixels and a range of 256 possible colors. so, p is one million, which is much larger than c log c. but, if p is smaller, or c larger (even if it is still less than p ), then c log c can become the larger quantity. thus, neither variable should be ignored.
besides time, space is the other computing resource that is commonly of concern to programmers. just as computers have become much faster over the years, they have also received greater allotments of memory. even so, the amount of available disk space or main memory can be signiﬁcant constraints for algorithm designers. the analysis techniques used to measure space requirements are similar to those used to measure time requirements. however, while time requirements are normally measured for an algorithm that manipulates a particular data structure, space requirements are normally determined for the data structure itself. the concepts of asymptotic analysis for growth rates on input size apply completely to measuring space requirements.
example 3.16 what are the space requirements for an array of n integers? if each integer requires c bytes, then the array requires cn bytes, which is Θ(n).
example 3.17 imagine that we want to keep track of friendships between n people. we can do this with an array of size n× n. each row of the array represents the friends of an individual, with the columns indicating who has that individual as a friend. for example, if person j is a friend of person i, then we place a mark in column j of row i in the array. likewise, we should also place a mark in column i of row j if we assume that friendship works both ways. for n people, the total size of the array is Θ(n2).
a data structure’s primary purpose is to store data in a way that allows efﬁcient access to those data. to provide efﬁcient access, it may be necessary to store additional information about where the data are within the data structure. for example, each node of a linked list must store a pointer to the next value on the list. all such information stored in addition to the actual data values is referred to as overhead. ideally, overhead should be kept to a minimum while allowing maximum access.
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
why can we not simply use the value of c for input size and say that the cost of the algorithm is Θ(c log c)? because, c is typically much less than p . for example, a picture might have 1000 × 1000 pixels and a range of 256 possible colors. so, p is one million, which is much larger than c log c. but, if p is smaller, or c larger (even if it is still less than p ), then c log c can become the larger quantity. thus, neither variable should be ignored.
besides time, space is the other computing resource that is commonly of concern to programmers. just as computers have become much faster over the years, they have also received greater allotments of memory. even so, the amount of available disk space or main memory can be signiﬁcant constraints for algorithm designers. the analysis techniques used to measure space requirements are similar to those used to measure time requirements. however, while time requirements are normally measured for an algorithm that manipulates a particular data structure, space requirements are normally determined for the data structure itself. the concepts of asymptotic analysis for growth rates on input size apply completely to measuring space requirements.
example 3.16 what are the space requirements for an array of n integers? if each integer requires c bytes, then the array requires cn bytes, which is Θ(n).
example 3.17 imagine that we want to keep track of friendships between n people. we can do this with an array of size n× n. each row of the array represents the friends of an individual, with the columns indicating who has that individual as a friend. for example, if person j is a friend of person i, then we place a mark in column j of row i in the array. likewise, we should also place a mark in column i of row j if we assume that friendship works both ways. for n people, the total size of the array is Θ(n2).
a data structure’s primary purpose is to store data in a way that allows efﬁcient access to those data. to provide efﬁcient access, it may be necessary to store additional information about where the data are within the data structure. for example, each node of a linked list must store a pointer to the next value on the list. all such information stored in addition to the actual data values is referred to as overhead. ideally, overhead should be kept to a minimum while allowing maximum access.
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
2t (n − 1) + c. t (n) = Θ(2n). but... there are only n(k + 1) subproblems to solve! clearly, there are many subproblems being solved repeatedly. store a n × k + 1 matrix to contain the solutions for all p (i, k). fill in the rows from i = 0 to n, left to right.
8 9 k1 =9 o − − − − − − − − i k2 =2 o − i − − − − − − o k3 =7 o − o − − − − i k4 =4 o − o − i − i o − o k5 =1 o i o i o i o i/o i o key:
-: no solution for p (i, k). o: solution(s) for p (i, k) with i omitted. i: solution(s) for p (i, k) with i included. i/o: solutions for p (i, k) with i included and omitted.
example: m(3, 9) contains o because p (2, 9) has a solution. it contains i because p (2, 2) = p (2, 9 − 7) has a solution. how can we ﬁnd a solution to p (5, 10)? how can we ﬁnd all solutions to p (5, 10)?
we next consider the problem of ﬁnding the shortest distance between all pairs of vertices in the graph, called the all-pairs shortest-paths problem. to be precise, for every u, v ∈ v, calculate d(u, v). one solution is to run dijkstra’s algorithm |v| times, each time computing the shortest path from a different start vertex. if g is sparse (that is, |e| = Θ(|v|)) then this is a good solution, because the total cost will be Θ(|v|2 + |v||e| log |v|) = Θ(|v|2 log |v|) for the version of dijkstra’s algorithm based on priority queues.
4. x’s rank is “usually” “close” to y ’s rank. we often give such algorithms names: 1. exact or deterministic algorithm. 2. approximation algorithm. 3. probabilistic algorithm. 4. heuristic. we can also sacriﬁce reliability for speed: 1. we ﬁnd the best, “usually” fast. 2. we ﬁnd the best fast, or we don’t get an answer at all (but fast). choose m elements at random, and pick the best. • for large n, if m = log n, the answer is pretty good. • cost is m − 1.
this just does the transform on one of the two polynomials. the full process is: 1. transform each polynomial. 2. multiply the resulting values (o(n) multiplies). 3. do the inverse transformation on the result.
16.4 the implementation for floyd’s algorithm given in section 16.2.2 is inefﬁcient for adjacency lists because the edges are visited in a bad order when initializing array d. what is the cost of of this initialization step for the adjacency list? how can this initialization step be revised so that it costs Θ(|v|2) in the worst case?
16.6 show the skip list that results from inserting the following values. draw the skip list after each insert. with each value, assume the depth of its corresponding node is as given in the list.
when analyzing these two code fragments, we will assume that n is a power of two. the ﬁrst code fragment has its outer for loop executed log n + 1 times because on each iteration k is multiplied by two until it reaches n. because the inner loop always executes n times, the total cost for i=0 n. note that a variable substitution takes place here to create the summation, with k = 2i. from equation 2.3, the solution for this summation is Θ(n log n). in the second code fragment, the outer loop is also executed log n + 1 times. the inner loop has cost k, which doubles each time. the summation can be expressed i=0 2i where n is assumed to be a power of two and again k = 2i.
what about other control statements? while loops are analyzed in a manner similar to for loops. the cost of an if statement in the worst case is the greater of the costs for the then and else clauses. this is also true for the average case, assuming that the size of n does not affect the probability of executing one of the clauses (which is usually, but not necessarily, true). for switch statements, the worst-case cost is that of the most expensive branch. for subroutine calls, simply add the cost of executing the subroutine.
there are rare situations in which the probability for executing the various branches of an if or switch statement are functions of the input size. for example, for input of size n, the then clause of an if statement might be executed with probability 1/n. an example would be an if statement that executes the then clause only for the smallest of n values. to perform an average-case analysis for such programs, we cannot simply count the cost of the if statement as being the cost of the more expensive branch. in such situations, the technique of amortized analysis (see section 14.3) can come to the rescue.
determining the execution time of a recursive subroutine can be difﬁcult. the running time for a recursive subroutine is typically best expressed by a recurrence relation. for example, the recursive factorial function fact of section 2.5 calls itself with a value one less than its input value. the result of this recursive call is
cost for a series of insert/delete operations relatively inexpensive, even though an occasional insert/delete operation might be expensive. to analyze the cost of dynamic array operations, we need to use a technique known as amortized analysis, which is discussed in section 14.3.
list users must decide whether they wish to store a copy of any given element on each list that contains it. for small elements such as an integer, this makes sense. if the elements are payroll records, it might be desirable for the list node to store a pointer to the record rather than store a copy of the record itself. this change would allow multiple list nodes (or other data structures) to point to the same record, rather than make repeated copies of the record. not only might this save space, but it also means that a modiﬁcation to an element’s value is automatically reﬂected at all locations where it is referenced. the disadvantage of storing a pointer to each element is that the pointer requires space of its own. if elements are never duplicated, then this additional space adds unnecessary overhead. java most naturally stores references to objects, meaning that only a single copy of an object such as a payroll record will be maintained, even if it is on multiple lists.
whether it is more advantageous to use references to shared elements or separate copies depends on the intended application. in general, the larger the elements and the more they are duplicated, the more likely that references to shared elements is the better approach.
a second issue faced by implementors of a list class (or any other data structure that stores a collection of user-deﬁned data elements) is whether the elements stored are all required to be of the same type. this is known as homogeneity in a data structure. in some applications, the user would like to deﬁne the class of the data element that is stored on a given list, and then never permit objects of a different class to be stored on that same list. in other applications, the user would like to permit the objects stored on a single list to be of differing types.
for the list implementations presented in this section, the compiler requires that all objects stored on the list be of the same type. besides java generics, there are other techniques that implementors of a list class can use to ensure that the element type for a given list remains ﬁxed, while still permitting different lists to store different element types. one approach is to store an object of the appropriate type in the header node of the list (perhaps an object of the appropriate type is supplied as a parameter to the list constructor), and then check that all insert operations on that list use the same element type.
n searches are performed. in other words, if we had known the series of (at least n) searches in advance and had stored the records in order of frequency so as to minimize the total cost for these accesses, this cost would be at least half the cost required by the move-to-front heuristic. (this will be proved using amortized analysis in section 14.3.) finally, move-to-front responds well to local changes in frequency of access, in that if a record is frequently accessed for a brief period of time it will be near the front of the list during that period of access. move-to-front does poorly when the records are processed in sequential order, especially if that sequential order is then repeated multiple times.
3. swap any record found with the record immediately preceding it in the list. this heuristic is called transpose. transpose is good for list implementations based on either linked lists or arrays. frequently used records will, over time, move to the front of the list. records that were once frequently accessed but are no longer used will slowly drift toward the back. thus, it appears to have good properties with respect to changing frequency of access. unfortunately, there are some pathological sequences of access that can make transpose perform poorly. consider the case where the last record of the list (call it x) is accessed. this record is then swapped with the next-to-last record (call it y), making y the last record. if y is now accessed, it swaps with x. a repeated series of accesses alternating between x and y will continually search to the end of the list, because neither record will ever make progress toward the front. however, such pathological cases are unusual in practice.
example 9.4 assume that we have eight records, with key values a to h, and that they are initially placed in alphabetical order. now, consider the result of applying the following access pattern:
and the total cost for the twelve accesses will be 45 comparisons. (assume that when a record’s frequency count goes up, it moves forward in the list to become the last record with that value for its frequency count. after the ﬁrst two accesses, f will be the ﬁrst record and d will be the second.)
this book contains many examples of asymptotic analysis of the time requirements for algorithms and the space requirements for data structures. often it is easy to invent an equation to model the behavior of the algorithm or data structure in question, and also easy to derive a closed-form solution for the equation should it contain a recurrence or summation.
sometimes an analysis proves more difﬁcult. it may take a clever insight to derive the right model, such as the snowplow argument for analyzing the average run length resulting from replacement selection (section 8.5.2). in this case, once the snowplow argument is understood, the resulting equations are simple. sometimes, developing the model is straightforward but analyzing the resulting equations is not. an example is the average-case analysis for quicksort. the equation given in section 7.5 simply enumerates all possible cases for the pivot position, summing corresponding costs for the recursive calls to quicksort. however, deriving a closed-form solution for the resulting recurrence relation is not as easy.
many iterative algorithms require that we compute a summation to determine the cost of a loop. techniques for ﬁnding closed-form solutions to summations are presented in section 14.1. time requirements for many algorithms based on recursion are best modeled by recurrence relations. a discussion of techniques for solving recurrences is provided in section 14.2. these sections extend the introduction to summations and recurrences provided in section 2.4, so the reader should already be familiar with that material.
section 14.3 provides an introduction to the topic of amortized analysis. amortized analysis deals with the cost of a series of operations. perhaps a single operation in the series has high cost, but as a result the cost of the remaining operations is limited in such a way that the entire series can be done efﬁciently. amortized analysis has been used successfully to analyze several of the algorithms presented in
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
1.4 deﬁne an adt for a list of integers. first, decide what functionality your adt should provide. example 1.4 should give you some ideas. then, specify your adt in java in the form of an abstract class declaration, showing the functions, their parameters, and their return types.
1.5 brieﬂy describe how integer variables are typically represented on a computer. (look up one’s complement and two’s complement arithmetic in an introductory computer science textbook if you are not familiar with these.) why does this representation for integers qualify as a data structure as deﬁned in section 1.2?
1.6 deﬁne an adt for a two-dimensional array of integers. specify precisely the basic operations that can be performed on such arrays. next, imagine an application that stores an array with 1000 rows and 1000 columns, where less than 10,000 of the array values are non-zero. describe two different implementations for such arrays that would be more space efﬁcient than a standard two-dimensional array implementation requiring one million positions.
1.7 you have been assigned to implement a sorting program. the goal is to make this program general purpose, in that you don’t want to deﬁne in advance what record or key types are used. describe ways to generalize a simple sorting algorithm (such as insertion sort, or any other sort you are familiar with) to support this generalization.
1.8 you have been assigned to implement a simple seqential search on an array. the problem is that you want the search to be as general as possible. this means that you need to support arbitrary record and key types. describe ways to generalize the search function to support this goal. consider the possibility that the function will be used multiple times in the same program, on differing record types. consider the possibility that the function will need to be used on different keys (possibly with the same or different types) of the same record. for example, a student data record might be searched by zip code, by name, by salary, or by gpa.
1.9 does every problem have an algorithm? 1.10 does every algorithm have a java program? 1.11 consider the design for a spelling checker program meant to run on a home computer. the spelling checker should be able to handle quickly a document of less than twenty pages. assume that the spelling checker comes with a dictionary of about 20,000 words. what primitive operations must be implemented on the dictionary, and what is a reasonable time constraint for each operation?
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
the typical customer opens and closes accounts far less often than he or she accesses the account. customers are willing to wait many minutes while accounts are created or deleted but are typically not willing to wait more than a brief time for individual account transactions such as a deposit or withdrawal. these observations can be considered as informal speciﬁcations for the time constraints on the problem.
it is common practice for banks to provide two tiers of service. human tellers or automated teller machines (atms) support customer access to account balances and updates such as deposits and withdrawals. special service representatives are typically provided (during restricted hours) to handle opening and closing accounts. teller and atm transactions are expected to take little time. opening or closing an account can take much longer (perhaps up to an hour from the customer’s perspective).
from a database perspective, we see that atm transactions do not modify the database signiﬁcantly. for simplicity, assume that if money is added or removed, this transaction simply changes the value stored in an account record. adding a new account to the database is allowed to take several minutes. deleting an account need have no time constraint, because from the customer’s point of view all that matters is that all the money be returned (equivalent to a withdrawal). from the bank’s point of view, the account record might be removed from the database system after business hours, or at the end of the monthly account cycle.
when considering the choice of data structure to use in the database system that manages customer accounts, we see that a data structure that has little concern for the cost of deletion, but is highly efﬁcient for search and moderately efﬁcient for insertion, should meet the resource constraints imposed by this problem. records are accessible by unique account number (sometimes called an exact-match query). one data structure that meets these requirements is the hash table described in chapter 9.4. hash tables allow for extremely fast exact-match search. a record can be modiﬁed quickly when the modiﬁcation does not affect its space requirements. hash tables also support efﬁcient insertion of new records. while deletions can also be supported efﬁciently, too many deletions lead to some degradation in performance for the remaining operations. however, the hash table can be reorganized periodically to restore the system to peak efﬁciency. such reorganization can occur ofﬂine so as not to affect atm transactions.
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
unfortunately, the bst can become unbalanced. even under relatively good conditions, the depth of leaf nodes can easily vary by a factor of two. this might not be a signiﬁcant concern when the tree is stored in main memory because the time required is still Θ(log n) for search and update. when the tree is stored on disk, however, the depth of nodes in the tree becomes crucial. every time a bst node b is visited, it is necessary to visit all nodes along the path from the root to b. each node on this path must be retrieved from disk. each disk access returns a block of information. if a node is on the same block as its parent, then the cost to ﬁnd that node is trivial once its parent is in main memory. thus, it is desirable to keep subtrees together on the same block. unfortunately, many times a node is not on the same block as its parent. thus, each access to a bst node could potentially require that another block to be read from disk. using a buffer pool to store multiple blocks in memory can mitigate disk access problems if bst accesses display good locality of reference. but a buffer pool cannot eliminate disk i/o entirely. the problem becomes greater if the bst is unbalanced, because nodes deep in the tree have the potential of causing many disk blocks to be read. thus, there are two signiﬁcant issues that must be addressed to have efﬁcient search from a disk-based bst. the ﬁrst is how to keep the tree balanced. the second is how to arrange the nodes on blocks so as to keep the number of blocks encountered on any path from the root to the leaves at a minimum.
we could select a scheme for balancing the bst and allocating bst nodes to blocks in a way that minimizes disk i/o, as illustrated by figure 10.7. however, maintaining such a scheme in the face of insertions and deletions is difﬁcult. in particular, the tree should remain balanced when an update takes place, but doing so might require much reorganization. each update should affect only a disk few blocks, or its cost will be too high. as you can see from figure 10.8, adopting a rule such as requiring the bst to be complete can cause a great deal of rearranging of data within the tree.
we can solve these problems by selecting another tree structure that automatically remains balanced after updates, and which is amenable to storing in blocks. there are a number of widely used balanced tree data structures, and there are also techniques for keeping bsts balanced. examples are the avl and splay trees discussed in section 13.2. as an alternative, section 10.4 presents the 2-3 tree, which has the property that its leaves are always at the same level. the main reason for discussing the 2-3 tree here in preference to the other balanced search trees is that
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
if we are willing to weaken the balance requirements, we can come up with alternative update routines that perform well both in terms of cost for the update and in balence for the resulting tree structure. the avl tree works in this way, using insertion and deletion routines altered from those of the bst to ensure that, for every node, the depths of the left and right subtrees differ by at most one. the avl tree is described in section 13.2.1.
a different approach to improving the performance of the bst is to not require that the tree always be balanced, but rather to expend some effort toward making the bst more balanced every time it is accessed. this is a little like the idea of path compression used by the union/find algorithm presented in section 6.2. one example of such a compromise is called the splay tree. the splay tree is described in section 13.2.2.
the avl tree (named for its inventors adelson-velskii and landis) should be viewed as a bst with the following additional property: for every node, the heights of its left and right subtrees differ by at most 1. as long as the tree maintains this property, if the tree contains n nodes, then it has a depth of at most o(log n). as a result, search for any node will cost o(log n), and if the updates can be done in time proportional to the depth of the node inserted or deleted, then updates will also cost o(log n), even in the worst case.
the key to making the avl tree work is to make the proper alterations to the insert and delete routines so as to maintain the balance property. of course, to be practical, we must be able to implement the revised update routines in Θ(log n) time.
consider what happens when we insert a node with key value 5, as shown in figure 13.4. the tree on the left meets the avl tree balance requirements. after the insertion, two nodes no longer meet the requirements. because the original tree met the balance requirement, nodes in the new tree can only be unbalanced by a difference of at most 2 in the subtrees. for the bottommost unbalanced node, call it s, there are 4 cases:
1. the extra node is in the left child of the left child of s. 2. the extra node is in the right child of the left child of s. 3. the extra node is in the left child of the right child of s. 4. the extra node is in the right child of the right child of s.
o(m log n) time for a tree of n nodes whenever m ≥ n. thus, a single insert or search operation could take o(n) time. however, m such operations are guaranteed to require a total of o(m log n) time, for an average cost of o(log n) per access operation. this is a desirable performance guarantee for any search-tree structure. unlike the avl tree, the splay tree is not guaranteed to be height balanced. what is guaranteed is that the total cost of the entire series of accesses will be cheap. ultimately, it is the cost of the series of operations that matters, not whether the tree is balanced. maintaining balance is really done only for the sake of reaching this time efﬁciency goal.
the splay tree access functions operate in a manner reminiscent of the moveto-front rule for self-organizing lists from section 9.2, and of the path compression technique for managing parent-pointer trees from section 6.2. these access functions tend to make the tree more balanced, but an individual access will not necessarily result in a more balanced tree.
whenever a node s is accessed (e.g., when s is inserted, deleted, or is the goal of a search), the splay tree performs a process called splaying. splaying moves s to the root of the bst. when s is being deleted, splaying moves the parent of s to the root. as in the avl tree, a splay of node s consists of a series of rotations. a rotation moves s higher in the tree by adjusting its position with respect to its parent and grandparent. a side effect of the rotations is a tendency to balance the tree. there are three types of rotation.
a single rotation is performed only if s is a child of the root node. the single rotation is illustrated by figure 13.7. it basically switches s with its parent in a way that retains the bst property. while figure 13.7 is slightly different from figure 13.5, in fact the splay tree single rotation is identical to the avl tree single rotation.
unlike the avl tree, the splay tree requires two types of double rotation. double rotations involve s, its parent (call it p), and s’s grandparent (call it g). the effect of a double rotation is to move s up two levels in the tree.
(a) show the result of building a bintree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (45, 25), h (45, 30), i (50, 30).
(b) show the result of deleting point c from the tree you built in part (a). (c) show the result of deleting point f from the resulting tree in part (b).
13.16 compare the trees constructed for exercises 12 and 15 in terms of the number of internal nodes, full leaf nodes, empty leaf nodes, and total depths of the two trees.
13.17 show the result of building a point quadtree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (31, 35), g (45, 26), h (44, 30), i (50, 30).
13.1 use the trie data structure to devise a program to sort variable-length strings. the program’s running time should be proportional to the total number of letters in all of the strings. note that some strings might be very long while most are short.
13.2 deﬁne the set of sufﬁx strings for a string s to be s, s without its ﬁrst character, s without its ﬁrst two characters, and so on. for example, the complete set of sufﬁx strings for “hello” would be
a sufﬁx tree is a pat trie that contains all of the sufﬁx strings for a given string, and associates each sufﬁx with the complete string. the advantage of a sufﬁx tree is that it allows a search for strings using “wildcards.” for example, the search key “th*” means to ﬁnd all strings with “th” as the ﬁrst two characters. this can easily be done with a regular trie. searching for “*th” is not efﬁcient in a regular trie, but it is efﬁcient in a sufﬁx tree. implement the sufﬁx tree for a dictionary of words or phrases, with support for wildcard search.
13.3 revise the bst class of section 5.4 to use the avl tree rotations. your new implementation should not modify the original bst class adt. compare your avl tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
|p| = 3 (because p has three members) and |q| = 2 (because q has two members). the union of p and q, written p ∪ q, is the set of elements in either p or q, which is {2, 3, 5, 10}. the intersection of p and q, written p ∩ q, is the set of elements that appear in both p and q, which is {5}. the set difference of p and q, written p − q, is the set of elements that occur in p but not in q, which is {2, 3}. note that p ∪ q = q ∪ p and that p ∩ q = q ∩ p, but in general p − q 6= q − p. in this example, q − p = {10}. note that the set {4, 3, 5} is indistinguishable from set p, because sets have no concept of order. likewise, set {4, 3, 4, 5} is also indistinguishable from p, because sets have no concept of duplicate elements. s = {a, b, c}. the powerset of s is
sometimes we wish to deﬁne a collection of elements with no order (like a set), but with duplicate-valued elements. such a collection is called a bag.1 to distinguish bags from sets, i use square brackets [] around a bag’s elements. for
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
the typical customer opens and closes accounts far less often than he or she accesses the account. customers are willing to wait many minutes while accounts are created or deleted but are typically not willing to wait more than a brief time for individual account transactions such as a deposit or withdrawal. these observations can be considered as informal speciﬁcations for the time constraints on the problem.
it is common practice for banks to provide two tiers of service. human tellers or automated teller machines (atms) support customer access to account balances and updates such as deposits and withdrawals. special service representatives are typically provided (during restricted hours) to handle opening and closing accounts. teller and atm transactions are expected to take little time. opening or closing an account can take much longer (perhaps up to an hour from the customer’s perspective).
from a database perspective, we see that atm transactions do not modify the database signiﬁcantly. for simplicity, assume that if money is added or removed, this transaction simply changes the value stored in an account record. adding a new account to the database is allowed to take several minutes. deleting an account need have no time constraint, because from the customer’s point of view all that matters is that all the money be returned (equivalent to a withdrawal). from the bank’s point of view, the account record might be removed from the database system after business hours, or at the end of the monthly account cycle.
when considering the choice of data structure to use in the database system that manages customer accounts, we see that a data structure that has little concern for the cost of deletion, but is highly efﬁcient for search and moderately efﬁcient for insertion, should meet the resource constraints imposed by this problem. records are accessible by unique account number (sometimes called an exact-match query). one data structure that meets these requirements is the hash table described in chapter 9.4. hash tables allow for extremely fast exact-match search. a record can be modiﬁed quickly when the modiﬁcation does not affect its space requirements. hash tables also support efﬁcient insertion of new records. while deletions can also be supported efﬁciently, too many deletions lead to some degradation in performance for the remaining operations. however, the hash table can be reorganized periodically to restore the system to peak efﬁciency. such reorganization can occur ofﬂine so as not to affect atm transactions.
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
if you want to be a successful java programmer, you need good reference manuals close at hand. david flanagan’s java in a nutshell [fla05] provides a good reference for those familiar with the basics of the language.
after gaining proﬁciency in the mechanics of program writing, the next step is to become proﬁcient in program design. good design is difﬁcult to learn in any discipline, and good design for object-oriented software is one of the most difﬁcult of arts. the novice designer can jump-start the learning process by studying wellknown and well-used design patterns. the classic reference on design patterns is design patterns: elements of reusable object-oriented software by gamma, helm, johnson, and vlissides [ghjv95] (this is commonly referred to as the “gang of four” book). unfortunately, this is an extremely difﬁcult book to understand, in part because the concepts are inherently difﬁcult. a number of web sites are available that discuss design patterns, and which provide study guides for the design patterns book. two other books that discuss object-oriented software design are object-oriented software design and construction with c++ by dennis kafura [kaf98], and object-oriented design heuristics by arthur j. riel [rie96].
the exercises for this chapter are different from those in the rest of the book. most of these exercises are answered in the following chapters. however, you should not look up the answers in other parts of the book. these exercises are intended to make you think about some of the issues to be covered later on. answer them to the best of your ability with your current knowledge.
1.1 think of a program you have used that is unacceptably slow. identify the speciﬁc operations that make the program slow. identify other basic operations that the program performs quickly enough.
1.2 most programming languages have a built-in integer data type. normally this representation has a ﬁxed size, thus placing a limit on how large a value can be stored in an integer variable. describe a representation for integers that has no size restriction (other than the limits of the computer’s available main memory), and thus no practical limit on how large an integer can be stored. brieﬂy show how your representation can be used to implement the operations of addition, multiplication, and exponentiation.
1.3 deﬁne an adt for character strings. your adt should consist of typical functions that can be performed on strings, with each function deﬁned in
1.4 deﬁne an adt for a list of integers. first, decide what functionality your adt should provide. example 1.4 should give you some ideas. then, specify your adt in java in the form of an abstract class declaration, showing the functions, their parameters, and their return types.
1.5 brieﬂy describe how integer variables are typically represented on a computer. (look up one’s complement and two’s complement arithmetic in an introductory computer science textbook if you are not familiar with these.) why does this representation for integers qualify as a data structure as deﬁned in section 1.2?
1.6 deﬁne an adt for a two-dimensional array of integers. specify precisely the basic operations that can be performed on such arrays. next, imagine an application that stores an array with 1000 rows and 1000 columns, where less than 10,000 of the array values are non-zero. describe two different implementations for such arrays that would be more space efﬁcient than a standard two-dimensional array implementation requiring one million positions.
1.7 you have been assigned to implement a sorting program. the goal is to make this program general purpose, in that you don’t want to deﬁne in advance what record or key types are used. describe ways to generalize a simple sorting algorithm (such as insertion sort, or any other sort you are familiar with) to support this generalization.
1.8 you have been assigned to implement a simple seqential search on an array. the problem is that you want the search to be as general as possible. this means that you need to support arbitrary record and key types. describe ways to generalize the search function to support this goal. consider the possibility that the function will be used multiple times in the same program, on differing record types. consider the possibility that the function will need to be used on different keys (possibly with the same or different types) of the same record. for example, a student data record might be searched by zip code, by name, by salary, or by gpa.
1.9 does every problem have an algorithm? 1.10 does every algorithm have a java program? 1.11 consider the design for a spelling checker program meant to run on a home computer. the spelling checker should be able to handle quickly a document of less than twenty pages. assume that the spelling checker comes with a dictionary of about 20,000 words. what primitive operations must be implemented on the dictionary, and what is a reasonable time constraint for each operation?
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
what is the running time for this code fragment? clearly it takes longer to run when n is larger. the basic operation in this example is the increment operation for variable sum. we can assume that incrementing takes constant time; call this time c2. (we can ignore the time required to initialize sum, and to increment the loop counters i and j. in practice, these costs can safely be bundled into time c2.) the total number of increment operations is n2. thus, we say that the running time is t(n) = c2n2.
the growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. figure 3.1 shows a graph for six equations, each meant to describe the running time for a particular program or algorithm. a variety of growth rates representative of typical algorithms are shown. the two equations labeled 10n and 20n are graphed by straight lines. a growth rate of cn (for c any positive constant) is often referred to as a linear growth rate or running time. this means that as the value of n grows, the running time of the algorithm grows in the same proportion. doubling the value of n roughly doubles the running time. an algorithm whose running-time equation has a highest-order term containing a factor of n2 is said to have a quadratic growth rate. in figure 3.1, the line labeled 2n2 represents a quadratic growth rate. the line labeled 2n represents an exponential growth rate. this name comes from the fact that n appears in the exponent. the line labeled n! is also growing exponentially.
as you can see from figure 3.1, the difference between an algorithm whose running time has cost t(n) = 10n and another with cost t(n) = 2n2 becomes tremendous as n grows. for n > 5, the algorithm with running time t(n) = 2n2 is already much slower. this is despite the fact that 10n has a greater constant factor than 2n2. comparing the two curves marked 20n and 2n2 shows that changing the constant factor for one of the equations only shifts the point at which the two curves cross. for n > 10, the algorithm with cost t(n) = 2n2 is slower than the algorithm with cost t(n) = 20n. this graph also shows that the equation t(n) = 5n log n grows somewhat more quickly than both t(n) = 10n and t(n) = 20n, but not nearly so quickly as the equation t(n) = 2n2. for constants a, b > 1, na grows faster than either logb n or log nb. finally, algorithms with cost t(n) = 2n or t(n) = n! are prohibitively expensive for even modest values of n. note that for constants a, b ≥ 1, an grows faster than nb.
figure 3.3 the increase in problem size that can be run in a ﬁxed period of time on a computer that is ten times faster. the ﬁrst column lists the right-hand sides for each of the ﬁve growth rate equations of figure 3.1. for the purpose of this example, arbitrarily assume that the old machine can run 10,000 basic operations in one hour. the second column shows the maximum value for n that can be run in 10,000 basic operations on the old machine. the third column shows the value for n0, the new maximum size for the problem that can be run in the same time on the new machine that is ten times faster. variable n0 is the greatest size for the problem that can run in 100,000 basic operations. the fourth column shows how the size of n changed to become n0 on the new machine. the ﬁfth column shows the increase in the problem size as the ratio of n0 to n.
problem size (as a proportion to the original size) gained by a faster computer. this relationship holds true regardless of the algorithm’s growth rate: constant factors never affect the relative improvement gained by a faster computer.
an algorithm with time equation t(n) = 2n2 does not receive nearly as great an improvement from the faster machine as an algorithm with linear growth rate. instead of an improvement by a factor of ten, the improvement is only the square 10 ≈ 3.16. thus, the algorithm with higher growth rate not only root of that: solves a smaller problem in a given time in the ﬁrst place, it also receives less of a speedup from a faster computer. as computers get ever faster, the disparity in problem sizes becomes ever greater.
the algorithm with growth rate t(n) = 5n log n improves by a greater amount than the one with quadratic growth rate, but not by as great an amount as the algorithms with linear growth rates.
note that something special happens in the case of the algorithm whose running time grows exponentially. in figure 3.1, the curve for the algorithm whose time is proportional to 2n goes up very quickly. in figure 3.3, the increase in problem size on the machine ten times as fast is shown to be about n + 3 (to be precise, it is n + log2 10). the increase in problem size for an algorithm with exponential growth rate is by a constant addition, not by a multiplicative factor. because the old value of n was 13, the new problem size is 16. if next year you buy another
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
many organizations are hierarchical in nature, such as the military and most businesses. consider a company with a president and some number of vice presidents who report to the president. each vice president has some number of direct subordinates, and so on. if we wanted to model this company with a data structure, it would be natural to think of the president in the root node of a tree, the vice presidents at level 1, and their subordinates at lower levels in the tree as we go down the organizational hierarchy.
because the number of vice presidents is likely to be more than two, this company’s organization cannot easily be represented by a binary tree. we need instead to use a tree whose nodes have an arbitrary number of children. unfortunately, when we permit trees to have nodes with an arbrary number of children, they become much harder to implement than binary trees. we consider such trees in this chapter. to distinguish them from the more commonly used binary tree, we use the term general tree.
section 6.1 presents general tree terminology. section 6.2 presents a simple representation for solving the important problem of processing equivalence classes. several pointer-based implementations for general trees are covered in section 6.3. aside from general trees and binary trees, there are also uses for trees whose internal nodes have a ﬁxed number k of children where k is something other than two. such trees are known as k-ary trees. section 6.4 generalizes the properties of binary trees to k-ary trees. sequential representations, useful for applications such as storing trees on disk, are covered in section 6.5.
a tree t is a ﬁnite set of one or more nodes such that there is one designated node r, called the root of t. if the set (t−{r}) is not empty, these nodes are partitioned
figure 5.1 an example binary tree. node a is the root. nodes b and c are a’s children. nodes b and d together form a subtree. node b has two children: its left child is the empty tree and its right child is d. nodes a, c, and e are ancestors of g. nodes d, e, and f make up level 2 of the tree; node a is at level 0. the edges from a to c to e to g form a path of length 3. nodes d, g, h, and i are leaves. nodes a, b, c, e, and f are internal nodes. the depth of i is 3. the height of this tree is 4.
of the path is k − 1. if there is a path from node r to node m, then r is an ancestor of m, and m is a descendant of r. thus, all nodes in the tree are descendants of the root of the tree, while the root is the ancestor of all nodes.
the depth of a node m in the tree is the length of the path from the root of the tree to m. the height of a tree is one more than the depth of the deepest node in the tree. all nodes of depth d are at level d in the tree. the root is the only node at level 0, and its depth is 0.
figure 5.1 illustrates the various terms used to identify parts of a binary tree. figure 5.2 illustrates an important point regarding the structure of binary trees. because all binary tree nodes have two children (one or both of which might be empty), the two binary trees of figure 5.2 are not the same.
two restricted forms of binary tree are sufﬁciently important to warrant special names. each node in a full binary tree is either (1) an internal node with exactly two non-empty children or (2) a leaf. a complete binary tree has a restricted shape obtained by starting at the root and ﬁlling the tree by levels from left to right. in the complete binary tree of height d, all levels except possibly level d−1 are completely full. the bottom level has its nodes ﬁlled in from the left side.
figure 5.2 two different binary trees. (a) a binary tree whose root has a nonempty left child. (b) a binary tree whose root has a non-empty right child. (c) the binary tree of (a) with the missing right child made explicit. (d) the binary tree of (b) with the missing left child made explicit.
figure 5.3 illustrates the differences between full and complete binary trees.1 there is no particular relationship between these two tree shapes; that is, the tree of figure 5.3(a) is full but not complete while the tree of figure 5.3(b) is complete but not full. the heap data structure (section 5.5) is an example of a complete binary tree. the huffman coding tree (section 5.6) is an example of a full binary tree.
1while these deﬁnitions for full and complete binary tree are the ones most commonly used, they are not universal. some textbooks even reverse these deﬁnitions! because the common meaning of the words “full” and “complete” are quite similar, there is little that you can do to distinguish between them other than to memorize the deﬁnitions. here is a memory aid that you might ﬁnd useful: “complete” is a wider word than “full,” and complete binary trees tend to be wider than full binary trees because each level of a complete binary tree is as wide as possible.
if p = d, the overhead drops to about one half of the total space. however, if only leaf nodes store useful information, the overhead fraction for this implementation is actually three quarters of the total space, because half of the “data” space is unused. if a full binary tree needs to store data only at the leaf nodes, a better implementation would have the internal nodes store two pointers and no data ﬁeld while the leaf nodes store only a data ﬁeld. this implementation requires 2p n + d(n + 1) units of space. if p = d, then the overhead is about 2p/(2p +d) = 2/3. it might seem counter-intuitive that the overhead ratio has gone up while the total amount of space has gone down. the reason is because we have changed our deﬁnition of “data” to refer only to what is stored in the leaf nodes, so while the overhead fraction is higher, it is from a total storage requirement that is lower.
there is one serious ﬂaw with this analysis. when using separate implementations for internal and leaf nodes, there must be a way to distinguish between the node types. when separate node types are implemented via java subclasses, the runtime environment stores information with each object allowing it to determine, for example, the correct subclass to use when the isleaf virtual function is called. thus, each node requires additional space. only one bit is truly necessary to distinguish the two possibilities. in rare applications where space is a critical resource, implementors can often ﬁnd a spare bit within the node’s value ﬁeld in which to store the node type indicator. an alternative is to use a spare bit within a node pointer to indicate node type. for example, this is often possible when the compiler requires that structures and objects start on word boundaries, leaving the last bit of a pointer value always zero. thus, this bit can be used to store the nodetype ﬂag and is reset to zero before the pointer is dereferenced. another alternative when the leaf value ﬁeld is smaller than a pointer is to replace the pointer to a leaf with that leaf’s value. when space is limited, such techniques can make the difference between success and failure. in any other situation, such “bit packing” tricks should be avoided because they are difﬁcult to debug and understand at best, and are often machine dependent at worst.2
the previous section points out that a large fraction of the space in a typical binary tree node implementation is devoted to structural overhead, not to storing data.
2in the early to mid 1980s, i worked on a geographic information system that stored spatial data in quadtrees (see section 13.3). at the time space was a critical resource, so we used a bit-packing approach where we stored the nodetype ﬂag as the last bit in the parent node’s pointer. this worked perfectly on various 32-bit workstations. unfortunately, in those days ibm pc-compatibles used 16-bit pointers. we never did ﬁgure out how to port our code to the 16-bit machine.
this section presents a simple, compact implementation for complete binary trees. recall that complete binary trees have all levels except the bottom ﬁlled out completely, and the bottom level has all of its nodes ﬁlled in from left to right. thus, a complete binary tree of n nodes has only one possible shape. you might think that a complete binary tree is such an unusual occurrence that there is no reason to develop a special implementation for it. however, the complete binary tree has practical uses, the most important being the heap data structure discussed in section 5.5. heaps are often used to implement priority queues (section 5.5) and for external sorting algorithms (section 8.5.2).
we begin by assigning numbers to the node positions in the complete binary tree, level by level, from left to right as shown in figure 5.12(a). an array can store the tree’s data values efﬁciently, placing each data value in the array position corresponding to that node’s position within the tree. figure 5.12(b) lists the array indices for the children, parent, and siblings of each node in figure 5.12(a). from figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives within the array. simple formulae can be derived for calculating the array index for each relative of a node r from r’s index. no explicit pointers are necessary to reach a node’s left or right child. this means there is no overhead to the array implementation if the array is selected to be of size n for a tree of n nodes.
the formulae for calculating the array indices of the various relatives of a node are as follows. the total number of nodes in the tree is n. the index of the node in question is r, which must fall in the range 0 to n − 1.
• parent(r) = b(r − 1)/2c if r 6= 0. • left child(r) = 2r + 1 if 2r + 1 < n. • right child(r) = 2r + 2 if 2r + 2 < n. • left sibling(r) = r − 1 if r is even. • right sibling(r) = r + 1 if r is odd and r + 1 < n.
section 4.4 presented the dictionary adt, along with dictionary implementations based on sorted and unsorted lists. when implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. however, searching an unsorted list for a particular record requires Θ(n) time in the average case. for a large database, this is probably much too slow. alternatively, the records can be stored in a sorted list. if the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. on the other hand, if we use a sorted
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
figure 5.1 an example binary tree. node a is the root. nodes b and c are a’s children. nodes b and d together form a subtree. node b has two children: its left child is the empty tree and its right child is d. nodes a, c, and e are ancestors of g. nodes d, e, and f make up level 2 of the tree; node a is at level 0. the edges from a to c to e to g form a path of length 3. nodes d, g, h, and i are leaves. nodes a, b, c, e, and f are internal nodes. the depth of i is 3. the height of this tree is 4.
of the path is k − 1. if there is a path from node r to node m, then r is an ancestor of m, and m is a descendant of r. thus, all nodes in the tree are descendants of the root of the tree, while the root is the ancestor of all nodes.
the depth of a node m in the tree is the length of the path from the root of the tree to m. the height of a tree is one more than the depth of the deepest node in the tree. all nodes of depth d are at level d in the tree. the root is the only node at level 0, and its depth is 0.
figure 5.1 illustrates the various terms used to identify parts of a binary tree. figure 5.2 illustrates an important point regarding the structure of binary trees. because all binary tree nodes have two children (one or both of which might be empty), the two binary trees of figure 5.2 are not the same.
two restricted forms of binary tree are sufﬁciently important to warrant special names. each node in a full binary tree is either (1) an internal node with exactly two non-empty children or (2) a leaf. a complete binary tree has a restricted shape obtained by starting at the root and ﬁlling the tree by levels from left to right. in the complete binary tree of height d, all levels except possibly level d−1 are completely full. the bottom level has its nodes ﬁlled in from the left side.
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
if p = d, the overhead drops to about one half of the total space. however, if only leaf nodes store useful information, the overhead fraction for this implementation is actually three quarters of the total space, because half of the “data” space is unused. if a full binary tree needs to store data only at the leaf nodes, a better implementation would have the internal nodes store two pointers and no data ﬁeld while the leaf nodes store only a data ﬁeld. this implementation requires 2p n + d(n + 1) units of space. if p = d, then the overhead is about 2p/(2p +d) = 2/3. it might seem counter-intuitive that the overhead ratio has gone up while the total amount of space has gone down. the reason is because we have changed our deﬁnition of “data” to refer only to what is stored in the leaf nodes, so while the overhead fraction is higher, it is from a total storage requirement that is lower.
there is one serious ﬂaw with this analysis. when using separate implementations for internal and leaf nodes, there must be a way to distinguish between the node types. when separate node types are implemented via java subclasses, the runtime environment stores information with each object allowing it to determine, for example, the correct subclass to use when the isleaf virtual function is called. thus, each node requires additional space. only one bit is truly necessary to distinguish the two possibilities. in rare applications where space is a critical resource, implementors can often ﬁnd a spare bit within the node’s value ﬁeld in which to store the node type indicator. an alternative is to use a spare bit within a node pointer to indicate node type. for example, this is often possible when the compiler requires that structures and objects start on word boundaries, leaving the last bit of a pointer value always zero. thus, this bit can be used to store the nodetype ﬂag and is reset to zero before the pointer is dereferenced. another alternative when the leaf value ﬁeld is smaller than a pointer is to replace the pointer to a leaf with that leaf’s value. when space is limited, such techniques can make the difference between success and failure. in any other situation, such “bit packing” tricks should be avoided because they are difﬁcult to debug and understand at best, and are often machine dependent at worst.2
the previous section points out that a large fraction of the space in a typical binary tree node implementation is devoted to structural overhead, not to storing data.
2in the early to mid 1980s, i worked on a geographic information system that stored spatial data in quadtrees (see section 13.3). at the time space was a critical resource, so we used a bit-packing approach where we stored the nodetype ﬂag as the last bit in the parent node’s pointer. this worked perfectly on various 32-bit workstations. unfortunately, in those days ibm pc-compatibles used 16-bit pointers. we never did ﬁgure out how to port our code to the 16-bit machine.
techniques in common use today. the next section presents one such approach to assigning variable-length codes, called huffman coding. while it is not commonly used in its simplest form for ﬁle compression (there are better methods), huffman coding gives the ﬂavor of such coding schemes.
huffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character. thus, it is a variable-length code. if the estimated frequencies for letters match the actual frequency found in an encoded message, then the length of that message will typically be less than if a ﬁxed-length code had been used. the huffman code for each letter is derived from a full binary tree called the huffman coding tree, or simply the huffman tree. each leaf of the huffman tree corresponds to a letter, and we deﬁne the weight of the leaf node to be the weight (frequency) of its associated letter. the goal is to build a tree with the minimum external path weight. deﬁne the weighted path length of a leaf to be its weight times its depth. the binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. a letter with high weight should have low depth, so that it will count the least against the total path length. as a result, another letter might be pushed deeper in the tree if it has less weight.
the process of building the huffman tree for n letters is quite simple. first, create a collection of n initial huffman trees, each of which is a single leaf node containing one of the letters. put the n partial trees onto a min-heap (a priority queue) organized by weight (frequency). next, remove the ﬁrst two trees (the ones with lowest weight) from the heap. join these two trees together to create a new tree whose root has the two trees as children, and whose weight is the sum of the weights of the two trees. put this new tree back on the heap. this process is repeated until all of the partial huffman trees have been combined into one.
example 5.8 figure 5.25 illustrates part of the huffman tree construction process for the eight letters of figure 5.24. ranking d and l arbitrarily by alphabetical order, the letters are ordered by frequency as
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
some binary tree implementations store data only at the leaf nodes, using the internal nodes to provide structure to the tree. more generally, binary tree implementations might require some amount of space for internal nodes, and a different amount for leaf nodes. thus, to analyze the space required by such implementations, it is useful to know the minimum and maximum fraction of the nodes that are leaves in a tree containing n internal nodes.
unfortunately, this fraction is not ﬁxed. a binary tree of n internal nodes might have only one leaf. this occurs when the internal nodes are arranged in a chain ending in a single leaf as shown in figure 5.4. in this case, the number of leaves is low because each internal node has only one non-empty child. to ﬁnd an upper bound on the number of leaves for a tree of n internal nodes, ﬁrst note that the upper bound will occur when each internal node has two non-empty children, that is, when the tree is full. however, this observation does not tell what shape of tree will yield the highest percentage of non-empty leaves. it turns out not to matter, because all full binary trees with n internal nodes have the same number of leaves. this fact allows us to compute the space requirements for a full binary tree implementation whose leaves require a different amount of space from its internal nodes.
proof: the proof is by mathematical induction on n, the number of internal nodes. this is an example of an induction proof where we reduce from an arbitrary instance of size n to an instance of size n − 1 that meets the induction hypothesis.
• base cases: the non-empty tree with zero internal nodes has one leaf node. a full binary tree with one internal node has two leaf nodes. thus, the base cases for n = 0 and n = 1 conform to the theorem.
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
aspects of binary trees that are relevent to all applications. for example, we must be able to initialize a binary tree, and we might wish to determine if the tree is empty. some activities might be unique to the application. for example, we might wish to combine two binary trees by making their roots be the children of a new root node. other activities are centered around the nodes. for example, we might need access to the left or right child of a node, or to the node’s data value.
clearly there are activities that relate to nodes (e.g., reach a node’s child or get a node’s value), and activities that relate to trees (e.g., tree initialization). this indicates that nodes and trees should be implemented as separate classes. for now, we concentrate on the class to implement binary tree nodes. this class will be used by some of the binary tree structures presented later.
figure 5.5 shows an interface for binary tree nodes, called binnode. class binnode is a generic with parameter e, which is the type for the data record stored in a node. member functions are provided that set or return the element value, return a reference to the left child, return a reference to the right child, or indicate whether the node is a leaf.
often we wish to process a binary tree by “visiting” each of its nodes, each time performing a speciﬁc action such as printing the contents of the node. any process for visiting all of the nodes in some order is called a traversal. any traversal that lists every node in the tree exactly once is called an enumeration of the tree’s nodes. some applications do not require that the nodes be visited in any particular order as long as each node is visited precisely once. for other applications, nodes
a more difﬁcult situation is illustrated by the following problem. given an arbitrary binary tree we wish to determine if, for every node a, are all nodes in a’s left subtree less than the value of a, and are all nodes in a’s right subtree greater than the value of a? (this happens to be the deﬁnition for a binary search tree, described in section 5.4.) unfortunately, to make this decision we need to know some context that is not available just by looking at the node’s parent or children. as shown by figure 5.6, it is not enough to verify that a’s left child has a value less than that of a, and that a’s right child has a greater value. nor is it enough to verify that a has a value consistent with that of its parent. in fact, we need to know information about what range of values is legal for a given node. that information might come from any ancestor for the node. thus, relevent range information must be passed down the tree. we can implement this function as follows.
in this section we will examine ways to implement binary tree nodes. we begin with some options for pointer-based binary tree node implementations. then comes a
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
figure 5.2 two different binary trees. (a) a binary tree whose root has a nonempty left child. (b) a binary tree whose root has a non-empty right child. (c) the binary tree of (a) with the missing right child made explicit. (d) the binary tree of (b) with the missing left child made explicit.
figure 5.3 illustrates the differences between full and complete binary trees.1 there is no particular relationship between these two tree shapes; that is, the tree of figure 5.3(a) is full but not complete while the tree of figure 5.3(b) is complete but not full. the heap data structure (section 5.5) is an example of a complete binary tree. the huffman coding tree (section 5.6) is an example of a full binary tree.
1while these deﬁnitions for full and complete binary tree are the ones most commonly used, they are not universal. some textbooks even reverse these deﬁnitions! because the common meaning of the words “full” and “complete” are quite similar, there is little that you can do to distinguish between them other than to memorize the deﬁnitions. here is a memory aid that you might ﬁnd useful: “complete” is a wider word than “full,” and complete binary trees tend to be wider than full binary trees because each level of a complete binary tree is as wide as possible.
for each of the following scenarios, which of these choices would be best? explain your answer. (a) the records are guaranteed to arrive already sorted from lowest to highest (i.e., whenever a record is inserted, its key value will always be greater than that of the last record inserted). a total of 1000 inserts will be interspersed with 1000 searches.
(b) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1,000,000 insertions are performed, followed by 10 searches.
(c) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are interspersed with 1000 searches.
(d) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are performed, followed by 1,000,000 searches.
5.2 one way to deal with the “problem” of null pointers in binary trees is to use that space for some other purpose. one example is the threaded binary tree. extending the node implementation of figure 5.7, the threaded binary tree stores with each node two additional bit ﬁelds that indicate if the child pointers lc and rc are regular pointers to child nodes or threads. if lc is not a pointer to a non-empty child (i.e., if it would be null in a regular binary tree), then it instead stores a pointer to the inorder predecessor of that node. the inorder predecessor is the node that would be printed immediately before the current node in an inorder traversal. if rc is not a pointer to a child, then it instead stores a pointer to the node’s inorder successor. the inorder successor is the node that would be printed immediately after the current node in an inorder traversal. the main advantage of threaded binary trees is that operations such as inorder traversal can be implemented without using recursion or a stack. reimplement the bst as a threaded binary tree, and include a non-recursive version of the preorder traversal
5.3 implement a city database using a bst to store the database records. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates.
one important aspect of algorithm design is referred to as the space/time tradeoff principle. the space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacriﬁce space or vice versa. many programs can be modiﬁed to reduce storage requirements by “packing” or encoding information. “unpacking” or decoding the information requires additional time. thus, the resulting program uses less space but runs slower. conversely, many programs can be modiﬁed to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. typically, such changes in time and space are both by a constant factor.
a classic example of a space/time tradeoff is the lookup table. a lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. for example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. if you are writing a program that often computes factorials, it is likely to be much more time efﬁcient to simply pre-compute the 12 storable values in a table. whenever the program needs the value of n! for n ≤ 12, it can simply check the lookup table. (if n > 12, the value is too large to store as an int variable anyway.) compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table.
lookup tables can also store approximations for an expensive function such as sine or cosine. if you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. note that initially building the lookup table requires a certain amount of time. your application must use the lookup table often enough to make this initialization worthwhile.
another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. here is a simple code fragment for sorting an array of integers. we assume that this is a special case where there are n integers whose values are a permutation of the integers from 0 to n − 1. this is an example of a binsort, which is discussed in section 7.7. binsort assigns each value to an array position corresponding to its value.
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
over the time required to ﬁnd the k largest elements using one of the other sorting methods described earlier. one situation where we are able to take advantage of this concept is in the implementation of kruskal’s minimum-cost spanning tree (mst) algorithm of section 11.5.2. that algorithm requires that edges be visited in ascending order (so, use a min-heap), but this process stops as soon as the mst is complete. thus, only a relatively small fraction of the edges need be sorted.
imagine that for the past year, as you paid your various bills, you then simply piled all the paperwork onto the top of a table somewhere. now the year has ended and its time to sort all of these papers by what the bill was for (phone, electricity, rent, etc.) and date. a pretty natural approach is to make some space on the ﬂoor, and as you go through the pile of papers, put the phone bills into one pile, the electric bills into another pile, and so on. once this initial assignment of bills to piles is done (in one pass), you can sort each pile by date relatively quickly because they are each fairly small. this is the basic idea behind a binsort. numbers 0 through n − 1:
here the key value is used to determine the position for a record in the ﬁnal sorted array. this is the most basic example of a binsort, where key values are used to assign records to bins. this algorithm is extremely efﬁcient, taking Θ(n) time regardless of the initial ordering of the keys. this is far better than the performance of any sorting algorithm that we have seen so far. the only problem is that this algorithm has limited use because it works only for a permutation of the numbers from 0 to n − 1.
we can extend this simple binsort algorithm to be more useful. because binsort must perform direct computation on the key value (as opposed to just asking which of two records comes ﬁrst as our previous sorting algorithms did), we will assume that the records use an integer key type.
the simplest extension is to allow for duplicate values among the keys. this can be done by turning array slots into arbitrary-length bins by turning b into an array of linked lists. in this way, all records with key value i can be placed in bin b[i]. a second extension allows for a key range greater than n. for example, a set of n records might have keys in the range 1 to 2n. the only requirement is
are real numbers or arbitrary length strings, then some care will be necessary in implementation. in particular, radix sort will need to be careful about deciding when the “last digit” has been found to distinguish among real numbers, or the last character in variable length strings. implementing the concept of radix sort with the trie data structure (section 13.1) is most appropriate for these situations.
at this point, the perceptive reader might begin to question our earlier assumption that key comparison takes constant time. if the keys are “normal integer” values stored in, say, an integer variable, what is the size of this variable compared to n? in fact, it is almost certain that 32 (the number of bits in a standard int variable) is greater than log n for any practical computation. in this sense, comparison of two long integers requires Ω(log n) work.
computers normally do arithmetic in units of a particular size, such as a 32-bit word. regardless of the size of the variables, comparisons use this native word size and require a constant amount of time. in practice, comparisons of two 32-bit values take constant time, even though 32 is much greater than log n. to some extent the truth of the proposition that there are constant time operations (such as integer comparison) is in the eye of the beholder. at the gate level of computer architecture, individual bits are compared. however, constant time comparison for integers is true in practice on most computers, and we rely on such assumptions as the basis for our analyses. in contrast, radix sort must do several arithmetic calculations on key values (each requiring constant time), where the number of such calculations is proportional to the key length. thus, radix sort truly does Ω(n log n) work to process n distinct key values.
which sorting algorithm is fastest? asymptotic complexity analysis lets us distinguish between Θ(n2) and Θ(n log n) algorithms, but it does not help distinguish between algorithms with the same asymptotic complexity. nor does asymptotic analysis say anything about which algorithm is best for sorting small lists. for answers to these questions, we can turn to empirical testing.
figure 7.13 shows timing results for actual implementations of the sorting algorithms presented in this chapter. the algorithms compared include insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort and radix sort. shellsort shows both the basic version from section 7.3 and another with increments based on division by three. mergesort shows both the basic implementation from section 7.4 and the optimized version with calls to insertion sort for lists of length below nine. for quicksort, two versions are compared: the basic implementation from section 7.5 and an optimized version that does not partition
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
figure 9.2 an illustration of open hashing for seven numbers stored in a ten-slot hash table using the hash function h(k) = k mod 10. the numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. two of the values hash to slot 0, one value hashes to slot 2, three of the values hash to slot 7, and one value hashes to slot 9.
open hashing is most appropriate when the hash table is kept in main memory, with the lists implemented by a standard in-memory linked list. storing an open hash table on disk in an efﬁcient way is difﬁcult, because members of a given linked list might be stored on different disk blocks. this would result in multiple disk accesses when searching for a particular key value, which defeats the purpose of using hashing.
there are similarities between open hashing and binsort. one way to view open hashing is that each record is simply placed in a bin. while multiple records may hash to the same bin, this initial binning should still greatly reduce the number of records accessed by a search operation. in a similar fashion, a simple binsort reduces the number of records in each bin to a small number that can be sorted in some other way.
closed hashing stores all records directly in the hash table. each record r with key value kr has a home position that is h(kr), the slot computed by the hash function. if r is to be inserted and another record already occupies r’s home position, then
the records in the a array using the corresponding value in the b array as the sort key and running a simple Θ(n) binsort. the conversion of sorting to pairing can be done in o(n) time, and likewise the conversion of the output of pairing can be converted to the correct output for sorting in o(n) time. thus, the cost of this “sorting algorithm” is dominated by the cost for pairing.
consider any two problems for which a suitable reduction from one to the other can be found. the ﬁrst problem takes an arbitrary instance of its input, which we will call i, and transforms i to a solution, which we will call sln. the second problem takes an arbitrary instance of its input, which we will call i0, and transforms i0 to a solution, which we will call sln0. we can deﬁne reduction more formally as a three-step process:
1. transform an arbitrary instance of the ﬁrst problem to an instance of the second problem. in other words, there must be a transformation from any instance i of the ﬁrst problem to an instance i0 of the second problem.
it is important to note that the reduction process does not give us an algorithm for solving either problem by itself. it merely gives us a method for solving the ﬁrst problem given that we already have a solution to the second. more importantly for the topics to be discussed in the remainder of this chapter, reduction gives us a way to understand the bounds of one problem in terms of another. speciﬁcally, given efﬁcient transformations, the upper bound of the ﬁrst problem is at most the upper bound of the second. conversely, the lower bound of the second problem is at least the lower bound of the ﬁrst.
as a second example of reduction, consider the simple problem of multiplying two n-digit numbers. the standard long-hand method for multiplication is to multiply the last digit of the ﬁrst number by the second number (taking Θ(n) time), multiply the second digit of the ﬁrst number by the second number (again taking Θ(n) time), and so on for each of the n digits of the ﬁrst number. finally, the intermediate results are added together. note that adding two numbers of length m and n can easily be done in Θ(m +n) time. because each digit of the ﬁrst number is multiplied against each digit of the second, this algorithm requires Θ(n2) time. asymptotically faster (but more complicated) algorithms are known, but none is so fast as to be in o(n).
next we ask the question: is squaring an n-digit number as difﬁcult as multiplying two n-digit numbers? we might hope that something about this special case
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? if there are 23 students, then the odds are about even that two will share a birthday. this is despite the fact that there are 365 days in which students can have birthdays (ignoring leap years), on most of which no student in the class has a birthday. with more students, the probability of a shared birthday increases. the mapping of students to days based on their birthday is similar to assigning records to slots in a table (of size 365) using the birthday as a hash function. note that this observation tells us nothing about which students share a birthday, or on which days of the year shared birthdays fall.
to be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space. typically, this means that the hash table will be around half full. because collisions are extremely likely to occur under these conditions (by chance, any record inserted into a table that is half full will have a collision half of the time), ddoes this mean that we need not worry about the ability of a hash function to avoid collisions? absolutely not. the difference between a good hash function and a bad hash function makes a big difference in practice. technically, any function that maps all possible key values to a slot in the hash table is a hash function. in the extreme case, even a function that maps all records to the same slot is a hash function, but it does nothing to help us ﬁnd records during a search operation.
we would like to pick a hash function that stores the actual records in the collection such that each slot in the hash table has equal probablility of being ﬁlled. unfortunately, we normally have no control over the key values of the actual records, so how well any particular hash function does this depends on the distribution of the keys within the allowable key range. in some cases, incoming data are well distributed across their key range. for example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table. however, in many applications the incoming records are highly clustered or otherwise poorly distributed. when input records are not well distributed throughout the key range it can be difﬁcult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance.
performance is required. an example is searching for data on a read-only cd. here the database will never change, the time for each access is expensive, and the database designer can build the hash table before issuing the cd.
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
• write(byte[] b): write some bytes at the current position in the ﬁle (overwriting the bytes already at that position). the current position moves forward as the bytes are written.
we now consider the problem of sorting collections of records too large to ﬁt in main memory. because the records must reside in peripheral or external memory, such sorting methods are called external sorts. this is in contrast to the internal sorts discussed in chapter 7 which assume that the records to be sorted are stored in main memory. sorting large collections of records is central to many applications, such as processing payrolls and other large business databases. as a consequence, many external sorting algorithms have been devised. years ago, sorting algorithm designers sought to optimize the use of speciﬁc hardware conﬁgurations, such as multiple tape or disk drives. most computing today is done on personal computers and low-end workstations with relatively powerful cpus, but only one or at most two disk drives. the techniques presented here are geared toward optimized processing on a single disk drive. this approach allows us to cover the most important issues in external sorting while skipping many less important machine-dependent details. readers who have a need to implement efﬁcient external sorting algorithms that take advantage of more sophisticated hardware conﬁgurations should consult the references in section 8.6.
when a collection of records is too large to ﬁt in main memory, the only practical way to sort it is to read some records from disk, do some rearranging, then write them back to disk. this process is repeated until the ﬁle is sorted, with each record read perhaps many times. given the high cost of disk i/o, it should come as no surprise that the primary goal of an external sorting algorithm is to minimize the amount of information that must be read from or written to disk. a certain amount of additional cpu processing can proﬁtably be traded for reduced disk access.
before discussing external sorting techniques, consider again the basic model for accessing information from disk. the ﬁle to be sorted is viewed by the programmer as a sequential series of ﬁxed-size blocks. assume (for simplicity) that each
quentially and write the output run ﬁles sequentially. for sequential processing and double buffering to be effective, however, it is necessary that there be a separate i/o head available for each ﬁle. this typically means that each of the input and output ﬁles must be on separate disk drives, requiring a total of four disk drives for maximum efﬁciency.
the external mergesort algorithm just described requires that log n passes be made to sort a ﬁle of n records. thus, each record must be read from disk and written to disk log n times. the number of passes can be signiﬁcantly reduced by observing that it is not necessary to use mergesort on small runs. a simple modiﬁcation is to read in a block of data, sort it in memory (perhaps using quicksort), and then output it as a single sorted run.
example 8.7 assume that we have blocks of size 4kb, and records are eight bytes with four bytes of data and a 4-byte key. thus, each block contains 512 records. standard mergesort would require nine passes to generate runs of 512 records, whereas processing each block as a unit can be done in one pass with an internal sort. these runs can then be merged by mergesort. standard mergesort requires eighteen passes to process 256k records. using an internal sort to create initial runs of 512 records reduces this to one initial pass to create the runs and nine merge passes to put them all together, approximately half as many passes.
we can extend this concept to improve performance even further. available main memory is usually much more than one block in size. if we process larger initial runs, then the number of passes required by mergesort is further reduced. for example, most modern computers can provide tens or even hundreds of megabytes of ram to the sorting program. if all of this memory (excepting a small amount for buffers and local variables) is devoted to building initial runs as large as possible, then quite large ﬁles can be processed in few passes. the next section presents a technique for producing large runs, typically twice as large as could ﬁt directly into main memory.
another way to reduce the number of passes required is to increase the number of runs that are merged together during each pass. while the standard mergesort algorithm merges two runs at a time, there is no reason why merging needs to be limited in this way. section 8.5.3 discusses the technique of multiway merging.
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
(b) if we change the dividing point computation in function binary from i = (l + r)/2 to i = r − 2, what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
3.21 design an algorithm to assemble a jigsaw puzzle. assume that each piece has four sides, and that each piece’s ﬁnal orientation is known (top, bottom, etc.). assume that you have available a function bool compare(piece a, piece b, side ad) that can tell, in constant time, whether piece a connects to piece b on a’s side ad and b’s opposite side bd. the input to your algorithm should consist of an n × m array of random pieces, along with dimensions n and m. the algorithm should put the pieces in their correct positions in the array. your algorithm should be as efﬁcient as possible in the asymptotic sense. write a summation for the running time of your algorithm on n pieces, and then derive a closed-form solution for the summation.
3.1 imagine that you are trying to store 32 boolean values, and must access them frequently. compare the time required to access boolean values stored alternatively as a single bit ﬁeld, a character, a short integer, or a long integer. there are two things to be careful of when writing your program. first, be sure that your program does enough variable accesses to make meaningful measurements. a single access is much smaller than the measurement rate for all four methods. second, be sure that your program spends as much time as possible doing variable accesses rather than other things such as calling timing functions or incrementing for loop counters.
3.2 implement sequential search and binary search algorithms on your computer. run timings for each algorithm on arrays of size n = 10i for i ranging from 1 to as large a value as your computer’s memory and compiler will allow. for both algorithms, store the values 0 through n − 1 in order in the array, and
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
this implementation contains calls to functions previsit and postvisit. these functions specify what activity should take place during the search. just as a preorder tree traversal requires action before the subtrees are visited, some graph traversals require that a vertex be processed before ones further along in the dfs. alternatively, some applications require activity after the remaining vertices are processed; hence the call to function postvisit. this would be a natural opportunity to make use of the visitor design pattern described in section 1.3.2.
dfs processes each edge once in a directed graph. in an undirected graph, dfs processes each edge from both directions. each vertex must be visited, but only once, so the total cost is Θ(|v| + |e|).
our second graph traversal algorithm is known as a breadth-ﬁrst search (bfs). bfs examines all vertices connected to the start vertex before visiting vertices further away. bfs is implemented similarly to dfs, except that a queue replaces the recursion stack. note that if the graph is a tree and the start vertex is at the root, bfs is equivalent to visiting vertices level by level from top to bottom. figure 11.10 provides an implementation for the bfs algorithm. figure 11.11 shows a graph and the corresponding breadth-ﬁrst search tree. figure 11.12 illustrates the bfs process for the graph of figure 11.11(a).
dequeue c. process (c, a). ignore. process (c, b). mark and enqueue b. print (c, b). process (c, d). mark and enqueue d. print (c, d). process (c, f). mark and enqueue f. print (c, f).
figure 11.12 a detailed illustration of the bfs process for the graph of figure 11.11(a) starting at vertex a. the steps leading to each change in the queue are described.
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
key value for each block in that cylinder, called the cylinder index. when new records are inserted, they are placed in the correct cylinder’s overﬂow area (in effect, a cylinder acts as a bucket). if a cylinder’s overﬂow area ﬁlls completely, then a system-wide overﬂow area is used. search proceeds by determining the proper cylinder from the system-wide table kept in main memory. the cylinder’s block table is brought in from disk and consulted to determine the correct block. if the record is found in that block, then the search is complete. otherwise, the cylinder’s overﬂow area is searched. if that is full, and the record is not found, then the system-wide overﬂow is searched.
after initial construction of the database, so long as no new records are inserted or deleted, access is efﬁcient because it rquires only two disk fetches. the ﬁrst disk fetch recovers the block table for the desired cylinder. the second disk fetch recovers the block that, under good conditions, contains the record. after many inserts, the overﬂow list becomes too long, resulting in signiﬁcant search time as the cylinder overﬂow area ﬁlls up. under extreme conditions, many searches might eventually lead to the system overﬂow area. the “solution” to this problem is to periodically reorganize the entire database. this means rebalancing the records among the cylinders, sorting the records within each cylinder, and updating both the system index table and the within-cylinder block table. such reorganization was typical of database systems during the 1960s and would normally be done each night or weekly.
linear indexing is efﬁcient when the database is static, that is, when records are inserted and deleted rarely or never. isam is adequate for a limited number of updates, but not for frequent changes. because it has essentially two levels of indexing, isam will also break down for a truly large database where the number of cylinders is too great for the top-level index to ﬁt in main memory.
1. large sets of records are frequently updated. 2. search is by one or a combination of several keys. 3. key range queries or min/max queries are used. for such databases, a better organization must be found. one approach would be to use the binary search tree (bst) to store primary and secondary key indices. bsts can store duplicate key values, they provide efﬁcient insertion and deletion as well as efﬁcient search, and they can perform efﬁcient range queries. when there
are greater than or equal to the value of the ﬁrst key. if there is a right subtree (equivalently, if the node stores two keys), then the values of all descendants in the center subtree are less than the value of the second key, while values in the right subtree are greater than or equal to the value of the second key. to maintain these shape and search properties requires that special action be taken when nodes are inserted and deleted. the 2-3 tree has the advantage over the bst in that the 2-3 tree can be kept height balanced at relatively low cost.
figure 10.9 illustrates the 2-3 tree. nodes are indicated as rectangular boxes with two key ﬁelds. (these nodes actually would contain complete records or pointers to complete records, but the ﬁgures will show oly the keys.) internal nodes with only two children have an empty right key ﬁeld. leaf nodes might contain either one or two keys. figure 10.10 is a class declaration for the 2-3 tree node.
note that this sample declaration does not distinguish between leaf and internal nodes and so is space inefﬁcient, because leaf nodes store three pointers each. the techniques of section 5.3.1 can be applied here to implement separate internal and leaf node types.
from the deﬁning rules for 2-3 trees we can derive relationships between the number of nodes in the tree and the depth of the tree. a 2-3 tree of height k has at least 2k−1 leaves, because if every internal node has two children it degenerates to the shape of a complete binary tree. a 2-3 tree of height k has at most 3k−1 leaves, because each internal node can have at most three children.
searching for a value in a 2-3 tree is similar to searching in a bst. search begins at the root. if the root does not contain the search key k, then the search progresses to the only subtree that can possibly contain k. the value(s) stored in the root node determine which is the correct subtree. for example, if searching for the value 30 in the tree of figure 10.9, we begin with the root node. because 30 is between 18 and 33, it can only be in the middle subtree. searching the middle child of the root node yields the desired record. if searching for 15, then the ﬁrst step is again to search the root node. because 15 is less than 18, the ﬁrst (left) branch is taken. at the next level, we take the second branch to the leaf node containing 15.
figure 10.11 simple insert into the 2-3 tree of figure 10.9. the value 14 is inserted into the tree at the leaf node containing 15. because there is room in the node for a second key, it is simply added to the left position with 15 moved to the right position.
insertion into a 2-3 tree is similar to insertion into a bst to the extent that the new record is placed in the appropriate leaf node. unlike bst insertion, a new child is not created to hold the record being inserted, that is, the 2-3 tree does not grow downward. the ﬁrst step is to ﬁnd the leaf node that would contain the record if it were in the tree. if this leaf node contains only one value, then the new record can be added to that node with no further modiﬁcation to the tree, as illustrated in figure 10.11. in this example, a record with key value 14 is inserted. searching from the root, we come to the leaf node that stores 15. we add 14 as the left value (pushing the record with key 15 to the rightmost position).
if we insert the new record into a leaf node l that already contains two records, then more space must be created. consider the two records of node l and the record to be inserted without further concern for which two were already in l and which is the new record. the ﬁrst step is to split l into two nodes. thus, a new node — call it l0 — must be created from free store. l receives the record with the least of the three key values. l0 receives the greatest of the three. the record
figure 10.12 a simple node-splitting insert for a 2-3 tree. the value 55 is added to the 2-3 tree of figure 10.9. this makes the node containing values 50 and 52 split, promoting value 52 to the parent node.
with the middle of the three key value is passed up to the parent node along with a pointer to l0. this is called a promotion. the promoted key is then inserted into the parent. if the parent currently contains only one record (and thus has only two children), then the promoted record and the pointer to l0 are simply added to the parent node. if the parent is full, then the split-and-promote process is repeated. figure 10.12 illustrates a simple promotion. figure 10.13 illustrates what happens when promotions require the root to split, adding a new level to the tree. in either case, all leaf nodes continue to have equal depth. figures 10.14 and 10.15 present an implementation for the insertion process.
note that inserthelp of figure 10.14 takes three parameters. the ﬁrst is a pointer to the root of the current subtree, named rt. the second is the key for the record to be inserted, and the third is the record itself. the return value for inserthelp is a pointer to a 2-3 tree node. if rt is unchanged, then a pointer to rt is returned. if rt is changed (due to the insertion causing the node to split), then a pointer to the new subtree root is returned, with the key value and record value in the leftmost ﬁelds, and a pointer to the (single) subtree in the center pointer ﬁeld. this revised node will then be added to the paren, as illustrated in figure 10.13.
when deleting a record from the 2-3 tree, there are three cases to consider. the simplest occurs when the record is to be removed from a leaf node containing two records. in this case, the record is simply removed, and no other nodes are affected. the second case occurs when the only record in a leaf node is to be removed. the third case occurs when a record is to be removed from an internal node. in both the second and the third cases, the deleted record is replaced with another that can take its place while maintaining the correct order, similar to removing a node from a bst. if the tree is sparse enough, there is no such record available that will allow all nodes to still maintain at least one record. in this situation, sibling nodes are merged together. the delete operation for the 2-3 tree is excessively complex and will not be described further. instead, a complete discussion of deletion will be
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
example 13.1 when searching for the value 7 (0000111 in binary) in the pat trie of figure 13.3, the root node indicates that bit position 0 (the leftmost bit) is checked ﬁrst. because the 0th bit for value 7 is 0, take the left branch. at level 1, branch depending on the value of bit 1, which again is 0. at level 2, branch depending on the value of bit 2, which again is 0. at level 3, the index stored in the node is 4. this means that bit 4 of the key is checked next. (the value of bit 3 is irrelevant, because all values stored in that subtree have the same value at bit position 3.) thus, the single branch that extends from the equivalent node in figure 13.1 is just skipped. for key value 7, bit 4 has value 1, so the rightmost branch is taken. because this leads to a leaf node, the search key is compared against the key stored in that node. if they match, then the desired record has been found.
note that during the search process, only a single bit of the search key is compared at each internal node. this is signiﬁcant, because the search key could be quite large. search in the pat trie requires only a single full-key comparison, which takes place once a leaf node has been reached.
example 13.2 consider the situation where we need to store a library of dna sequences. a dna sequence is a series of letters, usually many thousands of characters long, with the string coming from an alphabet of only four letters that stand for the four amino acids making up a dna strand. similar dna seqences might have long sections of ther string that are identical. the pat trie woudl avoid making multiple full key comparisons when searching for a speciﬁc sequence.
we have noted several times that the bst has a high risk of becoming unbalanced, resulting in excessively expensive search and update operations. one solution to this problem is to adopt another search tree structure such as the 2-3 tree. an alternative is to modify the bst access functions in some way to guarantee that the tree performs well. this is an appealing concept, and it works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. unfortunately, requiring that the bst always be in the shape of a complete binary tree requires excessive modiﬁcation to the tree during update, as discussed in section 10.3.
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
natural extension of the bst to multiple dimensions. it is a binary tree whose splitting decisions alternate among the key dimensions. like the bst, the k-d tree uses object space decomposition. the pr quadtree uses key space decomposition and so is a form of trie. it is a binary tree only for one-dimensional keys (in which case it is a trie with a binary alphabet). for d dimensions it has 2d branches. thus, in two dimensions, the pr quadtree has four branches (hence the name “quadtree”), splitting space into four equal-sized quadrants at each branch. section 13.3.3 brieﬂy mentions two other variations on these data structures, the bintree and the point quadtree. these four structures cover all four combinations of object versus key space decomposition on the one hand, and multi-level binary versus 2d-way branching on the other. section 13.3.4 brieﬂy discusses spatial data structures for storing other types of spatial data.
the k-d tree is a modiﬁcation to the bst that allows for efﬁcient processing of multidimensional keys. the k-d tree differs from the bst in that each level of the k-d tree makes branching decisions based on a particular search key associated with that level, called the discriminator. we deﬁne the discriminator at level i to be i mod k for k dimensions. for example, assume that we store data organized by xy-coordinates. in this case, k is 2 (there are two coordinates), with the xcoordinate ﬁeld arbitrarily designated key 0, and the y-coordinate ﬁeld designated key 1. at each level, the discriminator alternates between x and y. thus, a node n at level 0 (the root) would have in its left subtree only nodes whose x values are less than nx (because x is search key 0, and 0 mod 2 = 0). the right subtree would contain nodes whose x values are greater than nx. a node m at level 1 would have in its left subtree only nodes whose y values are less than my. there is no restriction on the relative values of mx and the x values of m’s descendants, because branching decisions made at m are based solely on the y coordinate. figure 13.11 shows an example of how a collection of two-dimensional points would be stored in a k-d tree. in figure 13.11 the region containing the points is (arbitrarily) restricted to a 128 × 128 square, and each internal node splits the search space. each split is shown by a line, vertical for nodes with x discriminators and horizontal for nodes with y discriminators. the root node splits the space into two parts; its children further subdivide the space into smaller parts. the children’s split lines do not cross the root’s split line. thus, each node in the k-d tree helps to decompose the space into rectangles that show the extent of where nodes can fall in the various subtrees.
mation for a node save considerable space, but avoiding storing such information in the nodes we enables a good design choice for empty leaf nodes, as discussed next.
how should we represent empty leaf nodes? on average, half of the leaf nodes in a pr quadtree are empty (i.e., do not store a data point). one implementation option is to use a null pointer in internal nodes to represent empty nodes. this will solve the problem of excessive space requirements. there is an unfortunate side effect that using a null pointer requires the pr quadtree processing methods to understand this convention. in other words, you are breaking encapsulation on the node representation because the tree now must know things about how the nodes are implemented. this is not too horrible for this particular application, because the node class can be considered private to the tree class, in which case the node implementation is completely invisible to the outside world. however, it is undesirable if there is another reasonable alternative.
fortunately, there is a good alternative. it is called the flyweight design pattern. in the pr quadtree, a ﬂyweight is a single empty leaf node that is reused in all places where an empty leaf node is needed. you simply have all of the internal nodes with empty leaf children point to the same node object. this node object is created once at the beginning of the program, and is never removed. the node class recognizes from the pointer value that the ﬂyweight is being accessed, and acts accordingly.
note that when using the flyweight design pattern, you cannot store coordinates for the node in the node. this is an example of the concept of intrinsic versus extrinsic state. intrinsic state for an object is state information stored in the object. if you stored the coordinates for a node in the node object, those coordinates would be intrinsic state. extrinsic state is state information about an object stored elsewhere in the environment, such as in global variables or passed to the method. if your recursive calls that process the tree pass in the coordinates for the current node, then the coordinates will be extrinsic state. a ﬂyweight can have in its intrinsic state only information that is accurate for all instances of the ﬂyweight. clearly coordinates do not qualify, because each empty leaf node has its own location. so, if you want to use a ﬂyweight, you must pass in coordinates.
another design choice is: who controls the work, the node class or the tree class? for example, on an insert operation, you could have the tree class control the ﬂow down the tree, looking at (querying) the nodes to see their type and reacting accordingly. this is the approach used by the bst implementation in section 5.4. an alternate approach is to have the node class do the work. that is, you have an insert method for the nodes. if the node is internal, it passes the city record to the appropriate child (recursively). if the node is a ﬂyweight, it replaces itself with a
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
set to be update[i]->forward[i], and the nodes stored in update[i] for indices 0 through 2 have their forward[i] pointers changed to point to the new node. this “splices” the new node into the skip list at all levels.
the remove function is left as an exercise. it is similar to inserting in that the update array is built as part of searching for the record to be deleted; then those nodes speciﬁed by the update array have their forward pointers adjusted to point around the node being deleted.
a newly inserted node could have a high level generated by randomlevel, or a low level. it is possible that many nodes in the skip list could have many pointers, leading to unnecessary insert cost and yielding poor (i.e., Θ(n)) performance during search, because not many nodes will be skipped. conversely, too many nodes could have a low level. in the worst case, all nodes could be at level 0, equivalent to a regular linked list. if so, search will again require Θ(n) time. however, the probability that performance will be poor is quite low. there is only once chance in 1024 that ten nodes in a row will be at level 0. the motto of probabilistic data structures such as the skip list is “don’t worry, be happy.” we simply accept the results of randomlevel and expect that probability will eventually work in our favor. the advantage of this approach is that the algorithms are simple, while requiring only Θ(log n) time for all operations in the average case.
in practice, the skip list will probably have better performance than a bst. the bst can have bad performance caused by the order in which data are inserted. for example, if n nodes are inserted into a bst in ascending order of their key value, then the bst will look like a linked list with the deepest node at depth n − 1. the skip list’s performance does not depend on the order in which values are inserted into the list. as the number of nodes in the skip list increases, the probability of encountering the worst case decreases geometrically. thus, the skip list illustrates a tension between the theoretical worst case (in this case, Θ(n) for a skip list operation), and a rapidly increasing probability of average-case performance of Θ(log n), that characterizes probabilistic data structures.
• raise a number to a power. • find common factors for two numbers. • tell whether a number is prime. • generate a random integer. • multiply two integers.
the node now containing g. precisely this situation occurs if we replace value 120 with the greatest value in the left subtree of figure 5.13(b). selecting the least value from the right subtree does not have a similar problem, because it does not violate the binary search tree property if equal values appear in the right subtree.
from the above, we see that if we want to remove the record stored in a node with two children, then we simply call deletemin on the node’s right subtree and substitute the record returned for the record being removed. figure 5.18 shows is the code for removehelp.
the cost for findhelp and inserthelp is the depth of the node found or inserted. the cost for removehelp is the depth of the node being removed, or in the case when this node has two children, the depth of the node with smallest value in its right subtree. thus, in the worst case, the cost of any one of these operations is the depth of the deepest node in the tree. this is why it is desirable to keep bsts balanced, that is, with least possible height. if a binary tree is balanced, then the height for a tree of n nodes is approximately log n. however, if the tree is completely unbalanced, for example in the shape of a linked list, then the height for a tree of n nodes can be as great as n. thus, a balanced bst will in the average case have operations costing Θ(log n), while a badly unbalanced bst can have operations in the worst case costing Θ(n). consider the situation where we
once the desired record is found, it is passed up the chain of recursive calls to findhelp in the third parameter, “it.” the return value for the function (true or false, depending on whether a suitable element has been found) is also simply passed back up the chain of recursive calls.
inserting a record with key value k requires that we ﬁrst ﬁnd where that record would have been if it were in the tree. this takes us to either a leaf node, or to an internal node with no child in the appropriate direction.3 call this node r 0. we then add a new node containing the new record as a child of r 0. figure 5.15 illustrates this operation. the value 35 is added as the right child of the node with value 32. here is the implementation for inserthelp: private bstnode<k,e> inserthelp(bstnode<k,e> rt, k k, e e) {
3this assumes that no node has a key value equal to the one being inserted. if we ﬁnd a node that duplicates the key value to be inserted, we have two options. if the application does not allow nodes with equal keys, then this insertion should be treated as an error (or ignored). if duplicate keys are allowed, our convention will be to insert the duplicate in the right subtree.
figure 5.15 an example of bst insertion. a record with value 35 is inserted into the bst of figure 5.13(a). the node with value 32 becomes the parent of the new node containing 35.
you should pay careful attention to the implementation for inserthelp. note that inserthelp returns a pointer to a bstnode. what is being returned is a subtree identical to the old subtree, except that it has been modiﬁed to contain the new record being inserted. each node along a path from the root to the parent of the new node added to the tree will have its appropriate child pointer assigned to it. except for the last node in the path, none of these nodes will actually change their child’s pointer value. in that sense, many of the assignments seem redundant. however, the cost of these additional assignments is worth paying to keep the insertion process simple. the alternative is to check if a given assignment is necessary, which is probably more expensive than the assignment!
the shape of a bst depends on the order in which elements are inserted. a new element is added to the bst as a new leaf node, potentially increasing the depth of the tree. figure 5.13 illustrates two bsts for a collection of values. it is possible for the bst containing n nodes to be a chain of nodes with height n. this would happen if, for example, all elements were inserted in sorted order. in general, it is preferable for a bst to be as shallow as possible. this keeps the average cost of a bst operation low.
removing a node from a bst is a bit trickier than inserting a node, but it is not complicated if all of the possible cases are considered individually. before tackling the general node removal process, let us ﬁrst discuss how to remove from a given subtree the node with the smallest key value. this routine will be used later by the general node removal function. to remove the node with the minimum key value from a subtree, ﬁrst ﬁnd that node by continuously moving down the left link until there is no further left link to follow. call this node s. to remove s, simply have the parent of s change its pointer to point to the right child of s. we know that s has no left child (because if s did have a left child, s would not be the node with
figure 5.15 an example of bst insertion. a record with value 35 is inserted into the bst of figure 5.13(a). the node with value 32 becomes the parent of the new node containing 35.
you should pay careful attention to the implementation for inserthelp. note that inserthelp returns a pointer to a bstnode. what is being returned is a subtree identical to the old subtree, except that it has been modiﬁed to contain the new record being inserted. each node along a path from the root to the parent of the new node added to the tree will have its appropriate child pointer assigned to it. except for the last node in the path, none of these nodes will actually change their child’s pointer value. in that sense, many of the assignments seem redundant. however, the cost of these additional assignments is worth paying to keep the insertion process simple. the alternative is to check if a given assignment is necessary, which is probably more expensive than the assignment!
the shape of a bst depends on the order in which elements are inserted. a new element is added to the bst as a new leaf node, potentially increasing the depth of the tree. figure 5.13 illustrates two bsts for a collection of values. it is possible for the bst containing n nodes to be a chain of nodes with height n. this would happen if, for example, all elements were inserted in sorted order. in general, it is preferable for a bst to be as shallow as possible. this keeps the average cost of a bst operation low.
removing a node from a bst is a bit trickier than inserting a node, but it is not complicated if all of the possible cases are considered individually. before tackling the general node removal process, let us ﬁrst discuss how to remove from a given subtree the node with the smallest key value. this routine will be used later by the general node removal function. to remove the node with the minimum key value from a subtree, ﬁrst ﬁnd that node by continuously moving down the left link until there is no further left link to follow. call this node s. to remove s, simply have the parent of s change its pointer to point to the right child of s. we know that s has no left child (because if s did have a left child, s would not be the node with
the node now containing g. precisely this situation occurs if we replace value 120 with the greatest value in the left subtree of figure 5.13(b). selecting the least value from the right subtree does not have a similar problem, because it does not violate the binary search tree property if equal values appear in the right subtree.
from the above, we see that if we want to remove the record stored in a node with two children, then we simply call deletemin on the node’s right subtree and substitute the record returned for the record being removed. figure 5.18 shows is the code for removehelp.
the cost for findhelp and inserthelp is the depth of the node found or inserted. the cost for removehelp is the depth of the node being removed, or in the case when this node has two children, the depth of the node with smallest value in its right subtree. thus, in the worst case, the cost of any one of these operations is the depth of the deepest node in the tree. this is why it is desirable to keep bsts balanced, that is, with least possible height. if a binary tree is balanced, then the height for a tree of n nodes is approximately log n. however, if the tree is completely unbalanced, for example in the shape of a linked list, then the height for a tree of n nodes can be as great as n. thus, a balanced bst will in the average case have operations costing Θ(log n), while a badly unbalanced bst can have operations in the worst case costing Θ(n). consider the situation where we
figure 5.13 two binary search trees for a collection of values. tree (a) results if values are inserted in the order 37, 24, 42, 7, 2, 40, 42, 32, 120. tree (b) results if the same values are inserted in the order 120, 42, 42, 7, 2, 32, 37, 24, 40.
figure 5.14 shows a class declaration for the bst that implements the dictionary adt. the public member functions include those required by the dictionary adt, along with a constructor and destructor.
to ﬁnd a record with key value k in a bst, begin at the root. if the root stores a record with key value k, then the search is over. if not, then we must search deeper in the tree. what makes the bst efﬁcient during search is that we need search only one of the node’s two subtrees. if k is less than the root node’s key value, we search only the left subtree. if k is greater than the root node’s key value, we search only the right subtree. this process continues until a record with key value k is found, or we reach a leaf node. if we reach a leaf node without encountering k, then no record exists in the bst whose key value is k.
example 5.5 consider searching for the node with key value 32 in the tree of figure 5.13(a). because 32 is less than the root value of 37, the search proceeds to the left subtree. because 32 is greater than 24, we search in 24’s right subtree. at this point the node containing 32 is found. if the search value were 35, the same path would be followed to the node containing 32. because this node has no children, we know that 35 is not in the bst.
notice that in figure 5.14, public member function find calls a private member function named findhelp. method find takes the search key as an explicit parameter and its bst as an implicit parameter, along with space to place a copy of
the record if it is found. however, the ﬁnd operation is most easily implemented as a recursive function whose parameters are the root of a bst subtree, the search key, and space for the element once it is found. member findhelp is of the desired form for this recursive subroutine and is implemented as follows:
figure 5.12 a complete binary tree and its array implementation. (a) the complete binary tree with twelve nodes. each node has been labeled with its position in the tree. (b) the positions for the relatives of each node. a dash indicates that the relative does not exist.
array-based list to implement the dictionary, then binary search can be used to ﬁnd a record in only Θ(log n) time. however, insertion will now require Θ(n) time on average because, once the proper location for the new record in the sorted list has been found, many records might be shifted to make room for the new record.
is there some way to organize a collection of records so that inserting records and searching for records can both be done quickly? this section presents the binary search tree (bst), which allows an improved solution to this problem.
a bst is a binary tree that conforms to the following condition, known as the binary search tree property: all nodes stored in the left subtree of a node whose key value is k have key values less than k. all nodes stored in the right subtree of a node whose key value is k have key values greater than or equal to k. figure 5.13 shows two bsts for a collection of values. one consequence of the binary search tree property is that if the bst nodes are printed using an inorder traversal (see section 5.2), the resulting enumeration will be in sorted order from lowest to highest.
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
are many approaches to hashing and it is easy to devise an inefﬁcient implementation. hashing is suitable for both in-memory and disk-based searching and is one of the two most widely used methods for organizing large databases stored on disk (the other is the b-tree, which is covered in chapter 10). as a simple (though unrealistic) example of hashing, consider storing n records, each with a unique key value in the range 0 to n − 1. in this simple case, a record with key k can be stored in ht[k], and the hash function is simply h(k) = k. to ﬁnd the record with key value k, simply look in ht[k].
typically, there are many more values in the key range than there are slots in the hash table. for a more realistic example, suppose that the key can take any value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that we expect to store approximately 1000 records at any given time. it is impractical in this situation to use a hash table with 65,536 slots, because most of the slots will be left empty. instead, we must devise a hash function that allows us to store the records in a much smaller table. because the possible key range is larger than the size of the table, at least some of the slots must be mapped to from multiple key values. given a hash function h and two keys k1 and k2, if h(k1) = β = h(k2) where β is a slot in the table, then we say that k1 and k2 have a collision at slot β under hash function h.
hashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. collisions occur when two records hash to the same slot in the table. if we are careful—or lucky—when selecting a hash function, then the actual number of collisions will be few. unfortunately, even under the best of circumstances, collisions are nearly unavoidable.1 for example, consider a classroom full of students. what is the
1the exception to this is perfect hashing. perfect hashing is a system in which records are hashed such that there are no collisions. a hash function is selected for the speciﬁc set of records being hashed, which requires that the entire collection of records be available before selecting the hash function. perfect hashing is efﬁcient because it always ﬁnds the record that we are looking for exactly where the hash function computes it to be, so only one access is required. selecting a perfect hash function can be expensive but might be worthwhile when extremely efﬁcient search
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
in the index. inverted lists reduce this problem, but they are only suitable for secondary key indices with many fewer secondary key values than records. the linear index would perform well as a primary key index if it could somehow be broken into pieces such that individual updates affect only a part of the index. this concept will be pursued throughout the rest of this chapter, eventually culminating in the b+-tree, the most widely used indexing method today. but ﬁrst, we begin by studying isam, an early attempt to solve the problem of large databases requiring frequent update. its weaknesses help to illustrate why the b+-tree works so well.
before the invention of effective tree indexing schemes, a variety of disk-based indexing methods were in use. all were rather cumbersome, largely because no adequate method for handling updates was known. typically, updates would cause the index to degrade in performance. isam is one example of such an index and was widely used by ibm prior to adoption of the b-tree.
isam is based on a modiﬁed form of the linear index, as illustrated by figure 10.6. records are stored in sorted order by primary key. the disk ﬁle is divided among a number of cylinders on disk.1 each cylindar holds a section of the list in sorted order. initially, each cylinder is not ﬁlled to capacity, and the extra space is set aside in the cylinder overﬂow. in memory is a table listing the lowest key value stored in each cylinder of the ﬁle. each cylinder contains a table listing the lowest
this section presents the b-tree. b-trees are usually attributed to r. bayer and e. mccreight who described the b-tree in a 1972 paper. by 1979, b-trees had replaced virtually all large-ﬁle access methods other than hashing. b-trees, or some variant of b-trees, are the standard ﬁle organization for applications requiring insertion, deletion, and key range searches. b-trees address effectively all of the major problems encountered when implementing disk-based search trees:
1. b-trees are always height balanced, with all leaf nodes at the same level. 2. update and search operations affect only a few disk blocks. the fewer the
3. b-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk i/o on searches due to locality of reference.
for an expanded discussion of the issues touched on in this chapter, see a general ﬁle processing text such as file structures: a conceptual toolkit by folk and zoellick [fz98]. in particular, folk and zoellick provide a good discussion of the relationship between primary and secondary indices. the most thorough discussion on various implementations for the b-tree is the survey article by comer [com79]. also see [sal88] for further details on implementing b-trees. see shaffer and brown [sb93] for a discussion of buffer pool management strategies for b+-tree-like data structures.
10.1 assume that a computer system has disk blocks of 1024 bytes, and that you are storing records that have 4-byte keys and 4-byte data ﬁelds. the records are sorted and packed sequentially into the disk ﬁle. (a) assume that a linear index uses 4 bytes to store the key and 4 bytes to store the block id for the associated records. what is the greatest number of records that can be stored in the ﬁle if a linear index of size 256kb is used?
(b) what is the greatest number of records that can be stored in the ﬁle if the linear index is also stored on disk (and thus its size is limited only by the second-level index) when using a second-level index of 1024 bytes (i.e., 256 key values) as illustrated by figure 10.2? each element of the second-level index references the smallest key value for a disk block of the linear index.
10.2 assume that a computer system has disk blocks of 4096 bytes, and that you are storing records that have 4-byte keys and 64-byte data ﬁelds. the records are sorted and packed sequentially into the disk ﬁle. (a) assume that a linear index uses 4 bytes to store the key and 4 bytes to store the block id for the associated records. what is the greatest number of records that can be stored in the ﬁle if a linear index of size 2mb is used?
(b) what is the greatest number of records that can be stored in the ﬁle if the linear index is also stored on disk (and thus its size is limited only by the second-level index) when using a second-level index of 4096 bytes (i.e., 1024 key values) as illustrated by figure 10.2? each element of the second-level index references the smallest key value for a disk block of the linear index.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
the asymptotic cost of search, insertion, and deletion of records from b-trees, b+-trees, and b∗-trees is Θ(log n) where n is the total number of records in the tree. however, the base of the log is the (average) branching factor of the tree. typical database applications use extremely high branching factors, perhaps 100 or more. thus, in practice the b-tree and its variants are extremely shallow.
as an illustration, consider a b+-tree of order 100 and leaf nodes that contain up to 100 records. a one-level b+-tree can have at most 100 records. a two-level b+-tree must have at least 100 records (2 leaves with 50 records each). it has at most 10,000 records (100 leaves with 100 records each). a three-level b+-tree must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes with 100 full children each). a four-level b+-tree must have at least 250,000 records and at most 100 million records. thus, it would require an extremely large database to generate a b+-tree of more than four levels.
we can reduce the number of disk fetches required for the b-tree even more by using the following methods. first, the upper levels of the tree can be stored in main memory at all times. because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. if the b-tree is only four levels deep, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record.
as mentioned earlier, a buffer pool should be used to manage nodes of the b-tree. several nodes of the tree would typically be in main memory at one time. the most straightforward approach is to use a standard method such as lru to do node replacement. however, sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool. in general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently.
2this concept can be extended further if higher space utilization is required. however, the update routines become much more complicated. i once worked on a project where we implemented 3-for-4 node split and merge routines. this gave better performance than the 2-for-3 node split and merge routines of the b∗-tree. however, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed!
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
figure 10.1 linear indexing for variable-length records. each record in the index ﬁle is of ﬁxed length and contains a pointer to the beginning of the corresponding record in the database ﬁle.
begins with a discussion of the variant normally referred to simply as a “b-tree.” section 10.5.1 presents the most widely implemented variant, the b+-tree.
a linear index is an index ﬁle organized as a sequence of key/pointer pairs where the keys are in sorted order and the pointers either (1) point to the position of the complete record on disk, (2) point to the position of the primary key in the primary index, or (3) are actually the value of the primary key. depending on its size, a linear index might be stored in main memory or on disk. a linear index provides a number of advantages. it provides convenient access to variable-length database records, because each entry in the index ﬁle contains a ﬁxed-length key ﬁeld and a ﬁxed-length pointer to the beginning of a (variable-length) record as shown in figure 10.1. a linear index also allows for efﬁcient search and random access to database records, becase it is amenable to binary search.
if the database contains enough records, the linear index might be too large to store in main memory. this makes binary search of the index more expensive because many disk accesses would typically be required by the search process. one solution to this problem is to store a second-level linear index in main memory that indicates which disk block in the index ﬁle stores a desired key. for example, the linear index on disk might reside in a series of 1024-byte blocks. if each key/pointer pair in the linear index requires 8 bytes, then 128 keys are stored per block. the second-level index, stored in main memory, consists of a simple table storing the value of the key in the ﬁrst position of each block in the linear index ﬁle. this arrangement is shown in figure 10.2. if the linear index requires 1024 disk blocks (1mb), the second-level index contains only 1024 entries, one per disk block. to ﬁnd which disk block contains a desired search key value, ﬁrst search through the
in the index. inverted lists reduce this problem, but they are only suitable for secondary key indices with many fewer secondary key values than records. the linear index would perform well as a primary key index if it could somehow be broken into pieces such that individual updates affect only a part of the index. this concept will be pursued throughout the rest of this chapter, eventually culminating in the b+-tree, the most widely used indexing method today. but ﬁrst, we begin by studying isam, an early attempt to solve the problem of large databases requiring frequent update. its weaknesses help to illustrate why the b+-tree works so well.
before the invention of effective tree indexing schemes, a variety of disk-based indexing methods were in use. all were rather cumbersome, largely because no adequate method for handling updates was known. typically, updates would cause the index to degrade in performance. isam is one example of such an index and was widely used by ibm prior to adoption of the b-tree.
isam is based on a modiﬁed form of the linear index, as illustrated by figure 10.6. records are stored in sorted order by primary key. the disk ﬁle is divided among a number of cylinders on disk.1 each cylindar holds a section of the list in sorted order. initially, each cylinder is not ﬁlled to capacity, and the extra space is set aside in the cylinder overﬂow. in memory is a table listing the lowest key value stored in each cylinder of the ﬁle. each cylinder contains a table listing the lowest
b-tree insertion is a generalization of 2-3 tree insertion. the ﬁrst step is to ﬁnd the leaf node that should contain the key to be inserted, space permitting. if there is room in this node, then insert the key. if there is not, then split the node into two and promote the middle key to the parent. if the parent becomes full, then it is split in turn, and its middle key promoted.
note that this insertion process is guaranteed to keep all nodes at least half full. for example, when we attempt to insert into a full internal node of a b-tree of order four, there will now be ﬁve children that must be dealt with. the node is split into two nodes containing two keys each, thus retaining the b-tree property. the middle of the ﬁve children is promoted to its parent.
the previous section mentioned that b-trees are universally used to implement large-scale disk-based systems. actually, the b-tree as described in the previous section is almost never implemented, nor is the 2-3 tree as described in section 10.4. what is most commonly implemented is a variant of the b-tree, called the b+-tree. when greater efﬁciency is required, a more complicated variant known as the b∗-tree is used.
the most signiﬁcant difference between the b+-tree and the bst or the 2-3 tree is that the b+-tree stores records only at the leaf nodes. internal nodes store key values, but these are used solely as placeholders to guide the search. this means that internal nodes are signiﬁcantly different in structure from leaf nodes. internal nodes store keys to guide the search, associating each key with a pointer to a child b+-tree node. leaf nodes store actual records, or else keys and pointers to actual records in a separate disk ﬁle if the b+-tree is being used purely as an index. depending on the size of a record as compared to the size of a key, a leaf node in a b+-tree of order m might have enough room to store more or less than m records. the requirement is simply that the leaf nodes store enough records to remain at least half full. the leaf nodes of a b+-tree are normally linked together to form a doubly linked list. thus, the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list. here is a java-like pseudocode representation for the b+-tree node interface. leaf node and internal node subclasses would implement this base class.
the asymptotic cost of search, insertion, and deletion of records from b-trees, b+-trees, and b∗-trees is Θ(log n) where n is the total number of records in the tree. however, the base of the log is the (average) branching factor of the tree. typical database applications use extremely high branching factors, perhaps 100 or more. thus, in practice the b-tree and its variants are extremely shallow.
as an illustration, consider a b+-tree of order 100 and leaf nodes that contain up to 100 records. a one-level b+-tree can have at most 100 records. a two-level b+-tree must have at least 100 records (2 leaves with 50 records each). it has at most 10,000 records (100 leaves with 100 records each). a three-level b+-tree must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes with 100 full children each). a four-level b+-tree must have at least 250,000 records and at most 100 million records. thus, it would require an extremely large database to generate a b+-tree of more than four levels.
we can reduce the number of disk fetches required for the b-tree even more by using the following methods. first, the upper levels of the tree can be stored in main memory at all times. because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. if the b-tree is only four levels deep, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record.
as mentioned earlier, a buffer pool should be used to manage nodes of the b-tree. several nodes of the tree would typically be in main memory at one time. the most straightforward approach is to use a standard method such as lru to do node replacement. however, sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool. in general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently.
2this concept can be extended further if higher space utilization is required. however, the update routines become much more complicated. i once worked on a project where we implemented 3-for-4 node split and merge routines. this gave better performance than the 2-for-3 node split and merge routines of the b∗-tree. however, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed!
figure 10.21 deletion from the b+-tree of figure 10.17 via borrowing from a sibling. the key with value 12 is deleted from the leftmost leaf, causing the record with key value 18 to shift to the leftmost leaf to take its place. note that the parent must be updated to properly indicate the key range within the subtrees. in this example, the parent node has its leftmost key value changed to 19.
determine if they have a spare record that can be used to ﬁll the gap. if so, then enough records are transferred from the sibling so that both nodes have about the same number of records. this is done so as to delay as long as possible the next time when a delete causes this node to underﬂow again. this process might require that the parent node has its placeholder key value revised to reﬂect the true ﬁrst key value in each node. figure 10.21 illustrates the process.
if neither sibling can lend a record to the underfull node (call it n), then n must give its records to a sibling and be removed from the tree. there is certainly room to do this, because the sibling is at most half full (remember that it had no records to contribute to the current node), and n has become less than half full because it is underﬂowing. this merge process combines two subtrees of the parent, which might cause it to underﬂow in turn. if the last two children of the root merge together, then the tree loses a level. figure 10.22 illustrates the node-merge deletion process. figure 10.23 shows java-like pseudocode for the b+-tree delete algorithm.
the b+-tree requires that all nodes be at least half full (except for the root). thus, the storage utilization must be at least 50%. this is satisfactory for many implementations, but note that keeping nodes fuller will result both in less space required (because there is less empty space in the disk ﬁle) and in more efﬁcient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). because b-trees have become so popular, many algorithm designers have tried to improve b-tree performance. one method for doing so is to use the b+-tree variant known as the b∗-tree. the b∗-tree is identical to the b+-tree, except for the rules used to split and merge nodes. instead of splitting a node in half when it overﬂows, the b∗-tree gives some records to its neighboring sibling, if possible. if the sibling is also full, then these two nodes split into three. similarly, when a node underﬂows, it is combined with its two siblings,
10.3 implement the dictionary adt of section 4.4 for a large ﬁle stored on disk by means of the b+-tree of section 10.5. assume that disk blocks are 1024 bytes, and thus both leaf nodes and internal nodes are also 1024 bytes. records should store a 4-byte (int) key value and a 60-byte data ﬁeld. internal nodes should store key value/pointer pairs where the “pointer” is actually the block number on disk for the child node. both internal nodes and leaf nodes will need room to store various information such as a count of the records stored on that node, and a pointer to the next node on that level. thus, leaf nodes will store 15 records, and internal nodes will have room to store about 120 to 125 children depending on how you implement them. use a buffer pool (section 8.3) to manage access to the nodes stored on disk.
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
properly used to describe our understanding of a problem or an algorithm, it is best to consider an example where you do not already know a lot about the problem.
let us look ahead to analyzing the problem of sorting to see how this process works. what is the least possible cost for any sorting algorithm in the worst case? the algorithm must at least look at every element in the input, just to determine that the input is truly sorted. it is also possible that each of the n values must be moved to another location in the sorted output. thus, any sorting algorithm must take at least cn time. for many problems, this observation that each of the n inputs must be looked at leads to an easy Ω(n) lower bound.
in your previous study of computer science, you have probably seen an example of a sorting algorithm whose running time is in o(n2) in the worst case. the simple bubble sort and insertion sort algorithms typically given as examples in a ﬁrst year programming course have worst case running times in o(n2). thus, the problem of sorting can be said to have an upper bound in o(n2). how do we close the gap between Ω(n) and o(n2)? can there be a better sorting algorithm? if you can think of no algorithm whose worst-case growth rate is better than o(n2), and if you have discovered no analysis technique to show that the least cost for the problem of sorting in the worst case is greater than Ω(n), then you cannot know for sure whether or not there is a better algorithm.
chapter 7 presents sorting algorithms whose running time is in o(n log n) for the worst case. this greatly narrows the gap. witht his new knowledge, we now have a lower bound in Ω(n) and an upper bound in o(n log n). should we search for a faster algorithm? many have tried, without success. fortunately (or perhaps unfortunately?), chapter 7 also includes a proof that any sorting algorithm must have running time in Ω(n log n) in the worst case.2 this proof is one of the most important results in the ﬁeld of algorithm analysis, and it means that no sorting algorithm can possibly run faster than cn log n for the worst-case input of size n. thus, we can conclude that the problem of sorting is Θ(n log n) in the worst case, because the upper and lower bounds have met.
knowing the lower bound for a problem does not give you a good algorithm. but it does help you to know when to stop looking. if the lower bound for the problem matches the upper bound for the algorithm (within a constant factor), then we know that we can ﬁnd an algorithm that is better only by a constant factor.
figure 7.2 an illustration of bubble sort. each column shows the array after the iteration with the indicated value of i in the outer for loop. values above the line in each column have been sorted. arrows indicate the swaps that take place during a given iteration.
7.2.2 bubble sort our next sort is called bubble sort. bubble sort is often taught to novice programmers in introductory computer science courses. this is unfortunate, because bubble sort has no redeeming features whatsoever. it is a relatively slow sort, it is no easier to understand than insertion sort, it does not correspond to any intuitive counterpart in “everyday” use, and it has a poor best-case running time. however, bubble sort serves as the basis for a better sort that will be presented in section 7.2.3.
bubble sort consists of a simple double for loop. the ﬁrst iteration of the inner for loop moves through the record array from bottom to top, comparing adjacent keys. if the lower-indexed key’s value is greater than its higher-indexed neighbor, then the two values are swapped. once the smallest value is encountered, this process will cause it to “bubble” up to the top of the array. the second pass through the array repeats this process. however, because we know that the smallest value reached the top of the array on the ﬁrst pass, there is no need to compare the top two elements on the second pass. likewise, each succeeding pass through the array compares adjacent elements, looking at one less value than the preceding pass. figure 7.2 illustrates bubble sort. a java implementation is as follows:
figure 7.5 summarizes the cost of insertion, bubble, and selection sort in terms of their required number of comparisons and swaps1 in the best, average, and worst cases. the running time for each of these sorts is Θ(n2) in the average and worst cases.
the remaining sorting algorithms presented in this chapter are signiﬁcantly better than these three under typical conditions. but before continuing on, it is instructive to investigate what makes these three sorts so slow. the crucial bottleneck is that only adjacent records are compared. thus, comparisons and moves (in all but selection sort) are by single steps. swapping adjacent records is called an exchange. thus, these sorts are sometimes referred to as exchange sorts. the cost of any exchange sort can be at best the total number of steps that the records in the array must move to reach their “correct” location (i.e., the number of inversions for each record). what is the average number of inversions? consider a list l containing n values. deﬁne lr to be l in reverse. l has n(n−1)/2 distinct pairs of values, each of which could potentially be an inversion. each such pair must either be an inversion in l or in lr. thus, the total number of inversions in l and lr together is exactly n(n− 1)/2 for an average of n(n− 1)/4 per list. we therefore know with certainty that any sorting algorithm which limits comparisons to adjacent items will cost at least n(n − 1)/4 = Ω(n2) in the average case.
1there is a slight anomaly with selection sort. the supposed advantage for selection sort is its low number of swaps required, yet selection sort’s best-case number of swaps is worse than that for insertion sort or bubble sort. this is because the implementation given for selection sort does not avoid a swap in the case where record i is already in position i. the reason is that it usually takes more time to repeatedly check for this situation than would be saved by avoiding such swaps.
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
in most applications, we have no means of knowing in advance the frequencies of access for the data records. to complicate matters further, certain records might be accessed frequently for a brief period of time, and then rarely thereafter. thus, the probability of access for records might change over time (in most database systems, this is to be expected). self-organizing lists seek to solve both of these problems.
self-organizing lists modify the order of records within the list based on the actual pattern of record access. self-organizing lists use a heuristic for deciding how to to reorder the list. these heuristics are similar to the rules for managing buffer pools (see section 8.3). in fact, a buffer pool is a form of self-organizing list. ordering the buffer pool by expected frequency of access is a good strategy, because typically we must search the contents of the buffers to determine if the desired information is already in main memory. when ordered by frequency of access, the buffer at the end of the list will be the one most appropriate for reuse when a new page of information must be read. below are three traditional heuristics for managing self-organizing lists:
1. the most obvious way to keep a list ordered by frequency would be to store a count of accesses to each record and always maintain records in this order. this method will be referred to as count. count is similar to the least frequently used buffer replacement strategy. whenever a record is accessed, it might move toward the front of the list if its number of accesses becomes greater than a record preceding it. thus, count will store the records in the order of frequency that has actually occurred so far. besides requiring space for the access counts, count does not react well to changing frequency of access over time. once a record has been accessed a large number of times under the frequency count system, it will remain near the front of the list regardless of further access history.
2. bring a record to the front of the list when it is found, pushing all the other records back one position. this is analogous to the least recently used buffer replacement strategy and is called move-to-front. this heuristic is easy to implement if the records are stored using a linked list. when records are stored in an array, bringing a record forward from near the end of the array will result in a large number of records changing position. move-to-front’s cost is bounded in the sense that it requires at most twice the number of accesses required by the optimal static ordering for n records when at least
unfortunately, the bst can become unbalanced. even under relatively good conditions, the depth of leaf nodes can easily vary by a factor of two. this might not be a signiﬁcant concern when the tree is stored in main memory because the time required is still Θ(log n) for search and update. when the tree is stored on disk, however, the depth of nodes in the tree becomes crucial. every time a bst node b is visited, it is necessary to visit all nodes along the path from the root to b. each node on this path must be retrieved from disk. each disk access returns a block of information. if a node is on the same block as its parent, then the cost to ﬁnd that node is trivial once its parent is in main memory. thus, it is desirable to keep subtrees together on the same block. unfortunately, many times a node is not on the same block as its parent. thus, each access to a bst node could potentially require that another block to be read from disk. using a buffer pool to store multiple blocks in memory can mitigate disk access problems if bst accesses display good locality of reference. but a buffer pool cannot eliminate disk i/o entirely. the problem becomes greater if the bst is unbalanced, because nodes deep in the tree have the potential of causing many disk blocks to be read. thus, there are two signiﬁcant issues that must be addressed to have efﬁcient search from a disk-based bst. the ﬁrst is how to keep the tree balanced. the second is how to arrange the nodes on blocks so as to keep the number of blocks encountered on any path from the root to the leaves at a minimum.
we could select a scheme for balancing the bst and allocating bst nodes to blocks in a way that minimizes disk i/o, as illustrated by figure 10.7. however, maintaining such a scheme in the face of insertions and deletions is difﬁcult. in particular, the tree should remain balanced when an update takes place, but doing so might require much reorganization. each update should affect only a disk few blocks, or its cost will be too high. as you can see from figure 10.8, adopting a rule such as requiring the bst to be complete can cause a great deal of rearranging of data within the tree.
we can solve these problems by selecting another tree structure that automatically remains balanced after updates, and which is amenable to storing in blocks. there are a number of widely used balanced tree data structures, and there are also techniques for keeping bsts balanced. examples are the avl and splay trees discussed in section 13.2. as an alternative, section 10.4 presents the 2-3 tree, which has the property that its leaves are always at the same level. the main reason for discussing the 2-3 tree here in preference to the other balanced search trees is that
4. b-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. this improves space efﬁciency while reducing the typical number of disk fetches necessary during a search or update operation.
a b-tree of order m is deﬁned to have the following shape properties: • the root is either a leaf or has at least two children. • each internal node, except for the root, has between dm/2e and m children. • all leaves are at the same level in the tree, so the tree is always height bal-
the b-tree is a generalization of the 2-3 tree. put another way, a 2-3 tree is a b-tree of order three. normally, the size of a node in the b-tree is chosen to ﬁll a disk block. a b-tree node implementation typically allows 100 or more children. thus, a b-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk ﬁle). in a typical application, b-tree block i/o will be managed using a buffer pool and a block-replacement scheme such as lru (see section 8.3).
1. perform a binary search on the records in the current node. if a record with the search key is found, then return that record. if the current node is a leaf node and the key is not found, then report an unsuccessful search.
for example, consider a search for the record with key value 47 in the tree of figure 10.16. the root node is examined and the second (right) branch taken. after
reading blocks stored on slower, secondary memory (such as on the disk drive). the disk stores the complete contents of the virtual memory. blocks are read into main memory as demanded by memory accesses. naturally, programs using virtual memory techniques are slower than programs whose data are stored completely in main memory. the advantage is reduced programmer effort because a good virtual memory system provides the appearance of larger main memory without modifying the program.
example 8.2 consider a virtual memory whose size is ten sectors, and which has a buffer pool of ﬁve buffers associated with it. we will use a lru replacement scheme. the following series of memory requests occurs.
after the ﬁrst ﬁve requests, the buffer pool will store the sectors in the order 6, 7, 1, 0, 9. because sector 6 is already at the front, the next request can be answered without reading new data from disk or reordering the buffers. the request to sector 8 requires emptying the contents of the least recently used buffer, which contains sector 9. the request to sector 1 brings the buffer holding sector 1’s contents back to the front. processing the remaining requests results in the buffer pool as shown in figure 8.5.
example 8.3 figure 8.5 illustrates a buffer pool of ﬁve blocks mediating a virtual memory of ten blocks. at any given moment, up to ﬁve sectors of information can be in main memory. assume that sectors 1, 7, 5, 3, and 8 are curently in the buffer pool, stored in this order, and that we use the lru buffer replacement strategy. if a request for sector 9 is then received, then one sector currently in the buffer pool must be replaced. because the buffer containing sector 8 is the least recently used buffer, its contents will be copied back to disk at sector 8. the contents of sector 9 are then copied into this buffer, and it is moved to the front of the buffer pool (leaving the buffer containing sector 3 as the new least-recently used buffer). if the next memory request were to sector 5, no data would need to be read from disk. instead, the buffer containing sector 5 would be moved to the front of the buffer pool.
when implementing buffer pools, there are two basic approaches that can be taken regarding the transfer of information between the user of the buffer pool and
where disk i/o is the bottleneck for the program, even the time to copy lots of information between the buffer pool user and the buffer might be inconsequential. another advantage to buffer passing is the reduction in unnecessary read operations for data that will be overwritten anyway.
you should note that the implementations for class bufferpool above are not generic. instead, the space parameter and the buffer pointer are declared to be byte[]. when a class is generic, that means that the record type is arbitrary, in contrast, using a byte[] but that the class knows what the record type is. pointer for the space means that not only is the record type arbitrary, but also the buffer pool does not even know what the user’s record type is. in fact, a given buffer pool might have many users who store many types of records.
in a buffer pool, the user decides where a given record will be stored but has no control over the precise mechanism by which data are transfered to the backing storage. this is in contrast to the memory manager described in section 12.3 in which the user passes a record to the manager and has no control at all over where the record is stored.
the java programmer’s logical view of a random access ﬁle is a single stream of bytes. interaction with a ﬁle can be viewed as a communications channel for issuing one of three instructions: read bytes from the current position in the ﬁle, write bytes to the current position in the ﬁle, and move the current position within the ﬁle. you do not normally see how the bytes are stored in sectors, clusters, and so forth. the mapping from logical to physical addresses is done by the ﬁle system, and sector-level buffering is done automatically by the disk controller.
when processing records in a disk ﬁle, the order of access can have a great effect on i/o time. a random access procedure processes records in an order independent of their logical order within the ﬁle. sequential access processes records in order of their logical appearance within the ﬁle. sequential processing requires less seek time if the physical layout of the disk ﬁle matches its logical layout, as would be expected if the ﬁle were created on a disk with a high percentage of free space.
takes maximum advantage of this microparallelism is double buffering. imagine that a ﬁle is being processed sequentially. while the ﬁrst sector is being read, the cpu cannot process that information and so must wait or ﬁnd something else to do in the meantime. once the ﬁrst sector is read, the cpu can start processing while the disk drive (in parallel) begins reading the second sector. if the time required for the cpu to process a sector is approximately the same as the time required by the disk controller to read a sector, it might be possible to keep the cpu continuously fed with data from the ﬁle. the same concept can also be applied to output, writing one sector to disk while the cpu is writing to a second output buffer in memory. thus, in computers that support double buffering, it pays to have at least two input buffers and two output buffers available.
caching information in memory is such a good idea that it is usually extended to multiple buffers. the operating system or an application program might store many buffers of information taken from some backing storage such as a disk ﬁle. this process of using buffers as an intermediary between a user and a disk ﬁle is called buffering the ﬁle. the information stored in a buffer is often called a page, and the collection of buffers is called a buffer pool. the goal of the buffer pool is to increase the amount of information stored in memory in hopes of increasing the likelihood that new information requests can be satisﬁed from the buffer pool rather than requiring new information to be read from disk.
as long as there is an unused buffer available in the buffer pool, new information can be read in from disk on demand. when an application continues to read new information from disk, eventually all of the buffers in the buffer pool will become full. once this happens, some decision must be made about what information in the buffer pool will be sacriﬁced to make room for newly requested information. when replacing information contained in the buffer pool, the goal is to select a buffer that has “unnecessary” information, that is, that information least likely to be requested again. because the buffer pool cannot know for certain what the pattern of future requests will look like, a decision based on some heuristic, or best guess, must be used. there are several approaches to making this decision.
one heuristic is “ﬁrst-in, ﬁrst-out” (fifo). this scheme simply orders the buffers in a queue. the buffer at the front of the queue is used next to store new information and then placed at the end of the queue. in this way, the buffer to be replaced is the one that has held its information the longest, in hopes that this information is no longer needed. this is a reasonable assumption when processing moves along the ﬁle at some steady pace in roughly sequential order. however, many programs work with certain key pieces of information over and over again, and the importance of information has little to do with how long ago the informa-
tion was ﬁrst accessed. typically it is more important to know how many times the information has been accessed, or how recently the information was last accessed. another approach is called “least frequently used” (lfu). lfu tracks the number of accesses to each buffer in the buffer pool. when a buffer must be reused, the buffer that has been accessed the fewest number of times is considered to contain the “least important” information, and so it is used next. lfu, while it seems intuitively reasonable, has many drawbacks. first, it is necessary to store and update access counts for each buffer. second, what was referenced many times in the past might now be irrelevant. thus, some time mechanism where counts “expire” is often desirable. this also avoids the problem of buffers that slowly build up big counts because they get used just often enough to avoid being replaced. an alternative is to maintain counts for all sectors ever read, not just the sectors currently in the buffer pool.
the third approach is called “least recently used” (lru). lru simply keeps the buffers in a list. whenever information in a buffer is accessed, this buffer is brought to the front of the list. when new information must be read, the buffer at the back of the list (the one least recently used) is taken and its “old” information is either discarded or written to disk, as appropriate. this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless special knowledge about information access patterns for an application suggests a special-purpose buffer management scheme.
the main purpose of a buffer pool is to minimize disk i/o. when the contents of a block are modiﬁed, we could write the updated information to disk immediately. but what if the block is changed again? if we write the block’s contents after every change, that might be a lot of disk write operations that can be avoided. it is more efﬁcient to wait until either the ﬁle is to be closed, or the buffer containing that block is ﬂushed from the buffer pool.
when a buffer’s contents are to be replaced in the buffer pool, we only want to write the contents to disk if it is necessary. that would be necessary only if the contents have changed since the block was read in originally from the ﬁle. the way to insure that the block is written when necessary, but only when necessary, is to maintain a boolean variable with the buffer (often referred to as the dirty bit) that is turned on when the buffer’s contents are modiﬁed by the client. at the time when the block is ﬂushed from the buffer pool, it is written to disk if and only if the dirty bit has been turned on.
modern operating systems support virtual memory. virtual memory is a technique that allows the programmer to pretend that there is more of the faster main memory (such as ram) than actually exists. this is done by means of a buffer pool
the next step is to deﬁne the adt for a list object in terms of a set of operations on that object. we will use the java notation of an interface to formally deﬁne the list adt. interface list deﬁnes the member functions that any list implementation inheriting from it must support, along with their parameters and return types. we increase the ﬂexibility of the list adt by making it a template.
true to the notion of an adt, an abstract class does not specify how operations are implemented. two complete implementations are presented later in this section, both of which use the same list adt to deﬁne their operations, but they are considerably different in approaches and in their space/time tradeoffs.
figure 4.1 presents our list adt. class list is a generic of one parameter, named e. e serves as a placeholder for whatever element type the user would like to store in a list. the comments given in figure 4.1 describe precisely what each member function is intended to do. however, some explanation of the basic design is in order. given that we wish to support the concept of a sequence, with access to any position in the list, the need for many of the member functions such as insert and movetopos is clear. the key design decision embodied in this adt is support for the concept of a current position. for example, member movetostart sets the current position to be the ﬁrst element on the list, while methods next and prev move the current position to the next and previous elements, respectively. the intention is that any implementation for this adt support the concept of a current position.
given that our adt deﬁnes lists to have a current position, it is helpful to modify our list display notation to indicate this position. i will use a vertical bar, such as h20, 23 | 12, 15i to indicate the list of four elements, with the current position immediately to the right of the bar. given this conﬁguration, calling insert with value 10 will change the list to be h20, 23 | 10, 12, 15i.
if you examine figure 4.1, you should ﬁnd that the list member functions provided allow you to build a list with elements in any desired order, and to access any desired position in the list. you might have noticed that the clear method is not necessary, in that it could be implemented by means of the other member functions in the same asymptotic time. it is included merely for convenience.
method getvalue returns a reference to the current element. it is considered a violation of getvalue’s preconditions to ask for the value of a non-existent element (i.e., there must be an element at the current position). in our concrete list implementations, the java’s assert mechanism will be used to enforce such preconditions. in a commercial implementation, such violations would be best implemented by the java’s exception mechanism.
freelists are particularly useful for linked lists that periodically grow and then shrink. the freelist will never grow larger than the largest size yet reached by the linked list. requests for new nodes (after the list has shrunk) can be handled by the freelist.
one approach to implementing freelists would be to create two new methods to handle requesting and freeing nodes. this requires that the user’s code, such as the linked list class implementation of figure 4.8, be modiﬁed to call these freelist operators. in the implementation shown here, the link class is augmented with methods get and release. figure 4.11 shows the reimplementation for the link class to support these methods. note how simple they are, because they need only remove and add an element to the front of the freelist, respectively.
in figure 4.11, you should note the use of the static deﬁnition for the freelist header. the purpose of the keyword static is to create a single variable shared among all instances of the link nodes. we want only a single freelist for all link nodes of a given type. a program might create multiple lists. if they are all of the same type (that is, their element types are the same), then they can and should share the same freelist. this will happen with the implementation of figure 4.11. if lists are created that have different element types, because this code is implemented with templates, the need for different list implementations will be discovered by the compiler at compile time. separate versions of the list class will be generated for each element type. thus, each element type will also get its own separate copy of the link class. and each distinct link class implementation will get a separate freelist.
let’s consider a more complex situation where we want separate freelists, but we don’t know this until runtime. for example, perhaps we are using lists to store variable-length strings. a given node gets space allocated for a string of a speciﬁc length. we’d like to be able to reuse the nodes by placing them on a freelist, instead of relying on the system free store operators. but we cannot reuse a given list node unless the size allocated for its string matches the length of the string we want to store in this node. thus, each speciﬁc node size must be stored on its own freelist. that is, the nodes for strings of length one are stored on one freelist, the nodes for strings of length two on another freelist, and so on. in this way, its easy to ﬁnd a node of the proper size.
unfortunately, we don’t know in advance what the various sizes of strings will be. if we want to support long strings, then there could be thousands of different node sizes, many of which are not used in any particular run of the program. thus,
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
given the speciﬁcations of the disk drive from example 8.1, we ﬁnd that it takes about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. it takes about 9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data. this is a good savings (slightly over half the time), but less than 1% of the data on the track are read. if we want to read only a single byte, it would save us effectively no time over that required to read an entire sector. for this reason, nearly all disk drives automatically read or write an entire sector’s worth of information whenever the disk is accessed, even when only one byte of information is requested.
once a sector is read, its information is stored in main memory. this is known as buffering or caching the information. if the next disk request is to that same sector, then it is not necessary to read from disk again because the information is already stored in main memory. buffering is an example of one method for minimizing disk accesses mentioned at the beginning of the chapter: bring off additional information from disk to satisfy future requests. if information from ﬁles were accessed at random, then the chance that two consecutive disk requests are to the same sector would be low. however, in practice most disk requests are close to the location (in the logical ﬁle at least) of the previous request. this means that the probability of the next request “hitting the cache” is much higher than chance would indicate.
this principle explains one reason why average access times for new disk drives are lower than in the past. not only is the hardware faster, but information is also now stored using better algorithms and larger caches that minimize the number of times information needs to be fetched from disk. this same concept is also used to store parts of programs in faster memory within the cpu, using the cpu cache that is prevalent in modern microprocessors.
sector-level buffering is normally provided by the operating system and is often built directly into the disk drive controller hardware. most operating systems maintain at least two buffers, one for input and one for output. consider what would happen if there were only one buffer during a byte-by-byte copy operation. the sector containing the ﬁrst byte would be read into the i/o buffer. the output operation would need to destroy the contents of the single i/o buffer to write this byte. then the buffer would need to be ﬁlled again from disk for the second byte, only to be destroyed during output. the simple solution to this problem is to keep one buffer for input, and a second for output.
most disk drive controllers operate independently from the cpu once an i/o request is received. this is useful because the cpu can typically execute millions of instructions during the time required for a single i/o operation. a technique that
where disk i/o is the bottleneck for the program, even the time to copy lots of information between the buffer pool user and the buffer might be inconsequential. another advantage to buffer passing is the reduction in unnecessary read operations for data that will be overwritten anyway.
you should note that the implementations for class bufferpool above are not generic. instead, the space parameter and the buffer pointer are declared to be byte[]. when a class is generic, that means that the record type is arbitrary, in contrast, using a byte[] but that the class knows what the record type is. pointer for the space means that not only is the record type arbitrary, but also the buffer pool does not even know what the user’s record type is. in fact, a given buffer pool might have many users who store many types of records.
in a buffer pool, the user decides where a given record will be stored but has no control over the precise mechanism by which data are transfered to the backing storage. this is in contrast to the memory manager described in section 12.3 in which the user passes a record to the manager and has no control at all over where the record is stored.
the java programmer’s logical view of a random access ﬁle is a single stream of bytes. interaction with a ﬁle can be viewed as a communications channel for issuing one of three instructions: read bytes from the current position in the ﬁle, write bytes to the current position in the ﬁle, and move the current position within the ﬁle. you do not normally see how the bytes are stored in sectors, clusters, and so forth. the mapping from logical to physical addresses is done by the ﬁle system, and sector-level buffering is done automatically by the disk controller.
when processing records in a disk ﬁle, the order of access can have a great effect on i/o time. a random access procedure processes records in an order independent of their logical order within the ﬁle. sequential access processes records in order of their logical appearance within the ﬁle. sequential processing requires less seek time if the physical layout of the disk ﬁle matches its logical layout, as would be expected if the ﬁle were created on a disk with a high percentage of free space.
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
they are not erased from disk and tape when the power is turned off. in contrast, ram used for main memory is usually volatile — all information is lost with the power. a second advantage is that ﬂoppy disks, cd-roms, and “ﬂash” drives can easily be transferred between computers. this provides a convenient way to take information from one computer to another.
in exchange for reduced storage costs, persistence, and portability, secondary storage devices pay a penalty in terms of increased access time. while not all accesses to disk take the same amount of time (more on this later), the typical time required to access a byte of storage from a disk drive in 2007 is around 9 ms (i.e., 9 thousandths of a second). this might not seem slow, but compared to the time required to access a byte from main memory, this is fantastically slow. typical access time from standard personal computer ram in 2007 is about 5-10 nanoseconds (i.e., 5-10 billionths of a second). thus, the time to access a byte of data from a disk drive is about six orders of magnitude greater than that required to access a byte from main memory. while disk drive and ram access times are both decreasing, they have done so at roughly the same rate. the relative speeds have remained the same for over twenty-ﬁve years, in that the difference in access time between ram and a disk drive has remained in the range between a factor of 100,000 and 1,000,000.
to gain some intuition for the signiﬁcance of this speed difference, consider the time that it might take for you to look up the entry for disk drives in the index of this book, and then turn to the appropriate page. call this your “primary memory” access time. if it takes you about 20 seconds to perform this access, then an access taking 500,000 times longer would require months.
it is interesting to note that while processing speeds have increased dramatically, and hardware prices have dropped dramatically, disk and memory access times have improved by less than an order of magnitude over the past ten years. however, the situation is really much better than that modest speedup would suggest. during the same time period, the size of both disk and main memory has increased by about three orders of magnitude. thus, the access times have actually decreased in the face of a massive increase in the density of these storage devices. due to the relatively slow access time for data on disk as compared to main memory, great care is required to create efﬁcient applications that process diskbased information. the million-to-one ratio of disk access time versus main memory access time makes the following rule of paramount importance when designing disk-based applications:
figure 8.3 the organization of a disk platter. dots indicate density of information. (a) nominal arrangement of tracks showing decreasing data density when moving outward from the center of the disk. (b) a “zoned” arrangement with the sector size and density periodically reset in tracks further away from the center.
rangement is illustrated by figure 8.3a. disk drives today actually group tracks into “zones” such that the tracks in the innermost zone adjust their data density going out to maintain the same radial data density, then the tracks of the next zone reset the data density to make better use of their storage ability, and so on. this arrangement is shown in figure 8.3b.
in contrast to the physical layout of a hard disk, a cd-rom consists of a single spiral track. bits of information along the track are equally spaced, so the information density is the same at both the outer and inner portions of the track. to keep
probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? if there are 23 students, then the odds are about even that two will share a birthday. this is despite the fact that there are 365 days in which students can have birthdays (ignoring leap years), on most of which no student in the class has a birthday. with more students, the probability of a shared birthday increases. the mapping of students to days based on their birthday is similar to assigning records to slots in a table (of size 365) using the birthday as a hash function. note that this observation tells us nothing about which students share a birthday, or on which days of the year shared birthdays fall.
to be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space. typically, this means that the hash table will be around half full. because collisions are extremely likely to occur under these conditions (by chance, any record inserted into a table that is half full will have a collision half of the time), ddoes this mean that we need not worry about the ability of a hash function to avoid collisions? absolutely not. the difference between a good hash function and a bad hash function makes a big difference in practice. technically, any function that maps all possible key values to a slot in the hash table is a hash function. in the extreme case, even a function that maps all records to the same slot is a hash function, but it does nothing to help us ﬁnd records during a search operation.
we would like to pick a hash function that stores the actual records in the collection such that each slot in the hash table has equal probablility of being ﬁlled. unfortunately, we normally have no control over the key values of the actual records, so how well any particular hash function does this depends on the distribution of the keys within the allowable key range. in some cases, incoming data are well distributed across their key range. for example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table. however, in many applications the incoming records are highly clustered or otherwise poorly distributed. when input records are not well distributed throughout the key range it can be difﬁcult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance.
performance is required. an example is searching for data on a read-only cd. here the database will never change, the time for each access is expensive, and the database designer can build the hash table before issuing the cd.
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
for each of the following scenarios, which of these choices would be best? explain your answer. (a) the records are guaranteed to arrive already sorted from lowest to highest (i.e., whenever a record is inserted, its key value will always be greater than that of the last record inserted). a total of 1000 inserts will be interspersed with 1000 searches.
(b) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1,000,000 insertions are performed, followed by 10 searches.
(c) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are interspersed with 1000 searches.
(d) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are performed, followed by 1,000,000 searches.
5.2 one way to deal with the “problem” of null pointers in binary trees is to use that space for some other purpose. one example is the threaded binary tree. extending the node implementation of figure 5.7, the threaded binary tree stores with each node two additional bit ﬁelds that indicate if the child pointers lc and rc are regular pointers to child nodes or threads. if lc is not a pointer to a non-empty child (i.e., if it would be null in a regular binary tree), then it instead stores a pointer to the inorder predecessor of that node. the inorder predecessor is the node that would be printed immediately before the current node in an inorder traversal. if rc is not a pointer to a child, then it instead stores a pointer to the node’s inorder successor. the inorder successor is the node that would be printed immediately after the current node in an inorder traversal. the main advantage of threaded binary trees is that operations such as inorder traversal can be implemented without using recursion or a stack. reimplement the bst as a threaded binary tree, and include a non-recursive version of the preorder traversal
5.3 implement a city database using a bst to store the database records. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates.
figure 13.13 function incircle must check the euclidean distance between it is possible for a record a to have x- and ya record and the query point. coordinates each within the query distance of the query point c, yet have a itself lie outside the query circle.
if the search process reaches a node whose key value for the discriminator is more than d above the corresponding value in the search key, then it is not possible that any record in the right subtree can be within distance d of the search key because all key values in that dimension are always too great. similarly, if the current node’s key value in the discriminator is d less than that for the search key value, then no record in the left subtree can be within the radius. in such cases, the subtree in question need not be searched, potentially saving much time. in the average case, the number of nodes that must be visited during a range query is linear on the number of data records that fall within the query circle.
example 13.7 find all cities in the k-d tree of figure 13.14 within 25 units of the point (25, 65). the search begins with the root node, which contains record a. because (40, 45) is exactly 25 units from the search point, it will be reported. the search procedure then determines which branches of the tree to take. the search circle extends to both the left and the right of a’s (vertical) dividing line, so both branches of the tree must be searched. the left subtree is processed ﬁrst. here, record b is checked and found to fall within the search circle. because the node storing b has no children, processing of the left subtree is complete. processing of a’s right subtree now begins. the coordinates of record c are checked and found not to fall within the circle. thus, it should not be reported. however, it is possible that cities within c’s subtrees could fall within the search circle even if c does not. as c is at level 1, the discriminator at this level is the y-coordinate. because 65 − 25 > 10, no record in c’s left subtree (i.e., records above c) could possibly be in the search circle. thus, c’s left subtree (if it had one) need not be searched. however, cities in c’s right subtree could fall within
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
tions, every other problem that is in np can also be solved in polynomial time on a regular computer! deﬁne a problem to be np-hard if any problem in np can be reduced to x in polynomial time. thus, x is as hard as any problem in np. a problem x is deﬁned to be np-complete if
1. x is in np, and 2. x is np-hard. the requirement that a problem be np-hard might seem to be impossible, but in fact there are hundreds of such problems, including traveling salesman. another such problem is called clique.
nobody knows whether there is a polynomial time solution for clique, but if such an algorithm is found for clique or for traveling salesman, then that solution can be modiﬁed to solve the other, or any other problem in np, in polynomial time. the primary theoretical advantage of knowing that a problem p1 is np-complete is that it can be used to show that another problem p2 is np-complete. this is done by ﬁnding a polynomial time reduction of p1 to p2. because we already know that all problems in np can be reduced to p1 in polynomial time (by the deﬁnition of np-complete), we now know that all problems can be reduced to p2 as well by the simple algorithm of reducing to p1 and then from there reducing to p2. there is a practical advantage to knowing that a problem is np-complete. it relates to knowing that if a polynomial time solution can be found for any problem that is np-complete, then a polynomial solution can be found for all such problems. the implication is that,
1. because no one has yet found such a solution, it must be difﬁcult or impos2. effort to ﬁnd a polynomial time solution for one np-complete problem can
how is np-completeness of practical signiﬁcance for typical programmers? well, if your boss demands that you provide a fast algorithm to solve a problem, she will not be happy if you come back saying that the best you could do was
example 17.2 in this example, we make use of a simple conversion between two graph problems. theorem 17.2 vertex cover is np-complete. proof: prove that vertex cover is in np: simply guess a subset of the graph and determine in polynomial time whether that subset is in fact a vertex cover of size k or less. prove that vertex cover is np-hard: we will assume that clique is already known to be np-complete. (we will see this proof in the next example. for now, just accept that it is true.) given that clique is np-complete, we need to ﬁnd a polynomialtime transformation from the input to clique to the input to vertex cover, and another polynomial-time transformation from the output for vertex cover to the output for clique. this turns out to be a simple matter, given the following observation. consider a graph g and a vertex cover s on g. denote by s0 the set of vertices in g but not in s. there can be no edge connecting any two vertices in s0 because, if there were, then s would not be a vertex cover. denote by g0 the inverse graph for g, that is, the graph formed from the edges not in g. if s is of size k, then s0 forms a clique of size n − k in graph g0. thus, we can reduce clique to vertex cover simply by converting graph g to g0, and asking if g0 has a vertex cover of size n − k or smaller. if yes, then there is a clique in g of size k; if no then there is not.
example 17.3 so far, our np-completenss proofs have involved transformations between inputs of the same “type,” such as from a boolean expression to a boolean expression or from a graph to a graph. sometimes an np-completeness proof involves a transformation between types of inputs, as shown next. theorem 17.3 clique is np-complete.
proof: clique is in np, because we can just guess a collection of k vertices and test in polynomial time if it is a clique. now we show that clique is np-hard by using a reduction from sat. an instance of sat is a boolean expression
that is, there is a vertex in g corresponding to every literal in boolean expression b. we will draw an edge between each pair of vertices v[i1, j1] and v[i2, j2] unless (1) they are two literals within the same clause (i1 = i2) or (2) they are opposite values for the same variable (i.e., one is negated and the other is not). set k = m. figure 17.4 shows an example of this transformation.
b is satisﬁable if and only if g has a clique of size k or greater. b being satisﬁable implies that there is a truth assignment such that at least one literal y[i, ji] is true for each i. if so, then these m literals must correspond to m vertices in a clique of size k = m. conversely, if g has a clique of size k or greater, then the clique must have size exactly k (because no two vertices corresponding to literals in the same clause can be in the clique) and there is one vertex v[i, ji] in the clique for each i. there is a truth assignment making each y[i, ji] true. that truth assignment satisﬁes b. we conclude that clique is np-hard, therefore np-complete. 2
17.2.3 coping with np-complete problems finding that your problem is np-complete might not mean that you can just forget about it. traveling salesmen need to ﬁnd reasonable sales routes regardless of the complexity of the problem. what do you do when faced with an np-complete problem that you must solve?
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
6.1 write classes that implement the general tree class declarations of figure 6.2 using the dynamic “left-child/right-sibling” representation described in section 6.3.4.
6.2 write classes that implement the general tree class declarations of figure 6.2 using the linked general tree implementation with child pointer arrays of figure 6.12. your implementation should support only ﬁxed-size nodes that do not change their number of children once they are created. then, reimplement these classes with the linked list of children representation of figure 6.13. how do the two implementations compare in space and time efﬁciency and ease of implementation?
6.3 write classes that implement the general tree class declarations of figure 6.2 using the linked general tree implementation with child pointer arrays of figure 6.12. your implementation must be able to support changes in the number of children for a node. when created, a node should be allocated with only enough space to store its initial set of children. whenever a new child is added to a node such that the array overﬂows, allocate a new array from free store that can store twice as many children.
6.4 implement a bst ﬁle archiver. your program should take a bst created in main memory using the implementation of figure 5.14 and write it out to disk using one of the sequential representations of section 6.5. it should also be able to read in disk ﬁles using your sequential representation and create the equivalent main memory representation.
6.5 use the union/find algorithm to implement a solution to the following problem. given a set of points represented by their xy-coordinates, assign the points to clusters. any two points are deﬁned to be in the same cluster if they are within a speciﬁed distance d of each other. for the purpose of this problem, clustering is an equivalence relationship. in other words, points a, b, and c are deﬁned to be in the same cluster if the distance between a and b is less than d and the distance between a and c is also less than d, even if the distance between b and c is greater than d. to solve the problem, compute the distance between each pair of points, using the equivalence processing algorithm to merge clusters whenever two points are within the speciﬁed distance. what is the asymptotic complexity of this algorithm? where is the bottleneck in processing?
6.6 in this project, you will run some empirical tests to deterimine if some variations on path compression in the union/find algorithm will lead to im-
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
example 3.18 the following is a true story. a few years ago, one of my graduate students had a big problem. his thesis work involved several intricate operations on a large database. he was now working on the ﬁnal step. “dr. shaffer,” he said, “i am running this program and it seems to be taking a long time.” after examining the algorithm we realized that its running time was Θ(n2), and that it would likely take one to two weeks to complete. even if we could keep the computer running uninterrupted for that long, he was hoping to complete his thesis and graduate before then. fortunately, we realized that there was a fairly easy way to convert the algorithm so that its running time was Θ(n log n). by the next day he had modiﬁed the program. it ran in only a few hours, and he ﬁnished his thesis on time.
while not nearly so important as changing an algorithm to reduce its growth rate, “code tuning” can also lead to dramatic improvements in running time. code tuning is the art of hand-optimizing a program to run faster or require less storage. for many programs, code tuning can reduce running time by a factor of ten, or cut the storage requirements by a factor of two or more. i once tuned a critical function in a program — without changing its basic algorithm — to achieve a factor of 200 speedup. to get this speedup, however, i did make major changes in the representation of the information, converting from a symbolic coding scheme to a numeric coding scheme on which i was able to do direct computation.
here are some suggestions for ways to speed up your programs by code tuning. the most important thing to realize is that most statements in a program do not have much effect on the running time of that program. there are normally just a few key subroutines, possibly even key lines of code within the key subroutines, that account for most of the running time. there is little point to cutting in half the running time of a subroutine that accounts for only 1% of the total running time. focus your attention on those parts of the program that have the most impact.
when tuning code, it is important to gather good timing statistics. many compilers and operating systems include proﬁlers and other special tools to help gather information on both time and space use. these are invaluable when trying to make a program more efﬁcient, because they can tell you where to invest your effort.
a lot of code tuning is based on the principle of avoiding work rather than speeding up work. a common situation occurs when we can test for a condition
another approach to analysis is simulation. the idea of simulation is to model the problem with a computer program and then run it to get a result. in the context of algorithm analysis, simulation is distinct from empirical comparison of two competitors because the purpose of the simulation is to perform analysis that might otherwise be too difﬁcult. a good example of this appears in figure 9.8. this ﬁgure shows the cost for inserting or deleting a record from a hash table under two different assumptions for the policy used to ﬁnd a free slot in the table. the y axes is the cost in number of hash table slots evaluated, and the x axes is the percentage of slots in the table that are full. the mathematical equations for these curves can be determined, but this is not so easy. a reasonable alternative is to write simple variations on hashing. by timing the cost of the program for various loading conditions, it is not difﬁcult to construct a plot similar to figure 9.8. the purpose of this analysis is not to determine which approach to hashing is most efﬁcient, so we are not doing empirical comparison of hashing alternatives. instead, the purpose is to analyze the proper loading factor that would be used in an efﬁcient hashing system to balance time cost versus hash table size (space cost).
pioneering works on algorithm analysis include the art of computer programming by donald e. knuth [knu97, knu98], and the design and analysis of computer algorithms by aho, hopcroft, and ullman [ahu74]. the alternate deﬁnition for Ω comes from [ahu83]. the use of the notation “t(n) is in o(f(n))” rather than the more commonly used “t(n) = o(f(n))” i derive from brassard and bratley [bb96], though certainly this use predates them. a good book to read for further information on algorithm analysis techniques is compared to what? by gregory j.e. rawlins [raw92].
bentley [ben88] describes one problem in numerical analysis for which, between 1945 and 1988, the complexity of the best known algorithm had decreased from o(n7) to o(n3). for a problem of size n = 64, this is roughly equivalent to the speedup achieved from all advances in computer hardware during the same time period.
while the most important aspect of program efﬁciency is the algorithm, much improvement can be gained from efﬁcient coding of a program. as cited by frederick p. brooks in the mythical man-month [bro95], an efﬁcient programmer can often produce programs that run ﬁve times faster than an inefﬁcient programmer, even when neither takes special efforts to speed up their code. for excellent and enjoyable essays on improving your coding efﬁciency, and ways to speed up your code when it really matters, see the books by jon bentley [ben82, ben00, ben88]. the
the initial cn term is the cost of doing the findpivot and partition steps, for some constant c. the closed-form solution to this recurrence relation is Θ(n log n). thus, quicksort has average-case cost Θ(n log n).
this is an unusual situation that the average case cost and the worst case cost have asymptotically different growth rates. consider what “average case” actually means. we compute an average cost for inputs of size n by summing up for every possible input of size n the product of the running time cost of that input times the probability that that input will occur. to simplify things, we assumed that every permutation is equally likely to occur. thus, ﬁnding the average means summing up the cost for every permutation and dividing by the number of inputs (n!). we know that some of these n! inputs cost o(n2). but the sum of all the permutation costs has to be (n!)(o(n log n)). given the extremely high cost of the worst inputs, there must be very few of them. in fact, there cannot be a constant fraction of the inputs with cost o(n2). even, say, 1% of the inputs with cost o(n2) would lead to an average cost of o(n2). thus, as n grows, the fraction of inputs with high cost must be going toward a limit of zero. we can conclude that quicksort will always have good behavior if we can avoid those very few bad input permutations.
the running time for quicksort can be improved (by a constant factor), and much study has gone into optimizing this algorithm. the most obvious place for improvement is the findpivot function. quicksort’s worst case arises when the pivot does a poor job of splitting the array into equal size subarrays. if we are willing to do more work searching for a better pivot, the effects of a bad pivot can be decreased or even eliminated. one good choice is to use the “median of three” algorithm, which uses as a pivot the middle of three randomly selected values. using a random number generator to choose the positions is relatively expensive, so a common compromise is to look at the ﬁrst, middle, and last positions of the current subarray. however, our simple findpivot function that takes the middle value as its pivot has the virtue of making it highly unlikely to get a bad input by chance, and it is quite cheap to implement. this is in sharp contrast to selecting the ﬁrst or last element as the pivot, which would yield bad performance for many permutations that are nearly sorted or nearly reverse sorted.
a signiﬁcant improvement can be gained by recognizing that quicksort is relatively slow when n is small. this might not seem to be relevant if most of the time we sort large arrays, nor should it matter how long quicksort takes in the rare instance when a small array is sorted because it will be fast anyway. but you should notice that quicksort itself sorts many, many small arrays! this happens as a natural by-product of the divide and conquer approach.
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
many algorithms (or their instantiations as programs), it is easy to come up with the equation that deﬁnes their runtime behavior. most algorithms presented in this book are well understood and we can almost always give a Θ analysis for them. however, chapter 17 discusses a whole class of algorithms for which we have no Θ analysis, just some unsatisfying big-oh and Ω analyses. exercise 3.14 presents a short, simple program fragment for which nobody currently knows the true upper or lower bounds.
while some textbooks and programmers will casually say that an algorithm is “order of” or “big-oh” of some cost function, it is generally better to use Θ notation rather than big-oh notation whenever we have sufﬁcient knowledge about an algorithm to be sure that the upper and lower bounds indeed match. throughout this book, Θ notation will be used in preference to big-oh notation whenever our state of knowledge makes that possible. limitations on our ability to analyze certain algorithms may require use of big-oh or Ω notations. in rare occasions when the discussion is explicitly about the upper or lower bound of a problem or algorithm, the corresponding notation will be used in preference to Θ notation.
once you determine the running-time equation for an algorithm, it really is a simple matter to derive the big-oh, Ω, and Θ expressions from the equation. you do not need to resort to the formal deﬁnitions of asymptotic analysis. instead, you can use the following rules to determine the simplest form.
1. if f(n) is in o(g(n)) and g(n) is in o(h(n)), then f(n) is in o(h(n)). 2. if f(n) is in o(kg(n)) for any constant k > 0, then f(n) is in o(g(n)). 3. if f1(n) is in o(g1(n)) and f2(n) is in o(g2(n)), then f1(n) + f2(n) is in
the ﬁrst rule says that if some function g(n) is an upper bound for your cost function, then any upper bound for g(n) is also an upper bound for your cost function. a similar property holds true for Ω notation: if g(n) is a lower bound for your cost function, then any lower bound for g(n) is also a lower bound for your cost function. likewise for Θ notation.
the signiﬁcance of rule (2) is that you can ignore any multiplicative constants in your equations when using big-oh notation. this rule also holds true for Ω and Θ notations.
3.15 does every algorithm have a Θ running-time equation? in other words, are the upper and lower bounds for the running time (on any speciﬁed class of inputs) always the same?
3.16 does every problem for which there exists some algorithm have a Θ runningtime equation? in other words, for every problem, and for any speciﬁed class of inputs, is there some algorithm whose upper bound is equal to the problem’s lower bound?
3.17 given an array storing integers ordered by value, modify the binary search routine to return the position of the ﬁrst integer with value k in the situation where k can appear multiple times in the array. be sure that your algorithm is Θ(log n), that is, do not resort to sequential search once an occurrence of k is found.
3.18 given an array storing integers ordered by value, modify the binary search routine to return the position of the integer with the greatest value less than k when k itself does not appear in the array. return error if the least value in the array is greater than k.
3.19 modify the binary search routine to support search in an array of inﬁnite size. in particular, you are given as input a sorted array and a key value k to search for. call n the position of the smallest value in the array that is equal to or larger than x. provide an algorithm that can determine n in o(log n) comparisons in the worst case. explain why your algorithm meets the required time bound.
3.20 it is possible to change the way that we pick the dividing point in a binary search, and still get a working search routine. however, where we pick the dividing point could affect the performance of the algorithm. (a) if we change the dividing point computation in function binary from i = (l + r)/2 to i = (l + ((r − l)/3)), what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
how can this be? programmers look at programs regularly to determine if they will halt. surely this can be automated. as a warning to those who believe any program can be analyzed in this way, carefully examine the following code fragment before reading on.
this is a famous piece of code. the sequence of values that is assigned to n by this code is sometimes called the collatz sequence for input value n. does this code fragment halt for all values of n? nobody knows the answer. every input that has been tried halts. but does it always halt? note that for this code fragment, because we do not know if it halts, we also do not know an upper bound for its running time. as for the lower bound, we can easily show Ω(log n)(see exercise 3.14).
personally, i have faith that someday some smart person will completely analyze the collitz function and prove once and for all that the code fragment halts for all values of n. doing so may well give us techniques that advance our ability to do algorithm analysis in general. unfortunately, proofs from computability — the branch of computer science that studies what is impossible to do with a computer — compel us to believe that there will always be another bit of program code that we cannot analyze. this comes as a result of the fact that the halting problem is unsolvable.
before proving that the halting problem is unsolvable, we ﬁrst prove that not all functions can be implemented as a computer program. this is so because the number of programs is much smaller than the number of possible functions.
a set is said to be countable (or countably inﬁnite if it is a set with inﬁnite members) if every member of the set can be uniquely assigned to a positive integer. a set is said to be uncountable (or uncountably inﬁnite) if it is not possible to assign every member of the set to a positive integer.
to understand what is meant when we say “assigned to a positive integer,” imagine that there is an inﬁnite row of bins, labeled 1, 2, 3, and so on. take a set and start placing members of the set into bins, with at most one member per bin. if we can ﬁnd a way to assign all of the members to bins, then the set is countable. for example, consider the set of positive even integers 2, 4, and so on. we can
17.4 further reading the classic text on the theory of np-completeness is computers and intractability: a guide to the theory of np-completeness by garey and johnston [gj79]. the traveling salesman problem, edited by lawler et al. [llks85], discusses many approaches to ﬁnding an acceptable solution to this particular np-complete problem in a reasonable amount of time.
for more information about the collatz function see “on the ups and downs of hailstone numbers” by b. hayes [hay84], and “the 3x + 1 problem and its generalizations” by j.c. lagarias [lag85].
17.1 consider this algorithm for ﬁnding the maximum element in an array: first sort the array and then select the last (maximum) element. what (if anything) does this reduction tell us about the upper and lower bounds to the problem of ﬁnding the maximum element in a sequence? why can we not reduce sorting to ﬁnding the maximum element? 17.2 use a reduction to prove that squaring an n × n matrix is just as expensive (asymptotically) as multiplying two n × n matrices. 17.3 use a reduction to prove that multiplying two upper triangular n × n matrices is just as expensive (asymptotically) as multiplying two arbitrary n × n matrices. (a) explain why computing the factorial of n by multiplying all values
(b) explain why computing an approximation to the factorial of n by making use of stirling’s formula (see section 2.2) is a polynomial time algorithm.
17.5 consider this algorithm for solving the clique problem. first, generate all subsets of the vertices containing exactly k vertices. there are o(nk) such subsets altogether. then, check whether any subgraphcs induced by these subsets is complete. if this algorithm ran in polynomial time, what would be its signiﬁcance? why is this not a polynomial-time algorithm for the clique problem?
bilities of modern compilers to make extremely good optimizations of expressions. “optimization of expressions” here means a rearrangement of arithmetic or logical expressions to run more efﬁciently. be careful not to damage the compiler’s ability to do such optimizations for you in an effort to optimize the expression yourself. always check that your “optimizations” really do improve the program by running the program before and after the change on a suitable benchmark set of input. many times i have been wrong about the positive effects of code tuning in my own programs. most often i am wrong when i try to optimize an expression. it is hard to do better than the compiler.
this chapter has focused on asymptotic analysis. this is an analytic tool, whereby we model the key aspects of an algorithm to determine the growth rate of the algorithm as the input size grows. as pointed out previously, there are many limitations to this approach. these include the effects at small problem size, determining the ﬁner distinctions between algorithms with the same growth rate, and the inherent difﬁculty of doing mathematical modeling for more complex problems.
an alternative to analytical approaches are empirical approaches. the most obvious empirical approach is simply to run two competitors and see which performs better. in this way we might overcome the deﬁciencies of analytical approaches.
be warned that comparative timing of programs is a difﬁcult business, often subject to experimental errors arising from uncontrolled factors (system load, the language or compiler used, etc.). the most important point is not to be biased in favor of one of the programs. if you are biased, this is certain to be reﬂected in the timings. one look at competing software or hardware vendors’ advertisements should convince you of this. the most common pitfall when writing two programs to compare their performance is that one receives more code-tuning effort than the other. as mentioned in section 3.10, code tuning can often reduce running time by a factor of ten. if the running times for two programs differ by a constant factor regardless of input size (i.e., their growth rates are the same), then differences in code tuning might account for any difference in running time. be suspicious of empirical comparisons in this situation.
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
bilities of modern compilers to make extremely good optimizations of expressions. “optimization of expressions” here means a rearrangement of arithmetic or logical expressions to run more efﬁciently. be careful not to damage the compiler’s ability to do such optimizations for you in an effort to optimize the expression yourself. always check that your “optimizations” really do improve the program by running the program before and after the change on a suitable benchmark set of input. many times i have been wrong about the positive effects of code tuning in my own programs. most often i am wrong when i try to optimize an expression. it is hard to do better than the compiler.
this chapter has focused on asymptotic analysis. this is an analytic tool, whereby we model the key aspects of an algorithm to determine the growth rate of the algorithm as the input size grows. as pointed out previously, there are many limitations to this approach. these include the effects at small problem size, determining the ﬁner distinctions between algorithms with the same growth rate, and the inherent difﬁculty of doing mathematical modeling for more complex problems.
an alternative to analytical approaches are empirical approaches. the most obvious empirical approach is simply to run two competitors and see which performs better. in this way we might overcome the deﬁciencies of analytical approaches.
be warned that comparative timing of programs is a difﬁcult business, often subject to experimental errors arising from uncontrolled factors (system load, the language or compiler used, etc.). the most important point is not to be biased in favor of one of the programs. if you are biased, this is certain to be reﬂected in the timings. one look at competing software or hardware vendors’ advertisements should convince you of this. the most common pitfall when writing two programs to compare their performance is that one receives more code-tuning effort than the other. as mentioned in section 3.10, code tuning can often reduce running time by a factor of ten. if the running times for two programs differ by a constant factor regardless of input size (i.e., their growth rates are the same), then differences in code tuning might account for any difference in running time. be suspicious of empirical comparisons in this situation.
example 1.6 when operating a car, the primary activities are steering, accelerating, and braking. on nearly all passenger cars, you steer by turning the steering wheel, accelerate by pushing the gas pedal, and brake by pushing the brake pedal. this design for cars can be viewed as an adt with operations “steer,” “accelerate,” and “brake.” two cars might implement these operations in radically different ways, say with different types of engine, or front- versus rear-wheel drive. yet, most drivers can operate many different cars because the adt presents a uniform method of operation that does not require the driver to understand the speciﬁcs of any particular engine or drive design. these differences are deliberately hidden.
the concept of an adt is one instance of an important principle that must be understood by any successful computer scientist: managing complexity through abstraction. a central theme of computer science is complexity and techniques for handling it. humans deal with complexity by assigning a label to an assembly of objects or concepts and then manipulating the label in place of the assembly. cognitive psychologists call such a label a metaphor. a particular label might be related to other pieces of information or other labels. this collection can in turn be given a label, forming a hierarchy of concepts and labels. this hierarchy of labels allows us to focus on important issues while ignoring unnecessary details.
example 1.7 we apply the label “hard drive” to a collection of hardware that manipulates data on a particular type of storage device, and we apply the label “cpu” to the hardware that controls execution of computer instructions. these and other labels are gathered together under the label “computer.” because even small home computers have millions of components, some form of abstraction is necessary to comprehend how a computer operates.
consider how you might go about the process of designing a complex computer program that implements and manipulates an adt. the adt is implemented in one part of the program by a particular data structure. while designing those parts of the program that use the adt, you can think in terms of operations on the data type without concern for the data structure’s implementation. without this ability to simplify your thinking about a complex program, you would have no hope of understanding or implementing it.
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
to understand once you have mastered this book is algorithms by robert sedgewick [sed03]. for an excellent and highly readable (but more advanced) teaching introduction to algorithms, their design, and their analysis, see introduction to algorithms: a creative approach by udi manber [man89]. for an advanced, encyclopedic approach, see introduction to algorithms by cormen, leiserson, and rivest [clrs01]. steven s. skiena’s the algorithm design manual [ski98] provides pointers to many implementations for data structures and algorithms that are available on the web.
for a gentle introduction to adts and program speciﬁcation, see abstract data types: their speciﬁcation, representation, and use by thomas, robinson, and emms [tre88].
the claim that all modern programming languages can implement the same algorithms (stated more precisely, any function that is computable by one programming language is computable by any programming language with certain standard capabilities) is a key result from computability theory. for an easy introduction to this ﬁeld see james l. hein, discrete structures, logic, and computability [hei03]. much of computer science is devoted to problem solving. indeed, this is what attracts many people to the ﬁeld. how to solve it by george p´olya [p´ol57] is considered to be the classic work on how to improve your problem-solving abilities. if you want to be a better student (as well as a better problem solver in general), see strategies for creative problem solving by folger and leblanc [fl95], effective problem solving by marvin levine [lev94], and problem solving & comprehension by arthur whimbey and jack lochhead [wl99].
see the origin of consciousness in the breakdown of the bicameral mind by julian jaynes [jay90] for a good discussion on how humans use the concept of metaphor to handle complexity. more directly related to computer science education and programming, see “cogito, ergo sum! cognitive processes of students dealing with data structures” by dan aharoni [aha00] for a discussion on moving from programming-context thinking to higher-level (and more design-oriented) programming-free thinking.
on a more pragmatic level, most people study data structures to write better programs. if you expect your program to work correctly and efﬁciently, it must ﬁrst be understandable to yourself and your co-workers. kernighan and pike’s the practice of programming [kp99] discusses a number of practical issues related to programming, including good coding and documentation style. for an excellent (and entertaining!) introduction to the difﬁculties involved with writing large programs, read the classic the mythical man-month: essays on software engineering by frederick p. brooks [bro95].
how can this be? programmers look at programs regularly to determine if they will halt. surely this can be automated. as a warning to those who believe any program can be analyzed in this way, carefully examine the following code fragment before reading on.
this is a famous piece of code. the sequence of values that is assigned to n by this code is sometimes called the collatz sequence for input value n. does this code fragment halt for all values of n? nobody knows the answer. every input that has been tried halts. but does it always halt? note that for this code fragment, because we do not know if it halts, we also do not know an upper bound for its running time. as for the lower bound, we can easily show Ω(log n)(see exercise 3.14).
personally, i have faith that someday some smart person will completely analyze the collitz function and prove once and for all that the code fragment halts for all values of n. doing so may well give us techniques that advance our ability to do algorithm analysis in general. unfortunately, proofs from computability — the branch of computer science that studies what is impossible to do with a computer — compel us to believe that there will always be another bit of program code that we cannot analyze. this comes as a result of the fact that the halting problem is unsolvable.
before proving that the halting problem is unsolvable, we ﬁrst prove that not all functions can be implemented as a computer program. this is so because the number of programs is much smaller than the number of possible functions.
a set is said to be countable (or countably inﬁnite if it is a set with inﬁnite members) if every member of the set can be uniquely assigned to a positive integer. a set is said to be uncountable (or uncountably inﬁnite) if it is not possible to assign every member of the set to a positive integer.
to understand what is meant when we say “assigned to a positive integer,” imagine that there is an inﬁnite row of bins, labeled 1, 2, 3, and so on. take a set and start placing members of the set into bins, with at most one member per bin. if we can ﬁnd a way to assign all of the members to bins, then the set is countable. for example, consider the set of positive even integers 2, 4, and so on. we can
17.4 further reading the classic text on the theory of np-completeness is computers and intractability: a guide to the theory of np-completeness by garey and johnston [gj79]. the traveling salesman problem, edited by lawler et al. [llks85], discusses many approaches to ﬁnding an acceptable solution to this particular np-complete problem in a reasonable amount of time.
for more information about the collatz function see “on the ups and downs of hailstone numbers” by b. hayes [hay84], and “the 3x + 1 problem and its generalizations” by j.c. lagarias [lag85].
17.1 consider this algorithm for ﬁnding the maximum element in an array: first sort the array and then select the last (maximum) element. what (if anything) does this reduction tell us about the upper and lower bounds to the problem of ﬁnding the maximum element in a sequence? why can we not reduce sorting to ﬁnding the maximum element? 17.2 use a reduction to prove that squaring an n × n matrix is just as expensive (asymptotically) as multiplying two n × n matrices. 17.3 use a reduction to prove that multiplying two upper triangular n × n matrices is just as expensive (asymptotically) as multiplying two arbitrary n × n matrices. (a) explain why computing the factorial of n by multiplying all values
(b) explain why computing an approximation to the factorial of n by making use of stirling’s formula (see section 2.2) is a polynomial time algorithm.
17.5 consider this algorithm for solving the clique problem. first, generate all subsets of the vertices containing exactly k vertices. there are o(nk) such subsets altogether. then, check whether any subgraphcs induced by these subsets is complete. if this algorithm ran in polynomial time, what would be its signiﬁcance? why is this not a polynomial-time algorithm for the clique problem?
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
distance to every such vertex, rather than recording the actual path. recording the path requires modiﬁcations to the algorithm that are left as an exercise.
computer networks provide an application for the single-source shortest-paths problem. the goal is to ﬁnd the cheapest way for one computer to broadcast a message to all other computers on the network. the network can be modeled by a graph with edge weights indicating time or cost to send a message to a neighboring computer.
for unweighted graphs (or whenever all edges have the same cost), the singlesource shortest paths can be found using a simple breadth-ﬁrst search. when weights are added, bfs will not give the correct answer.
one approach to solving this problem when the edges have differing weights might be to process the vertices in a ﬁxed order. label the vertices v0 to vn−1, with s = v0. when processing vertex v1, we take the edge connecting v0 and v1. when processing v2, we consider the shortest distance from v0 to v2 and compare that to the shortest distance from v0 to v1 to v2. when processing vertex vi, we consider the shortest path for vertices v0 through vi−1 that have already been processed. unfortunately, the true shortest path to vi might go through vertex vj for j > i. such a path will not be considered by this algorithm. however, the problem would not occur if we process the vertices in order of distance from s. assume that we have processed in order of distance from s to the ﬁrst i − 1 vertices that are closest to s; call this set of vertices s. we are now about to process the ith closest vertex; call it x. a shortest path from s to x must have its next-to-last vertex in s. thus,
in other words, the shortest path from s to x is the minimum over all paths that go from s to u, then have an edge from u to x, where u is some vertex in s.
this solution is usually referred to as dijkstra’s algorithm. it works by maintaining a distance estimate d(x) for all vertices x in v. the elements of d are initialized to the value infinite. vertices are processed in order of distance from s. whenever a vertex v is processed, d(x) is updated for every neighbor x of v. figure 11.16 shows an implementation for dijkstra’s algorithm. at the end, array d will contain the shortest distance values.
there are two reasonable solutions to the key issue of ﬁnding the unvisited vertex with minimum distance value during each pass through the main for loop. the ﬁrst method is simply to scan through the list of |v| vertices searching for the minimum value, as follows:
figure 11.19 a graph and its mst. all edges appear in the original graph. those edges drawn with heavy lines indicate the subset making up the mst. note that edge (c, f) could be replaced with edge (d, f) to form a different mst with equal cost.
the ﬁrst of our two algorithms for ﬁnding msts is commonly referred to as prim’s algorithm. prim’s algorithm is very simple. start with any vertex n in the graph, setting the mst to be n initially. pick the least-cost edge connected to n. this edge connects n to another vertex; call this m. add vertex m and edge (n, m) to the mst. next, pick the least-cost edge coming from either n or m to any other vertex in the graph. add this edge and the new vertex it reaches to the mst. this process continues, at each step expanding the mst by selecting the least-cost edge from a vertex currently in the mst to a vertex not currently in the mst.
prim’s algorithm is quite similar to dijkstra’s algorithm for ﬁnding the singlesource shortest paths. the primary difference is that we are seeking not the next closest vertex to the start vertex, but rather the next closest vertex to any vertex currently in the mst. thus we replae the lines if (d[w] > (d[v] + g->weight(v, w)))
figure 11.20 shows an implementation for prim’s algorithm that searches the distance matrix for the next closest vertex. for each vertex i, when i is processed by prim’s algorithm, an edge going to i is added to the mst that we are building.
11.10 show the shortest paths generated by running dijkstra’s shortest-paths algorithm on the graph of figure 11.25, beginning at vertex 4. show the d values as each vertex is processed, as in figure 11.18.
11.12 the root of a dag is a vertex r such that every vertex of the dag can be reached by a directed path from r. write an algorithm that takes a directed graph as input and determines the root (if there is one) for the graph. the running time of your algorithm should be Θ(|v| + |e|).
11.13 write an algorithm to ﬁnd the longest path in a dag, where the length of the path is measured by the number of edges that it contains. what is the asymptotic complexity of your algorithm? 11.14 write an algorithm to determine whether a directed graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v| + |e|) time. 11.15 write an algorithm to determine whether an undirected graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v|) time.
11.16 the single-destination shortest-paths problem for a directed graph is to ﬁnd the shortest path from every vertex to a speciﬁed vertex v. write an algorithm to solve the single-destination shortest-paths problem.
11.17 list the order in which the edges of the graph in figure 11.25 are visited when running prim’s mst algorithm starting at vertex 3. show the ﬁnal mst.
11.18 list the order in which the edges of the graph in figure 11.25 are visited when running kruskal’s mst algorithm. each time an edge is added to the mst, show the result on the equivalence array, (e.g., show the array as in figure 6.7).
2t (n − 1) + c. t (n) = Θ(2n). but... there are only n(k + 1) subproblems to solve! clearly, there are many subproblems being solved repeatedly. store a n × k + 1 matrix to contain the solutions for all p (i, k). fill in the rows from i = 0 to n, left to right.
8 9 k1 =9 o − − − − − − − − i k2 =2 o − i − − − − − − o k3 =7 o − o − − − − i k4 =4 o − o − i − i o − o k5 =1 o i o i o i o i/o i o key:
-: no solution for p (i, k). o: solution(s) for p (i, k) with i omitted. i: solution(s) for p (i, k) with i included. i/o: solutions for p (i, k) with i included and omitted.
example: m(3, 9) contains o because p (2, 9) has a solution. it contains i because p (2, 2) = p (2, 9 − 7) has a solution. how can we ﬁnd a solution to p (5, 10)? how can we ﬁnd all solutions to p (5, 10)?
we next consider the problem of ﬁnding the shortest distance between all pairs of vertices in the graph, called the all-pairs shortest-paths problem. to be precise, for every u, v ∈ v, calculate d(u, v). one solution is to run dijkstra’s algorithm |v| times, each time computing the shortest path from a different start vertex. if g is sparse (that is, |e| = Θ(|v|)) then this is a good solution, because the total cost will be Θ(|v|2 + |v||e| log |v|) = Θ(|v|2 log |v|) for the version of dijkstra’s algorithm based on priority queues.
figure 16.1 an example of k-paths in floyd’s algorithm. path 1, 3 is a 0-path by deﬁnition. path 3, 0, 2 is not a 0-path, but it is a 1-path (as well as a 2-path, a 3-path, and a 4-path) because the largest intermediate vertex is 0. path 1, 3, 2 is a 4-path, but not a 3-path because the intermediate vertex is 3. all paths in this graph are 4-paths.
for a dense graph, the priority queue version of dijkstra’s algorithm yields a cost of Θ(|v|3 log |v|), but the version using minvertex yields a cost of Θ(|v|3).
another solution that limits processing time to Θ(|v|3) regardless of the number of edges is known as floyd’s algorithm. deﬁne a k-path from vertex v to vertex u to be any path whose intermediate vertices (aside from v and u) all have indices less than k. a 0-path is deﬁned to be a direct edge from v to u. figure 16.1 illustrates the concept of k-paths.
deﬁne dk(v, u) to be the length of the shortest k-path from vertex v to vertex u. assume that we already know the shortest k-path from v to u. the shortest (k + 1)path either goes through vertex k or it does not. if it does go through k, then the best path is the best k-path from v to k followed by the best k-path from k to u. otherwise, we should keep the best k-path seen before. floyd’s algorithm simply checks all of the possibilities in a triple loop. here is the implementation for floyd’s algorithm. at the end of the algorithm, array d stores the all-pairs shortest distances.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
11.3.3 topological sort assume that we need to schedule a series of tasks, such as classes or construction jobs, where we cannot start one task until after its prerequisites are completed. we wish to organize the tasks into a linear order that allows us to complete them one at a time without violating any prerequisites. we can model the problem using a dag. the graph is directed because one task is a prerequisite of another — the vertices have a directed relationship. it is acyclic because a cycle would indicate a conﬂicting series of prerequisites that could not be completed without violating at least one prerequisite. the process of laying out the vertices of a dag in a linear order to meet the prerequisite rules is called a topological sort. figure 11.13 illustrates the problem. an acceptable topological sort for this example is j1, j2, j3, j4, j5, j6, j7.
a topological sort may be found by performing a dfs on the graph. when a vertex is visited, no action is taken (i.e., function previsit does nothing). when the recursion pops back to that vertex, function postvisit prints the vertex. this yields a topological sort in reverse order. it does not matter where the sort starts, as long as all vertices are visited in the end. here is an implementation for the dfs-based algorithm.
using this algorithm starting at j1 and visiting adjacent neighbors in alphabetic order, vertices of the graph in figure 11.13 are printed out in the order j7, j5, j4, j6, j2, j3, j1. when reversed, this yields the legal topological sort j1, j3, j2, j6, j4, j5, j7.
we can also implement topological sort using a queue instead of recursion. to do so, we ﬁrst visit all edges, counting the number of edges that lead to each vertex (i.e., count the number of prerequisites for each vertex). all vertices with no
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
1. to design an algorithm that is easy to understand, code, and debug. 2. to design an algorithm that makes efﬁcient use of the computer’s resources.
ideally, the resulting program is true to both of these goals. we might say that such a program is “elegant.” while the algorithms and program code examples presented here attempt to be elegant in this sense, it is not the purpose of this book to explicitly treat issues related to goal (1). these are primarily concerns of the discipline of software engineering. rather, this book is mostly about issues relating to goal (2).
how do we measure efﬁciency? chapter 3 describes a method for evaluating the efﬁciency of an algorithm or computer program, called asymptotic analysis. asymptotic analysis also allows you to measure the inherent difﬁculty of a problem. the remaining chapters use asymptotic analysis techniques for every algorithm presented. this allows you to see how each algorithm compares to other algorithms for solving the same problem in terms of its efﬁciency.
this ﬁrst chapter sets the stage for what is to follow, by presenting some higherorder issues related to the selection and use of data structures. we ﬁrst examine the process by which a designer selects a data structure appropriate to the task at hand. we then consider the role of abstraction in program design. we brieﬂy consider the concept of a design pattern and see some examples. the chapter ends with an exploration of the relationship between problems, algorithms, and programs.
you might think that with ever more powerful computers, program efﬁciency is becoming less important. after all, processor speed and memory size still seem to double every couple of years. won’t any efﬁciency problem we might have today be solved by tomorrow’s hardware?
as we develop more powerful computers, our history so far has always been to use additional computing power to tackle more complex problems, be it in the form of more sophisticated user interfaces, bigger problem sizes, or new problems previously deemed computationally infeasible. more complex problems demand more computation, making the need for efﬁcient programs even greater. worse yet, as tasks become more complex, they become less like our everyday experience. today’s computer scientists must be trained to have a thorough understanding of the principles behind efﬁcient program design, because their ordinary life experiences often do not apply when designing computer programs.
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
figure 8.5 an illustration of virtual memory. the complete collection of information resides in the slowe, secondary storage (on disk). those sectors recently accessed are held in the fast main memory (in ram). in this example, copies of sectors 1, 7, 5, 3, and 8 from secondary storage are currently stored in the main memory. if a memory access to sector 9 is received, one of the sectors currently in main memory must be replaced.
the buffer pool class itself. the ﬁrst approach is to pass “messages” between the two. this approach is illustrated by the following abstract class:
this simple class provides an interface with two member functions, insert and getbytes. the information is passed between the buffer pool user and the buffer pool through the space parameter. this is storage space, provided by the bufferpool client and at least sz bytes long, which the buffer pool can take information from (the insert function) or put information into (the getbytes function). parameter pos indicates where the information will be placed in the
simple lists and arrays are the right tool for the many applications. other situations require support for operations that cannot be implemented efﬁciently by the standard list representations of chapter 4. this chapter presents advanced implementations for lists and arrays that overcome some of the problems of simple linked list and contiguous array representations. a wide range of topics are covered, whose unifying thread is that the data structures are all list- or array-like. this chapter should also serve to reinforce the concept of logical representation versus physical implementation, as some of the “list” implementations have quite different organizations internally.
section 12.1 describes a series of representations for multilists, which are lists that may contain sublists. section 12.2 discusses representations for implementing sparse matrices, large matrices where most of the elements have zero values. section 12.3 discusses memory management techniques, which are essentially a way of allocating variable-length sections from a large array.
recall from chapter 4 that a list is a ﬁnite, ordered sequence of items of the form hx0, x1, ..., xn−1i where n ≥ 0. we can represent the empty list by null or hi. in chapter 4 we assumed that all list elements had the same data type. in this section, we extend the deﬁnition of lists to allow elements to be arbitrary in nature. in general, list elements are one of two types.
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
in polynomial time), and thus the problem can be solved in polynomial time by our hypothetical magical computer. another view of this concept is that if you cannot get the answer to a problem in polynomial time by guessing the right answer and then checking it, then you cannot do it in polynomial time in any other way.
the idea of “guessing” the right answer to a problem — or checking all possible solutions in parallel to determine which is correct — is called non-determinism. an algorithm that works in this manner is called a non-deterministic algorithm, and any problem with an algorithm that runs on a non-deterministic machine in polynomial time is given a special name: it is said to be a problem in np. thus, problems in np are those problems that can be solved in polynomial time on a non-deterministic machine.
not all problems requiring exponential time on a regular computer are in np. for example, towers of hanoi is not in np, because it must print out o(2n) moves for n disks. a non-deterministic machine cannot “guess” and print the correct answer in less time.
figure 17.2 illustrates this problem. five vertices are shown, with edges and associated costs between each pair of edges. (for simplicity, we assume that the cost is the same in both directions, though this need not be the case.) if the salesman visits the cities in the order abcdea, he will travel a total distance of 13. a better route would be abdcea, with cost 11. the best route for this particular graph would be abedca, with cost 9.
we cannot solve this problem in polynomial time with a guess-and-test nondeterministic computer. the problem is that, given a candidate cycle, while we can quickly check that the answer is indeed a cycle of the appropriate form, and while we can quickly calculate the length of the cycle, we have no easy way of knowing if it is in fact the shortest such cycle. however, we can solve a variant of this problem cast in the form of a decision problem. a decision problem is simply one whose answer is either yes or no. the decision problem form of traveling salesman is as follows:
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
that is, if a decision problem f yields yes on input i, then there is a language l containing string i0 where i0 is some suitable transformation of input i. conversely, if f would give answer no for input i, then i’s transformed version i0 is not in the language l.
turing machines are a simple model of computation for writing programs that are language acceptors. there is a “universal” turing machine that can take as input a description for a turing machine, and an input string, and return the execution of that machine on that string. this turing machine in turn can be cast as a boolean expression such that the expression is satisﬁable if and only if the turing machine yields accept for that string. cook used turing machines in his proof because they are simple enough that he could develop this transformation of turing machines to boolean expressions, but rich enough to be able to compute any function that a regular computer can compute. the signiﬁcance of this transformation is that any decision problem that is performable by the turing machine is transformable to sat. thus, sat is np-hard. as explained above, to show that a decision problem x is np-complete, we prove that x is in np (normally easy, and normally done by giving a suitable polynomial-time, nondeterministic algorithm) and then prove that x is np-hard. to prove that x is np-hard, we choose a known np-complete problem, say a. we describe a polynomial-time transformation that takes an arbitrary instance i of a to an instance i0 of x. we then describe a polynomial-time transformation from s0 to s such that s is the solution for i. the following example provides a model for how an np-completeness proof is done.
example 17.1 3 sat is a special case of sat. is 3 sat easier than sat? not if we can prove it to be np-complete. theorem 17.1 3 sat is np-complete. proof: prove that 3 sat is in np: guess (nondeterministically) truth values for the variables. the correctness of the guess can be veriﬁed in polynomial time. prove that 3 sat is np-hard: we need a polynomial-time reduction from sat to 3 sat. let e = c1 · c2 · ... · ck be any instance of sat. our strategy is to replace any clause ci that does not have exactly three literals
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
bit for all vertices is cleared. the mark bit for a vertex is set when the vertex is ﬁrst visited during the traversal. if a marked vertex is encountered during traversal, it is not visited a second time. this keeps the program from going into an inﬁnite loop when it encounters a cycle.
once the traversal algorithm completes, we can check to see if all vertices have been processed by checking the mark bit array. if not all vertices are marked, we can continue the traversal from another unmarked vertex. note that this process works regardless of whether the graph is directed or undirected. to ensure visiting all vertices, graphtraverse could be called as follows on a graph g:
11.3.1 depth-first search the ﬁrst method of organized graph traversal is called depth-ﬁrst search (dfs). whenever a vertex v is visited during the search, dfs will recursively visit all of v’s unvisited neighbors. equivalently, dfs will add all edges leading out of v to a stack. the next vertex to be visited is determined by popping the stack and following that edge. the effect is to follow one branch through the graph to its conclusion, then it will back up and follow another branch, and so on. the dfs process can be used to deﬁne a depth-ﬁrst search tree. this tree is composed of the edges that were followed to any new (unvisited) vertex during the traversal and leaves out the edges that lead to already visited vertices. dfs can be applied to directed or undirected graphs. here is an implementation for the dfs algorithm:
figure 11.9 a detailed illustration of the dfs process for the graph of figure 11.8(a) starting at vertex a. the steps leading to each change in the recursion stack are described.
(b) draw the adjacency list representation for the same graph. (c) if a pointer requires four bytes, a vertex label requires two bytes, and an edge weight requires two bytes, which representation requires more space for this graph?
(d) if a pointer requires four bytes, a vertex label requires one byte, and an edge weight requires two bytes, which representation requires more space for this graph?
11.4 show the dfs tree for the graph of figure 11.25, starting at vertex 1. 11.5 wright a pseudocode algorithm to create a dfs tree for an undirected, con-
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
4.19 a common problem for compilers and text editors is to determine if the parentheses (or other brackets) in a string are balanced and properly nested. for example, the string “((())())()” contains properly nested pairs of parentheses, but the string “)()(” does not, and the string “())” does not contain properly matching parentheses. (a) give an algorithm that returns true if a string contains properly nested and balanced parentheses, and false otherwise. use a stack to keep track of the number of left parentheses seen so far. hint: at no time while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses.
(b) give an algorithm that returns the position in the string of the ﬁrst offending parenthesis if the string is not properly nested and balanced. that is, if an excess right parenthesis is found, return its position; if there are too many left parentheses, return the position of the ﬁrst excess left parenthesis. return −1 if the string is properly balanced and nested. use a stack to keep track of the number and positions of left parentheses seen so far.
4.20 imagine that you are designing an application where you need to perform the operations insert, delete maximum, and delete minimum. for this application, the cost of inserting is not important, because it can be done off-line prior to startup of the time-critical section, but the performance of the two deletion operations are critical. repeated deletions of either kind must work as fast as possible. suggest a data structure that can support this application, and justify your suggestion. what is the time complexity for each of the three key operations?
4.1 a deque (pronounced “deck”) is like a queue, except that items may be added and removed from both the front and the rear. write either an array-based or linked implementation for the deque.
4.2 one solution to the problem of running out of space for an array-based list implementation is to replace the array with a larger array whenever the original array overﬂows. a good rule that leads to an implementation that is both space and time efﬁcient is to double the current size of the array when there is an overﬂow. reimplement the array-based list class of figure 4.2 to support this array-doubling rule.
there are two fundamental approaches to dealing with the relationship between a collection of actions and a hierarchy of object types. first consider the typical procedural approach. say we have a base class for page layout entities, with a subclass hierarchy to deﬁne speciﬁc subtypes (page, columns, rows, ﬁgures, characters, etc.). and say there are actions to be performed on a collection of such objects (such as rendering the objects to the screen). the procedural design approach is for each action to be implemented as a method that takes as a parameter a pointer to the base class type. each such method will traverse through the collection of objects, visiting each object in turn. each method contains something like a case statement that deﬁnes the details of the action for each subclass in the collection (e.g., page, column, row, character). we can cut the code down some by using the visitor design pattern so that we only need to write the traversal once, and then write a visitor subroutine for each action that might be applied to the collection of objects. but each such visitor subroutine must still contain logic for dealing with each of the possible subclasses.
in our page composition application, there are only a few activities that we would like to perform on the page representation. we might render the objects in full detail. or we might want a “rough draft” rendering that prints only the bounding boxes of the objects. if we come up with a new activity to apply to the collection of objects, we do not need to change any of the code that implements the existing activities. but adding new activities won’t happen often for this application. in contrast, there could be many object types, and we might frequently add new object types to our implementation. unfortunately, adding a new object type requires that we modify each activity, and the subroutines implementing the activities get rather long case statements to distinguish the behavior of the many subclasses.
an alternative design is to have each object subclass in the hierarchy embody the action for each of the various activities that might be performed. each subclass will have code to perform each activity (such as full rendering or bounding box rendering). then, if we wish to apply the activity to the collection, we simply call the ﬁrst object in the collection and specify the action (as a method call on that object). in the case of our page layout and its hierarchical collection of objects, those objects that contain other objects (such as a row objects that contains letters) will call the appropriate method for each child. if we want to add a new activity with this organization, we have to change the code for every subclass. but this is relatively rare for our text compositing application. in contrast, adding a new object into the subclass hierarchy (which for this application is far more likely than adding a new rendering function) is easy. adding a new subclass does not require changing
this second design approach of burying the functional activity in the subclasses is called the composite design pattern. a detailed example for using the composite design pattern is presented in section 5.3.1.
our ﬁnal example of a design pattern lets us encapsulate and make interchangeable a set of alternative actions that might be performed as part of some larger activity. again continuing our text compositing example, each output device that we wish to render to will require its own function for doing the actual rendering. that is, the objects will be broken down into constituent pixels or strokes, but the actual mechanics of rendering a pixel or stroke will depend on the output device. we don’t want to build this rendering functionality into the object subclasses. instead, we want to pass to the subroutine performing the rendering action a method or class that does the appropriate rendering details for that output device. that is, we wish to hand to the object the appropriate “strategy” for accomplishing the details of the rendering task. thus, we call this approach the strategy design pattern.
the strategy design pattern will be discussed further in chapter 7. there, a sorting function is given a class (called a comparator) that understands how to extract and compare the key values for records to be sorted. in this way, the sorting function does not need to know any details of how its record type is implemented. one of the biggest challenges to understanding design patterns is that many of them appear to be pretty much the same. for example, you might be confused about the difference between the composite pattern and the visitor pattern. the distinction is that the composite design pattern is about whether to give control of the traversal process to the nodes of the tree or to the tree itself. both approaches can make use of the visitor design pattern to avoid rewriting the traversal function many times, by encapsulating the activity performed at each node.
but isn’t the strategy design pattern doing the same thing? the difference between the visitor pattern and the strategy pattern is more subtle. here the difference is primarily one of intent and focus. in both the strategy design pattern and the visitor design pattern, an activity is being passed in as a parameter. the strategy design pattern is focused on encapsulating an activity that is part of a larger process, so that different ways of performing that activity can be substituted. the visitor design pattern is focused on encapsulating an activity that will be performed on all members of a collection so that completely different activities can be substituted within a generic method that accesses all of the collection members.
figure 5.10 presents a sample node implementation. it includes two classes derived from class varbinnode, named leafnode and intlnode. class intlnode accesses its children through pointers of type varbinnode. function traverse illustrates the use of these classes. where traverse calls method “root.isleaf(),” java’s runtime environment determines which subclass this particular instance of root happens to be and calls that subclass’s version of isleaf. method isleaf then provides the actual node type to its caller. the other member functions for the derived subclasses are accessed by type-casting the base class pointer as appropriate, as shown in function traverse.
there is another approach that we can take to represent separate leaf and internal nodes, also using a virtual base class and separate node classes for the two types. this is to implement nodes using the composite design pattern. this approach is noticeably different from the one of figure 5.10 in that the node classes themselves implement the functionality of traverse. figure 5.11 shows the implementation. here, base class varbinnode declares a member function traverse that each subclass must implement. each subclass then implements its own appropriate behavior for its role in a traversal. the whole traversal process is called by invoking traverse on the root node, which in turn invokes traverse on its children.
when comparing the implementations of figures 5.10 and 5.11, each has advantages and disadvantages. the ﬁrst does not require that the node classes know about the traverse function. with this approach, it is easy to add new methods to the tree class that do other traversals or other operations on nodes of the tree. however, we see that traverse in figure 5.10 does need to be familiar with each node subclass. adding a new node subclass would therefore require modiﬁcations to the traverse function. in contrast, the approach of figure 5.11 requires that any new operation on the tree that requires a traversal also be implemented in the node subclasses. on the other hand, the approach of figure 5.11 avoids the need for the traverse function to know anything about the distinct abilities of the node subclasses. those subclasses handle the responsibility of performing a traversal on themselves. a secondary beneﬁt is that there is no need for traverse to explicitly enumerate all of the different node subclasses, directing appropriate action for each. with only two node classes this is a minor point. but if there were many such subclasses, this could become a bigger problem. a disadvantage is that the traversal operation must not be called on a null pointer, because there is no object to catch the call. this problem could be avoided by using a ﬂyweight to implement empty nodes.
typically, the version of figure 5.10 would be preferred in this example if traverse is a member function of the tree class, and if the node subclasses are
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
figure 1.1 the relationship between data items, abstract data types, and data structures. the adt deﬁnes the logical form of the data type. the data structure implements the physical form of the data type.
a design pattern embodies and generalizes important design concepts for a recurring problem. a primary goal of design patterns is to quickly transfer the knowledge gained by expert designers to newer programmers. another goal is to allow for efﬁcient communication between programmers. its much easier to discuss a design issue when you share a vocabulary relevant to the topic.
speciﬁc design patterns emerge from the discovery that a particular design problem appears repeatedly in many contexts. they are meant to solve real problems. design patterns are a bit like generics: they describe the structure for a design solution, with the details ﬁlled in for any given problem. design patterns are a bit like data structures: each one provides costs and beneﬁts, which implies that tradeoffs are possible. therefore, a given design pattern might have variations on its application to match the various tradeoffs inherent in a given situation.
the flyweight design pattern is meant to solve the following problem. you have an application with many objects. some of these objects are identical in the information that they contain, and the role that they play. but they must be reached from various places, and conceptually they really are distinct objects. because so much information is shared, we would like to take advantage of the opportunity to reduce memory cost by sharing space. an example comes from representing the
layout for a document. the letter “c” might reasonably be represented by an object that describes that character’s strokes and bounding box. however, we don’t want to create a separate “c” object everywhere in the document that a “c” appears. the solution is to allocate a single copy of the shared representation for “c” object. then, every place in the document that needs a “c” in a given font, size, and typeface will reference this single copy. the various instances of references to “c” are called ﬂyweights. a ﬂyweight includes the reference to the shared information, and might include additional information speciﬁc to that instance.
we could imagine describing the layout of text on a page by using a tree structure. the root of the tree is a node representing the page. the page has multiple child nodes, one for each column. the column nodes have child nodes for each row. and the rows have child nodes for each character. these representations for characters are the ﬂyweights. the ﬂyweight includes the reference to the shared shape information, and might contain additional information speciﬁc to that instance. for example, each instance for “c” will contain a reference to the shared information about strokes and shapes, and it might also contain the exact location for that instance of the character on the page.
flyweights are used in the implementation for the pr quadtree data structure for storing collections of point objects, described in section 13.3. in a pr quadtree, we again have a tree with leaf nodes. many of these leaf nodes (the ones that represent empty areas) contain the same information. these identical nodes can be implemented using the flyweight design pattern for better memory efﬁciency.
given a tree of objects to describe a page layout, we might wish to perform some activity on every node in the tree. section 5.2 discusses tree traversal, which is the process of visiting every node in the tree in a deﬁned order. a simple example for our text composition application might be to count the number of nodes in the tree that represents the page. at another time, we might wish to print a listing of all the nodes for debugging purposes.
we could write a separate traversal function for each such activity that we intend to perform on the tree. a better approach would be to write a generic traversal function, and pass in the activity to be performed at each node. this organization constitutes the visitor design pattern. the visitor design pattern is used in sections 5.2 (tree traversal) and 11.3 (graph traversal).
figure 5.10 presents a sample node implementation. it includes two classes derived from class varbinnode, named leafnode and intlnode. class intlnode accesses its children through pointers of type varbinnode. function traverse illustrates the use of these classes. where traverse calls method “root.isleaf(),” java’s runtime environment determines which subclass this particular instance of root happens to be and calls that subclass’s version of isleaf. method isleaf then provides the actual node type to its caller. the other member functions for the derived subclasses are accessed by type-casting the base class pointer as appropriate, as shown in function traverse.
there is another approach that we can take to represent separate leaf and internal nodes, also using a virtual base class and separate node classes for the two types. this is to implement nodes using the composite design pattern. this approach is noticeably different from the one of figure 5.10 in that the node classes themselves implement the functionality of traverse. figure 5.11 shows the implementation. here, base class varbinnode declares a member function traverse that each subclass must implement. each subclass then implements its own appropriate behavior for its role in a traversal. the whole traversal process is called by invoking traverse on the root node, which in turn invokes traverse on its children.
when comparing the implementations of figures 5.10 and 5.11, each has advantages and disadvantages. the ﬁrst does not require that the node classes know about the traverse function. with this approach, it is easy to add new methods to the tree class that do other traversals or other operations on nodes of the tree. however, we see that traverse in figure 5.10 does need to be familiar with each node subclass. adding a new node subclass would therefore require modiﬁcations to the traverse function. in contrast, the approach of figure 5.11 requires that any new operation on the tree that requires a traversal also be implemented in the node subclasses. on the other hand, the approach of figure 5.11 avoids the need for the traverse function to know anything about the distinct abilities of the node subclasses. those subclasses handle the responsibility of performing a traversal on themselves. a secondary beneﬁt is that there is no need for traverse to explicitly enumerate all of the different node subclasses, directing appropriate action for each. with only two node classes this is a minor point. but if there were many such subclasses, this could become a bigger problem. a disadvantage is that the traversal operation must not be called on a null pointer, because there is no object to catch the call. this problem could be avoided by using a ﬂyweight to implement empty nodes.
typically, the version of figure 5.10 would be preferred in this example if traverse is a member function of the tree class, and if the node subclasses are
for each of the following scenarios, which of these choices would be best? explain your answer. (a) the records are guaranteed to arrive already sorted from lowest to highest (i.e., whenever a record is inserted, its key value will always be greater than that of the last record inserted). a total of 1000 inserts will be interspersed with 1000 searches.
(b) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1,000,000 insertions are performed, followed by 10 searches.
(c) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are interspersed with 1000 searches.
(d) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are performed, followed by 1,000,000 searches.
5.2 one way to deal with the “problem” of null pointers in binary trees is to use that space for some other purpose. one example is the threaded binary tree. extending the node implementation of figure 5.7, the threaded binary tree stores with each node two additional bit ﬁelds that indicate if the child pointers lc and rc are regular pointers to child nodes or threads. if lc is not a pointer to a non-empty child (i.e., if it would be null in a regular binary tree), then it instead stores a pointer to the inorder predecessor of that node. the inorder predecessor is the node that would be printed immediately before the current node in an inorder traversal. if rc is not a pointer to a child, then it instead stores a pointer to the node’s inorder successor. the inorder successor is the node that would be printed immediately after the current node in an inorder traversal. the main advantage of threaded binary trees is that operations such as inorder traversal can be implemented without using recursion or a stack. reimplement the bst as a threaded binary tree, and include a non-recursive version of the preorder traversal
5.3 implement a city database using a bst to store the database records. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates.
this second design approach of burying the functional activity in the subclasses is called the composite design pattern. a detailed example for using the composite design pattern is presented in section 5.3.1.
our ﬁnal example of a design pattern lets us encapsulate and make interchangeable a set of alternative actions that might be performed as part of some larger activity. again continuing our text compositing example, each output device that we wish to render to will require its own function for doing the actual rendering. that is, the objects will be broken down into constituent pixels or strokes, but the actual mechanics of rendering a pixel or stroke will depend on the output device. we don’t want to build this rendering functionality into the object subclasses. instead, we want to pass to the subroutine performing the rendering action a method or class that does the appropriate rendering details for that output device. that is, we wish to hand to the object the appropriate “strategy” for accomplishing the details of the rendering task. thus, we call this approach the strategy design pattern.
the strategy design pattern will be discussed further in chapter 7. there, a sorting function is given a class (called a comparator) that understands how to extract and compare the key values for records to be sorted. in this way, the sorting function does not need to know any details of how its record type is implemented. one of the biggest challenges to understanding design patterns is that many of them appear to be pretty much the same. for example, you might be confused about the difference between the composite pattern and the visitor pattern. the distinction is that the composite design pattern is about whether to give control of the traversal process to the nodes of the tree or to the tree itself. both approaches can make use of the visitor design pattern to avoid rewriting the traversal function many times, by encapsulating the activity performed at each node.
but isn’t the strategy design pattern doing the same thing? the difference between the visitor pattern and the strategy pattern is more subtle. here the difference is primarily one of intent and focus. in both the strategy design pattern and the visitor design pattern, an activity is being passed in as a parameter. the strategy design pattern is focused on encapsulating an activity that is part of a larger process, so that different ways of performing that activity can be substituted. the visitor design pattern is focused on encapsulating an activity that will be performed on all members of a collection so that completely different activities can be substituted within a generic method that accesses all of the collection members.
layout for a document. the letter “c” might reasonably be represented by an object that describes that character’s strokes and bounding box. however, we don’t want to create a separate “c” object everywhere in the document that a “c” appears. the solution is to allocate a single copy of the shared representation for “c” object. then, every place in the document that needs a “c” in a given font, size, and typeface will reference this single copy. the various instances of references to “c” are called ﬂyweights. a ﬂyweight includes the reference to the shared information, and might include additional information speciﬁc to that instance.
we could imagine describing the layout of text on a page by using a tree structure. the root of the tree is a node representing the page. the page has multiple child nodes, one for each column. the column nodes have child nodes for each row. and the rows have child nodes for each character. these representations for characters are the ﬂyweights. the ﬂyweight includes the reference to the shared shape information, and might contain additional information speciﬁc to that instance. for example, each instance for “c” will contain a reference to the shared information about strokes and shapes, and it might also contain the exact location for that instance of the character on the page.
flyweights are used in the implementation for the pr quadtree data structure for storing collections of point objects, described in section 13.3. in a pr quadtree, we again have a tree with leaf nodes. many of these leaf nodes (the ones that represent empty areas) contain the same information. these identical nodes can be implemented using the flyweight design pattern for better memory efﬁciency.
given a tree of objects to describe a page layout, we might wish to perform some activity on every node in the tree. section 5.2 discusses tree traversal, which is the process of visiting every node in the tree in a deﬁned order. a simple example for our text composition application might be to count the number of nodes in the tree that represents the page. at another time, we might wish to print a listing of all the nodes for debugging purposes.
we could write a separate traversal function for each such activity that we intend to perform on the tree. a better approach would be to write a generic traversal function, and pass in the activity to be performed at each node. this organization constitutes the visitor design pattern. the visitor design pattern is used in sections 5.2 (tree traversal) and 11.3 (graph traversal).
there are two fundamental approaches to dealing with the relationship between a collection of actions and a hierarchy of object types. first consider the typical procedural approach. say we have a base class for page layout entities, with a subclass hierarchy to deﬁne speciﬁc subtypes (page, columns, rows, ﬁgures, characters, etc.). and say there are actions to be performed on a collection of such objects (such as rendering the objects to the screen). the procedural design approach is for each action to be implemented as a method that takes as a parameter a pointer to the base class type. each such method will traverse through the collection of objects, visiting each object in turn. each method contains something like a case statement that deﬁnes the details of the action for each subclass in the collection (e.g., page, column, row, character). we can cut the code down some by using the visitor design pattern so that we only need to write the traversal once, and then write a visitor subroutine for each action that might be applied to the collection of objects. but each such visitor subroutine must still contain logic for dealing with each of the possible subclasses.
in our page composition application, there are only a few activities that we would like to perform on the page representation. we might render the objects in full detail. or we might want a “rough draft” rendering that prints only the bounding boxes of the objects. if we come up with a new activity to apply to the collection of objects, we do not need to change any of the code that implements the existing activities. but adding new activities won’t happen often for this application. in contrast, there could be many object types, and we might frequently add new object types to our implementation. unfortunately, adding a new object type requires that we modify each activity, and the subroutines implementing the activities get rather long case statements to distinguish the behavior of the many subclasses.
an alternative design is to have each object subclass in the hierarchy embody the action for each of the various activities that might be performed. each subclass will have code to perform each activity (such as full rendering or bounding box rendering). then, if we wish to apply the activity to the collection, we simply call the ﬁrst object in the collection and specify the action (as a method call on that object). in the case of our page layout and its hierarchical collection of objects, those objects that contain other objects (such as a row objects that contains letters) will call the appropriate method for each child. if we want to add a new activity with this organization, we have to change the code for every subclass. but this is relatively rare for our text compositing application. in contrast, adding a new object into the subclass hierarchy (which for this application is far more likely than adding a new rendering function) is easy. adding a new subclass does not require changing
another issue to consider when designing a traversal is how to deﬁne the visitor function that is to be executed on every node. one approach is simply to write a new version of the traversal for each such visitor function as needed. the disadvantage to this is that whatever function does the traversal must have access to the binnode class. it is probably better design to permit only the tree class to have access to the binnode class.
another approach is for the tree class to supply a generic traversal function which takes the visitor as a function parameter. this is known as the visitor design pattern. a major contraint on this approach is that the signature for all visitor functions, that is, their return type and parameters, must be ﬁxed in advance. thus, the designer of the generic traversal function must be able to adequately judge what parameters and return type will likely be needed by potential visitor functions.
properly handling information ﬂow between parts of a program can often be a signiﬁcant design challenge. this issue tends to be particularly confusing when dealing with recursive functions such as tree traversals. in general, we can run into trouble either with passing in the correct information needed by the function to do its work, or with returning information to the recursive function’s caller. we will see many examples throughout the book that illustrate methods for passing information in and out of recursive functions as they traverse a tree structure. before leaving this section, we will study a few simple examples.
example 5.4 we wish to write a function that counts the number of nodes in a binary tree. the key insight is that the total count for any (non-empty) subtree is one for the root plus the counts for the left and right subtrees. we can implement the function as follows.
another problem that occurs when recursively processing data collections is controlling which members of the collection will be visited. for example, some tree “traversals” might in fact visit only some tree nodes, while avoiding processing of others. exercise 5.20 must solve exactly this problem in the context of a binary search tree. it must visit only those children of a given node that might possibly
this implementation contains calls to functions previsit and postvisit. these functions specify what activity should take place during the search. just as a preorder tree traversal requires action before the subtrees are visited, some graph traversals require that a vertex be processed before ones further along in the dfs. alternatively, some applications require activity after the remaining vertices are processed; hence the call to function postvisit. this would be a natural opportunity to make use of the visitor design pattern described in section 1.3.2.
dfs processes each edge once in a directed graph. in an undirected graph, dfs processes each edge from both directions. each vertex must be visited, but only once, so the total cost is Θ(|v| + |e|).
our second graph traversal algorithm is known as a breadth-ﬁrst search (bfs). bfs examines all vertices connected to the start vertex before visiting vertices further away. bfs is implemented similarly to dfs, except that a queue replaces the recursion stack. note that if the graph is a tree and the start vertex is at the root, bfs is equivalent to visiting vertices level by level from top to bottom. figure 11.10 provides an implementation for the bfs algorithm. figure 11.11 shows a graph and the corresponding breadth-ﬁrst search tree. figure 11.12 illustrates the bfs process for the graph of figure 11.11(a).
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
in this example, the list has four elements. the second element is the sublist hy1,ha1, a2i, y3i and the third is the sublist hz1, z2i. the sublist hy1,ha1, a2i, y3i itself contains a sublist. if a list l has one or more sublists, we call l a multilist. lists with no sublists are often referred to as linear lists or chains. note that this deﬁnition for multilist ﬁts well with our deﬁnition of sets from deﬁnition 2.1, where a set’s members can be either primitive elements or sets.
we can restrict the sublists of a multilist in various ways, depending on whether the multilist should have the form of a tree, a dag, or a generic graph. a pure list is a list structure whose graph corresponds to a tree, such as in figure 12.1. in other words, there is exactly one path from the root to any node, which is equivalent to saying that no object may appear more than once in the list. in the pure list, each pair of angle brackets corresponds to an internal node of the tree. the members of the list correspond to the children for the node. atoms on the list correspond to leaf nodes.
a reentrant list is a list structure whose graph corresponds to a dag. nodes might be accessible from the root by more than one path, which is equivalent to saying that objects (including sublists) may appear multiple times in the list as long as no cycles are formed. all edges point downward, from the node representing a list or sublist to its elements. figure 12.2 illustrates a reentrant list. to write out
figure 12.17 garbage cycle example. all memory elements in the cycle have non-zero reference counts because each element has one pointer to it, even though the entire cycle is garbage.
another approach to garbage collection is the mark/sweep strategy. here, each memory object needs only a single mark bit rather than a reference counter ﬁeld. when free store is exhausted, a separate garbage collection phase takes place as follows.
1. clear all mark bits. 2. perform depth-ﬁrst search (dfs) following pointers from each variable on the system’s list of variables. each memory element encountered during the dfs has its mark bit turned on.
the advantages of the mark/sweep approach are that it needs less space than is necessary for reference counts, and it works for cycles. however, there is a major disadvantage. this is a “hidden” space requirement needed to do the processing. dfs is a recursive algorithm: either it must be implemented recursively, in which case the compiler’s runtime system maintains a stack, or else the memory manager can maintain its own stack. what happens if all memory is contained in a single linked list? then the depth of the recursion (or the size of the stack) is the number of memory cells! unfortunately, the space for the dfs stack must be available at the worst conceivable time, that is, when free memory has been exhausted.
fortunately, a clever technique allows dfs to be performed without requiring additional space for a stack. instead, the structure being traversed is used to hold the stack. at each step deeper into the traversal, instead of storing a pointer on the stack, we “borrow” the pointer being followed. this pointer is set to point back to the node we just came from in the previous step, as illustrated by figure 12.18. each borrowed pointer stores an additional bit to tell us whether we came down the left branch or the right branch of the link node being pointed to. at any given instant we have passed down only one path from the root, and we can follow the trail of pointers back up. as we return (equivalent to popping the recursion stack), we set the pointer back to its original position so as to return the structure to its
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
earlier chapters presented basic data structures and algorithms that operate on data stored in main memory. some applications require that large amounts of information be stored and processed — so much information that it cannot all ﬁt into main memory. in that case, the information must reside on disk and be brought into main memory selectively for processing.
you probably already realize that main memory access is much faster than access to data stored on disk or other storage devices. the relative difference in access times is so great that efﬁcient disk-based programs require a different approach to algorithm design than most programmers are used to. as a result, many programmers do a poor job when it comes to ﬁle processing applications.
this chapter presents the fundamental issues relating to the design of algorithms and data structures for disk-based applications.1 we begin with a description of the signiﬁcant differences between primary memory and secondary storage. section 8.2 discusses the physical aspects of disk drives. section 8.3 presents basic methods for managing buffer pools. section 8.4 discusses the java model for random access to data stored on disk. section 8.5 discusses the basic principles for sorting collections of records too large to ﬁt in main memory.
1computer technology changes rapidly. i provide examples of disk drive speciﬁcations and other hardware performance numbers that are reasonably up to date as of the time when the book was written. when you read it, the numbers might seem out of date. however, the basic principles do not change. the approximate ratios for time, space, and cost between memory and disk have remained surprisingly steady for over 20 years.
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
8.11 at the end of 2004, the fastest disk drive i could ﬁnd speciﬁcations for was the maxtor atlas. this drive had a nominal capacity of 73.4gb using 4 platters (8 surfaces) or 9.175gb/surface. assume there are 16,384 tracks with an average of 1170 sectors/track and 512 bytes/sector.3 the disk turns at 15,000 rpm. the track-to-track seek time is 0.4 ms and the average seek time is 3.6 ms. how long will it take on average to read a 6mb ﬁle, assuming that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on contiguous tracks, and that the ﬁle completely ﬁlls each track on which it is found. show your calculations.
8.12 using the speciﬁcations for the disk drive given in exercise 8.11, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
8.15 assume that a ﬁle contains one million records sorted by key value. a query to the ﬁle returns a single record containing the requested key value. files are stored on disk in sectors each containing 100 records. assume that the average time to read a sector selected at random is 10.0 ms. in contrast, it takes only 2.0 ms to read the sector adjacent to the current position of the i/o head. the “batch” algorithm for processing queries is to ﬁrst sort the queries by order of appearance in the ﬁle, and then read the entire ﬁle sequentially, processing all queries in sequential order as the ﬁle is read. this algorithm implies that the queries must all be available before processing begins. the “interactive” algorithm is to process each query in order of its arrival, searching for the requested sector each time (unless by chance two queries in a row are to the same sector). carefully deﬁne under what conditions the batch method is more efﬁcient than the interactive method.
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
a great discussion on external sorting methods can be found in salzberg’s book. the presentation in this chapter is similar in spirit to salzberg’s.
for details on disk drive modeling and measurement, see the article by ruemmler and wilkes, “an introduction to disk drive modeling” [rw94]. see andrew s. tanenbaum’s structured computer organization [tan06] for an introduction to computer hardware and organization. an excellent, detailed description of memory and hard disk drives can be found online at “the pc guide,” by charles m. kozierok [koz05] (www.pcguide.com). the pc guide also gives detailed descriptions of the microsoft windows and unix (linux) ﬁle systems.
see “outperforming lru with an adaptive replacement cache algorithm” by megiddo and modha for an example of a more sophisticated algorithm than lru for managing buffer pools.
8.1 computer memory and storage prices change rapidly. find out what the current prices are for the media listed in figure 8.1. does your information change any of the basic conclusions regarding disk processing?
8.2 assume a disk drive from the late 1990s is conﬁgured as follows. the total storage is approximately 675mb divided among 15 surfaces. each surface has 612 tracks; there are 144 sectors/track, 512 bytes/sector, and 8 sectors/cluster. the disk turns at 3600 rpm. the track-to-track seek time is 20 ms, and the average seek time is 80 ms. now assume that there is a 360kb ﬁle on the disk. on average, how long does it take to read all of the data in the ﬁle? assume that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on adjacent tracks, and that the ﬁle completely ﬁlls each track on which it is found. a seek must be performed each time the i/o head moves to a new track. show your calculations.
8.3 using the speciﬁcations for the disk drive given in exercise 8.2, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
of the data on disk. likewise, when writing to a particular logical byte position with respect to the beginning of the ﬁle, this position must be converted by the ﬁle manager into the corresponding physical location on the disk. to gain some appreciation for the the approximate time costs for these operations, you need to understand the physical structure and basic workings of a disk drive.
disk drives are often referred to as direct access storage devices. this means that it takes roughly equal time to access any record in the ﬁle. this is in contrast to sequential access storage devices such as tape drives, which require the tape reader to process data from the beginning of the tape until the desired position has been reached. as you will see, the disk drive is only approximately direct access: at any given time, some records are more quickly accessible than others.
8.2.1 disk drive architecture a hard disk drive is composed of one or more round platters, stacked one on top of another and attached to a central spindle. platters spin continuously at a constant rate. each usable surface of each platter is assigned a read/write head or i/o head through which data are read or written, somewhat like the arrangement of a phonograph player’s arm “reading” sound from a phonograph record. unlike a phonograph needle, the disk read/write head does not actually touch the surface of a hard disk. instead, it remains slightly above the surface, and any contact during normal operation would damage the disk. this distance is very small, much smaller than the height of a dust particle. it can be likened to a 5000-kilometer airplane trip across the united states, with the plane ﬂying at a height of one meter!
a hard disk drive typically has several platters and several read/write heads, as shown in figure 8.2(a). each head is attached to an arm, which connects to the boom. the boom moves all of the heads in or out together. when the heads are in some position over the platters, there are data on each platter directly accessible to each head. the data on a single platter that are accessible to any one position of the head for that platter are collectively called a track, that is, all data on a platter that are a ﬁxed distance from the spindle, as shown in figure 8.2(b). the collection of all tracks that are a ﬁxed distance from the spindle is called a cylinder. thus, a cylinder is all of the data that can be read when the arms are in a particular position. each track is subdivided into sectors. between each sector there are intersector gaps in which no data are stored. these gaps allow the read head to recognize the end of a sector. note that each sector contains the same amount of data. because the outer tracks have greater length, they contain fewer bits per inch than do the inner tracks. thus, about half of the potential storage space is wasted, because only the innermost tracks are stored at the highest possible data density. this ar-
in the index. inverted lists reduce this problem, but they are only suitable for secondary key indices with many fewer secondary key values than records. the linear index would perform well as a primary key index if it could somehow be broken into pieces such that individual updates affect only a part of the index. this concept will be pursued throughout the rest of this chapter, eventually culminating in the b+-tree, the most widely used indexing method today. but ﬁrst, we begin by studying isam, an early attempt to solve the problem of large databases requiring frequent update. its weaknesses help to illustrate why the b+-tree works so well.
before the invention of effective tree indexing schemes, a variety of disk-based indexing methods were in use. all were rather cumbersome, largely because no adequate method for handling updates was known. typically, updates would cause the index to degrade in performance. isam is one example of such an index and was widely used by ibm prior to adoption of the b-tree.
isam is based on a modiﬁed form of the linear index, as illustrated by figure 10.6. records are stored in sorted order by primary key. the disk ﬁle is divided among a number of cylinders on disk.1 each cylindar holds a section of the list in sorted order. initially, each cylinder is not ﬁlled to capacity, and the extra space is set aside in the cylinder overﬂow. in memory is a table listing the lowest key value stored in each cylinder of the ﬁle. each cylinder contains a table listing the lowest
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
sort routine such as the unix qsort function. interestingly, quicksort is hampered by exceedingly poor worst-case performance, thus making it inappropriate for certain applications.
before we get to quicksort, consider for a moment the practicality of using a binary search tree for sorting. you could insert all of the values to be sorted into the bst one by one, then traverse the completed tree using an inorder traversal. the output would form a sorted list. this approach has a number of drawbacks, including the extra space required by bst pointers and the amount of time required to insert nodes into the tree. however, this method introduces some interesting ideas. first, the root of the bst (i.e., the ﬁrst node inserted) splits the list into two sublits: the left subtree contains those values in the list less than the root value while the right subtree contains those values in the list greater than or equal to the root value. thus, the bst implicitly implements a “divide and conquer” approach to sorting the left and right subtrees. quicksort implements this concept in a much more efﬁcient way.
quicksort ﬁrst selects a value called the pivot. assume that the input array contains k values less than the pivot. the records are then rearranged in such a way that the k values less than the pivot are placed in the ﬁrst, or leftmost, k positions in the array, and the values greater than or equal to the pivot are placed in the last, or rightmost, n − k positions. this is called a partition of the array. the values placed in a given partition need not (and typically will not) be sorted with respect to each other. all that is required is that all values end up in the correct partition. the pivot value itself is placed in position k. quicksort then proceeds to sort the resulting subarrays now on either side of the pivot, one of size k and the other of size n − k − 1. how are these values sorted? because quicksort is such a good algorithm, using quicksort on the subarrays would be appropriate.
unlike some of the sorts that we have seen earlier in this chapter, quicksort might not seem very “natural” in that it is not an approach that a person is likely to use to sort real objects. but it should not be too suprising that a really efﬁcient sort for huge numbers of abstract objects on a computer would be rather different from our experiences with sorting a relatively few physical objects.
the java code for quicksort is as follows. parameters i and j deﬁne the left and right indices, respectively, for the subarray being sorted. the initial call to quicksort would be qsort(array, 0, n-1).
figure 7.8 illustrates partition. initially, variables l and r are immediately outside the actual bounds of the subarray being partitioned. each pass through the outer do loop moves the counters l and r inwards, until eventually they meet. note that at each iteration of the inner while loops, the bounds are moved prior to checking against the pivot value. this ensures that progress is made by each while loop, even when the two values swapped on the last iteration of the do loop were equal to the pivot. also note the check that r > l in the second while loop. this ensures that r does not run off the low end of the partition in the case where the pivot is the least value in that partition. function partition returns the ﬁrst index of the right partition so that the subarray bound for the recursive calls to qsort can be determined. figure 7.9 illustrates the complete quicksort algorithm.
to analyze quicksort, we ﬁrst analyze the findpivot and partition functions operating on a subarray of length k. clearly, findpivot takes constant time. function partition contains a do loop with two nested while loops. the total cost of the partition operation is constrained by how far l and r can move inwards. in particular, these two bounds variables together can move a total of s steps for a subarray of length s. however, this does not directly tell us how much work is done by the nested while loops. the do loop as a whole is guaranteed to move both l and r inward at least one position on each ﬁrst pass. each while loop moves its variable at least once (except in the special case where r is at the left edge of the array, but this can happen only once). thus, we see that the do loop can be executed at most s times, the total amount of work done moving l and r is s, and each while loop can fail its test at most s times. the total work for the entire partition function is therefore Θ(s).
knowing the cost of findpivot and partition, we can determine the cost of quicksort. we begin with a worst-case analysis. the worst case will occur when the pivot does a poor job of breaking the array, that is, when there are no elements in one partition, and n − 1 elements in the other. in this case, the divide and conquer strategy has done a poor job of dividing, so the conquer phase will work on a subproblem only one less than the size of the original problem. if this
the initial cn term is the cost of doing the findpivot and partition steps, for some constant c. the closed-form solution to this recurrence relation is Θ(n log n). thus, quicksort has average-case cost Θ(n log n).
this is an unusual situation that the average case cost and the worst case cost have asymptotically different growth rates. consider what “average case” actually means. we compute an average cost for inputs of size n by summing up for every possible input of size n the product of the running time cost of that input times the probability that that input will occur. to simplify things, we assumed that every permutation is equally likely to occur. thus, ﬁnding the average means summing up the cost for every permutation and dividing by the number of inputs (n!). we know that some of these n! inputs cost o(n2). but the sum of all the permutation costs has to be (n!)(o(n log n)). given the extremely high cost of the worst inputs, there must be very few of them. in fact, there cannot be a constant fraction of the inputs with cost o(n2). even, say, 1% of the inputs with cost o(n2) would lead to an average cost of o(n2). thus, as n grows, the fraction of inputs with high cost must be going toward a limit of zero. we can conclude that quicksort will always have good behavior if we can avoid those very few bad input permutations.
the running time for quicksort can be improved (by a constant factor), and much study has gone into optimizing this algorithm. the most obvious place for improvement is the findpivot function. quicksort’s worst case arises when the pivot does a poor job of splitting the array into equal size subarrays. if we are willing to do more work searching for a better pivot, the effects of a bad pivot can be decreased or even eliminated. one good choice is to use the “median of three” algorithm, which uses as a pivot the middle of three randomly selected values. using a random number generator to choose the positions is relatively expensive, so a common compromise is to look at the ﬁrst, middle, and last positions of the current subarray. however, our simple findpivot function that takes the middle value as its pivot has the virtue of making it highly unlikely to get a bad input by chance, and it is quite cheap to implement. this is in sharp contrast to selecting the ﬁrst or last element as the pivot, which would yield bad performance for many permutations that are nearly sorted or nearly reverse sorted.
a signiﬁcant improvement can be gained by recognizing that quicksort is relatively slow when n is small. this might not seem to be relevant if most of the time we sort large arrays, nor should it matter how long quicksort takes in the rare instance when a small array is sorted because it will be fast anyway. but you should notice that quicksort itself sorts many, many small arrays! this happens as a natural by-product of the divide and conquer approach.
element in l, that is, we check elements l[j], l[2j], and so on. so long as k is greater than the values we are checking, we continue on. but when we reach a value in l greater than k, we do a linear search on the piece of length j − 1 that we know brackets k if it is in the list. if mj ≤ n < (m+1)j, then the total cost of this algorithm is at most m+ j−1 3-way comparisons. therefore, the cost to run the algorithm on n items with a jump of size j is
take the derivative and solve for f0(j) = 0 to ﬁnd the minimum, which is √ this example teaches us some lessons about algorithm design. we want to balance the work done while selecting a sublist with the work done while searching a sublist. in general, it is a good strategy to make subproblems of equal effort. this is an example of a divide and conquer algorithm. what if we extend this idea to three levels? we would ﬁrst make jumps of some size j to ﬁnd a sublist of size j − 1 whose end vlaues bracket value k. ww would then work through this sublist by making jumps of some smaller size, say j1. finally, once we ﬁnd a bracketed sublist of size j1 − 1, we would do sequential search to complete the process.
this probably sounds convoluted to do two levels of jumping to be followed by a sequential search. while it might make sense to do a two-level algorithm (that is, jump search jumps to ﬁnd a sublist and then does sequential search on the sublist), it almost never seems to make sense to do a three-level algorithm. instead, when we go beyond two levels, we nearly always generalize by using recursion. this leads us to the most commonly used search algorithm for sorted arrays, the binary search described in section 3.5.
if we know nothing about the distribution of key values, then binary search is the best algorithm available for searching a sorted array (see exercise 9.22). however, sometimes we do know something about the expected key distribution. consider the typical behavior of a person looking up a word in a large dictionary. most people certainly do not use sequential search! typically, people use a modiﬁed form of binary search, at least until they get close to the word that they are
example 14.8 our next example comes from the algorithm to build a heap. recall from section 5.5 that to build a heap, we ﬁrst heapify the two subheaps, then push down the root to its proper position. the cost is:
14.2.3 divide and conquer recurrences the third approach to solving recurrences is to take advantage of known theorems that describe the solution for classes of recurrences. one useful example is a theorem that gives the answer for a class known as divide and conquer recurrences. these have the form
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
takes maximum advantage of this microparallelism is double buffering. imagine that a ﬁle is being processed sequentially. while the ﬁrst sector is being read, the cpu cannot process that information and so must wait or ﬁnd something else to do in the meantime. once the ﬁrst sector is read, the cpu can start processing while the disk drive (in parallel) begins reading the second sector. if the time required for the cpu to process a sector is approximately the same as the time required by the disk controller to read a sector, it might be possible to keep the cpu continuously fed with data from the ﬁle. the same concept can also be applied to output, writing one sector to disk while the cpu is writing to a second output buffer in memory. thus, in computers that support double buffering, it pays to have at least two input buffers and two output buffers available.
caching information in memory is such a good idea that it is usually extended to multiple buffers. the operating system or an application program might store many buffers of information taken from some backing storage such as a disk ﬁle. this process of using buffers as an intermediary between a user and a disk ﬁle is called buffering the ﬁle. the information stored in a buffer is often called a page, and the collection of buffers is called a buffer pool. the goal of the buffer pool is to increase the amount of information stored in memory in hopes of increasing the likelihood that new information requests can be satisﬁed from the buffer pool rather than requiring new information to be read from disk.
as long as there is an unused buffer available in the buffer pool, new information can be read in from disk on demand. when an application continues to read new information from disk, eventually all of the buffers in the buffer pool will become full. once this happens, some decision must be made about what information in the buffer pool will be sacriﬁced to make room for newly requested information. when replacing information contained in the buffer pool, the goal is to select a buffer that has “unnecessary” information, that is, that information least likely to be requested again. because the buffer pool cannot know for certain what the pattern of future requests will look like, a decision based on some heuristic, or best guess, must be used. there are several approaches to making this decision.
one heuristic is “ﬁrst-in, ﬁrst-out” (fifo). this scheme simply orders the buffers in a queue. the buffer at the front of the queue is used next to store new information and then placed at the end of the queue. in this way, the buffer to be replaced is the one that has held its information the longest, in hopes that this information is no longer needed. this is a reasonable assumption when processing moves along the ﬁle at some steady pace in roughly sequential order. however, many programs work with certain key pieces of information over and over again, and the importance of information has little to do with how long ago the informa-
figure 8.6 a simple external mergesort algorithm. input records are divided equally between two input ﬁles. the ﬁrst runs from each input ﬁle are merged and placed into the ﬁrst output ﬁle. the second runs from each input ﬁle are merged and placed in the second output ﬁle. merging alternates between the two output ﬁles until the input ﬁles are empty. the roles of input and output ﬁles are then reversed, allowing the runlength to be doubled with each pass.
5. repeat until ﬁnished, alternating output between the two output run buffers. whenever the end of an input block is reached, read the next block from the appropriate input ﬁle. when an output buffer is full, write it to the appropriate output ﬁle.
6. repeat steps 2 through 5, using the original output ﬁles as input ﬁles. on the second pass, the ﬁrst two records of each input run ﬁle are already in sorted order. thus, these two runs may be merged and output as a single run of four elements.
example 8.6 using the input of figure 8.6, we ﬁrst create runs of length one split between two input ﬁles. we then process these two input ﬁles sequentially, making runs of length two. the ﬁrst run has the values 20 and 36, which are output to the ﬁrst output ﬁle. the next run has 13 and 17, which is ouput to the second ﬁle. the run 14, 28 is sent to the ﬁrst ﬁle, then run 15, 23 is sent to the second ﬁle, and so on. once this pass has completed, the roles of the input ﬁles and output ﬁles are reversed. the next pass will merge runs of length two into runs of length four. runs 20, 36 and 13, 17 are merged to send 13, 17, 20, 36 to the ﬁrst output ﬁle. then runs 14, 28 and 15, 23 are merged to send run 14, 15, 23, 28 to the second output ﬁle. in the ﬁnal pass, these runs are merged to form the ﬁnal run 13, 14, 15, 17, 20, 23, 28, 36.
this algorithm can easily take advantage of the double buffering techniques described in section 8.3. note that the various passes read the input run ﬁles se-
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
depending on the pattern of record accesses, it might be possible to reduce the expected cost of access even in the face of collisions. recall the 80/20 rule: 80% of the accesses will come to 20% of the data. in other words, some records are accessed more frequently. if two records hash to the same home position, which would be better placed in the home position, and which in a slot further down the probe sequence? the answer is that the record with higher frequency of access should be placed in the home position, because this will reduce the total number of record accesses. ideally, records along a probe sequence will be ordered by their frequency of access.
one approach to approximating this goal is to modify the order of records along the probe sequence whenever a record is accessed. if a search is made to a record that is not in its home position, a self-organizing list heuristic can be used. for example, if the linear probing collision resolution policy is used, then whenever a record is located that is not in its home position, it can be swapped with the record preceding it in the probe sequence. that other record will now be further from its home position, but hopefully it will be accessed less frequently. note that this approach will not work for the other collision resolution policies presented in this section, because swapping a pair of records to improve access to one might remove the other from its probe sequence.
another approach is to keep access counts for records and periodically rehash the entire table. the records should be inserted into the hash table in frequency order, ensuring that records that were frequently accessed during the last series of requests have the best chance of being near their home positions.
1. deleting a record must not hinder later searches. in other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. for example, in figure 9.6(a), keys 9877 and 2037 both hash to slot 7. key 2037 is placed in slot 8 by the collision resolution policy. if 9877 is deleted from the table, a search for 2037 must still pass through slot 7 as it probes to slot 8.
both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. the tombstone indicates that a record once
this chapter presents mathematical notation, background, and techniques used throughout the book. this material is provided primarily for review and reference. you might wish to return to the relevant sections when you encounter unfamiliar notation or mathematical techniques in later chapters.
section 2.7 on estimating might be unfamiliar to many readers. estimating is not a mathematical technique, but rather a general engineering skill. it is enormously useful to computer scientists doing design work, because any proposed solution whose estimated resource requirements fall well outside the problem’s resource constraints can be discarded immediately.
the concept of a set in the mathematical sense has wide application in computer science. the notations and techniques of set theory are commonly used when describing and implementing algorithms because the abstractions associated with sets often help to clarify and simplify algorithm design.
a set is a collection of distinguishable members or elements. the members are typically drawn from some larger population known as the base type. each member of a set is either a primitive element of the base type or is a set itself. there is no concept of duplication in a set. each value from the base type is either in the set or not in the set. for example, a set named p might be the three integers 7, 11, and 42. in this case, p’s members are 7, 11, and 42, and the base type is integer. figure 2.1 shows the symbols commonly used to express sets and their relationships. here are some examples of this notation in use. first deﬁne two sets, p and q.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
cost for a series of insert/delete operations relatively inexpensive, even though an occasional insert/delete operation might be expensive. to analyze the cost of dynamic array operations, we need to use a technique known as amortized analysis, which is discussed in section 14.3.
list users must decide whether they wish to store a copy of any given element on each list that contains it. for small elements such as an integer, this makes sense. if the elements are payroll records, it might be desirable for the list node to store a pointer to the record rather than store a copy of the record itself. this change would allow multiple list nodes (or other data structures) to point to the same record, rather than make repeated copies of the record. not only might this save space, but it also means that a modiﬁcation to an element’s value is automatically reﬂected at all locations where it is referenced. the disadvantage of storing a pointer to each element is that the pointer requires space of its own. if elements are never duplicated, then this additional space adds unnecessary overhead. java most naturally stores references to objects, meaning that only a single copy of an object such as a payroll record will be maintained, even if it is on multiple lists.
whether it is more advantageous to use references to shared elements or separate copies depends on the intended application. in general, the larger the elements and the more they are duplicated, the more likely that references to shared elements is the better approach.
a second issue faced by implementors of a list class (or any other data structure that stores a collection of user-deﬁned data elements) is whether the elements stored are all required to be of the same type. this is known as homogeneity in a data structure. in some applications, the user would like to deﬁne the class of the data element that is stored on a given list, and then never permit objects of a different class to be stored on that same list. in other applications, the user would like to permit the objects stored on a single list to be of differing types.
for the list implementations presented in this section, the compiler requires that all objects stored on the list be of the same type. besides java generics, there are other techniques that implementors of a list class can use to ensure that the element type for a given list remains ﬁxed, while still permitting different lists to store different element types. one approach is to store an object of the appropriate type in the header node of the list (perhaps an object of the appropriate type is supplied as a parameter to the list constructor), and then check that all insert operations on that list use the same element type.
cost for a series of insert/delete operations relatively inexpensive, even though an occasional insert/delete operation might be expensive. to analyze the cost of dynamic array operations, we need to use a technique known as amortized analysis, which is discussed in section 14.3.
list users must decide whether they wish to store a copy of any given element on each list that contains it. for small elements such as an integer, this makes sense. if the elements are payroll records, it might be desirable for the list node to store a pointer to the record rather than store a copy of the record itself. this change would allow multiple list nodes (or other data structures) to point to the same record, rather than make repeated copies of the record. not only might this save space, but it also means that a modiﬁcation to an element’s value is automatically reﬂected at all locations where it is referenced. the disadvantage of storing a pointer to each element is that the pointer requires space of its own. if elements are never duplicated, then this additional space adds unnecessary overhead. java most naturally stores references to objects, meaning that only a single copy of an object such as a payroll record will be maintained, even if it is on multiple lists.
whether it is more advantageous to use references to shared elements or separate copies depends on the intended application. in general, the larger the elements and the more they are duplicated, the more likely that references to shared elements is the better approach.
a second issue faced by implementors of a list class (or any other data structure that stores a collection of user-deﬁned data elements) is whether the elements stored are all required to be of the same type. this is known as homogeneity in a data structure. in some applications, the user would like to deﬁne the class of the data element that is stored on a given list, and then never permit objects of a different class to be stored on that same list. in other applications, the user would like to permit the objects stored on a single list to be of differing types.
for the list implementations presented in this section, the compiler requires that all objects stored on the list be of the same type. besides java generics, there are other techniques that implementors of a list class can use to ensure that the element type for a given list remains ﬁxed, while still permitting different lists to store different element types. one approach is to store an object of the appropriate type in the header node of the list (perhaps an object of the appropriate type is supplied as a parameter to the list constructor), and then check that all insert operations on that list use the same element type.
the third issue that users of the list implementations must face is primarily of concern when programming in languages that do not support automatic garbage collection. that is how to deal with the memory of the objects stored on the list when the list is deleted or the clear method is called. the list destructor and the clear method are problematic in that there is a potential that they will be misused, thus causing a memory leak. deleting listarray in the array-based implementation, or deleting a link node in the linked list implementation, might remove the only reference to an object, leaving its memory space inaccessible. unfortunately, there is no way for the list implementation to know whether a given object is pointed to in another part of the program or not. thus, the user of the list must be responsible for deleting these objects when that is appropriate.
the singly linked list presented in section 4.1.2 allows for direct access from a list node only to the next node in the list. a doubly linked list allows convenient access from a list node to the next node and also to the preceding node on the list. the doubly linked list node accomplishes this in the obvious way by storing two pointers: one to the node following it (as in the singly linked list), and a second pointer to the node preceding it. the most common reason to use a doubly linked list is because it is easier to implement than a singly linked list. while the code for the doubly linked implementation is a little longer than for the singly linked version, it tends to be a bit more “obvious” in its intention, and so easier to implement and debug. figure 4.12 illustrates the doubly linked list concept.
like our singly linked list implementation, the doubly linked list implementation makes use of a header node. we also add a tailer node to the end of the list. the tailer is similar to the header, in that it is a node that contains no value, and it always exists. when the doubly linked list is initialized, the header and tailer nodes are created. data member head points to the header node, and tail points to the tailer node. the purpose of these nodes is to simplify the insert, append, and remove methods by eliminating all need for special-case code when the list is empty.
whether a list implementation is doubly or singly linked should be hidden from the list class user. figure 4.13 shows the complete implementation for a link
examining the managed memory pool to determine which parts are still being used and which parts are garbage. in particular, a list is kept of all program variables, and any memory locations not reachable from one of these variables are considered to be garbage. when the garbage collector executes, all unused memory locations are placed in free store for future access. this approach has the advantage that it allows for easy collection of garbage. it has the disadvantage, from a user’s point of view, that every so often the system must halt while it performs garbage collection. for example, garbage collection is noticeable in the emacs text editor, which is normally implemented in lisp. occasionally the user must wait for a moment while the memory management system performs garbage collection.
the java programming language also makes use of garbage collection. as in lisp, it is common practice in java to allocate dynamic memory as needed, and to later drop all references to that memory. the garbage collector is responsible for reclaiming such unused space as necessary. this might require extra time when running the program, but it makes life considerably easier for the programmer. in contrast, many large applications written in c++ (even commonly used commercial software) contain memory leaks that will in time cause the program to fail.
several algorithms have been used for garbage collection. one is the reference count algorithm. here, every dynamically allocated memory block includes space for a count ﬁeld. whenever a pointer is directed to a memory block, the reference count is increased. whenever a pointer is directed away from a memory block, the reference count is decreased. if the count ever becomes zero, then the memory block is considered garbage and is immediately placed in free store. this approach has the advantage that it does not require an explicit garbage collection phase, because information is put in free store immediately when it becomes garbage.
the reference count algorithm is used by the unix ﬁle system. files can have multiple names, called links. the ﬁle system keeps a count of the number of links to each ﬁle. whenever a ﬁle is “deleted,” in actuality its link ﬁeld is simply reduced by one. if there is another link to the ﬁle, then no space is recovered by the ﬁle system. whenever the number of links goes to zero, the ﬁle’s space becomes available for reuse.
reference counts have several major disadvantages. first, a reference count must be maintained for each memory object. this works well when the objects are large, such as a ﬁle. however, it will not work well in a system such as lisp where the memory objects typically consist of two pointers or a value (an atom). another major problem occurs when garbage contains cycles. consider figure 12.17. here each memory object is pointed to once, but the collection of objects is still garbage because no pointer points to the collection. thus, reference counts only work when
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
example 2.1 for the integers, = is an equivalence relation that partitions each element into a distinct subset. in other words, for any integer a, three things are true.
1. a = a, 2. if a = b then b = a, and 3. if a = b and b = c, then a = c. of course, for distinct integers a, b, and c there are never cases where a = b, b = a, or b = c. so the claims that = is symmetric and transitive are vacuously true (there are never examples in the relation where these events occur). but becausethe requirements for symmetry and transitivity are not violated, the relation is symmetric and transitive.
example 2.2 if we clarify the deﬁnition of sibling to mean that a person is a sibling of him- or herself, then the sibling relation is an equivalence relation that partitions the set of people.
example 2.3 we can use the modulus function (deﬁned in the next section) to deﬁne an equivalence relation. for the set of integers, use the modulus function to deﬁne a binary relation such that two numbers x and y are in the relation if and only if x mod m = y mod m. thus, for m = 4, h1, 5i is in the relation because 1 mod 4 = 5 mod 4. we see that modulus used in this way deﬁnes an equivalence relation on the integers, and this relation can be used to partition the integers into m equivalence classes. this relation is an equivalence relation because
a binary relation is called a partial order if it is antisymmetric and transitive.2 the set on which the partial order is deﬁned is called a partially ordered set or a poset. elements x and y of a set are comparable under a given relation if either
2not all authors use this deﬁnition for partial order. i have seen at least three signiﬁcantly different deﬁnitions in the literature. i have selected the one that lets < and ≤ both deﬁne partial orders on the integers, becausethis seems the most natural to me.
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
many organizations are hierarchical in nature, such as the military and most businesses. consider a company with a president and some number of vice presidents who report to the president. each vice president has some number of direct subordinates, and so on. if we wanted to model this company with a data structure, it would be natural to think of the president in the root node of a tree, the vice presidents at level 1, and their subordinates at lower levels in the tree as we go down the organizational hierarchy.
because the number of vice presidents is likely to be more than two, this company’s organization cannot easily be represented by a binary tree. we need instead to use a tree whose nodes have an arbitrary number of children. unfortunately, when we permit trees to have nodes with an arbrary number of children, they become much harder to implement than binary trees. we consider such trees in this chapter. to distinguish them from the more commonly used binary tree, we use the term general tree.
section 6.1 presents general tree terminology. section 6.2 presents a simple representation for solving the important problem of processing equivalence classes. several pointer-based implementations for general trees are covered in section 6.3. aside from general trees and binary trees, there are also uses for trees whose internal nodes have a ﬁxed number k of children where k is something other than two. such trees are known as k-ary trees. section 6.4 generalizes the properties of binary trees to k-ary trees. sequential representations, useful for applications such as storing trees on disk, are covered in section 6.5.
a tree t is a ﬁnite set of one or more nodes such that there is one designated node r, called the root of t. if the set (t−{r}) is not empty, these nodes are partitioned
objects are in different sets, and function union merges two sets together. a private function find is used to ﬁnd the ultimate root for an object.
an application using the union/find operations should store a set of n objects, where each object is assigned a unique index in the range 0 to n − 1. the indices refer to the corresponding parent pointers in the array. class parptrtree creates and initializes the union/find array, and functions differ and union take array indices as inputs.
figure 6.5 illustrates the parent pointer implementation. note that the nodes can appear in any order within the array, and the array can store up to n separate trees. for example, figure 6.5 shows two trees stored in the same array. thus, a single array can store a collection of items distributed among an arbitrary (and changing) number of disjoint subsets.
consider the problem of assigning the members of a set to disjoint subsets called equivalence classes. recall from section 2.1 that an equivalence relation is reﬂexive, symmetric, and transitive. thus, if objects a and b are equivalent, and objects b and c are equivalent, we must be able to recognize that objects a and c are also equivalent.
there are many practical uses for disjoint sets and representing equivalences. for example, consider figure 6.6 which shows a graph of ten nodes labeled a through j. notice that for nodes a through i, there is some series of edges that connects any pair of the nodes, but node j is disconnected from the rest of the nodes. such a graph might be used to represent connections such as wires between components on a circuit board, or roads between cities. we can consider two nodes of the graph to be equivalent if there is a path between them. thus, nodes a, h, and e would be equivalent in figure 6.6, but j is not equivalent to any other. a subset of equivalent (connected) edges in a graph is called a connected component. the goal is to quickly classify the objects into disjoint sets that correspond to the connected components. another application for union/find occurs in kruskal’s algorithm for computing the minimal cost spanning tree for a graph (section 11.5.2).
the input to the union/find algorithm is typically a series of equivalence pairs. in the case of the connected components example, the equivalence pairs would simply be the set of edges in the graph. an equivalence pair might say that object c is equivalent to object a. if so, c and a are placed in the same subset. if a later equivalence relates a and b, then by implication c is also equivalent to b. thus, an equivalence pair may cause two subsets to merge, each of which contains several objects.
union rule for joining sets) is Θ(n log∗ n). the notation “log∗ n” means the number of times that the log of n must be taken before n ≤ 1. for example, log∗ 65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and ﬁnally log 2 = 1. thus, log∗ n grows very slowly, so the cost for a series of n find operations is very close to n.
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth Θ(log∗ n). one can devise a series of equivalence operations that yields Θ(log n) depth for the resulting tree. however, many of the equivalences in such a series will look only at the roots of the trees being merged, requiring little processing time. the total amount of processing time required for n operations will be Θ(n log∗ n), yielding nearly constant time for each equivalence operation. this is an example of the technique of amortized analysis, discussed further in section 14.3.
we now tackle the problem of devising an implementation for general trees that allows efﬁcient processing of all member functions of the adt shown in figure 6.2. this section presents several approaches to implementing general trees. each implementation yields advantages and disadvantages in the amount of space required to store a node and the relative ease with which key operations can be performed. general tree implementations should place no restriction on how many children a node may have. in some applications, once a node is created the number of children never changes. in such cases, a ﬁxed amount of space can be allocated for the node when it is created, based on the number of children for the node. matters become more complicated if children can be added to or deleted from a node, requiring that the node’s space allocation be adjusted accordingly.
will use the “)” symbol) to indicate the end of a child list. all leaf nodes are followed by a “)” symbol because they have no children. a leaf node that is also the last child for its parent would indicate this by two or more successive “)” symbols.
note that f is followed by three “)” marks, because it is a leaf, the last node of b’s rightmost subtree, and the last node of r’s rightmost subtree.
note that this representation for serializing general trees cannot be used for binary trees. this is because a binary tree is not merely a restricted form of general tree with at most two children. every binary tree node has a left and a right child, though either or both might be empty. for example, the representation of example 6.8 cannot let us distinguish whether node d in figure 6.17 is the left or right child of node b.
6.6 further reading the expression log∗ n cited in section 6.2 is closely related to the inverse of ackermann’s function. for more information about ackermann’s function and the cost of path compression, see robert e. tarjan’s paper “on the efﬁciency of a good but not linear set merging algorithm” [tar75]. the article “data structures and algorithms for disjoint set union problems” by galil and italiano [gi91] covers many aspects of the equivalence class problem.
foundations of multidimensional and metric data structures by hanan samet [sam06] treats various implementations of tree structures in detail within the context of k-ary trees. samet covers sequential implementations as well as the linked and array implementations such as those described in this chapter and chapter 5. while these books are ostensibly concerned with spatial data structures, many of the concepts treated are relevant to anyone who must implement tree structures.
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
figure 11.22 prim’s mst algorithm proof. the left oval contains that portion of the graph where prim’s mst and the “true” mst t agree. the right oval contains the rest of the graph. the two portions of the graph are connected by (at least) edges ej (selected by prim’s algorithm to be in the mst) and e0 (the “correct” edge to be placed in the mst). note that the path from vw to vj cannot include any marked vertex vi, i ≤ j, because to do so would form a cycle.
and (d, f) happen to have equal cost, it is an arbitrary decision as to which gets selected. let’s pick (c, f). the next step marks vertex e and adds edge (f, e) to the mst. following in this manner, vertex b (through edge (c, b)) is marked. at this point, the algorithm terminates.
our next mst algorithm is commonly referred to as kruskal’s algorithm. kruskal’s algorithm is also a simple, greedy algorithm. we ﬁrst partition the set of vertices into |v| equivalence classes (see section 6.2), each consisting of one vertex. we then process the edges in order of weight. an edge is added to the mst, and the two equivalence classes combined, if the edge connects two vertices in different equivalence classes. this process is repeated until only one equivalence class remains.
example 11.4 figure 11.23 shows the ﬁrst three steps of kruskal’s algorithm for the graph of figure 11.19. edge (c, d) has the least cost, and because c and d are currently in separate msts, they are combined. we next select edge (e, f) to process, and combine these vertices into a single
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
11.10 show the shortest paths generated by running dijkstra’s shortest-paths algorithm on the graph of figure 11.25, beginning at vertex 4. show the d values as each vertex is processed, as in figure 11.18.
11.12 the root of a dag is a vertex r such that every vertex of the dag can be reached by a directed path from r. write an algorithm that takes a directed graph as input and determines the root (if there is one) for the graph. the running time of your algorithm should be Θ(|v| + |e|).
11.13 write an algorithm to ﬁnd the longest path in a dag, where the length of the path is measured by the number of edges that it contains. what is the asymptotic complexity of your algorithm? 11.14 write an algorithm to determine whether a directed graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v| + |e|) time. 11.15 write an algorithm to determine whether an undirected graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v|) time.
11.16 the single-destination shortest-paths problem for a directed graph is to ﬁnd the shortest path from every vertex to a speciﬁed vertex v. write an algorithm to solve the single-destination shortest-paths problem.
11.17 list the order in which the edges of the graph in figure 11.25 are visited when running prim’s mst algorithm starting at vertex 3. show the ﬁnal mst.
11.18 list the order in which the edges of the graph in figure 11.25 are visited when running kruskal’s mst algorithm. each time an edge is added to the mst, show the result on the equivalence array, (e.g., show the array as in figure 6.7).
at each iteration of the algorithm. the relative costs of these two variants depend on who sparse or dense the graph is. they might also depend on whether the graph is implemented using an adjacency list or adjacency matrix. design and implement a study to compare the effects on performance for three variables: (i) the two graph representations (adjacency list and adjacency matrix); (ii) the two implementations for djikstra’s shortest paths algorithm (searching the table of vertex distances or using a priority queue to track the distances), and (iii) sparse versus dense graphs. be sure to test your implementations on a variety of graphs that are sufﬁciently large to generate meaningful times.
11.4 the example implementations for dfs and bfs show calls to functions previsit and postvisit. better is to implement bfs and dfs using the visitor design pattern, with the visitor functions being passed in as either function or template parameters. reimplement the bfs and dfs functions to make use of the visitor design pattern.
11.5 write a program to label the connected components for an undirected graph. in other words, all vertices of the ﬁrst component are given the ﬁrst component’s label, all vertices of the second component are given the second component’s label, and so on. your algorithm should work by deﬁning any two vertices connected by an edge to be members of the same equivalence class. once all of the edges have been processed, all vertices in a given equivalence class will be connected. use the union/find implementation from section 6.2 to implement equivalence classes.
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
for more about estimating techniques, see two programming pearls by john louis bentley entitled the back of the envelope and the envelope is back [ben84, ben00, ben86, ben88]. genius: the life and science of richard feynman by james gleick [gle92] gives insight into how important back of the envelope calculation was to the developers of the atomic bomb, and to modern theoretical physics in general.
each of the properties reﬂexive, symmetric, antisymmetric, and transitive. (a) “isbrotherof” on the set of people. (b) “isfatherof” on the set of people. (c) the relation r = {hx, yi| x2 + y2 = 1} for real numbers x and y. (d) the relation r = {hx, yi| x2 = y2} for real numbers x and y. (e) the relation r = {hx, yi| x mod y = 0} for x, y ∈ {1, 2, 3, 4}. (f) the empty relation ∅ (i.e., the relation with no ordered pairs for which (g) the empty relation ∅ (i.e., the relation with no ordered pairs for which
relation or prove that it is not an equivalence relation. (a) for integers a and b, a ≡ b if and only if a + b is even. (b) for integers a and b, a ≡ b if and only if a + b is odd. (c) for nonzero rational numbers a and b, a ≡ b if and only if a × b > 0. (d) for nonzero rational numbers a and b, a ≡ b if and only if a/b is an (e) for rational numbers a and b, a ≡ b if and only if a − b is an integer. (f) for rational numbers a and b, a ≡ b if and only if |a − b| ≤ 2.
why or why not. (a) “isfatherof” on the set of people. (b) “isancestorof” on the set of people. (c) “isolderthan” on the set of people. (d) “issisterof” on the set of people. (e) {ha, bi,ha, ai,hb, ai} on the set {a, b}.
figure 5.13 two binary search trees for a collection of values. tree (a) results if values are inserted in the order 37, 24, 42, 7, 2, 40, 42, 32, 120. tree (b) results if the same values are inserted in the order 120, 42, 42, 7, 2, 32, 37, 24, 40.
figure 5.14 shows a class declaration for the bst that implements the dictionary adt. the public member functions include those required by the dictionary adt, along with a constructor and destructor.
to ﬁnd a record with key value k in a bst, begin at the root. if the root stores a record with key value k, then the search is over. if not, then we must search deeper in the tree. what makes the bst efﬁcient during search is that we need search only one of the node’s two subtrees. if k is less than the root node’s key value, we search only the left subtree. if k is greater than the root node’s key value, we search only the right subtree. this process continues until a record with key value k is found, or we reach a leaf node. if we reach a leaf node without encountering k, then no record exists in the bst whose key value is k.
example 5.5 consider searching for the node with key value 32 in the tree of figure 5.13(a). because 32 is less than the root value of 37, the search proceeds to the left subtree. because 32 is greater than 24, we search in 24’s right subtree. at this point the node containing 32 is found. if the search value were 35, the same path would be followed to the node containing 32. because this node has no children, we know that 35 is not in the bst.
notice that in figure 5.14, public member function find calls a private member function named findhelp. method find takes the search key as an explicit parameter and its bst as an implicit parameter, along with space to place a copy of
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
figure 13.1 the binary trie for the collection of values 2, 7, 24, 31, 37, 40, 42, 120. all data values are stored in the leaf nodes. edges are labeled with the value of the bit used to determine the branching direction of each node. the binary form of the key value determines the path to the record, assuming that each key is represented as a 7-bit value representing a number in the range 0 to 127.
one application for tries is storing a dictionary of words. such a trie will be referred to as an alphabet trie. for simplicity, our examples will ignore case in letters. we add a special character ($) to the 26 standard english letters. the $ character is used to represent the end of a string. thus, the branching factor for each node is (up to) 27. once constructed, the alphabet trie is used to determine if a given word is in the dictionary. consider searching for a word in the alphabet trie of figure 13.2. the ﬁrst letter of the search word determines which branch to take from the root, the second letter determines which branch to take at the next level, and so on. only the letters that lead to a word are shown as branches. in figure 13.2(b) the leaf nodes of the trie store a copy of the actual words, while in figure 13.2(a) the word is built up from the letters associated with each branch. one way to implement a node of the alphabet trie is as an array of 27 pointers indexed by letter. because most nodes have branches to only a small fraction of the possible letters in the alphabet, an alternate implementation is to use a linked list of pointers to the child nodes, as in figure 6.9.
the depth of a leaf node in the alphabet trie of figure 13.2(b) has little to do with the number of nodes in the trie. rather, a node’s depth depends on the number of characters required to distinguish this node’s word from any other. for example, if the words “anteater” and “antelope” are both stored in the trie, it is not until the ﬁfth letter that the two words can be distinguished. thus, these words must be stored at least as deep as level ﬁve. in general, the limiting factor on the depth of nodes in the alphabet trie is the length of the words stored.
this chapter presents mathematical notation, background, and techniques used throughout the book. this material is provided primarily for review and reference. you might wish to return to the relevant sections when you encounter unfamiliar notation or mathematical techniques in later chapters.
section 2.7 on estimating might be unfamiliar to many readers. estimating is not a mathematical technique, but rather a general engineering skill. it is enormously useful to computer scientists doing design work, because any proposed solution whose estimated resource requirements fall well outside the problem’s resource constraints can be discarded immediately.
the concept of a set in the mathematical sense has wide application in computer science. the notations and techniques of set theory are commonly used when describing and implementing algorithms because the abstractions associated with sets often help to clarify and simplify algorithm design.
a set is a collection of distinguishable members or elements. the members are typically drawn from some larger population known as the base type. each member of a set is either a primitive element of the base type or is a set itself. there is no concept of duplication in a set. each value from the base type is either in the set or not in the set. for example, a set named p might be the three integers 7, 11, and 42. in this case, p’s members are 7, 11, and 42, and the base type is integer. figure 2.1 shows the symbols commonly used to express sets and their relationships. here are some examples of this notation in use. first deﬁne two sets, p and q.
example 2.17 we would like to prove that function fact does indeed compute the factorial function. there are two distinct steps to such a proof. the ﬁrst is to prove that the function always terminates. the second is to prove that the function returns the correct value. theorem 2.8 function fact will terminate for any value of n. proof: for the base case, we observe that fact will terminate directly whenever n ≤ 0. the induction hypothesis is that fact will terminate for n − 1. for n, we have two possibilities. one possibility is that n ≥ 12. in that case, fact will terminate directly because it will fail its assertion test. otherwise, fact will make a recursive call to fact(n-1). by the induction hypothesis, fact(n-1) must terminate. 2 theorem 2.9 function fact does compute the factorial function for any value in the range 0 to 12. proof: to prove the base case, observe that when n = 0 or n = 1, fact(n) returns the correct value of 1. the induction hypothesis is that fact(n-1) returns the correct value of (n − 1)!. for any value n within the legal range, fact(n) returns n ∗ fact(n-1). by the induction hypothesis, fact(n-1) = (n− 1)!, and because n∗ (n− 1)! = n!, we have proved that fact(n) produces the correct result.
we can use a similar process to prove many recursive programs correct. the general form is to show that the base cases perform correctly, and then to use the induction hypothesis to show that the recursive step also produces the correct result. prior to this, we must prove that the function always terminates, which might also be done using an induction proof.
one of the most useful life skills that you can gain from your computer science training is knowing how to perform quick estimates. this is sometimes known as “back of the napkin” or “back of the envelope” calculation. both nicknames suggest that only a rough estimate is produced. estimation techniques are a standard part of engineering curricula but are often neglected in computer science. estimation is no substitute for rigorous, detailed analysis of a problem, but it can serve to indicate when a rigorous analysis is warranted: if the initial estimate indicates that the solution is unworkable, then further analysis is probably unnecessary.
for more about estimating techniques, see two programming pearls by john louis bentley entitled the back of the envelope and the envelope is back [ben84, ben00, ben86, ben88]. genius: the life and science of richard feynman by james gleick [gle92] gives insight into how important back of the envelope calculation was to the developers of the atomic bomb, and to modern theoretical physics in general.
each of the properties reﬂexive, symmetric, antisymmetric, and transitive. (a) “isbrotherof” on the set of people. (b) “isfatherof” on the set of people. (c) the relation r = {hx, yi| x2 + y2 = 1} for real numbers x and y. (d) the relation r = {hx, yi| x2 = y2} for real numbers x and y. (e) the relation r = {hx, yi| x mod y = 0} for x, y ∈ {1, 2, 3, 4}. (f) the empty relation ∅ (i.e., the relation with no ordered pairs for which (g) the empty relation ∅ (i.e., the relation with no ordered pairs for which
relation or prove that it is not an equivalence relation. (a) for integers a and b, a ≡ b if and only if a + b is even. (b) for integers a and b, a ≡ b if and only if a + b is odd. (c) for nonzero rational numbers a and b, a ≡ b if and only if a × b > 0. (d) for nonzero rational numbers a and b, a ≡ b if and only if a/b is an (e) for rational numbers a and b, a ≡ b if and only if a − b is an integer. (f) for rational numbers a and b, a ≡ b if and only if |a − b| ≤ 2.
why or why not. (a) “isfatherof” on the set of people. (b) “isancestorof” on the set of people. (c) “isolderthan” on the set of people. (d) “issisterof” on the set of people. (e) {ha, bi,ha, ai,hb, ai} on the set {a, b}.
(b) what is the average number of “1” bits for an n-bit random number? (c) what is the expected value for the position of the leftmost “1” bit? in other words, how many positions on average must we examine when moving from left to right before encountering a “1” bit? show the appropriate summation.
2.35 what is the total volume of your body in liters (or, if you prefer, gallons)? 2.36 an art historian has a database of 20,000 full-screen color images.
(a) about how much space will this require? how many cd-roms would be required to store the database? (a cd-rom holds about 600mb of data). be sure to explain all assumptions you made to derive your answer.
(b) now, assume that you have access to a good image compression technique that can store the images in only 1/10 of the space required for an uncompressed image. will the entire database ﬁt onto a single cdrom if the images are compressed?
2.37 how many cubic miles of water ﬂow out of the mouth of the mississippi river each day? do not look up the answer or any supplemental facts. be sure to describe all assumptions made in arriving at your answer.
2.38 when buying a home mortgage, you often have the option of paying some money in advance (called “discount points”) to get a lower interest rate. assume that you have the choice between two 15-year mortgages: one at 8%, and the other at 7 3 4% with an up-front charge of 1% of the mortgage value. how long would it take to recover the 1% charge when you take the mortgage at the lower rate? as a second, more precise estimate, how long would it take to recover the charge plus the interest you would have received if you had invested the equivalent of the 1% charge in the bank at 5% interest while paying the higher rate? do not use a calculator to help you answer this question.
2.39 when you build a new house, you sometimes get a “construction loan” which is a temporary line of credit out of which you pay construction costs as they occur. at the end of the construction period, you then replace the construction loan with a regular mortgage on the house. during the construction loan, you only pay each month for the interest charged against the actual amount borrowed so far. assume that your house construction project starts at the beginning of april, and is complete at the end of six months. assume that the total construction cost will be $300,000 with the costs occurring at the beginning of each month in $50,000 increments. the construction loan charges 6% interest. estimate the total interest payments that must be paid over the life of the construction loan.
2.40 here are some questions that test your working knowledge of how fast computers operate. is disk drive access time normally measured in milliseconds (thousandths of a second) or microseconds (millionths of a second)? does your ram memory access a word in more or less than one microsecond? how many instructions can your cpu execute in one year if the machine is left running at full speed all the time? do not use paper or a calculator to derive your answers.
2.44 how many cities and towns are there in the united states? 2.45 how many steps would it take to walk from boston to san francisco? 2.46 a man begins a car trip to visit his in-laws. the total distance is 60 miles, and he starts off at a speed of 60 miles per hour. after driving exactly 1 mile, he loses some of his enthusiasm for the journey, and (instantaneously) slows
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
examining class ualdict (ual stands for “unsorted array-based list), we can easily see that insert is a constant time operation, because it simply inserts the new record at the end of the list. however, find, and remove both require Θ(n) time in the average and worst cases, because we need to do a sequential search. method remove in particular must touch every record in the list, because once the desired record is found, the remaining records must be shifted down in the list to ﬁll the gap. method removeany removes the last record from the list, so this is a constant-time operation.
as an alternative, we could implement the dictionary using a linked list. the implementation would be quite similar to that shown in figure 4.31, and the cost of the functions should be the same asymptotically.
another alternative would be to implement the dictionary with a sorted list. the advantage of this approach would be that we might be able to speed up the find operation by using a binary search. to do so, ﬁrst we must deﬁne a variation on the list adt to support sorted lists. a sorted list is somewhat different from an unsorted list in that it cannot permit the user to control where elements get inserted. thus, the insert method must be quite different in a sorted list than in an unsorted list. likewise, the user cannot be permitted to append elements onto the list. for these reasons, a sorted list cannot be implemented with straightforward inheritance from the list adt.
the cost for find in a sorted list is Θ(log n) for a list of length n. this is a great improvement over the cost of find in an unsorted list. unfortunately, the cost of insert changes from constant time in the unsorted list to Θ(n) time in the sorted list. whether the sorted list implementation for the dictionary adt is more or less efﬁcient than the unsorted list implementation depends on the relative number of insert and find operations to be performed. if many more find operations than insert operations are used, then it might be worth using a sorted list to implement the dictionary. in both cases, remove requires Θ(n) time in the worst and average cases. even if we used binary search to cut down on the time to ﬁnd the record prior to removal, we would still need to shift down the remaining records in the list to ﬁll the gap left by the remove operation.
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
9.5 implement a database stored on disk using bucket hashing. deﬁne records to be 128 bytes long with a 4-byte key and 120 bytes of data. the remaining 4 bytes are available for you to store necessary information to support the hash table. a bucket in the hash table will be 1024 bytes long, so each bucket has space for 8 records. the hash table should consist of 27 buckets (total space for 216 records with slots indexed by positions 0 to 215) followed by the overﬂow bucket at record position 216 in the ﬁle. the hash function for key value k should be k mod 213. (note that this means the last three slots in the table will not be home positions for any record.) the collision resolution function should be linear probing with wrap-around within the bucket. for example, if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if a bucket is full, the record should be placed in the overﬂow section at the end of the ﬁle. your hash table should implement the dictionary adt of section 4.4. when you do your testing, assume that the system is meant to store about 100 or so records at a time.
9.6 implement the dictionary adt of section 4.4 by means of a hash table with linear probing as the collision resolution policy. you might wish to begin with the code of figure 9.7. using empirical simulation, determine the cost of insert and delete as α grows (i.e., reconstruct the dashed lines of figure 9.8). then, repeat the experiment using quadratic probing and pseudorandom probing. what can you say about the relative performance of these three collision resolution policies?
10.3 implement the dictionary adt of section 4.4 for a large ﬁle stored on disk by means of the b+-tree of section 10.5. assume that disk blocks are 1024 bytes, and thus both leaf nodes and internal nodes are also 1024 bytes. records should store a 4-byte (int) key value and a 60-byte data ﬁeld. internal nodes should store key value/pointer pairs where the “pointer” is actually the block number on disk for the child node. both internal nodes and leaf nodes will need room to store various information such as a count of the records stored on that node, and a pointer to the next node on that level. thus, leaf nodes will store 15 records, and internal nodes will have room to store about 120 to 125 children depending on how you implement them. use a buffer pool (section 8.3) to manage access to the nodes stored on disk.
this chapter talks about some fundamental patterns of algorithms. we discuss greed algorithms, dynamic programming, randomized algorithms, numerical algorithms, and the concept of a transform.
section 16.3.1 presents the skip list, a probabilistic data structure that can be used to implement the dictionary adt. the skip list is comparable in complexity to the bst, yet often outperforms the bst, because the skip list’s efﬁciency is not tied to the values of the dataset being stored.
observe that the following are all greedy algorithms (that work!): kruskal’s mst, prim’s mst, dijkstra’s shortest paths, huffman’s coding algorithm. various greedy knapsack algorithms, such as continuous-knapsack problem (see johnsonbaugh and schaefer, sec 7.6).
see the treatment by kleinberg & tardos. consider that a heap has a “greedy” deﬁnition: the value of any node a is bigger than its children. the bst’s deﬁnition is that the value of any node a is greater than all nodes in the left subtree, and less than all nodes in the right subtree. if we try a greedy deﬁnition (a is greater than its left child and less than its right child), we can get a tree that meets this deﬁnition but is not a bst. see the example in section 5.2.
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
the ﬁrst two lines of the function constitute the base cases. if n ≤ 1, then one of the base cases computes a solution for the problem. if n > 1, then fact calls a function that knows how to ﬁnd the factorial of n − 1. of course, the function that knows how to compute the factorial of n − 1 happens to be fact itself. but we should not think too hard about this while writing the algorithm. the design for recursive algorithms can always be approached in this way. first write the base cases. then think about solving the problem by combining the results of one or more smaller — but similar — subproblems. if the algorithm you write is correct, then certainly you can rely on it (recursively) to solve the smaller subproblems. the secret to success is: do not worry about how the recursive call solves the subproblem. simply accept that it will solve it correctly, and use this result to in turn correctly solve the original problem. what could be simpler?
recursion has no counterpart in everyday problem solving. the concept can be difﬁcult to grasp because it requires you to think about problems in a new way. to use recursion effectively, it is necessary to train yourself to stop analyzing the recursive process beyond the recursive call. the subproblems will take care of themselves. you just worry about the base cases and how to recombine the subproblems.
the recursive version of the factorial function might seem unnecessarily complicated to you because the same effect can be achieved by using a while loop. here is another example of recursion, based on a famous puzzle called “towers of hanoi.” the natural algorithm to solve this problem has multiple recursive calls. it cannot be rewritten easily using while loops.
the towers of hanoi puzzle begins with three poles and n rings, where all rings start on the leftmost pole (labeled pole 1). the rings each have a different size, and are stacked in order of decreasing size with the largest ring at the bottom, as shown in figure 2.2.a. the problem is to move the rings from the leftmost pole to the rightmost pole (labeled pole 3) in a series of steps. at each step the top ring on some pole is moved to another pole. there is one limitation on where rings may be moved: a ring can never be moved on top of a smaller ring.
example 2.17 we would like to prove that function fact does indeed compute the factorial function. there are two distinct steps to such a proof. the ﬁrst is to prove that the function always terminates. the second is to prove that the function returns the correct value. theorem 2.8 function fact will terminate for any value of n. proof: for the base case, we observe that fact will terminate directly whenever n ≤ 0. the induction hypothesis is that fact will terminate for n − 1. for n, we have two possibilities. one possibility is that n ≥ 12. in that case, fact will terminate directly because it will fail its assertion test. otherwise, fact will make a recursive call to fact(n-1). by the induction hypothesis, fact(n-1) must terminate. 2 theorem 2.9 function fact does compute the factorial function for any value in the range 0 to 12. proof: to prove the base case, observe that when n = 0 or n = 1, fact(n) returns the correct value of 1. the induction hypothesis is that fact(n-1) returns the correct value of (n − 1)!. for any value n within the legal range, fact(n) returns n ∗ fact(n-1). by the induction hypothesis, fact(n-1) = (n− 1)!, and because n∗ (n− 1)! = n!, we have proved that fact(n) produces the correct result.
we can use a similar process to prove many recursive programs correct. the general form is to show that the base cases perform correctly, and then to use the induction hypothesis to show that the recursive step also produces the correct result. prior to this, we must prove that the function always terminates, which might also be done using an induction proof.
one of the most useful life skills that you can gain from your computer science training is knowing how to perform quick estimates. this is sometimes known as “back of the napkin” or “back of the envelope” calculation. both nicknames suggest that only a rough estimate is produced. estimation techniques are a standard part of engineering curricula but are often neglected in computer science. estimation is no substitute for rigorous, detailed analysis of a problem, but it can serve to indicate when a rigorous analysis is warranted: if the initial estimate indicates that the solution is unworkable, then further analysis is probably unnecessary.
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
one important aspect of algorithm design is referred to as the space/time tradeoff principle. the space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacriﬁce space or vice versa. many programs can be modiﬁed to reduce storage requirements by “packing” or encoding information. “unpacking” or decoding the information requires additional time. thus, the resulting program uses less space but runs slower. conversely, many programs can be modiﬁed to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. typically, such changes in time and space are both by a constant factor.
a classic example of a space/time tradeoff is the lookup table. a lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. for example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. if you are writing a program that often computes factorials, it is likely to be much more time efﬁcient to simply pre-compute the 12 storable values in a table. whenever the program needs the value of n! for n ≤ 12, it can simply check the lookup table. (if n > 12, the value is too large to store as an int variable anyway.) compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table.
lookup tables can also store approximations for an expensive function such as sine or cosine. if you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. note that initially building the lookup table requires a certain amount of time. your application must use the lookup table often enough to make this initialization worthwhile.
another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. here is a simple code fragment for sorting an array of integers. we assume that this is a special case where there are n integers whose values are a permutation of the integers from 0 to n − 1. this is an example of a binsort, which is discussed in section 7.7. binsort assigns each value to an array position corresponding to its value.
as an interesting aside, writing a correct binary search algorithm is not easy. knuth [knu98] notes that while the ﬁrst binary search was published in 1946, the ﬁrst bug-free algorithm was not published until 1962! bentley (“writing correct programs” in [ben00]) has found that 90% of the computer professionals he tested could not write a bug-free binary search in two hours.
see stirling’s approximation in section 2.2 for help in classifying n!. (a) suppose that a particular algorithm has time complexity t(n) = 3 × 2n, and that executing an implementation of it on a particular machine takes t seconds for n inputs. now suppose that we are presented with a machine that is 64 times as fast. how many inputs could we process on the new machine in t seconds?
(b) suppose that another algorithm has time complexity t(n) = n2, and that executing an implementation of it on a particular machine takes t seconds for n inputs. now suppose that we are presented with a machine that is 64 times as fast. how many inputs could we process on the new machine in t seconds?
(c) a third algorithm has time complexity t(n) = 8n. executing an implementation of it on a particular machine takes t seconds for n inputs. given a new machine that is 64 times as fast, how many inputs could we process in t seconds?
3.5 hardware vendor xyz corp. claims that their latest computer will run 100 times faster than that of their competitor, prunes, inc. if the prunes, inc. computer can execute a program on input of size n in one hour, what size
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
17.4 further reading the classic text on the theory of np-completeness is computers and intractability: a guide to the theory of np-completeness by garey and johnston [gj79]. the traveling salesman problem, edited by lawler et al. [llks85], discusses many approaches to ﬁnding an acceptable solution to this particular np-complete problem in a reasonable amount of time.
for more information about the collatz function see “on the ups and downs of hailstone numbers” by b. hayes [hay84], and “the 3x + 1 problem and its generalizations” by j.c. lagarias [lag85].
17.1 consider this algorithm for ﬁnding the maximum element in an array: first sort the array and then select the last (maximum) element. what (if anything) does this reduction tell us about the upper and lower bounds to the problem of ﬁnding the maximum element in a sequence? why can we not reduce sorting to ﬁnding the maximum element? 17.2 use a reduction to prove that squaring an n × n matrix is just as expensive (asymptotically) as multiplying two n × n matrices. 17.3 use a reduction to prove that multiplying two upper triangular n × n matrices is just as expensive (asymptotically) as multiplying two arbitrary n × n matrices. (a) explain why computing the factorial of n by multiplying all values
(b) explain why computing an approximation to the factorial of n by making use of stirling’s formula (see section 2.2) is a polynomial time algorithm.
17.5 consider this algorithm for solving the clique problem. first, generate all subsets of the vertices containing exactly k vertices. there are o(nk) such subsets altogether. then, check whether any subgraphcs induced by these subsets is complete. if this algorithm ran in polynomial time, what would be its signiﬁcance? why is this not a polynomial-time algorithm for the clique problem?
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
use a variety of random search values in the range 0 to n − 1 on each size n. graph the resulting times. when is sequential search faster than binary search for a sorted array?
3.3 implement a program that runs and gives timings for the two fibonacci sequence functions provided in exercise 2.11. graph the resulting running times for as many values of n as your computer can handle.
operation) and removed from the front (called a dequeue operation). queues operate like standing in line at a movie theater ticket counter.1 if nobody cheats, then newcomers go to the back of the line. the person at the front of the line is the next to be served. thus, queues release their elements in order of arrival. accountants have used queues since long before the existence of computers. they call a queue a “fifo” list, which stands for “first-in, first-out.” figure 4.22 shows a sample queue adt. this section presents two implementations for queues: the array-based queue and the linked queue.
assume that there are n elements in the queue. by analogy to the array-based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only Θ(1) time because the front element of the queue (the one being removed) is the last element in the array. however,
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
when all inserts and releases follow a simple pattern, such as last requested, ﬁrst released (stack order), or ﬁrst requested, ﬁrst released (queue order), memory management is fairly easy. we are concerned in this section with the general case where blocks of any size might be requested and released in any order. this is known as dynamic storage allocation. one example of dynamic storage allocation is managing free store for a compiler’s runtime environment, such as the systemlevel new operations in java. another example is managing main memory in a multitasking operating system. here, a program might require a certain amount of space, and the memory manager must keep track of which programs are using which parts of the main memory. yet another example is the ﬁle manager for a disk drive. when a disk ﬁle is created, expanded, or deleted, the ﬁle manager must allocate or deallocate disk space.
a block of memory or disk space managed in this way is sometimes referred to as a heap. the term “heap” is being used here in a different way than the heap data structure discussed in section 5.5. here “heap” refers to the memory controlled by a dynamic memory management scheme.
in the rest of this section, we ﬁrst study techniques for dynamic memory management. we then tackle the issue of what to do when no single block of memory in the memory pool is large enough to honor a given request.
12.3.1 dynamic storage allocation for the purpose of dynamic storage allocation, we view memory as a single array broken into a series of variable-size blocks, where some of the blocks are free and some are reserved or already allocated. the free blocks are linked together to form
figure 12.9 dynamic storage allocation model. memory is made up of a series of variable-size blocks, some allocated and some free. in this example, shaded areas represent memory currently allocated and unshaded areas represent unused memory available for future allocation.
a freelist used for servicing future memory requests. figure 12.9 illustrates the situation that can arise after a series of memory allocations and deallocations.
when a memory request is received by the memory manager, some block on the freelist must be found that is large enough to service the request. if no such block is found, then the memory manager must resort to a failure policy such as discussed in section 12.3.2.
if there is a request for m words, and no block exists of exactly size m, then a larger block must be used instead. one possibility in this case is that the entire block is given away to the memory allocation request. this might be desirable when the size of the block is only slightly larger than the request. this is because saving a tiny block that is too small to be useful for a future memory request might not be worthwhile. alternatively, for a free block of size k, with k > m, up to k − m space may be retained by the memory manager to form a new free block, while the rest is used to service the request.
memory managers can suffer from two types of fragmentation. external fragmentation occurs when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. internal fragmentation occurs when more than m words are allocated to a request for m words, wasting free storage. this is equivalent to the internal fragmentation that occurs when ﬁles are allocated in multiples of the cluster size. the difference between internal and external fragmentation is illustrated by figure 12.10.
some memory management schemes sacriﬁce space to internal fragmentation to make memory management easier (and perhaps reduce external fragmentation). for example, external fragmentation does not happen in ﬁle management systems
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
figure 8.11 a comparison of three external sorts on a collection of small records for ﬁles of various sizes. each entry in the table shows time in seconds and total number of blocks read and written by the program. file sizes are in megabytes. for the third sorting algorithm, on ﬁle size of 4mb, the time and blocks shown in the last column are for a 32-way merge. 32 is used instead of 16 because 32 is a root of the number of blocks in the ﬁle (while 16 is not), thus allowing the same number of runs to be merged at every pass.
merges for r beyond about 4 or 8 runs does not help much because a lot of time is spent determining which is the next smallest element among the r runs.
we see from this experiment that building large initial runs reduces the running time to slightly more than one third that of standard mergesort, depending on ﬁle and memory sizes. using a multiway merge further cuts the time nearly in half. in summary, a good external sorting algorithm will seek to do the following: • make the initial runs as long as possible. • at all stages, overlap input, processing, and output as much as possible. • use as much working memory as possible. applying more memory usually speeds processing. in fact, more memory will have a greater effect than a faster disk. a faster cpu is unlikely to yield much improvement in running time for external sorting, because disk i/o speed is the limiting factor.
a good general text on ﬁle processing is folk and zoellig’s file structures: a conceptual toolkit [fz98]. a somewhat more advanced discussion on key issues in ﬁle processing is betty salzberg’s file structures: an analytical approach [sal88].
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
for an expanded discussion of the issues touched on in this chapter, see a general ﬁle processing text such as file structures: a conceptual toolkit by folk and zoellick [fz98]. in particular, folk and zoellick provide a good discussion of the relationship between primary and secondary indices. the most thorough discussion on various implementations for the b-tree is the survey article by comer [com79]. also see [sal88] for further details on implementing b-trees. see shaffer and brown [sb93] for a discussion of buffer pool management strategies for b+-tree-like data structures.
10.1 assume that a computer system has disk blocks of 1024 bytes, and that you are storing records that have 4-byte keys and 4-byte data ﬁelds. the records are sorted and packed sequentially into the disk ﬁle. (a) assume that a linear index uses 4 bytes to store the key and 4 bytes to store the block id for the associated records. what is the greatest number of records that can be stored in the ﬁle if a linear index of size 256kb is used?
(b) what is the greatest number of records that can be stored in the ﬁle if the linear index is also stored on disk (and thus its size is limited only by the second-level index) when using a second-level index of 1024 bytes (i.e., 256 key values) as illustrated by figure 10.2? each element of the second-level index references the smallest key value for a disk block of the linear index.
10.2 assume that a computer system has disk blocks of 4096 bytes, and that you are storing records that have 4-byte keys and 64-byte data ﬁelds. the records are sorted and packed sequentially into the disk ﬁle. (a) assume that a linear index uses 4 bytes to store the key and 4 bytes to store the block id for the associated records. what is the greatest number of records that can be stored in the ﬁle if a linear index of size 2mb is used?
(b) what is the greatest number of records that can be stored in the ﬁle if the linear index is also stored on disk (and thus its size is limited only by the second-level index) when using a second-level index of 4096 bytes (i.e., 1024 key values) as illustrated by figure 10.2? each element of the second-level index references the smallest key value for a disk block of the linear index.
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
computer storage devices are typically classiﬁed into primary or main memory and secondary or peripheral storage. primary memory usually refers to random access memory (ram), while secondary storage refers to devices such as hard disk drives, removeable “ﬂash” drives, ﬂoppy disks, cds, dvds, and tape drives. primary memory also includes registers, cache, and video memories, but we will ignore them for this discussion because their existence does not affect the principal differences between primary and secondary memory.
along with a faster cpu, every new model of computer seems to come with more main memory. as memory size continues to increase, is it possible that relatively slow disk storage will be unnecessary? probably not, because the desire to store and process larger ﬁles grows at least as fast as main memory size. prices for both main memory and peripheral storage devices have dropped dramatically in recent years, as demonstrated by figure 8.1. however, the cost for disk drive storage per megabyte is about two orders of magnitude less than ram and has been for many years.
there is now a wide range of removable media available for transfering data or storing data ofﬂine in relative safety. these include ﬂoppy disks (now largely obsolete), writable cds and dvds, “ﬂash” drives, and magnetic tape. optical storage such as cds and dvds costs roughly half the price of hard disk drive space per megabyte, and have become practical for use as backup storage within the past few years. tape used to be much cheaper than other media, and was the preferred means of backup. “flash” drives cost the most per megabyte, but due to their storage capacity and ﬂexibility, have now replaced ﬂoppy disks as the primary storage device for transferring data between computer when direct network transfer is not available.
secondary storage devices have at least two other advantages over ram memory. perhaps most importantly, disk and tape ﬁles are persistent, meaning that
they are not erased from disk and tape when the power is turned off. in contrast, ram used for main memory is usually volatile — all information is lost with the power. a second advantage is that ﬂoppy disks, cd-roms, and “ﬂash” drives can easily be transferred between computers. this provides a convenient way to take information from one computer to another.
in exchange for reduced storage costs, persistence, and portability, secondary storage devices pay a penalty in terms of increased access time. while not all accesses to disk take the same amount of time (more on this later), the typical time required to access a byte of storage from a disk drive in 2007 is around 9 ms (i.e., 9 thousandths of a second). this might not seem slow, but compared to the time required to access a byte from main memory, this is fantastically slow. typical access time from standard personal computer ram in 2007 is about 5-10 nanoseconds (i.e., 5-10 billionths of a second). thus, the time to access a byte of data from a disk drive is about six orders of magnitude greater than that required to access a byte from main memory. while disk drive and ram access times are both decreasing, they have done so at roughly the same rate. the relative speeds have remained the same for over twenty-ﬁve years, in that the difference in access time between ram and a disk drive has remained in the range between a factor of 100,000 and 1,000,000.
to gain some intuition for the signiﬁcance of this speed difference, consider the time that it might take for you to look up the entry for disk drives in the index of this book, and then turn to the appropriate page. call this your “primary memory” access time. if it takes you about 20 seconds to perform this access, then an access taking 500,000 times longer would require months.
it is interesting to note that while processing speeds have increased dramatically, and hardware prices have dropped dramatically, disk and memory access times have improved by less than an order of magnitude over the past ten years. however, the situation is really much better than that modest speedup would suggest. during the same time period, the size of both disk and main memory has increased by about three orders of magnitude. thus, the access times have actually decreased in the face of a massive increase in the density of these storage devices. due to the relatively slow access time for data on disk as compared to main memory, great care is required to create efﬁcient applications that process diskbased information. the million-to-one ratio of disk access time versus main memory access time makes the following rule of paramount importance when designing disk-based applications:
figure 8.3 the organization of a disk platter. dots indicate density of information. (a) nominal arrangement of tracks showing decreasing data density when moving outward from the center of the disk. (b) a “zoned” arrangement with the sector size and density periodically reset in tracks further away from the center.
rangement is illustrated by figure 8.3a. disk drives today actually group tracks into “zones” such that the tracks in the innermost zone adjust their data density going out to maintain the same radial data density, then the tracks of the next zone reset the data density to make better use of their storage ability, and so on. this arrangement is shown in figure 8.3b.
in contrast to the physical layout of a hard disk, a cd-rom consists of a single spiral track. bits of information along the track are equally spaced, so the information density is the same at both the outer and inner portions of the track. to keep
2t (n − 1) + c. t (n) = Θ(2n). but... there are only n(k + 1) subproblems to solve! clearly, there are many subproblems being solved repeatedly. store a n × k + 1 matrix to contain the solutions for all p (i, k). fill in the rows from i = 0 to n, left to right.
8 9 k1 =9 o − − − − − − − − i k2 =2 o − i − − − − − − o k3 =7 o − o − − − − i k4 =4 o − o − i − i o − o k5 =1 o i o i o i o i/o i o key:
-: no solution for p (i, k). o: solution(s) for p (i, k) with i omitted. i: solution(s) for p (i, k) with i included. i/o: solutions for p (i, k) with i included and omitted.
example: m(3, 9) contains o because p (2, 9) has a solution. it contains i because p (2, 2) = p (2, 9 − 7) has a solution. how can we ﬁnd a solution to p (5, 10)? how can we ﬁnd all solutions to p (5, 10)?
we next consider the problem of ﬁnding the shortest distance between all pairs of vertices in the graph, called the all-pairs shortest-paths problem. to be precise, for every u, v ∈ v, calculate d(u, v). one solution is to run dijkstra’s algorithm |v| times, each time computing the shortest path from a different start vertex. if g is sparse (that is, |e| = Θ(|v|)) then this is a good solution, because the total cost will be Θ(|v|2 + |v||e| log |v|) = Θ(|v|2 log |v|) for the version of dijkstra’s algorithm based on priority queues.
4. x’s rank is “usually” “close” to y ’s rank. we often give such algorithms names: 1. exact or deterministic algorithm. 2. approximation algorithm. 3. probabilistic algorithm. 4. heuristic. we can also sacriﬁce reliability for speed: 1. we ﬁnd the best, “usually” fast. 2. we ﬁnd the best fast, or we don’t get an answer at all (but fast). choose m elements at random, and pick the best. • for large n, if m = log n, the answer is pretty good. • cost is m − 1.
this just does the transform on one of the two polynomials. the full process is: 1. transform each polynomial. 2. multiply the resulting values (o(n) multiplies). 3. do the inverse transformation on the result.
16.4 the implementation for floyd’s algorithm given in section 16.2.2 is inefﬁcient for adjacency lists because the edges are visited in a bad order when initializing array d. what is the cost of of this initialization step for the adjacency list? how can this initialization step be revised so that it costs Θ(|v|2) in the worst case?
16.6 show the skip list that results from inserting the following values. draw the skip list after each insert. with each value, assume the depth of its corresponding node is as given in the list.
this book describes many data structures that can be used in a wide variety of problems. there are also many examples of efﬁcient algorithms. in general, our search algorithms strive to be at worst in o(log n) to ﬁnd a record, while our sorting algorithms strive to be in o(n log n). a few algorithms have higher asymptotic complexity, such as floyd’s all-pairs shortest-paths algorithm, whose running time is Θ(n3).
we can solve many problems efﬁciently because we have available (and choose to use) efﬁcient algorithms. given any problem for which you know some algorithm, it is always possible to write an inefﬁcient algorithm to “solve” the problem. for example, consider a sorting algorithm that tests every possible permutation of its input until it ﬁnds the correct permutation that provides a sorted list. the running time for this algorithm would be unacceptably high, because it is proportional to the number of permutations which is n! for n inputs. when solving the minimumcost spanning tree problem, if we were to test every possible subset of edges to see which forms the shortest minimum spanning tree, the amount of work would be proportional to 2|e| for a graph with |e| edges. fortunately, for both of these problems we have more clever algorithms that allow us to ﬁnd answers (relatively) quickly without explicitly testing every possible solution.
unfortunately, there are many computing problems for which the best possible algorithm takes a long time to run. a simple example is the towers of hanoi problem, which requires 2n moves to “solve” a tower with n disks. it is not possible for any computer program that solves the towers of hanoi problem to run in less than Ω(2n) time, because that many moves must be printed out.
besides those problems whose solutions must take a long time to run, there are also many problems for which we simply do not know if there are efﬁcient algorithms or not. the best algorithms that we know for such problems are very
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
figure 12.9 dynamic storage allocation model. memory is made up of a series of variable-size blocks, some allocated and some free. in this example, shaded areas represent memory currently allocated and unshaded areas represent unused memory available for future allocation.
a freelist used for servicing future memory requests. figure 12.9 illustrates the situation that can arise after a series of memory allocations and deallocations.
when a memory request is received by the memory manager, some block on the freelist must be found that is large enough to service the request. if no such block is found, then the memory manager must resort to a failure policy such as discussed in section 12.3.2.
if there is a request for m words, and no block exists of exactly size m, then a larger block must be used instead. one possibility in this case is that the entire block is given away to the memory allocation request. this might be desirable when the size of the block is only slightly larger than the request. this is because saving a tiny block that is too small to be useful for a future memory request might not be worthwhile. alternatively, for a free block of size k, with k > m, up to k − m space may be retained by the memory manager to form a new free block, while the rest is used to service the request.
memory managers can suffer from two types of fragmentation. external fragmentation occurs when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. internal fragmentation occurs when more than m words are allocated to a request for m words, wasting free storage. this is equivalent to the internal fragmentation that occurs when ﬁles are allocated in multiples of the cluster size. the difference between internal and external fragmentation is illustrated by figure 12.10.
some memory management schemes sacriﬁce space to internal fragmentation to make memory management easier (and perhaps reduce external fragmentation). for example, external fragmentation does not happen in ﬁle management systems
assumes that memory is of size 2n for some integer n. both free and reserved blocks will always be of size 2k for k ≤ n. at any given time, there might be both free and reserved blocks of various sizes. the buddy system keeps a separate list for free blocks of each size. there can be at most n such lists, because there can only be n distinct block sizes. when a request comes in for m words, we ﬁrst determine the smallest value of k such that 2k ≥ m. a block of size 2k is selected from the free list for that block size if one exists. the buddy system does not worry about internal fragmentation: the entire block of size 2k is allocated.
if no block of size 2k exists, the next larger block is located. this block is split in half (repeatedly if necessary) until the desired block of size 2k is created. any other blocks generated as a by-product of this splitting process are placed on the appropriate freelists.
the disadvantage of the buddy system is that it allows internal fragmentation. for example, a request for 257 words will require a block of size 512. the primary advantages of the buddy system are (1) there is less external fragmentation; (2) search for a block of the right size is cheaper than, say, best ﬁt because we need only ﬁnd the ﬁrst available block on the block list for blocks of size 2k; and (3) merging adjacent free blocks is easy.
the reason why this method is called the buddy system is because of the way that merging takes place. the buddy for any block of size 2k is another block of the same size, and with the same address (i.e., the byte position in memory, read as a binary value) except that the kth bit is reversed. for example, the block of size 8 with beginning address 0000 in figure 12.14(a) has buddy with address 1000. likewise, in figure 12.14(b), the block of size 4 with address 0000 has buddy 0100. if free blocks are sorted by address value, the buddy can be found by searching the correct block size list. merging simply requires that the address for the combined buddies be moved to the freelist for the next larger block size.
in addition to sequential-ﬁt and buddy methods, there are many ad hoc approaches to memory management. if the application is sufﬁciently complex, it might be desirable to break available memory into several memory zones, each with a different memory management scheme. for example, some zones might have a simple memory access pattern of ﬁrst-in, ﬁrst-out. this zone can therefore be managed efﬁciently by using a simple stack. another zone might allocate only records of ﬁxed size, and so can be managed with a simple freelist as described in section 4.1.2. other zones might need one of the general-purpose memory allocation methods
figure 12.15 using handles for dynamic memory management. the memory manager returns the address of the handle in response to a memory request. the handle stores the address of the actual memory block. in this way, the memory block might be moved (with its address updated in the handle) without disrupting the application program.
ﬁt memory allocation method, where external fragmentation has led to a series of small blocks that collectively could service the request. in this case, it might be possible to compact memory by moving the reserved blocks around so that the free space is collected into a single block. a problem with this approach is that the application must somehow be able to deal with the fact that all of its data have now been moved to different locations. if the application program relies on the absolute positions of the data in any way, this would be disastrous. one approach for dealing with this problem is the use of handles. a handle is a second level of indirection to a memory location. the memory allocation routine does not return a pointer to the block of storage, but rather a pointer to a variable that in turn points to the storage. this variable is the handle. the handle never moves its position, but the position of the block might be moved and the value of the handle updated. figure 12.15 illustrates the concept.
another failure policy that might work in some applications is to defer the memory request until sufﬁcient memory becomes available. for example, a multitasking operating system could adopt the strategy of not allowing a process to run until there is sufﬁcient memory available. while such a delay might be annoying to the user, it is better than halting the entire system. the assumption here is that other processes will eventually terminate, freeing memory.
another option might be to allocate more memory to the memory manager. in a zoned memory allocation system where the memory manager is part of a larger system, this might be a viable option. in a java program that implements its own memory manager, it might be possible to get more memory from the system-level new operator, such as is done by the freelist of section 4.1.2.
figure 12.9 dynamic storage allocation model. memory is made up of a series of variable-size blocks, some allocated and some free. in this example, shaded areas represent memory currently allocated and unshaded areas represent unused memory available for future allocation.
a freelist used for servicing future memory requests. figure 12.9 illustrates the situation that can arise after a series of memory allocations and deallocations.
when a memory request is received by the memory manager, some block on the freelist must be found that is large enough to service the request. if no such block is found, then the memory manager must resort to a failure policy such as discussed in section 12.3.2.
if there is a request for m words, and no block exists of exactly size m, then a larger block must be used instead. one possibility in this case is that the entire block is given away to the memory allocation request. this might be desirable when the size of the block is only slightly larger than the request. this is because saving a tiny block that is too small to be useful for a future memory request might not be worthwhile. alternatively, for a free block of size k, with k > m, up to k − m space may be retained by the memory manager to form a new free block, while the rest is used to service the request.
memory managers can suffer from two types of fragmentation. external fragmentation occurs when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. internal fragmentation occurs when more than m words are allocated to a request for m words, wasting free storage. this is equivalent to the internal fragmentation that occurs when ﬁles are allocated in multiples of the cluster size. the difference between internal and external fragmentation is illustrated by figure 12.10.
some memory management schemes sacriﬁce space to internal fragmentation to make memory management easier (and perhaps reduce external fragmentation). for example, external fragmentation does not happen in ﬁle management systems
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
figure 12.9 dynamic storage allocation model. memory is made up of a series of variable-size blocks, some allocated and some free. in this example, shaded areas represent memory currently allocated and unshaded areas represent unused memory available for future allocation.
a freelist used for servicing future memory requests. figure 12.9 illustrates the situation that can arise after a series of memory allocations and deallocations.
when a memory request is received by the memory manager, some block on the freelist must be found that is large enough to service the request. if no such block is found, then the memory manager must resort to a failure policy such as discussed in section 12.3.2.
if there is a request for m words, and no block exists of exactly size m, then a larger block must be used instead. one possibility in this case is that the entire block is given away to the memory allocation request. this might be desirable when the size of the block is only slightly larger than the request. this is because saving a tiny block that is too small to be useful for a future memory request might not be worthwhile. alternatively, for a free block of size k, with k > m, up to k − m space may be retained by the memory manager to form a new free block, while the rest is used to service the request.
memory managers can suffer from two types of fragmentation. external fragmentation occurs when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. internal fragmentation occurs when more than m words are allocated to a request for m words, wasting free storage. this is equivalent to the internal fragmentation that occurs when ﬁles are allocated in multiples of the cluster size. the difference between internal and external fragmentation is illustrated by figure 12.10.
some memory management schemes sacriﬁce space to internal fragmentation to make memory management easier (and perhaps reduce external fragmentation). for example, external fragmentation does not happen in ﬁle management systems
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
freelists are particularly useful for linked lists that periodically grow and then shrink. the freelist will never grow larger than the largest size yet reached by the linked list. requests for new nodes (after the list has shrunk) can be handled by the freelist.
one approach to implementing freelists would be to create two new methods to handle requesting and freeing nodes. this requires that the user’s code, such as the linked list class implementation of figure 4.8, be modiﬁed to call these freelist operators. in the implementation shown here, the link class is augmented with methods get and release. figure 4.11 shows the reimplementation for the link class to support these methods. note how simple they are, because they need only remove and add an element to the front of the freelist, respectively.
in figure 4.11, you should note the use of the static deﬁnition for the freelist header. the purpose of the keyword static is to create a single variable shared among all instances of the link nodes. we want only a single freelist for all link nodes of a given type. a program might create multiple lists. if they are all of the same type (that is, their element types are the same), then they can and should share the same freelist. this will happen with the implementation of figure 4.11. if lists are created that have different element types, because this code is implemented with templates, the need for different list implementations will be discovered by the compiler at compile time. separate versions of the list class will be generated for each element type. thus, each element type will also get its own separate copy of the link class. and each distinct link class implementation will get a separate freelist.
let’s consider a more complex situation where we want separate freelists, but we don’t know this until runtime. for example, perhaps we are using lists to store variable-length strings. a given node gets space allocated for a string of a speciﬁc length. we’d like to be able to reuse the nodes by placing them on a freelist, instead of relying on the system free store operators. but we cannot reuse a given list node unless the size allocated for its string matches the length of the string we want to store in this node. thus, each speciﬁc node size must be stored on its own freelist. that is, the nodes for strings of length one are stored on one freelist, the nodes for strings of length two on another freelist, and so on. in this way, its easy to ﬁnd a node of the proper size.
unfortunately, we don’t know in advance what the various sizes of strings will be. if we want to support long strings, then there could be thousands of different node sizes, many of which are not used in any particular run of the program. thus,
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
figure 11.18 illustrates dijkstra’s algorithm. the start vertex is a. all vertices except a have an initial value of ∞. after processing vertex a, its neighbors have their d estimates updated to be the direct distance from a. after processing c (the closest vertex to a), vertices b and e are updated to reﬂect the shortest path through c. the remaining vertices are processed in order b, d, and e.
this section presents two algorithms for determining the minimum-cost spanning tree (mst) for a graph. the mst problem takes as input a connected, undirected graph g, where each edge has a distance or weight measure attached. the mst is the graph containing the vertices of g along with the subset of g’s edges that (1) has minimum total cost as measured by summing the values for all of the edges in the subset, and (2) keeps the vertices connected. applications where a solution to this problem is useful include soldering the shortest set of wires needed to connect a set of terminals on a circuit board, and connecting a set of cities by telephone lines in such a way as to require the least amount of cable.
the mst contains no cycles. if a proposed set of edges did have a cycle, a cheaper mst could be had by removing any one of the edges in the cycle. thus, the mst is a free tree with |v|−1 edges. the name “minimum-cost spanning tree” comes from the fact that the required set of edges forms a tree, it spans the vertices (i.e., it connects them together), and it has minimum cost. figure 11.19 shows the mst for an example graph.
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
some binary tree implementations store data only at the leaf nodes, using the internal nodes to provide structure to the tree. more generally, binary tree implementations might require some amount of space for internal nodes, and a different amount for leaf nodes. thus, to analyze the space required by such implementations, it is useful to know the minimum and maximum fraction of the nodes that are leaves in a tree containing n internal nodes.
unfortunately, this fraction is not ﬁxed. a binary tree of n internal nodes might have only one leaf. this occurs when the internal nodes are arranged in a chain ending in a single leaf as shown in figure 5.4. in this case, the number of leaves is low because each internal node has only one non-empty child. to ﬁnd an upper bound on the number of leaves for a tree of n internal nodes, ﬁrst note that the upper bound will occur when each internal node has two non-empty children, that is, when the tree is full. however, this observation does not tell what shape of tree will yield the highest percentage of non-empty leaves. it turns out not to matter, because all full binary trees with n internal nodes have the same number of leaves. this fact allows us to compute the space requirements for a full binary tree implementation whose leaves require a different amount of space from its internal nodes.
proof: the proof is by mathematical induction on n, the number of internal nodes. this is an example of an induction proof where we reduce from an arbitrary instance of size n to an instance of size n − 1 that meets the induction hypothesis.
• base cases: the non-empty tree with zero internal nodes has one leaf node. a full binary tree with one internal node has two leaf nodes. thus, the base cases for n = 0 and n = 1 conform to the theorem.
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
example 6.5 for the binary tree of figure 6.17, the corresponding sequential representation would be as follows (assuming that ‘/’ stands for null):
to reconstruct the tree structure from this node list, we begin by setting node a to be the root. a’s left child will be node b. node b’s left child is a null pointer, so node d must be b’s right child. node d has two null children, so node c must be the right child of node a.
to illustrate the difﬁculty involved in using the sequential tree representation for processing, consider searching for the right child of the root node. we must ﬁrst move sequentially through the node list of the left subtree. only at this point do we reach the value of the root’s right child. clearly the sequential representation is space efﬁcient, but not time efﬁcient for descending through the tree along some arbitrary path.
assume that each node value takes a constant amount of space. an example would be if the node value is a positive integer and null is indicated by the value zero. from the full binary tree theorem of section 5.1.1, we know that the size of the node list will be about twice the number of nodes (i.e., the overhead fraction is 1/2). the extra space is required by the null pointers. we should be able to store the node list more compactly. however, any sequential implementation must recognize when a leaf node has been reached, that is, a leaf node indicates the end of a subtree. one way to do this is to explicitly list with each node whether it is an internal node or a leaf. if a node x is an internal node, then we know that its
figure 4.7 insertion using a header node, with curr pointing one node head of the current element. (a) linked list before insertion. the current node contains 12. (b) linked list after inserting the node containing 10.
inserting a new element is a three-step process. first, the new list node is created and the new element is stored into it. second, the next ﬁeld of the new list node is assigned to point to the current node (the one (after) the node that curr points to). third, the next ﬁeld of node pointed to by curr is assigned to point to the newly inserted node. the following line in the insert method of figure 4.8 actually does all three of these steps. curr.setnext(new link<e>(it, curr.next())); operator new creates the new link node and calls the constructor for the link class, which takes two parameters. the ﬁrst is the element. the second is the value to be placed in the list node’s next ﬁeld, in this case “curr.next().” figure 4.9 illustrates this three-step process. once the new node is added, tail is pushed forward if the new element was added to the end of the list. insertion requires Θ(1) time.
removing a node from the linked list requires only that the appropriate pointer be redirected around the node to be deleted. this memory eventually be reclaimed by the garbage collector. the following lines from the remove method of figure 4.8 do precisely this.
many organizations are hierarchical in nature, such as the military and most businesses. consider a company with a president and some number of vice presidents who report to the president. each vice president has some number of direct subordinates, and so on. if we wanted to model this company with a data structure, it would be natural to think of the president in the root node of a tree, the vice presidents at level 1, and their subordinates at lower levels in the tree as we go down the organizational hierarchy.
because the number of vice presidents is likely to be more than two, this company’s organization cannot easily be represented by a binary tree. we need instead to use a tree whose nodes have an arbitrary number of children. unfortunately, when we permit trees to have nodes with an arbrary number of children, they become much harder to implement than binary trees. we consider such trees in this chapter. to distinguish them from the more commonly used binary tree, we use the term general tree.
section 6.1 presents general tree terminology. section 6.2 presents a simple representation for solving the important problem of processing equivalence classes. several pointer-based implementations for general trees are covered in section 6.3. aside from general trees and binary trees, there are also uses for trees whose internal nodes have a ﬁxed number k of children where k is something other than two. such trees are known as k-ary trees. section 6.4 generalizes the properties of binary trees to k-ary trees. sequential representations, useful for applications such as storing trees on disk, are covered in section 6.5.
a tree t is a ﬁnite set of one or more nodes such that there is one designated node r, called the root of t. if the set (t−{r}) is not empty, these nodes are partitioned
6.1 write classes that implement the general tree class declarations of figure 6.2 using the dynamic “left-child/right-sibling” representation described in section 6.3.4.
6.2 write classes that implement the general tree class declarations of figure 6.2 using the linked general tree implementation with child pointer arrays of figure 6.12. your implementation should support only ﬁxed-size nodes that do not change their number of children once they are created. then, reimplement these classes with the linked list of children representation of figure 6.13. how do the two implementations compare in space and time efﬁciency and ease of implementation?
6.3 write classes that implement the general tree class declarations of figure 6.2 using the linked general tree implementation with child pointer arrays of figure 6.12. your implementation must be able to support changes in the number of children for a node. when created, a node should be allocated with only enough space to store its initial set of children. whenever a new child is added to a node such that the array overﬂows, allocate a new array from free store that can store twice as many children.
6.4 implement a bst ﬁle archiver. your program should take a bst created in main memory using the implementation of figure 5.14 and write it out to disk using one of the sequential representations of section 6.5. it should also be able to read in disk ﬁles using your sequential representation and create the equivalent main memory representation.
6.5 use the union/find algorithm to implement a solution to the following problem. given a set of points represented by their xy-coordinates, assign the points to clusters. any two points are deﬁned to be in the same cluster if they are within a speciﬁed distance d of each other. for the purpose of this problem, clustering is an equivalence relationship. in other words, points a, b, and c are deﬁned to be in the same cluster if the distance between a and b is less than d and the distance between a and c is also less than d, even if the distance between b and c is greater than d. to solve the problem, compute the distance between each pair of points, using the equivalence processing algorithm to merge clusters whenever two points are within the speciﬁed distance. what is the asymptotic complexity of this algorithm? where is the bottleneck in processing?
6.6 in this project, you will run some empirical tests to deterimine if some variations on path compression in the union/find algorithm will lead to im-
figure 6.1 notation for general trees. node p is the parent of nodes v, s1, and s2. thus, v, s1, and s2 are children of p. nodes r and p are ancestors of v. nodes v, s1, and s2 are called siblings. the oval surrounds the subtree having v as its root.
into n ≥ 0 disjoint subsets t0, t1, ..., tn−1, each of which is a tree, and whose roots r1, r2, ..., rn, respectively, are children of r. the subsets ti (0 ≤ i < n) are said to be subtrees of t. these subtrees are ordered in that ti is said to come before tj if i < j. by convention, the subtrees are arranged from left to right with subtree t0 called the leftmost child of r. a node’s out degree is the number of children for that node. a forest is a collection of one or more trees. figure 6.1 presents further tree notation generalized from the notation for binary trees presented in chapter 5. each node in a tree has precisely one parent, except for the root, which has no parent. from this observation, it immediately follows that a tree with n nodes must have n − 1 edges because each node, aside from the root, has one edge connecting that node to its parent.
6.1.1 an adt for general tree nodes before discussing general tree implementations, we should ﬁrst make precise what operations such implementations must support. any implementation must be able to initialize a tree. given a tree, we need access to the root of that tree. there must be some way to access the children of a node. in the case of the adt for binary tree nodes, this was done by providing member functions that give explicit
/** general tree adt */ interface gentree<e> { public void clear(); public gtnode<e> root(); // make the tree have a new root, give first child and sib public void newroot(e value, gtnode<e> first,
access to the left and right child pointers. unfortunately, because we do not know in advance how many children a given node will have in the general tree, we cannot give explicit functions to access each child. an alternative must be found that works for any number of children.
one choice would be to provide a function that takes as its parameter the index for the desired child. that combined with a function that returns the number of children for a given node would support the ability to access any node or process all children of a node. unfortunately, this view of access tends to bias the choice for node implementations in favor of an array-based approach, because these functions favor random access to a list of children. in practice, an implementation based on a linked list is often preferred.
an alternative is to provide access to the ﬁrst (or leftmost) child of a node, and to provide access to the next (or right) sibling of a node. figure 6.2 shows class declarations for general trees and their nodes. based on these two access functions, the children of a node can be traversed like a list. trying to ﬁnd the next sibling of the rightmost sibling would return null.
in section 5.2, three tree traversals were presented for binary trees: preorder, postorder, and inorder. for general trees, preorder and postorder traversals are deﬁned with meanings similar to their binary tree counterparts. preorder traversal of a general tree ﬁrst visits the root of the tree, then performs a preorder traversal of each subtree from left to right. a postorder traversal of a general tree performs a postorder traversal of the root’s subtrees from left to right, then visits the root. inorder traversal does not have a natural deﬁnition for the general tree, because there is no particular number of children for an internal node. an arbitrary deﬁnition — such as visit the leftmost subtree in inorder, then the root, then visit the remaining subtrees in inorder — can be invented. however, inorder traversals are generally not useful with general trees.
6.2 write an algorithm to determine if two binary trees are identical when the ordering of the subtrees for a node is ignored. for example, if a tree has root node with value r, left child with value a and right child with value b, this would be considered identical to another tree with root node value r, left child value b, and right child value a. make the algorithm as efﬁcient as you can. analyze your algorithm’s running time. how much harder would it be to make this algorthm work on a general tree?
6.4 write a function that takes as input a general tree and returns the number of nodes in that tree. write your function to use the gentree and gtnode adts of figure 6.2.
6.5 describe how to implement the weighted union rule efﬁciently. in particular, describe what information must be stored with each node and how this information is updated when two trees are merged. modify the implementation of figure 6.4 to support the weighted union rule.
6.6 a potential alternatative to the weighted union rule for combining two trees is the height union rule. the height union rule requires that the root of the tree with greater height become the root of the union. explain why the height union rule can lead to worse average time behavior than the weighted union rule.
6.7 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (0, 2) (1, 2) (3, 4) (3, 1) (3, 5) (9, 11) (12, 14) (3, 9) (4, 14) (6, 7) (8, 10) (8, 7) (7, 0) (10, 15) (10, 13)
6.8 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (2, 3) (4, 5) (6, 5) (3, 5) (1, 0) (7, 8) (1, 8) (3, 8) (9, 10) (11, 14) (11, 10) (12, 13) (11, 13) (14, 1)
figure 6.12 a dynamic general tree representation with ﬁxed-size arrays for the child pointers. (a) the general tree. (b) the tree representation. for each node, the ﬁrst ﬁeld stores the node value while the second ﬁeld stores the size of the child pointer array.
space, and the old space is then returned to free store. as an alternative to relying on the system’s garbage collector, a memory manager for variable size storage units can be implemented, as described in section 12.3. another possibility is to use a collection of free lists, one for each array size, as described in section 4.1.2. note in figure 6.12 that the current number of children for each node is stored explicitly in a size ﬁeld. the child pointers are stored in an array with size elements.
another approach that is more ﬂexible, but which requires more space, is to store a linked list of child pointers with each node as illustrated by figure 6.13. this implementation is essentially the same as the “list of children” implementation of section 6.3.1, but with dynamically allocated nodes rather than storing the nodes in an array.
6.3.4 dynamic “left-child/right-sibling” implementation the “left-child/right-sibling” implementation of section 6.3.2 stores a ﬁxed number of pointers with each node. this can be readily adapted to a dynamic implementation. in essence, we substitute a binary tree for a general tree. each node of the “left-child/right-sibling” implementation points to two “children” in a new binary tree structure. the left child of this new structure is the node’s ﬁrst child in the general tree. the right child is the node’s right sibling. we can easily extend this conversion to a forest of general trees, because the roots of the trees can be considered siblings. converting from a forest of general trees to a single binary tree is illustrated by figure 6.14. here we simply include links from each node to its right sibling and remove links to all children except the leftmost child. figure 6.15 shows how this might look in an implementation with two pointers at each node.
figure 6.14 converting from a forest of general trees to a single binary tree. each node stores pointers to its left child and right sibling. the tree roots are assumed to be siblings for the purpose of converting.
compared with the implementation illustrated by figure 6.13 which requires overhead of three pointers/node, the implementation of figure 6.15 only requires two pointers per node.
because each node of the general tree now contains a ﬁxed number of pointers, and because each function of the general tree adt can now be implemented efﬁciently, the dynamic “left-child/right-sibling” implementation is preferred to the other general tree implementations described in sections 6.3.1 to 6.3.3.
k-ary trees are trees whose internal nodes all have exactly k children. thus, a full binary tree is a 2-ary tree. the pr quadtree discussed in section 13.3 is an
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
union rule for joining sets) is Θ(n log∗ n). the notation “log∗ n” means the number of times that the log of n must be taken before n ≤ 1. for example, log∗ 65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and ﬁnally log 2 = 1. thus, log∗ n grows very slowly, so the cost for a series of n find operations is very close to n.
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth Θ(log∗ n). one can devise a series of equivalence operations that yields Θ(log n) depth for the resulting tree. however, many of the equivalences in such a series will look only at the roots of the trees being merged, requiring little processing time. the total amount of processing time required for n operations will be Θ(n log∗ n), yielding nearly constant time for each equivalence operation. this is an example of the technique of amortized analysis, discussed further in section 14.3.
we now tackle the problem of devising an implementation for general trees that allows efﬁcient processing of all member functions of the adt shown in figure 6.2. this section presents several approaches to implementing general trees. each implementation yields advantages and disadvantages in the amount of space required to store a node and the relative ease with which key operations can be performed. general tree implementations should place no restriction on how many children a node may have. in some applications, once a node is created the number of children never changes. in such cases, a ﬁxed amount of space can be allocated for the node when it is created, based on the number of children for the node. matters become more complicated if children can be added to or deleted from a node, requiring that the node’s space allocation be adjusted accordingly.
figure 6.14 converting from a forest of general trees to a single binary tree. each node stores pointers to its left child and right sibling. the tree roots are assumed to be siblings for the purpose of converting.
compared with the implementation illustrated by figure 6.13 which requires overhead of three pointers/node, the implementation of figure 6.15 only requires two pointers per node.
because each node of the general tree now contains a ﬁxed number of pointers, and because each function of the general tree adt can now be implemented efﬁciently, the dynamic “left-child/right-sibling” implementation is preferred to the other general tree implementations described in sections 6.3.1 to 6.3.3.
k-ary trees are trees whose internal nodes all have exactly k children. thus, a full binary tree is a 2-ary tree. the pr quadtree discussed in section 13.3 is an
with the “list of children” implementation, it is difﬁcult to access a node’s right sibling. figure 6.10 presents an improvement. here, each node stores its value and pointers to its parent, leftmost child, and right sibling. thus, each of the basic adt operations can be implemented by reading a value directly from the node. if two trees are stored within the same node array, then adding one as the subtree of the other simply requires setting three pointers. combining trees in this way is illustrated by figure 6.11. this implementation is more space efﬁcient than the “list of children” implementation, and each node requires a ﬁxed amount of space in the node array.
the two general tree implementations just described use an array to store the collection of nodes. in contrast, our standard implementation for binary trees stores each node as a separate dynamic object containing its value and pointers to its two children. unfortunately, nodes of a general tree can have any number of children, and this number may change during the life of the node. a general tree node implementation must support these properties. one solution is simply to limit the number
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
figure 6.9 the “list of children” implementation for general trees. the column of numbers to the left of the node array labels the array indices. the column labeled “val” stores node values. the column labeled “par” stores pointers to the parents. for clarity, these pointers are shown as array indices. the last column stores pointers to the linked list of children for each internal node. each element on the linked list stores a pointer to one of the node’s children (shown as the array index of the target node).
our ﬁrst attempt to create a general tree implementation is called the “list of children” implementation for general trees. it simply stores with each internal node a linked list of its children, in order from left to right. this is illustrated by figure 6.9. the “list of children” implementation stores the tree nodes in an array. each node contains a value, a pointer to its parent, and a pointer to a linked list of the node’s children, stored from left to right. each list element contains a pointer to one child. thus, the leftmost child of a node can be found directly because it is the ﬁrst element in the linked list. however, to ﬁnd the right sibling for a node is more difﬁcult. consider the case of a node m and its parent p. to ﬁnd m’s right sibling, we must move down the child list of p until the linked list element storing the pointer to m has been found. going one step further takes us to the linked list element that stores a pointer to m’s right sibling. thus, in the worst case, to ﬁnd m’s right sibling requires that all children of m’s parent be searched.
combining trees using this representation is difﬁcult if each tree is stored in a separate node array. if the nodes of both trees are stored in a single node array, then
with the “list of children” implementation, it is difﬁcult to access a node’s right sibling. figure 6.10 presents an improvement. here, each node stores its value and pointers to its parent, leftmost child, and right sibling. thus, each of the basic adt operations can be implemented by reading a value directly from the node. if two trees are stored within the same node array, then adding one as the subtree of the other simply requires setting three pointers. combining trees in this way is illustrated by figure 6.11. this implementation is more space efﬁcient than the “list of children” implementation, and each node requires a ﬁxed amount of space in the node array.
the two general tree implementations just described use an array to store the collection of nodes. in contrast, our standard implementation for binary trees stores each node as a separate dynamic object containing its value and pointers to its two children. unfortunately, nodes of a general tree can have any number of children, and this number may change during the life of the node. a general tree node implementation must support these properties. one solution is simply to limit the number
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
o(m log n) time for a tree of n nodes whenever m ≥ n. thus, a single insert or search operation could take o(n) time. however, m such operations are guaranteed to require a total of o(m log n) time, for an average cost of o(log n) per access operation. this is a desirable performance guarantee for any search-tree structure. unlike the avl tree, the splay tree is not guaranteed to be height balanced. what is guaranteed is that the total cost of the entire series of accesses will be cheap. ultimately, it is the cost of the series of operations that matters, not whether the tree is balanced. maintaining balance is really done only for the sake of reaching this time efﬁciency goal.
the splay tree access functions operate in a manner reminiscent of the moveto-front rule for self-organizing lists from section 9.2, and of the path compression technique for managing parent-pointer trees from section 6.2. these access functions tend to make the tree more balanced, but an individual access will not necessarily result in a more balanced tree.
whenever a node s is accessed (e.g., when s is inserted, deleted, or is the goal of a search), the splay tree performs a process called splaying. splaying moves s to the root of the bst. when s is being deleted, splaying moves the parent of s to the root. as in the avl tree, a splay of node s consists of a series of rotations. a rotation moves s higher in the tree by adjusting its position with respect to its parent and grandparent. a side effect of the rotations is a tendency to balance the tree. there are three types of rotation.
a single rotation is performed only if s is a child of the root node. the single rotation is illustrated by figure 13.7. it basically switches s with its parent in a way that retains the bst property. while figure 13.7 is slightly different from figure 13.5, in fact the splay tree single rotation is identical to the avl tree single rotation.
unlike the avl tree, the splay tree requires two types of double rotation. double rotations involve s, its parent (call it p), and s’s grandparent (call it g). the effect of a double rotation is to move s up two levels in the tree.
many organizations are hierarchical in nature, such as the military and most businesses. consider a company with a president and some number of vice presidents who report to the president. each vice president has some number of direct subordinates, and so on. if we wanted to model this company with a data structure, it would be natural to think of the president in the root node of a tree, the vice presidents at level 1, and their subordinates at lower levels in the tree as we go down the organizational hierarchy.
because the number of vice presidents is likely to be more than two, this company’s organization cannot easily be represented by a binary tree. we need instead to use a tree whose nodes have an arbitrary number of children. unfortunately, when we permit trees to have nodes with an arbrary number of children, they become much harder to implement than binary trees. we consider such trees in this chapter. to distinguish them from the more commonly used binary tree, we use the term general tree.
section 6.1 presents general tree terminology. section 6.2 presents a simple representation for solving the important problem of processing equivalence classes. several pointer-based implementations for general trees are covered in section 6.3. aside from general trees and binary trees, there are also uses for trees whose internal nodes have a ﬁxed number k of children where k is something other than two. such trees are known as k-ary trees. section 6.4 generalizes the properties of binary trees to k-ary trees. sequential representations, useful for applications such as storing trees on disk, are covered in section 6.5.
a tree t is a ﬁnite set of one or more nodes such that there is one designated node r, called the root of t. if the set (t−{r}) is not empty, these nodes are partitioned
figure 6.1 notation for general trees. node p is the parent of nodes v, s1, and s2. thus, v, s1, and s2 are children of p. nodes r and p are ancestors of v. nodes v, s1, and s2 are called siblings. the oval surrounds the subtree having v as its root.
into n ≥ 0 disjoint subsets t0, t1, ..., tn−1, each of which is a tree, and whose roots r1, r2, ..., rn, respectively, are children of r. the subsets ti (0 ≤ i < n) are said to be subtrees of t. these subtrees are ordered in that ti is said to come before tj if i < j. by convention, the subtrees are arranged from left to right with subtree t0 called the leftmost child of r. a node’s out degree is the number of children for that node. a forest is a collection of one or more trees. figure 6.1 presents further tree notation generalized from the notation for binary trees presented in chapter 5. each node in a tree has precisely one parent, except for the root, which has no parent. from this observation, it immediately follows that a tree with n nodes must have n − 1 edges because each node, aside from the root, has one edge connecting that node to its parent.
6.1.1 an adt for general tree nodes before discussing general tree implementations, we should ﬁrst make precise what operations such implementations must support. any implementation must be able to initialize a tree. given a tree, we need access to the root of that tree. there must be some way to access the children of a node. in the case of the adt for binary tree nodes, this was done by providing member functions that give explicit
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
1. natural distributions are geometric. for example, consider the populations of the 100 largest cities in the united states. if you plot these populations on a numberline, most of them will be clustered toward the low side, with a few outliers on the high side. this is an example of a zipf distribution (see section 9.2). viewed the other way, the home town for a given person is far more likely to be a particular large city than a particular small town.
1. we know nothing about the distribution of the incoming keys. in this case, we wish to select a hash function that evenly distributes the key range across the hash table, while avoiding obvious opportunities for clustering such as hash functions that are sensitive to the high- or low-order bits of the key value.
2. we know something about the distribution of the incoming keys. in this case, we should use a distribution-dependent hash function that avoids assigning clusters of related key values to the same hash table slot. for example, if hashing english words, we should not hash on the value of the ﬁrst character because this is likely to be unevenly distributed.
the value returned by this hash function depends solely on the least signiﬁcant four bits of the key. because these bits are likely to be poorly distributed (as an example, a high percentage of the keys might be even numbers, which means that the low order bit is zero), the result will also be poorly distributed. this example shows that the size of the table m can have a big effect on the performance of a hash system because this value is typically used as the modulus.
we next try to follow forward[1] of 31 to reach the node with value 58. because 58 is smaller than 62, we follow 58’s forward[1] pointer to 69. because 69 is too big, follow 58’s level 0 pointer to 62. because 62 is not less than 62, we fall out of the while loop and move one step forward to the node with value 62. the ideal skip list of figure 16.2(c) has been organized so that (if the ﬁrst and last nodes are not counted) half of the nodes have only one pointer, one quarter have two, one eighth have three, and so on. the distances are equally spaced; in effect this is a “perfectly balanced” skip list. maintaining such balance would be expensive during the normal process of insertions and deletions. the key to skip lists is that we do not worry about any of this. whenever inserting a node, we assign it a level (i.e., some number of pointers). the assignment is random, using a geometric distribution yielding a 50% probability that the node will have one pointer, a 25% probability that it will have two, and so on. the following function determines the level based on such a distribution:
once the proper level for the node has been determined, the next step is to ﬁnd where the node should be inserted and link it in as appropriate at all of its levels. figure 16.4 shows an implementation for inserting a new value into the skip list. in this example, we begin by inserting a node with value 10 into an empty skip list. assume that randomlevel returns a value of 1 (i.e., the node is at level 1, with 2 pointers). because the empty skip list has no nodes, the level of the list (and thus the level of the header node) must be set to 1. the new node is inserted, yielding the skip list of figure 16.5(a).
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
some applications must represent a large, two-dimensional matrix where many of the elements have a value of zero. one example is the lower triangular matrix that results from solving systems of simultaneous equations. a lower triangular matrix stores zero values at positions [r, c] such that r < c, as shown in figure 12.6(a). thus, the upper-right triangle of the matrix is always zero. another example is the representation of undirected graphs in an adjacency matrix (see project 11.2). because all edges between vertices i and j go in both directions, there is no need to store both. instead we can just store one edge going from the higher-indexed vertex to the lower-indexed vertex. in this case, only the lower triangle of the matrix can have non-zero values. we can take advantage of this fact to save space. instead of storing n(n + 1)/2 pieces of information in an n × n array, it would save space to use a list of length n(n + 1)/2. this is only practical if some means can be found to locate within the list the element that would correspond to position [r, c] in the original matrix.
to derive an equation to do this computation, note that row 0 of the matrix has one non-zero value, row 1 has two non-zero values, and so on. thus, row r k=1 k = (r2 + r)/2 non-zero elements. adding c to reach the cth position in the rth row yields the following equation to convert position [r, c] in the original matrix to the correct position in the list.
a similar equation can be used to store an upper triangular matrix, that is, a matrix with zero values at positions [r, c] such that r > c, as shown in figure 12.6(b). for an n × n upper triangular matrix, the equation would be
a more difﬁcult situation arises when the vast majority of values stored in an n × n matrix are zero, but there is no restriction on which positions are zero and which are non-zero. this is known as a sparse matrix.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
figure 11.3 two graph representations. (a) a directed graph. (b) the adjacency matrix for the graph of (a). (c) the adjacency list for the graph of (a).
example 11.1 the entry for vertex 0 in figure 11.3(c) stores 1 and 4 because there are two edges in the graph leaving vertex 0, with one going to vertex 1 and one going to vertex 4. the list for vertex 2 stores an entry for vertex 4 because there is an edge from vertex 2 to vertex 4, but no entry for vertex 3 because this edge comes into vertex 2 rather than going out.
the storage requirements for the adjacency list depend on both the number of edges and the number of vertices in the graph. there must be an array entry for each vertex (even if the vertex is not adjacent to any other vertex and thus has no elements on its linked list), and each edge must appear on one of the lists. thus, the cost is Θ(|v| + |e|).
both the adjacency matrix and the adjacency list can be used to store directed or undirected graphs. each edge of an undirected graph connecting vertices u and v is represented by two directed edges: one from u to v and one from v to u. figure 11.4 illustrates the use of the adjacency matrix and the adjacency list for undirected graphs.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
figure 11.3 two graph representations. (a) a directed graph. (b) the adjacency matrix for the graph of (a). (c) the adjacency list for the graph of (a).
example 11.1 the entry for vertex 0 in figure 11.3(c) stores 1 and 4 because there are two edges in the graph leaving vertex 0, with one going to vertex 1 and one going to vertex 4. the list for vertex 2 stores an entry for vertex 4 because there is an edge from vertex 2 to vertex 4, but no entry for vertex 3 because this edge comes into vertex 2 rather than going out.
the storage requirements for the adjacency list depend on both the number of edges and the number of vertices in the graph. there must be an array entry for each vertex (even if the vertex is not adjacent to any other vertex and thus has no elements on its linked list), and each edge must appear on one of the lists. thus, the cost is Θ(|v| + |e|).
both the adjacency matrix and the adjacency list can be used to store directed or undirected graphs. each edge of an undirected graph connecting vertices u and v is represented by two directed edges: one from u to v and one from v to u. figure 11.4 illustrates the use of the adjacency matrix and the adjacency list for undirected graphs.
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
public void init(int n); public int n(); public int e(); public int first(int v); public int next(int v, int w); // get v’s next neighbor public void setedge(int i, int j, int wght); // set weight public void deledge(int i, int j); // delete edge (i, j) public boolean isedge(int i, int j); // if (i,j) an edge? public int weight(int i, int j); public void setmark(int v, int val); // set mark for v public int getmark(int v); // get mark for v
figure 11.5 a graph adt. this adt assumes that the number of vertices is ﬁxed when the graph is created, but that edges can be added and removed. it also supports a mark array to aid graph traversal algorithms.
the adjacency matrix often requires a higher asymptotic cost for an algorithm than would result if the adjacency list were used. the reason is that it is common for a graph algorithm to visit each neighbor of each vertex. using the adjacency list, only the actual edges connecting a vertex to its neighbors are examined. however, the adjacency matrix must look at each of its |v| potential edges, yielding a total cost of Θ(|v2|) time when the algorithm might otherwise require only Θ(|v|+|e|) time. this is a considerable disadvantage when the graph is sparse, but not when the graph is closer to full.
we next turn to the problem of implementing a graph class. figure 11.5 shows an abstract class deﬁning an adt for graphs. vertices are deﬁned by an integer index value. in other words, there is a vertex 0, vertex 1, and so on. we can assume that a graph application stores any additional information of interest about a given vertex elsewhere, such as a name or application-dependent value. note that this adt is not implemented using a template, because it is the graph class users’ responsibility to maintain information related to the vertices themselves. the graph class has no knowledge of the type or content of the information associated with a vertex, only the index number for that vertex.
abstract class graph has methods to return the number of vertices and edges (methods n and e, respectively). function weight returns the weight of a given edge, with that edge identiﬁed by its two incident vertices. for example, calling weight(0, 4) on the graph of figure 11.1 (c) would return 4. if no such edge
functions setedge and deledge set the weight of an edge and remove an edge from the graph, respectively. again, an edge is identiﬁed by its two incident vertices. setedge does not permit the user to set the weight to be 0, because this value is used to indicate a non-existent edge, nor are negative edge weights permitted. functions getmark and setmark get and set, respectively, a requested value in the mark array (described below) for vertex v.
nearly every graph algorithm presented in this chapter will require visits to all neighbors of a given vertex. two methods are provided to support this. they work in a manner similar to linked list access functions. function first takes as input a vertex v, and returns the edge to the ﬁrst neighbor for v (we assume the neighbor list is sorted by vertex number). function next takes as input vertices v1 and v2 and returns the index for the vertex forming the next edge with v1 after v2 on v1’s edge list. function next will return a value of n = |v| once the end of the edge list for v1 has been reached. the following line appears in many graph algorithms:
this for loop gets the ﬁrst neighbor of v, then works through the remaining neighbors of v until a value equal to g->n() is returned, signaling that all neighbors of v have been visited. for example, first(1) in figure 11.4 would return 0. next(1, 0) would return 3. next(0, 3) would return 4. next(1, 4) would return 5, which is not a vertex in the graph.
it is reasonably straightforward to implement our graph and edge adts using either the adjacency list or adjacency matrix. the sample implementations presented here do not address the issue of how the graph is actually created. the user of these implementations must add functionality for this purpose, perhaps reading the graph description from a ﬁle. the graph can be built up by using the setedge function provided by the adt.
figure 11.6 shows an implementation for the adjacency matrix. array mark stores the information manipulated by the setmark and getmark functions. the edge matrix is implemented as an integer array of size n × n for a graph of n vertices. position (i, j) in the matrix stores the weight for edge (i, j) if it exists. a weight of zero for edge (i, j) is used to indicate that no edge connects vertices i and j.
given a vertex v, function first locates the position in matrix of the ﬁrst edge (if any) of v by beginning with edge (v, 0) and scanning through row v until an edge is found. if no edge is incident on v, then first returns n.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
at each iteration of the algorithm. the relative costs of these two variants depend on who sparse or dense the graph is. they might also depend on whether the graph is implemented using an adjacency list or adjacency matrix. design and implement a study to compare the effects on performance for three variables: (i) the two graph representations (adjacency list and adjacency matrix); (ii) the two implementations for djikstra’s shortest paths algorithm (searching the table of vertex distances or using a priority queue to track the distances), and (iii) sparse versus dense graphs. be sure to test your implementations on a variety of graphs that are sufﬁciently large to generate meaningful times.
11.4 the example implementations for dfs and bfs show calls to functions previsit and postvisit. better is to implement bfs and dfs using the visitor design pattern, with the visitor functions being passed in as either function or template parameters. reimplement the bfs and dfs functions to make use of the visitor design pattern.
11.5 write a program to label the connected components for an undirected graph. in other words, all vertices of the ﬁrst component are given the ﬁrst component’s label, all vertices of the second component are given the second component’s label, and so on. your algorithm should work by deﬁning any two vertices connected by an edge to be members of the same equivalence class. once all of the edges have been processed, all vertices in a given equivalence class will be connected. use the union/find implementation from section 6.2 to implement equivalence classes.
figure 11.1 examples of graphs and terminology. (a) a graph. (b) a directed graph (digraph). (c) a labeled (directed) graph with weights associated with the edges. in this example, there is a simple path from vertex 0 to vertex 3 containing vertices 0, 1, and 3. vertices 0, 1, 3, 2, 4, and 1 also form a path, but not a simple path because vertex 1 appears twice. vertices 1, 3, 2, 4, and 1 form a simple cycle.
a graph g = (v, e) consists of a set of vertices v and a set of edges e, such that each edge in e is a connection between a pair of vertices in v.1 the number of vertices is written |v|, and the number of edges is written |e|. |e| can range from zero to a maximum of |v|2 − |v|. a graph with relatively few edges is called sparse, while a graph with many edges is called dense. a graph containing all possible edges is said to be complete.
a graph with edges directed from one vertex to another (as in figure 11.1(b)) is called a directed graph or digraph. a graph whose edges are not directed is called an undirected graph (as illustrated by figure 11.1(a)). a graph with labels associated with its vertices (as in figure 11.1(c)) is called a labeled graph. two vertices are adjacent if they are joined by an edge. such vertices are also called neighbors. an edge connecting vertices u and v is written (u, v). such an edge is said to be incident on vertices u and v. associated with each edge may be a cost or weight. graphs whose edges have weights (as in figure 11.1(c)) are said to be weighted. a sequence of vertices v1, v2, ..., vn forms a path of length n − 1 if there exist edges from vi to vi+1 for 1 ≤ i < n. a path is simple if all vertices on the path are distinct. the length of a path is the number of edges it contains. a cycle is a path of length three or more that connects some vertex v1 to itself. a cycle is simple if the path is simple, except for the ﬁrst and last vertices being the same.
1some graph applications require that a given pair of vertices can have multiple edges connecting them, or that a vertex can have an edge to itself. however, the applications discussed in this book do not require either of these special cases, so for simplicity we will assume that they cannot occur.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
public void init(int n); public int n(); public int e(); public int first(int v); public int next(int v, int w); // get v’s next neighbor public void setedge(int i, int j, int wght); // set weight public void deledge(int i, int j); // delete edge (i, j) public boolean isedge(int i, int j); // if (i,j) an edge? public int weight(int i, int j); public void setmark(int v, int val); // set mark for v public int getmark(int v); // get mark for v
figure 11.5 a graph adt. this adt assumes that the number of vertices is ﬁxed when the graph is created, but that edges can be added and removed. it also supports a mark array to aid graph traversal algorithms.
the adjacency matrix often requires a higher asymptotic cost for an algorithm than would result if the adjacency list were used. the reason is that it is common for a graph algorithm to visit each neighbor of each vertex. using the adjacency list, only the actual edges connecting a vertex to its neighbors are examined. however, the adjacency matrix must look at each of its |v| potential edges, yielding a total cost of Θ(|v2|) time when the algorithm might otherwise require only Θ(|v|+|e|) time. this is a considerable disadvantage when the graph is sparse, but not when the graph is closer to full.
we next turn to the problem of implementing a graph class. figure 11.5 shows an abstract class deﬁning an adt for graphs. vertices are deﬁned by an integer index value. in other words, there is a vertex 0, vertex 1, and so on. we can assume that a graph application stores any additional information of interest about a given vertex elsewhere, such as a name or application-dependent value. note that this adt is not implemented using a template, because it is the graph class users’ responsibility to maintain information related to the vertices themselves. the graph class has no knowledge of the type or content of the information associated with a vertex, only the index number for that vertex.
abstract class graph has methods to return the number of vertices and edges (methods n and e, respectively). function weight returns the weight of a given edge, with that edge identiﬁed by its two incident vertices. for example, calling weight(0, 4) on the graph of figure 11.1 (c) would return 4. if no such edge
function next locates the edge following edge (i, j) (if any) by continuing down the row of vertex i starting at position j + 1, looking for an edge. if no such edge exists, next returns n. functions setedge and deledge adjust the appropriate value in the array. function weight returns the value stored in the appropriate position in the array.
figure 11.7 presents an implementation of the adjacency list representation for graphs. its main data structure is an array of linked lists, one linked list for each vertex. these linked lists store objects of type edge, which merely stores the index for the vertex pointed to by the edge, along with the weight of the edge.
implementation for graphl member functions is straightforward in principle, with the key functions being setedge, deledge, and weight. the simplest implementation would start at the beginning of the adjacency list and move along it until the desired vertex has been found. however, many graph algorithms work by taking advantage of the first and next functions to process all edges extending from a given vertex in turn. thus, there is a signiﬁcant time savings if setedge, deledge, and weight ﬁrst check to see if the desired edge is the current one on the relevant linked list. the implementation of figure 11.7 does exactly this.
often it is useful to visit the vertices of a graph in some speciﬁc order based on the graph’s topology. this is known as a graph traversal and is similar in concept to a tree traversal. recall that tree traversals visit every node exactly once, in some speciﬁed order such as preorder, inorder, or postorder. multiple tree traversals exist because various applications require the nodes to be visited in a particular order. for example, to print a bst’s nodes in ascending order requires an inorder traversal as opposed to some other traversal. standard graph traversal orders also exist. each is appropriate for solving certain problems. for example, many problems in artiﬁcial intelligence programming are modeled using graphs. the problem domain may consist of a large collection of states, with connections between various pairs
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
function next locates the edge following edge (i, j) (if any) by continuing down the row of vertex i starting at position j + 1, looking for an edge. if no such edge exists, next returns n. functions setedge and deledge adjust the appropriate value in the array. function weight returns the value stored in the appropriate position in the array.
figure 11.7 presents an implementation of the adjacency list representation for graphs. its main data structure is an array of linked lists, one linked list for each vertex. these linked lists store objects of type edge, which merely stores the index for the vertex pointed to by the edge, along with the weight of the edge.
implementation for graphl member functions is straightforward in principle, with the key functions being setedge, deledge, and weight. the simplest implementation would start at the beginning of the adjacency list and move along it until the desired vertex has been found. however, many graph algorithms work by taking advantage of the first and next functions to process all edges extending from a given vertex in turn. thus, there is a signiﬁcant time savings if setedge, deledge, and weight ﬁrst check to see if the desired edge is the current one on the relevant linked list. the implementation of figure 11.7 does exactly this.
often it is useful to visit the vertices of a graph in some speciﬁc order based on the graph’s topology. this is known as a graph traversal and is similar in concept to a tree traversal. recall that tree traversals visit every node exactly once, in some speciﬁed order such as preorder, inorder, or postorder. multiple tree traversals exist because various applications require the nodes to be visited in a particular order. for example, to print a bst’s nodes in ascending order requires an inorder traversal as opposed to some other traversal. standard graph traversal orders also exist. each is appropriate for solving certain problems. for example, many problems in artiﬁcial intelligence programming are modeled using graphs. the problem domain may consist of a large collection of states, with connections between various pairs
on a road map, a road connecting two towns is typically labeled with its distance. we can model a road network as a directed graph whose edges are labeled with real numbers. these numbers represent the distance (or other cost metric, such as travel time) between two vertices. these labels may be called weights, costs, or distances, depending on the application. given such a graph, a typical problem is to ﬁnd the total length of the shortest path between two speciﬁed vertices. this is not a trivial problem, because the shortest path may not be along the edge (if any) connecting two vertices, but rather may be along a path involving one or more intermediate vertices. for example, in figure 11.15, the cost of the path from a to b to d is 15. the cost of the edge directly from a to d is 20. the cost of the path from a to c to b to d is 10. thus, the shortest path from a to d is 10 (not along the edge connecting a to d). we use the notation d(a, d) = 10 to indicate that the shortest distance from a to d is 10. in figure 11.15, there is no path from e to b, so we set d(e, b) = ∞. we deﬁne w(a, d) = 20 to be the weight of edge (a, d), that is, the weight of the direct connection from a to d. because there is no edge from e to b, w(e, b) = ∞. note that w(d, a) = ∞ because the graph of figure 11.15 is directed. we assume that all weights are positive.
11.4.1 single-source shortest paths this section presents an algorithm to solve the single-source shortest-paths problem. given vertex s in graph g, ﬁnd a shortest path from s to every other vertex in g. we might want only the shortest path between two vertices, s and t. however in the worst case, while ﬁnding the shortest path from s to t, we might ﬁnd the shortest paths from s to every other vertex as well. so there is no better algorithm (in the worst case) for ﬁnding the shortest path to a single vertex than to ﬁnd shortest paths to all vertices. the algorithm described here will only compute the
distance to every such vertex, rather than recording the actual path. recording the path requires modiﬁcations to the algorithm that are left as an exercise.
computer networks provide an application for the single-source shortest-paths problem. the goal is to ﬁnd the cheapest way for one computer to broadcast a message to all other computers on the network. the network can be modeled by a graph with edge weights indicating time or cost to send a message to a neighboring computer.
for unweighted graphs (or whenever all edges have the same cost), the singlesource shortest paths can be found using a simple breadth-ﬁrst search. when weights are added, bfs will not give the correct answer.
one approach to solving this problem when the edges have differing weights might be to process the vertices in a ﬁxed order. label the vertices v0 to vn−1, with s = v0. when processing vertex v1, we take the edge connecting v0 and v1. when processing v2, we consider the shortest distance from v0 to v2 and compare that to the shortest distance from v0 to v1 to v2. when processing vertex vi, we consider the shortest path for vertices v0 through vi−1 that have already been processed. unfortunately, the true shortest path to vi might go through vertex vj for j > i. such a path will not be considered by this algorithm. however, the problem would not occur if we process the vertices in order of distance from s. assume that we have processed in order of distance from s to the ﬁrst i − 1 vertices that are closest to s; call this set of vertices s. we are now about to process the ith closest vertex; call it x. a shortest path from s to x must have its next-to-last vertex in s. thus,
in other words, the shortest path from s to x is the minimum over all paths that go from s to u, then have an edge from u to x, where u is some vertex in s.
this solution is usually referred to as dijkstra’s algorithm. it works by maintaining a distance estimate d(x) for all vertices x in v. the elements of d are initialized to the value infinite. vertices are processed in order of distance from s. whenever a vertex v is processed, d(x) is updated for every neighbor x of v. figure 11.16 shows an implementation for dijkstra’s algorithm. at the end, array d will contain the shortest distance values.
there are two reasonable solutions to the key issue of ﬁnding the unvisited vertex with minimum distance value during each pass through the main for loop. the ﬁrst method is simply to scan through the list of |v| vertices searching for the minimum value, as follows:
figure 11.18 illustrates dijkstra’s algorithm. the start vertex is a. all vertices except a have an initial value of ∞. after processing vertex a, its neighbors have their d estimates updated to be the direct distance from a. after processing c (the closest vertex to a), vertices b and e are updated to reﬂect the shortest path through c. the remaining vertices are processed in order b, d, and e.
this section presents two algorithms for determining the minimum-cost spanning tree (mst) for a graph. the mst problem takes as input a connected, undirected graph g, where each edge has a distance or weight measure attached. the mst is the graph containing the vertices of g along with the subset of g’s edges that (1) has minimum total cost as measured by summing the values for all of the edges in the subset, and (2) keeps the vertices connected. applications where a solution to this problem is useful include soldering the shortest set of wires needed to connect a set of terminals on a circuit board, and connecting a set of cities by telephone lines in such a way as to require the least amount of cable.
the mst contains no cycles. if a proposed set of edges did have a cycle, a cheaper mst could be had by removing any one of the edges in the cycle. thus, the mst is a free tree with |v|−1 edges. the name “minimum-cost spanning tree” comes from the fact that the required set of edges forms a tree, it spans the vertices (i.e., it connects them together), and it has minimum cost. figure 11.19 shows the mst for an example graph.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
public void init(int n); public int n(); public int e(); public int first(int v); public int next(int v, int w); // get v’s next neighbor public void setedge(int i, int j, int wght); // set weight public void deledge(int i, int j); // delete edge (i, j) public boolean isedge(int i, int j); // if (i,j) an edge? public int weight(int i, int j); public void setmark(int v, int val); // set mark for v public int getmark(int v); // get mark for v
figure 11.5 a graph adt. this adt assumes that the number of vertices is ﬁxed when the graph is created, but that edges can be added and removed. it also supports a mark array to aid graph traversal algorithms.
the adjacency matrix often requires a higher asymptotic cost for an algorithm than would result if the adjacency list were used. the reason is that it is common for a graph algorithm to visit each neighbor of each vertex. using the adjacency list, only the actual edges connecting a vertex to its neighbors are examined. however, the adjacency matrix must look at each of its |v| potential edges, yielding a total cost of Θ(|v2|) time when the algorithm might otherwise require only Θ(|v|+|e|) time. this is a considerable disadvantage when the graph is sparse, but not when the graph is closer to full.
we next turn to the problem of implementing a graph class. figure 11.5 shows an abstract class deﬁning an adt for graphs. vertices are deﬁned by an integer index value. in other words, there is a vertex 0, vertex 1, and so on. we can assume that a graph application stores any additional information of interest about a given vertex elsewhere, such as a name or application-dependent value. note that this adt is not implemented using a template, because it is the graph class users’ responsibility to maintain information related to the vertices themselves. the graph class has no knowledge of the type or content of the information associated with a vertex, only the index number for that vertex.
abstract class graph has methods to return the number of vertices and edges (methods n and e, respectively). function weight returns the weight of a given edge, with that edge identiﬁed by its two incident vertices. for example, calling weight(0, 4) on the graph of figure 11.1 (c) would return 4. if no such edge
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
figure 11.2 an undirected graph with three connected components. vertices 0, 1, 2, 3, and 4 form one connected component. vertices 5 and 6 form a second connected component. vertex 7 by itself forms a third connected component.
a subgraph s is formed from graph g by selecting a subset vs of g’s vertices and a subset es of g’s edges such that for every edge e in es, both of its vertices are in vs.
an undirected graph is connected if there is at least one path from any vertex to any other. the maximally connected subgraphs of an undirected graph are called connected components. for example, figure 11.2 shows an undirected graph with three connected components.
there are two commonly used methods for representing graphs. the adjacency matrix is illustrated by figure 11.3(b). the adjacency matrix for a graph is a |v| × |v| array. assume that |v| = n and that the vertices are labeled from v0 through vn−1. row i of the adjacency matrix contains entries for vertex vi. column j in row i is marked if there is an edge from vi to vj and is not marked otherwise. thus, the adjacency matrix requires one bit at each position. alternatively, if we wish to associate a number with each edge, such as the weight or distance between two vertices, then each matrix position must store that number. in either case, the space requirements for the adjacency matrix are Θ(|v|2).
the second common representation for graphs is the adjacency list, illustrated by figure 11.3(c). the adjacency list is an array of linked lists. the array is |v| items long, with position i storing a pointer to the linked list of edges for vertex vi. this linked list represents the edges by the vertices that are adjacent to vertex vi. the adjacency list is therefore a generalization of the “list of children” representation for trees described in section 6.3.1.
figure 11.1 examples of graphs and terminology. (a) a graph. (b) a directed graph (digraph). (c) a labeled (directed) graph with weights associated with the edges. in this example, there is a simple path from vertex 0 to vertex 3 containing vertices 0, 1, and 3. vertices 0, 1, 3, 2, 4, and 1 also form a path, but not a simple path because vertex 1 appears twice. vertices 1, 3, 2, 4, and 1 form a simple cycle.
a graph g = (v, e) consists of a set of vertices v and a set of edges e, such that each edge in e is a connection between a pair of vertices in v.1 the number of vertices is written |v|, and the number of edges is written |e|. |e| can range from zero to a maximum of |v|2 − |v|. a graph with relatively few edges is called sparse, while a graph with many edges is called dense. a graph containing all possible edges is said to be complete.
a graph with edges directed from one vertex to another (as in figure 11.1(b)) is called a directed graph or digraph. a graph whose edges are not directed is called an undirected graph (as illustrated by figure 11.1(a)). a graph with labels associated with its vertices (as in figure 11.1(c)) is called a labeled graph. two vertices are adjacent if they are joined by an edge. such vertices are also called neighbors. an edge connecting vertices u and v is written (u, v). such an edge is said to be incident on vertices u and v. associated with each edge may be a cost or weight. graphs whose edges have weights (as in figure 11.1(c)) are said to be weighted. a sequence of vertices v1, v2, ..., vn forms a path of length n − 1 if there exist edges from vi to vi+1 for 1 ≤ i < n. a path is simple if all vertices on the path are distinct. the length of a path is the number of edges it contains. a cycle is a path of length three or more that connects some vertex v1 to itself. a cycle is simple if the path is simple, except for the ﬁrst and last vertices being the same.
1some graph applications require that a given pair of vertices can have multiple edges connecting them, or that a vertex can have an edge to itself. however, the applications discussed in this book do not require either of these special cases, so for simplicity we will assume that they cannot occur.
some applications must represent a large, two-dimensional matrix where many of the elements have a value of zero. one example is the lower triangular matrix that results from solving systems of simultaneous equations. a lower triangular matrix stores zero values at positions [r, c] such that r < c, as shown in figure 12.6(a). thus, the upper-right triangle of the matrix is always zero. another example is the representation of undirected graphs in an adjacency matrix (see project 11.2). because all edges between vertices i and j go in both directions, there is no need to store both. instead we can just store one edge going from the higher-indexed vertex to the lower-indexed vertex. in this case, only the lower triangle of the matrix can have non-zero values. we can take advantage of this fact to save space. instead of storing n(n + 1)/2 pieces of information in an n × n array, it would save space to use a list of length n(n + 1)/2. this is only practical if some means can be found to locate within the list the element that would correspond to position [r, c] in the original matrix.
to derive an equation to do this computation, note that row 0 of the matrix has one non-zero value, row 1 has two non-zero values, and so on. thus, row r k=1 k = (r2 + r)/2 non-zero elements. adding c to reach the cth position in the rth row yields the following equation to convert position [r, c] in the original matrix to the correct position in the list.
a similar equation can be used to store an upper triangular matrix, that is, a matrix with zero values at positions [r, c] such that r > c, as shown in figure 12.6(b). for an n × n upper triangular matrix, the equation would be
a more difﬁcult situation arises when the vast majority of values stored in an n × n matrix are zero, but there is no restriction on which positions are zero and which are non-zero. this is known as a sparse matrix.
figure 11.1 examples of graphs and terminology. (a) a graph. (b) a directed graph (digraph). (c) a labeled (directed) graph with weights associated with the edges. in this example, there is a simple path from vertex 0 to vertex 3 containing vertices 0, 1, and 3. vertices 0, 1, 3, 2, 4, and 1 also form a path, but not a simple path because vertex 1 appears twice. vertices 1, 3, 2, 4, and 1 form a simple cycle.
a graph g = (v, e) consists of a set of vertices v and a set of edges e, such that each edge in e is a connection between a pair of vertices in v.1 the number of vertices is written |v|, and the number of edges is written |e|. |e| can range from zero to a maximum of |v|2 − |v|. a graph with relatively few edges is called sparse, while a graph with many edges is called dense. a graph containing all possible edges is said to be complete.
a graph with edges directed from one vertex to another (as in figure 11.1(b)) is called a directed graph or digraph. a graph whose edges are not directed is called an undirected graph (as illustrated by figure 11.1(a)). a graph with labels associated with its vertices (as in figure 11.1(c)) is called a labeled graph. two vertices are adjacent if they are joined by an edge. such vertices are also called neighbors. an edge connecting vertices u and v is written (u, v). such an edge is said to be incident on vertices u and v. associated with each edge may be a cost or weight. graphs whose edges have weights (as in figure 11.1(c)) are said to be weighted. a sequence of vertices v1, v2, ..., vn forms a path of length n − 1 if there exist edges from vi to vi+1 for 1 ≤ i < n. a path is simple if all vertices on the path are distinct. the length of a path is the number of edges it contains. a cycle is a path of length three or more that connects some vertex v1 to itself. a cycle is simple if the path is simple, except for the ﬁrst and last vertices being the same.
1some graph applications require that a given pair of vertices can have multiple edges connecting them, or that a vertex can have an edge to itself. however, the applications discussed in this book do not require either of these special cases, so for simplicity we will assume that they cannot occur.
/** constructors */ public huffnode() {left = right = null; } public huffnode(e val) { left = right = null; element = val; } public huffnode(e val, huffnode<e> l, huffnode<e> r) { left = l; right = r; element = val; }
figure 5.27 implementation for huffman tree nodes. internal nodes and leaf nodes are represented by separate classes, each derived from an abstract base class.
huffman tree building is an example of a greedy algorithm. at each step, the algorithm makes a “greedy” decision to merge the two subtrees with least weight. this makes the algorithm simple, but does it give the desired result? this section concludes with a proof that the huffman tree indeed gives the most efﬁcient arrangement for the set of letters.
lemma 5.1 for any huffman tree built by function buildhuff containing at least two letters, the two letters with least frequency are stored in siblings nodes whose depth is at least as deep as any other leaf nodes in the tree.
int v = minvertex(g, d); g.setmark(v, visited); if (v != s) addedgetomst(v[v], v); if (d[v] == integer.max value) return; // unreachable for (int w = g.first(v); w < g.n(); w = g.next(v, w))
array v[i] stores the previously visited vertex that is closest to vertex i. this information lets us know which edge goes into the mst when vertex i is processed. the implementation of figure 11.20 also contains calls to addedgetomst to indicate which edges are actually added to the mst.
alternatively, we can implement prim’s algorithm using a priority queue to ﬁnd the next closest vertex, as shown in figure 11.21. as with the priority queue version of dijkstra’s algorithm, the heap’s elem type stores a dijkelem object.
prim’s algorithm is an example of a greedy algorithm. at each step in the for loop, we select the least-cost edge that connects some marked vertex to some unmarked vertex. the algorithm does not otherwise check that the mst really should include this least-cost edge. this leads to an important question: does prim’s algorithm work correctly? clearly it generates a spanning tree (because each pass through the for loop adds one edge and one unmarked vertex to the spanning tree until all vertices have been added), but does this tree have minimum cost?
theorem 11.1 prim’s algorithm produces a minimum-cost spanning tree. proof: we will use a proof by contradiction. let g = (v, e) be a graph for which prim’s algorithm does not generate an mst. deﬁne an ordering on the vertices according to the order in which they were added by prim’s algorithm to the mst: v0, v1, ..., vn−1. let edge ei connect (vx, vi) for some x < i and i ≥ 1. let ej be the lowest numbered (ﬁrst) edge added by prim’s algorithm such that the set of edges selected so far cannot be extended to form an mst for g. in other words, ej is the
figure 11.22 prim’s mst algorithm proof. the left oval contains that portion of the graph where prim’s mst and the “true” mst t agree. the right oval contains the rest of the graph. the two portions of the graph are connected by (at least) edges ej (selected by prim’s algorithm to be in the mst) and e0 (the “correct” edge to be placed in the mst). note that the path from vw to vj cannot include any marked vertex vi, i ≤ j, because to do so would form a cycle.
and (d, f) happen to have equal cost, it is an arbitrary decision as to which gets selected. let’s pick (c, f). the next step marks vertex e and adds edge (f, e) to the mst. following in this manner, vertex b (through edge (c, b)) is marked. at this point, the algorithm terminates.
our next mst algorithm is commonly referred to as kruskal’s algorithm. kruskal’s algorithm is also a simple, greedy algorithm. we ﬁrst partition the set of vertices into |v| equivalence classes (see section 6.2), each consisting of one vertex. we then process the edges in order of weight. an edge is added to the mst, and the two equivalence classes combined, if the edge connects two vertices in different equivalence classes. this process is repeated until only one equivalence class remains.
example 11.4 figure 11.23 shows the ﬁrst three steps of kruskal’s algorithm for the graph of figure 11.19. edge (c, d) has the least cost, and because c and d are currently in separate msts, they are combined. we next select edge (e, f) to process, and combine these vertices into a single
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
seen so far. it is reasonable to assume that it takes a ﬁxed amount of time to do one such comparison, regardless of the value of the two integers or their positions in the array.
because the most important factor affecting running time is normally size of the input, for a given input size n we often express the time t to run the algorithm as a function of n, written as t(n). we will always assume t(n) is a non-negative value.
let us call c the amount of time required to compare two integers in function largest. we do not care right now what the precise value of c might be. nor are we concerned with the time required to increment variable i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. we just want a reasonable approximation for the time taken to execute the algorithm. the total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. we say that function largest (and the largest-value sequential search algorithm in general) has a running time expressed by the equation
example 3.2 the running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. we can assume this assignment takes a constant amount of time regardless of the value. let us call c1 the amount of time necessary to copy an integer. no matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. thus, the equation for this algorithm is simply
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
as an interesting aside, writing a correct binary search algorithm is not easy. knuth [knu98] notes that while the ﬁrst binary search was published in 1946, the ﬁrst bug-free algorithm was not published until 1962! bentley (“writing correct programs” in [ben00]) has found that 90% of the computer professionals he tested could not write a bug-free binary search in two hours.
see stirling’s approximation in section 2.2 for help in classifying n!. (a) suppose that a particular algorithm has time complexity t(n) = 3 × 2n, and that executing an implementation of it on a particular machine takes t seconds for n inputs. now suppose that we are presented with a machine that is 64 times as fast. how many inputs could we process on the new machine in t seconds?
(b) suppose that another algorithm has time complexity t(n) = n2, and that executing an implementation of it on a particular machine takes t seconds for n inputs. now suppose that we are presented with a machine that is 64 times as fast. how many inputs could we process on the new machine in t seconds?
(c) a third algorithm has time complexity t(n) = 8n. executing an implementation of it on a particular machine takes t seconds for n inputs. given a new machine that is 64 times as fast, how many inputs could we process in t seconds?
3.5 hardware vendor xyz corp. claims that their latest computer will run 100 times faster than that of their competitor, prunes, inc. if the prunes, inc. computer can execute a program on input of size n in one hour, what size
computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. if you had a second program whose growth rate is 2n and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! thus, an exponential growth rate is radically different than the other growth rates shown in figure 3.3. the signiﬁcance of this difference is explored in chapter 17.
instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to n2 with a new algorithm whose running time is proportional to n log n. in the graph of figure 3.1, a ﬁxed amount of time would appear as a horizontal line. if the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. an algorithm with running time t(n) = n2 requires 1024 × 1024 = 1, 048, 576 time steps for an input of size n = 1024. an algorithm with running time t(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of size n = 1024, which is an improvement of much more than a factor of ten when compared to the algorithm with running time t(n) = n2. because n2 > 10n log n whenever n > 58, if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater beneﬁt in terms of larger problem size that can run in a certain time on the new computer.
despite the larger constant for the curve labeled 10n in figure 3.1, the curve labeled 2n2 crosses it at the relatively small value of n = 5. what if we double the value of the constant in front of the linear equation? as shown in the graph, the curve labeled 20n is surpassed by the curve labeled 2n2 once n = 10. the additional factor of two for the linear growth rate does not much matter; it only doubles the x-coordinate for the intersection point. in general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross.
when you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. the time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. for these reasons, we usually ignore the constants
when we want an estimate of the running time or other resource requirements of an algorithm. this simpliﬁes the analysis and keeps us thinking about the most important aspect: the growth rate. this is called asymptotic algorithm analysis. to be precise, asymptotic analysis refers to the study of an algorithm as the input size “gets big” or reaches a limit (in the calculus sense). however, it has proved to be so useful to ignore all constant factors that asymptotic analysis is used for most algorithm comparisons.
it is not always reasonable to ignore the constants. when comparing algorithms meant to run on small values of n, the constant can have a large effect. for example, if the problem is to sort a collection of exactly ﬁve records, then an algorithm designed for sorting thousands of records is probably not appropriate, even if its asymptotic analysis indicates good performance. there are rare cases where the constants for two algorithms under comparison can differ by a factor of 1000 or more, making the one with lower growth rate impractical for most purposes due to its large constant. asymptotic analysis is a form of “back of the envelope” estimation for algorithm resource consumption. it provides a simpliﬁed model of the running time or other resource needs of an algorithm. this simpliﬁcation usually helps you understand the behavior of your algorithms. just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important.
several terms are used to describe the running-time equation for an algorithm. these terms — and their associated symbols — indicate precisely what aspect of the algorithm’s behavior is being described. one is the upper bound for the growth of the algorithm’s running time. it indicates the upper or highest growth rate that the algorithm can have.
to make any statement about the upper bound of an algorithm, we must be making it about some class of inputs of size n. we measure this upper bound nearly always on the best-case, average-case, or worst-case inputs. thus, we cannot say, “this algorithm has an upper bound to its growth rate of n2.” we must say something like, “this algorithm has an upper bound to its growth rate of n2 in the average case.”
because the phrase “has an upper bound to its growth rate of f(n)” is long and often used when discussing algorithms, we adopt a special notation, called big-oh notation. if the upper bound for an algorithm’s growth rate (for, say, the worst case) is f(n), then we would write that this algorithm is “in the set o(f(n))in the worst case” (or just “in o(f(n))in the worst case”). for example, if n2 grows as
seen so far. it is reasonable to assume that it takes a ﬁxed amount of time to do one such comparison, regardless of the value of the two integers or their positions in the array.
because the most important factor affecting running time is normally size of the input, for a given input size n we often express the time t to run the algorithm as a function of n, written as t(n). we will always assume t(n) is a non-negative value.
let us call c the amount of time required to compare two integers in function largest. we do not care right now what the precise value of c might be. nor are we concerned with the time required to increment variable i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. we just want a reasonable approximation for the time taken to execute the algorithm. the total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. we say that function largest (and the largest-value sequential search algorithm in general) has a running time expressed by the equation
example 3.2 the running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. we can assume this assignment takes a constant amount of time regardless of the value. let us call c1 the amount of time necessary to copy an integer. no matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. thus, the equation for this algorithm is simply
the following is a precise deﬁnition for an upper bound. t(n) represents the true running time of the algorithm. f(n) is some expression for the upper bound.
for t(n) a non-negatively valued function, t(n) is in set o(f(n)) if there exist two positive constants c and n0 such that t(n) ≤ cf(n) for all n > n0.
constant n0 is the smallest value of n for which the claim of an upper bound holds true. usually n0 is small, such as 1, but does not need to be. you must also be able to pick some constant c, but it is irrelevant what the value for c actually is. in other words, the deﬁnition says that for all inputs of the type in question (such as the worst case for all inputs of size n) that are large enough (i.e., n > n0), the algorithm always executes in less than cf(n) steps for some constant c.
example 3.4 consider the sequential search algorithm for ﬁnding a speciﬁed value in an array of integers. if visiting and examining one value in the array requires cs steps where cs is a positive number, and if the value we search for has equal probability of appearing in any position in the array, then in the average case t(n) = csn/2. for all values of n > 1, csn/2 ≤ csn. therefore, by the deﬁnition, t(n) is in o(n) for n0 = 1 and c = cs.
example 3.5 for a particular algorithm, t(n) = c1n2 + c2n in the average case where c1 and c2 are positive numbers. then, c1n2 + c2n ≤ c1n2 + c2n2 ≤ (c1 + c2)n2 for all n > 1. so, t(n) ≤ cn2 for c = c1 + c2, and n0 = 1. therefore, t(n) is in o(n2) by the deﬁnition.
example 3.6 assigning the value from the ﬁrst position of an array to a variable takes constant time regardless of the size of the array. thus, t(n) = c (for the best, worst, and average cases). we could say in this case that t(n) is in o(c). however, it is traditional to say that an algorithm whose running time has a constant upper bound is in o(1).
just knowing that something is in o(f(n)) says only how bad things can get. perhaps things are not nearly so bad. because we know sequential search is in o(n) in the worst case, it is also true to say that sequential search is in o(n2). but
what is the running time for this code fragment? clearly it takes longer to run when n is larger. the basic operation in this example is the increment operation for variable sum. we can assume that incrementing takes constant time; call this time c2. (we can ignore the time required to initialize sum, and to increment the loop counters i and j. in practice, these costs can safely be bundled into time c2.) the total number of increment operations is n2. thus, we say that the running time is t(n) = c2n2.
the growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. figure 3.1 shows a graph for six equations, each meant to describe the running time for a particular program or algorithm. a variety of growth rates representative of typical algorithms are shown. the two equations labeled 10n and 20n are graphed by straight lines. a growth rate of cn (for c any positive constant) is often referred to as a linear growth rate or running time. this means that as the value of n grows, the running time of the algorithm grows in the same proportion. doubling the value of n roughly doubles the running time. an algorithm whose running-time equation has a highest-order term containing a factor of n2 is said to have a quadratic growth rate. in figure 3.1, the line labeled 2n2 represents a quadratic growth rate. the line labeled 2n represents an exponential growth rate. this name comes from the fact that n appears in the exponent. the line labeled n! is also growing exponentially.
as you can see from figure 3.1, the difference between an algorithm whose running time has cost t(n) = 10n and another with cost t(n) = 2n2 becomes tremendous as n grows. for n > 5, the algorithm with running time t(n) = 2n2 is already much slower. this is despite the fact that 10n has a greater constant factor than 2n2. comparing the two curves marked 20n and 2n2 shows that changing the constant factor for one of the equations only shifts the point at which the two curves cross. for n > 10, the algorithm with cost t(n) = 2n2 is slower than the algorithm with cost t(n) = 20n. this graph also shows that the equation t(n) = 5n log n grows somewhat more quickly than both t(n) = 10n and t(n) = 20n, but not nearly so quickly as the equation t(n) = 2n2. for constants a, b > 1, na grows faster than either logb n or log nb. finally, algorithms with cost t(n) = 2n or t(n) = n! are prohibitively expensive for even modest values of n. note that for constants a, b ≥ 1, an grows faster than nb.
figure 3.3 the increase in problem size that can be run in a ﬁxed period of time on a computer that is ten times faster. the ﬁrst column lists the right-hand sides for each of the ﬁve growth rate equations of figure 3.1. for the purpose of this example, arbitrarily assume that the old machine can run 10,000 basic operations in one hour. the second column shows the maximum value for n that can be run in 10,000 basic operations on the old machine. the third column shows the value for n0, the new maximum size for the problem that can be run in the same time on the new machine that is ten times faster. variable n0 is the greatest size for the problem that can run in 100,000 basic operations. the fourth column shows how the size of n changed to become n0 on the new machine. the ﬁfth column shows the increase in the problem size as the ratio of n0 to n.
problem size (as a proportion to the original size) gained by a faster computer. this relationship holds true regardless of the algorithm’s growth rate: constant factors never affect the relative improvement gained by a faster computer.
an algorithm with time equation t(n) = 2n2 does not receive nearly as great an improvement from the faster machine as an algorithm with linear growth rate. instead of an improvement by a factor of ten, the improvement is only the square 10 ≈ 3.16. thus, the algorithm with higher growth rate not only root of that: solves a smaller problem in a given time in the ﬁrst place, it also receives less of a speedup from a faster computer. as computers get ever faster, the disparity in problem sizes becomes ever greater.
the algorithm with growth rate t(n) = 5n log n improves by a greater amount than the one with quadratic growth rate, but not by as great an amount as the algorithms with linear growth rates.
note that something special happens in the case of the algorithm whose running time grows exponentially. in figure 3.1, the curve for the algorithm whose time is proportional to 2n goes up very quickly. in figure 3.3, the increase in problem size on the machine ten times as fast is shown to be about n + 3 (to be precise, it is n + log2 10). the increase in problem size for an algorithm with exponential growth rate is by a constant addition, not by a multiplicative factor. because the old value of n was 13, the new problem size is 16. if next year you buy another
computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. if you had a second program whose growth rate is 2n and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! thus, an exponential growth rate is radically different than the other growth rates shown in figure 3.3. the signiﬁcance of this difference is explored in chapter 17.
instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to n2 with a new algorithm whose running time is proportional to n log n. in the graph of figure 3.1, a ﬁxed amount of time would appear as a horizontal line. if the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. an algorithm with running time t(n) = n2 requires 1024 × 1024 = 1, 048, 576 time steps for an input of size n = 1024. an algorithm with running time t(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of size n = 1024, which is an improvement of much more than a factor of ten when compared to the algorithm with running time t(n) = n2. because n2 > 10n log n whenever n > 58, if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater beneﬁt in terms of larger problem size that can run in a certain time on the new computer.
despite the larger constant for the curve labeled 10n in figure 3.1, the curve labeled 2n2 crosses it at the relatively small value of n = 5. what if we double the value of the constant in front of the linear equation? as shown in the graph, the curve labeled 20n is surpassed by the curve labeled 2n2 once n = 10. the additional factor of two for the linear growth rate does not much matter; it only doubles the x-coordinate for the intersection point. in general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross.
when you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. the time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. for these reasons, we usually ignore the constants
what is the running time for this code fragment? clearly it takes longer to run when n is larger. the basic operation in this example is the increment operation for variable sum. we can assume that incrementing takes constant time; call this time c2. (we can ignore the time required to initialize sum, and to increment the loop counters i and j. in practice, these costs can safely be bundled into time c2.) the total number of increment operations is n2. thus, we say that the running time is t(n) = c2n2.
the growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. figure 3.1 shows a graph for six equations, each meant to describe the running time for a particular program or algorithm. a variety of growth rates representative of typical algorithms are shown. the two equations labeled 10n and 20n are graphed by straight lines. a growth rate of cn (for c any positive constant) is often referred to as a linear growth rate or running time. this means that as the value of n grows, the running time of the algorithm grows in the same proportion. doubling the value of n roughly doubles the running time. an algorithm whose running-time equation has a highest-order term containing a factor of n2 is said to have a quadratic growth rate. in figure 3.1, the line labeled 2n2 represents a quadratic growth rate. the line labeled 2n represents an exponential growth rate. this name comes from the fact that n appears in the exponent. the line labeled n! is also growing exponentially.
as you can see from figure 3.1, the difference between an algorithm whose running time has cost t(n) = 10n and another with cost t(n) = 2n2 becomes tremendous as n grows. for n > 5, the algorithm with running time t(n) = 2n2 is already much slower. this is despite the fact that 10n has a greater constant factor than 2n2. comparing the two curves marked 20n and 2n2 shows that changing the constant factor for one of the equations only shifts the point at which the two curves cross. for n > 10, the algorithm with cost t(n) = 2n2 is slower than the algorithm with cost t(n) = 20n. this graph also shows that the equation t(n) = 5n log n grows somewhat more quickly than both t(n) = 10n and t(n) = 20n, but not nearly so quickly as the equation t(n) = 2n2. for constants a, b > 1, na grows faster than either logb n or log nb. finally, algorithms with cost t(n) = 2n or t(n) = n! are prohibitively expensive for even modest values of n. note that for constants a, b ≥ 1, an grows faster than nb.
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. if you had a second program whose growth rate is 2n and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! thus, an exponential growth rate is radically different than the other growth rates shown in figure 3.3. the signiﬁcance of this difference is explored in chapter 17.
instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to n2 with a new algorithm whose running time is proportional to n log n. in the graph of figure 3.1, a ﬁxed amount of time would appear as a horizontal line. if the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. an algorithm with running time t(n) = n2 requires 1024 × 1024 = 1, 048, 576 time steps for an input of size n = 1024. an algorithm with running time t(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of size n = 1024, which is an improvement of much more than a factor of ten when compared to the algorithm with running time t(n) = n2. because n2 > 10n log n whenever n > 58, if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater beneﬁt in terms of larger problem size that can run in a certain time on the new computer.
despite the larger constant for the curve labeled 10n in figure 3.1, the curve labeled 2n2 crosses it at the relatively small value of n = 5. what if we double the value of the constant in front of the linear equation? as shown in the graph, the curve labeled 20n is surpassed by the curve labeled 2n2 once n = 10. the additional factor of two for the linear growth rate does not much matter; it only doubles the x-coordinate for the intersection point. in general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross.
when you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. the time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. for these reasons, we usually ignore the constants
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
what is the running time for this code fragment? clearly it takes longer to run when n is larger. the basic operation in this example is the increment operation for variable sum. we can assume that incrementing takes constant time; call this time c2. (we can ignore the time required to initialize sum, and to increment the loop counters i and j. in practice, these costs can safely be bundled into time c2.) the total number of increment operations is n2. thus, we say that the running time is t(n) = c2n2.
the growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. figure 3.1 shows a graph for six equations, each meant to describe the running time for a particular program or algorithm. a variety of growth rates representative of typical algorithms are shown. the two equations labeled 10n and 20n are graphed by straight lines. a growth rate of cn (for c any positive constant) is often referred to as a linear growth rate or running time. this means that as the value of n grows, the running time of the algorithm grows in the same proportion. doubling the value of n roughly doubles the running time. an algorithm whose running-time equation has a highest-order term containing a factor of n2 is said to have a quadratic growth rate. in figure 3.1, the line labeled 2n2 represents a quadratic growth rate. the line labeled 2n represents an exponential growth rate. this name comes from the fact that n appears in the exponent. the line labeled n! is also growing exponentially.
as you can see from figure 3.1, the difference between an algorithm whose running time has cost t(n) = 10n and another with cost t(n) = 2n2 becomes tremendous as n grows. for n > 5, the algorithm with running time t(n) = 2n2 is already much slower. this is despite the fact that 10n has a greater constant factor than 2n2. comparing the two curves marked 20n and 2n2 shows that changing the constant factor for one of the equations only shifts the point at which the two curves cross. for n > 10, the algorithm with cost t(n) = 2n2 is slower than the algorithm with cost t(n) = 20n. this graph also shows that the equation t(n) = 5n log n grows somewhat more quickly than both t(n) = 10n and t(n) = 20n, but not nearly so quickly as the equation t(n) = 2n2. for constants a, b > 1, na grows faster than either logb n or log nb. finally, algorithms with cost t(n) = 2n or t(n) = n! are prohibitively expensive for even modest values of n. note that for constants a, b ≥ 1, an grows faster than nb.
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. if you had a second program whose growth rate is 2n and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! thus, an exponential growth rate is radically different than the other growth rates shown in figure 3.3. the signiﬁcance of this difference is explored in chapter 17.
instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to n2 with a new algorithm whose running time is proportional to n log n. in the graph of figure 3.1, a ﬁxed amount of time would appear as a horizontal line. if the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. an algorithm with running time t(n) = n2 requires 1024 × 1024 = 1, 048, 576 time steps for an input of size n = 1024. an algorithm with running time t(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of size n = 1024, which is an improvement of much more than a factor of ten when compared to the algorithm with running time t(n) = n2. because n2 > 10n log n whenever n > 58, if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater beneﬁt in terms of larger problem size that can run in a certain time on the new computer.
despite the larger constant for the curve labeled 10n in figure 3.1, the curve labeled 2n2 crosses it at the relatively small value of n = 5. what if we double the value of the constant in front of the linear equation? as shown in the graph, the curve labeled 20n is surpassed by the curve labeled 2n2 once n = 10. the additional factor of two for the linear growth rate does not much matter; it only doubles the x-coordinate for the intersection point. in general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross.
when you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. the time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. for these reasons, we usually ignore the constants
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
example 3.18 the following is a true story. a few years ago, one of my graduate students had a big problem. his thesis work involved several intricate operations on a large database. he was now working on the ﬁnal step. “dr. shaffer,” he said, “i am running this program and it seems to be taking a long time.” after examining the algorithm we realized that its running time was Θ(n2), and that it would likely take one to two weeks to complete. even if we could keep the computer running uninterrupted for that long, he was hoping to complete his thesis and graduate before then. fortunately, we realized that there was a fairly easy way to convert the algorithm so that its running time was Θ(n log n). by the next day he had modiﬁed the program. it ran in only a few hours, and he ﬁnished his thesis on time.
while not nearly so important as changing an algorithm to reduce its growth rate, “code tuning” can also lead to dramatic improvements in running time. code tuning is the art of hand-optimizing a program to run faster or require less storage. for many programs, code tuning can reduce running time by a factor of ten, or cut the storage requirements by a factor of two or more. i once tuned a critical function in a program — without changing its basic algorithm — to achieve a factor of 200 speedup. to get this speedup, however, i did make major changes in the representation of the information, converting from a symbolic coding scheme to a numeric coding scheme on which i was able to do direct computation.
here are some suggestions for ways to speed up your programs by code tuning. the most important thing to realize is that most statements in a program do not have much effect on the running time of that program. there are normally just a few key subroutines, possibly even key lines of code within the key subroutines, that account for most of the running time. there is little point to cutting in half the running time of a subroutine that accounts for only 1% of the total running time. focus your attention on those parts of the program that have the most impact.
when tuning code, it is important to gather good timing statistics. many compilers and operating systems include proﬁlers and other special tools to help gather information on both time and space use. these are invaluable when trying to make a program more efﬁcient, because they can tell you where to invest your effort.
a lot of code tuning is based on the principle of avoiding work rather than speeding up work. a common situation occurs when we can test for a condition
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
some input for which p halts. we could modify our compiler (or write a function as part of a program) to take p and some input string w, and modify it so that w is hardcoded inside p, with p reading no input. call this modiﬁed program p 0. now, p 0 always behaves the same way regardless of its input, because it ignores all input. however, because w is now hardwired inside of p 0, the behavior we get is that of p when given w as input. so, p 0 will halt on any arbitrary input if and only if p would halt on input w. we now feed p 0 to function ahalt. if ahalt could determine that p 0 halts on some input, then that is the same as determining that p halts on input w. but we know that that is impossible. therefore, ahalt cannot exist. 2
there are many things that we would like to have a computer do that are unsolvable. many of these have to do with program behavior. for example, proving that an arbitrary program is “correct,” that is, proving that a program computes a particular function, is a proof regarding program behavior. as such, what can be accomplished is severely limited. some other unsolvable problems include:
• does a program halt on every input? • does a program compute a particular function? • do two programs compute the same function? • does a particular line in a program get executed? this does not mean that a computer program cannot be written that works on special cases, possibly even on most programs that we would be interested in checking. for example, some c compilers will check if the control expression for a while loop is a constant expression that evaluates to false. if it is, the compiler will issue a warning that the while loop code will never be executed. however, it is not possible to write a computer program that can check for all input programs whether a speciﬁed line of code will be executed when the program is given some speciﬁed input.
another unsolvable problem is whether a program contains a computer virus. the property “contains a computer virus” is a matter of behavior. thus, it is not possible to determine positively whether an arbitrary program contains a computer virus. fortunately, there are many good heuristics for determining if a program is likely to contain a virus, and it is usually possible to determine if a program contains a particular virus, at least for the ones that are now known. real virus checkers do a pretty good job, but, it will always be possible for malicious people to invent new viruses that no existing virus checker can recognize.
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
figure 12.15 using handles for dynamic memory management. the memory manager returns the address of the handle in response to a memory request. the handle stores the address of the actual memory block. in this way, the memory block might be moved (with its address updated in the handle) without disrupting the application program.
ﬁt memory allocation method, where external fragmentation has led to a series of small blocks that collectively could service the request. in this case, it might be possible to compact memory by moving the reserved blocks around so that the free space is collected into a single block. a problem with this approach is that the application must somehow be able to deal with the fact that all of its data have now been moved to different locations. if the application program relies on the absolute positions of the data in any way, this would be disastrous. one approach for dealing with this problem is the use of handles. a handle is a second level of indirection to a memory location. the memory allocation routine does not return a pointer to the block of storage, but rather a pointer to a variable that in turn points to the storage. this variable is the handle. the handle never moves its position, but the position of the block might be moved and the value of the handle updated. figure 12.15 illustrates the concept.
another failure policy that might work in some applications is to defer the memory request until sufﬁcient memory becomes available. for example, a multitasking operating system could adopt the strategy of not allowing a process to run until there is sufﬁcient memory available. while such a delay might be annoying to the user, it is better than halting the entire system. the assumption here is that other processes will eventually terminate, freeing memory.
another option might be to allocate more memory to the memory manager. in a zoned memory allocation system where the memory manager is part of a larger system, this might be a viable option. in a java program that implements its own memory manager, it might be possible to get more memory from the system-level new operator, such as is done by the freelist of section 4.1.2.
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
this section presents the concept of amortized analysis, which is the analysis for a series of operations taken as a whole. in particular, amortized analysis allows us to deal with the situation where the worst-case cost for n operations is less than n times the worst-case cost of any one operation. rather than focusing on the individual cost of each operation independently and summing them, amortized analysis looks at the cost of the entire series and “charges” each individual operation with a share of the total cost.
we can apply the technique of amortized analysis in the case of a series of sequential searches in an unsorted array. for n random searches, the average-case cost for each search is n/2, and so the expected total cost for the series is n2/2. unfortunately, in the worst case all of the searches would be to the last item in the array. in this case, each search costs n for a total worst-case cost of n2. compare this to the cost for a series of n searches such that each item in the array is searched for precisely once. in this situation, some of the searches must be expensive, but also some searches must be cheap. the total number of searches, in the best, avi=i i ≈ n2/2. this is a factor
the typical customer opens and closes accounts far less often than he or she accesses the account. customers are willing to wait many minutes while accounts are created or deleted but are typically not willing to wait more than a brief time for individual account transactions such as a deposit or withdrawal. these observations can be considered as informal speciﬁcations for the time constraints on the problem.
it is common practice for banks to provide two tiers of service. human tellers or automated teller machines (atms) support customer access to account balances and updates such as deposits and withdrawals. special service representatives are typically provided (during restricted hours) to handle opening and closing accounts. teller and atm transactions are expected to take little time. opening or closing an account can take much longer (perhaps up to an hour from the customer’s perspective).
from a database perspective, we see that atm transactions do not modify the database signiﬁcantly. for simplicity, assume that if money is added or removed, this transaction simply changes the value stored in an account record. adding a new account to the database is allowed to take several minutes. deleting an account need have no time constraint, because from the customer’s point of view all that matters is that all the money be returned (equivalent to a withdrawal). from the bank’s point of view, the account record might be removed from the database system after business hours, or at the end of the monthly account cycle.
when considering the choice of data structure to use in the database system that manages customer accounts, we see that a data structure that has little concern for the cost of deletion, but is highly efﬁcient for search and moderately efﬁcient for insertion, should meet the resource constraints imposed by this problem. records are accessible by unique account number (sometimes called an exact-match query). one data structure that meets these requirements is the hash table described in chapter 9.4. hash tables allow for extremely fast exact-match search. a record can be modiﬁed quickly when the modiﬁcation does not affect its space requirements. hash tables also support efﬁcient insertion of new records. while deletions can also be supported efﬁciently, too many deletions lead to some degradation in performance for the remaining operations. however, the hash table can be reorganized periodically to restore the system to peak efﬁciency. such reorganization can occur ofﬂine so as not to affect atm transactions.
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
unfortunately, there is more than one way to assign values to q and r, depending on how integer division is interpreted. the most common mathematical deﬁnition computes the mod function as n mod m = n − mbn/mc. in this case, −3 mod 5 = 2. however, java and c++ compilers typically use the underlying processor’s machine instruction for computing integer arithmetic. on many computers this is done by truncating the resulting fraction, meaning n mod m = n − m(trunc(n/m)). under this deﬁnition, −3 mod 5 = −3.
unfortunately, for many applications this is not what the user wants or expects. for example, many hash systems will perform some computation on a record’s key value and then take the result modulo the hash table size. the expectation here would be that the result is a legal index into the hash table, not a negative number. implementors of hash functions must either insure that the result of the computation is always postive, or else add the hash table size to the result of the modulo function when that result is negative.
a logarithm of base b for value y is the power to which b is raised to get y. normally, this is written as logb y = x. thus, if logb y = x then bx = y, and blogby = y.
example 2.6 many programs require an encoding for a collection of objects. what is the minimum number of bits needed to represent n distinct code values? the answer is dlog2 ne bits. for example, if you have 1000 codes to store, you will require at least dlog2 1000e = 10 bits to have 1000 different codes (10 bits provide 1024 distinct code values).
example 2.7 consider the binary search algorithm for ﬁnding a given value within an array sorted by value from lowest to highest. binary search ﬁrst looks at the middle element and determines if the value being searched for is in the upper half or the lower half of the array. the algorithm then continues splitting the appropriate subarray in half until the desired value is found. (binary search is described in more detail in section 3.5.) how many times can an array of size n be split in half until only one element remains in the ﬁnal subarray? the answer is dlog2 ne times.
n values. if we implement sequential search as a program and run it many times on many different arrays of size n, or search for many different values of k within the same array, we expect the algorithm on average to go halfway through the array before ﬁnding the value we seek. on average, the algorithm examines about n/2 values. we call this the average case for this algorithm.
when analyzing an algorithm, should we study the best, worst, or average case? normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. in other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. however, there are rare instances where a best-case analysis is useful — in particular, when the best case has high probability of occurring. in chapter 7 you will see some examples where taking advantage of the best-case running time for one sorting algorithm makes a second more efﬁcient.
how about the worst case? the advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. this is especially important for real-time applications, such as for the computers that monitor an air trafﬁc control system. here, it would not be acceptable to use an algorithm that can handle n airplanes quickly enough most of the time, but which fails to perform quickly enough when all n airplanes are coming from the same direction.
for other applications — particularly when we wish to aggregate the cost of running the program many times on many different inputs — worst-case analysis might not be a representative measure of the algorithm’s performance. often we prefer to know the average-case running time. this means that we would like to know the typical behavior of the algorithm on inputs of size n. unfortunately, average-case analysis is not always possible. average-case analysis ﬁrst requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. for example, it was stated previously that the sequential search algorithm on average examines half of the array values. this is only true if the element with value k is equally likely to appear in any position in the array. if this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. see section 9.2 for further discussion regarding the effects of data distribution on the sequential search algorithm.
the characteristics of a data distribution have a signiﬁcant effect on many search algorithms, such as those based on hashing (section 9.4) and search trees (e.g., see section 5.4). incorrect assumptions about data distribution can have dis-
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
occupied the slot but does so no longer. if a tombstone is encountered when searching through a probe sequence, the search procedure is to continue with the search. when a tombstone is encountered during insertion, that slot can be used to store the new record. however, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. however, the new record would actually be inserted into the slot of the ﬁrst tombstone encountered. the use of tombstones allows searches to work correctly and allows reuse of deleted slots. however, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. this will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. a typical database application will ﬁrst load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. after the table is loaded with the initial collection of records, the ﬁrst few deletions will lengthen the average probe sequence distance for records (it will add tombstones). over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by ﬁlling in tombstone slots. for example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). after a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. this seems like a small increase, but it is three times longer on average beyond the home position than before deletions.
two possible solutions to this problem are 1. do a local reorganization upon deletion to try to shorten the average path length. for example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove a key from its probe sequence). this will not work for all collision resolution policies. 2. periodically rehash the table by reinserting all records into a new hash table. not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions.
for a comparison of the efﬁciencies for various self-organizing techniques, see bentley and mcgeoch, “amortized analysis of self-organizing sequential search heuristics” [bm85]. the text compression example of section 9.2 comes from
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
this section presents the b-tree. b-trees are usually attributed to r. bayer and e. mccreight who described the b-tree in a 1972 paper. by 1979, b-trees had replaced virtually all large-ﬁle access methods other than hashing. b-trees, or some variant of b-trees, are the standard ﬁle organization for applications requiring insertion, deletion, and key range searches. b-trees address effectively all of the major problems encountered when implementing disk-based search trees:
1. b-trees are always height balanced, with all leaf nodes at the same level. 2. update and search operations affect only a few disk blocks. the fewer the
3. b-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk i/o on searches due to locality of reference.
one approach to representing a sparse matrix is to concatenate (or otherwise combine) the row and column coordinates into a single value and use this as a key in a hash table. thus, if we want to know the value of a particular position in the matrix, we search the hash table for the appropriate key. if a value for this position is not found, it is assumed to be zero. this is an ideal approach when all queries to the matrix are in terms of access by speciﬁed position. however, if we wish to ﬁnd the ﬁrst non-zero element in a given row, or the next non-zero element below the current one in a given column, then the hash table requires us to check sequentially through all possible positions in some row or column.
another approach is to implement the matrix as an orthogonal list, as illustrated in figure 12.7. here we have a list of row headers, each of which contains a pointer to a list of matrix records. a second list of column headers also contains pointers to matrix records. each non-zero matrix element stores pointers to its non-zero neighbors in the row, both following and preceding it. each non-zero
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equivalent to the number required to successfully search for it. to insert a record, an empty slot along the record’s probe sequence must be found. this is equivalent to an unsuccessful search for the record (recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table).
when the hash table is empty, the ﬁrst record inserted will always ﬁnd its home position free. thus, it will require only one record access to ﬁnd a free slot. if all records are stored in their home positions, then successful searches will also require only one record access. as the table begins to ﬁll up, the probability that a record can be inserted in its home position decreases. if a record hashes to an occupied slot, then the collision resolution policy must locate another slot in which to store it. finding records not stored in their home position also requires additional record accesses as the record is searched for along its probe sequence. as the table ﬁlls up, more and more records are likely to be located ever further from their home positions.
from this discussion, we see that the expected cost of hashing is a function of how full the table is. deﬁne the load factor for the table as α = n/m, where n is the number of records currently in the table.
an estimate of the expected cost for an insertion (or an unsuccessful search) can be derived analytically as a function of α in the case where we assume that the probe sequence follows a random permutation of the slots in the hash table. assuming that every slot in the table has equal probability of being the home slot for the next record, the probability of ﬁnding the home position occupied is α. the probability of ﬁnding both the home position occupied and the next slot on the probe sequence occupied is n (n−1)
depending on the pattern of record accesses, it might be possible to reduce the expected cost of access even in the face of collisions. recall the 80/20 rule: 80% of the accesses will come to 20% of the data. in other words, some records are accessed more frequently. if two records hash to the same home position, which would be better placed in the home position, and which in a slot further down the probe sequence? the answer is that the record with higher frequency of access should be placed in the home position, because this will reduce the total number of record accesses. ideally, records along a probe sequence will be ordered by their frequency of access.
one approach to approximating this goal is to modify the order of records along the probe sequence whenever a record is accessed. if a search is made to a record that is not in its home position, a self-organizing list heuristic can be used. for example, if the linear probing collision resolution policy is used, then whenever a record is located that is not in its home position, it can be swapped with the record preceding it in the probe sequence. that other record will now be further from its home position, but hopefully it will be accessed less frequently. note that this approach will not work for the other collision resolution policies presented in this section, because swapping a pair of records to improve access to one might remove the other from its probe sequence.
another approach is to keep access counts for records and periodically rehash the entire table. the records should be inserted into the hash table in frequency order, ensuring that records that were frequently accessed during the last series of requests have the best chance of being near their home positions.
1. deleting a record must not hinder later searches. in other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. for example, in figure 9.6(a), keys 9877 and 2037 both hash to slot 7. key 2037 is placed in slot 8 by the collision resolution policy. if 9877 is deleted from the table, a search for 2037 must still pass through slot 7 as it probes to slot 8.
both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. the tombstone indicates that a record once
figure 9.3 an illustration of bucket hashing for seven numbers stored in a ﬁvebucket hash table using the hash function h(k) = k mod 5. each bucket contains two slots. the numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. two of the values hash to bucket 0, three values hash to bucket 2, one value hashes to bucket 3, and one value hashes to bucket 4. because bucket 2 cannot hold three values, the third one ends up in the overﬂow bucket.
r will be stored at some other slot in the table. it is the business of the collision resolution policy to determine which slot that will be. naturally, the same policy must be followed during search as during insertion, so that any record not found in its home position can be recovered by repeating the collision resolution process.
bucket hashing one implementation for closed hashing groups hash table slots into buckets. the m slots of the hash table are divided into b buckets, with each bucket consisting of m/b slots. the hash function assigns each record to the ﬁrst slot within one of the buckets. if this slot is already occupied, then the bucket slots are searched sequentially until an open slot is found. if a bucket is entirely full, then the record is stored in an overﬂow bucket of inﬁnite capacity at the end of the table. all buckets share the same overﬂow bucket. a good implementation will use a hash function that distributes the records evenly among the buckets so that as few records as possible go into the overﬂow bucket. figure 9.3 illustrates bucket hashing.
when searching for a record, the ﬁrst step is to hash the key to determine which bucket should contain the record. the records in this bucket are then searched. if
the desired key value is not found and the bucket still has free slots, then the search is complete. if the bucket is full, then it is possible that the desired record is stored in the overﬂow bucket. in this case, the overﬂow bucket must be searched until the record is found or all records in the overﬂow bucket have been checked. if many records are in the overﬂow bucket, this will be an expensive process.
a simple variation on bucket hashing is to hash a key value to some slot in the hash table as though bucketing were not being used. if the home position is full, then the collision resolution process is to move down through the table toward the end of the bucket will searching for a free slot in which to store the record. if the bottom of the bucket is reached, then the collision resolution routine wraps around to the top of the bucket to continue the search for an open slot. for example, assume that buckets contain eight records, with the ﬁrst bucket consisting of slots 0 through 7. if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if all slots in this bucket are full, then the record is assigned to the overﬂow bucket. the advantage of this approach is that initial collisions are reduced, because any slot can be a home position rather than just the ﬁrst slot in the bucket.
bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. whenever search or insertion occurs, the entire bucket is read into memory. because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. if the bucket is full, then the overﬂow bucket must be retrieved from disk as well. naturally, overﬂow should be kept small to minimize unnecessary disk accesses.
we now turn to the most commonly used form of hashing: closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hash table.
during insertion, the goal of collision resolution is to ﬁnd a free slot in the hash table when the home position for the record is already occupied. we can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. the ﬁrst slot in the sequence will be the home position for the key. if the home position is occupied, then the collision resolution policy goes to the next slot in the sequence. if this is occupied as well, then another slot must be found, and so on. this sequence of slots is known as the probe sequence, and it is generated by some probe function that we will call p. the insert function is shown in figure 9.4. insertion works as follows:
9.5 implement a database stored on disk using bucket hashing. deﬁne records to be 128 bytes long with a 4-byte key and 120 bytes of data. the remaining 4 bytes are available for you to store necessary information to support the hash table. a bucket in the hash table will be 1024 bytes long, so each bucket has space for 8 records. the hash table should consist of 27 buckets (total space for 216 records with slots indexed by positions 0 to 215) followed by the overﬂow bucket at record position 216 in the ﬁle. the hash function for key value k should be k mod 213. (note that this means the last three slots in the table will not be home positions for any record.) the collision resolution function should be linear probing with wrap-around within the bucket. for example, if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if a bucket is full, the record should be placed in the overﬂow section at the end of the ﬁle. your hash table should implement the dictionary adt of section 4.4. when you do your testing, assume that the system is meant to store about 100 or so records at a time.
9.6 implement the dictionary adt of section 4.4 by means of a hash table with linear probing as the collision resolution policy. you might wish to begin with the code of figure 9.7. using empirical simulation, determine the cost of insert and delete as α grows (i.e., reconstruct the dashed lines of figure 9.8). then, repeat the experiment using quadratic probing and pseudorandom probing. what can you say about the relative performance of these three collision resolution policies?
and the next four bytes (“bbbb”) will be interpreted as the integer value 1,650,614,882. their sum is 3,284,386,755 (when treated as an unsigned integer). if the table size is 101 then the modulus function will cause this key to hash to slot 75 in the table. note that for any sufﬁciently long string, the sum for the integer quantities will typically cause a 32-bit integer to overﬂow (thus losing some of the high-order bits) because the resulting values are so large. but this causes no problems when the goal is to compute a hash function.
while the goal of a hash function is to minimize collisions, collisions are normally unavoidable in practice. thus, hashing implementations must include some form of collision resolution policy. collision resolution techniques can be broken into two classes: open hashing (also called separate chaining) and closed hashing (also called open addressing).3 the difference between the two has to do with whether collisions are stored outside the table (open hashing), or whether collisions result in storing one of the records at another slot in the table (closed hashing). open hashing is treated in this section, and closed hashing in section 9.4.3.
the simplest form of open hashing deﬁnes each slot in the hash table to be the head of a linked list. all records that hash to a particular slot are placed on that slot’s linked list. figure 9.2 illustrates a hash table where each slot stores one record and a link pointer to the rest of the list.
records within a slot’s list can be ordered in several ways: by insertion order, by key value order, or by frequency-of-access order. ordering the list by key value provides an advantage in the case of an unsuccessful search, because we know to stop searching the list once we encounter a key that is greater than the one being searched for. if records on the list are unordered or ordered by frequency, then an unsuccessful search will need to visit every record on the list.
given a table of size m storing n records, the hash function will (ideally) spread the records evenly among the m positions in the table, yielding on average n/m records for each list. assuming that the table has more slots than there are records to be stored, we can hope that few slots will contain more than one record. in the case where a list is empty or has only one record, a search requires only one access to the list. thus, the average cost for hashing should be Θ(1). however, if clustering causes many records to hash to only a few of the slots, then the cost to
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
9.13 assume that you are hashing key k to a hash table of n slots (indexed from 0 to n − 1). for each of the following functions h(k), is the function acceptable as a hash function (i.e., would the hash program work correctly for both insertions and searches), and if so, is it a good hash function? function random(n) returns a random integer between 0 and n − 1, inclusive. (a) h(k) = k/n where k and n are integers. (b) h(k) = 1. (c) h(k) = (k + random(n)) mod n. (d) h(k) = k mod n where n is a prime number.
9.14 assume that you have a seven-slot closed hash table (the slots are numbered 0 through 6). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 7 and linear probing on this list of numbers: 3, 12, 9, 2. after inserting the record with key value 2, list for each empty slot the probability that it will be the next one ﬁlled.
9.15 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and quadratic probing on this list of numbers: 3, 12, 9, 2, 79, 46. after inserting the record with key value 46, list for each empty slot the probability that it will be the next one ﬁlled.
9.16 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and pseudo-random probing on this list of numbers: 3, 12, 9, 2, 79, 44. the permutation of offsets to be used by the pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. after inserting the record with key value 44, list for each empty slot the probability that it will be the next one ﬁlled.
9.18 using closed hashing, with double hashing to resolve collisions, insert the following keys into a hash table of thirteen slots (the slots are numbered 0 through 12). the hash functions to be used are h1 and h2, deﬁned below. you should show the hash table after all eight keys have been inserted.
are many approaches to hashing and it is easy to devise an inefﬁcient implementation. hashing is suitable for both in-memory and disk-based searching and is one of the two most widely used methods for organizing large databases stored on disk (the other is the b-tree, which is covered in chapter 10). as a simple (though unrealistic) example of hashing, consider storing n records, each with a unique key value in the range 0 to n − 1. in this simple case, a record with key k can be stored in ht[k], and the hash function is simply h(k) = k. to ﬁnd the record with key value k, simply look in ht[k].
typically, there are many more values in the key range than there are slots in the hash table. for a more realistic example, suppose that the key can take any value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that we expect to store approximately 1000 records at any given time. it is impractical in this situation to use a hash table with 65,536 slots, because most of the slots will be left empty. instead, we must devise a hash function that allows us to store the records in a much smaller table. because the possible key range is larger than the size of the table, at least some of the slots must be mapped to from multiple key values. given a hash function h and two keys k1 and k2, if h(k1) = β = h(k2) where β is a slot in the table, then we say that k1 and k2 have a collision at slot β under hash function h.
hashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. collisions occur when two records hash to the same slot in the table. if we are careful—or lucky—when selecting a hash function, then the actual number of collisions will be few. unfortunately, even under the best of circumstances, collisions are nearly unavoidable.1 for example, consider a classroom full of students. what is the
1the exception to this is perfect hashing. perfect hashing is a system in which records are hashed such that there are no collisions. a hash function is selected for the speciﬁc set of records being hashed, which requires that the entire collection of records be available before selecting the hash function. perfect hashing is efﬁcient because it always ﬁnds the record that we are looking for exactly where the hash function computes it to be, so only one access is required. selecting a perfect hash function can be expensive but might be worthwhile when extremely efﬁcient search
figure 9.2 an illustration of open hashing for seven numbers stored in a ten-slot hash table using the hash function h(k) = k mod 10. the numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. two of the values hash to slot 0, one value hashes to slot 2, three of the values hash to slot 7, and one value hashes to slot 9.
open hashing is most appropriate when the hash table is kept in main memory, with the lists implemented by a standard in-memory linked list. storing an open hash table on disk in an efﬁcient way is difﬁcult, because members of a given linked list might be stored on different disk blocks. this would result in multiple disk accesses when searching for a particular key value, which defeats the purpose of using hashing.
there are similarities between open hashing and binsort. one way to view open hashing is that each record is simply placed in a bin. while multiple records may hash to the same bin, this initial binning should still greatly reduce the number of records accessed by a search operation. in a similar fashion, a simple binsort reduces the number of records in each bin to a small number that can be sorted in some other way.
closed hashing stores all records directly in the hash table. each record r with key value kr has a home position that is h(kr), the slot computed by the hash function. if r is to be inserted and another record already occupies r’s home position, then
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
depending on the pattern of record accesses, it might be possible to reduce the expected cost of access even in the face of collisions. recall the 80/20 rule: 80% of the accesses will come to 20% of the data. in other words, some records are accessed more frequently. if two records hash to the same home position, which would be better placed in the home position, and which in a slot further down the probe sequence? the answer is that the record with higher frequency of access should be placed in the home position, because this will reduce the total number of record accesses. ideally, records along a probe sequence will be ordered by their frequency of access.
one approach to approximating this goal is to modify the order of records along the probe sequence whenever a record is accessed. if a search is made to a record that is not in its home position, a self-organizing list heuristic can be used. for example, if the linear probing collision resolution policy is used, then whenever a record is located that is not in its home position, it can be swapped with the record preceding it in the probe sequence. that other record will now be further from its home position, but hopefully it will be accessed less frequently. note that this approach will not work for the other collision resolution policies presented in this section, because swapping a pair of records to improve access to one might remove the other from its probe sequence.
another approach is to keep access counts for records and periodically rehash the entire table. the records should be inserted into the hash table in frequency order, ensuring that records that were frequently accessed during the last series of requests have the best chance of being near their home positions.
1. deleting a record must not hinder later searches. in other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. for example, in figure 9.6(a), keys 9877 and 2037 both hash to slot 7. key 2037 is placed in slot 8 by the collision resolution policy. if 9877 is deleted from the table, a search for 2037 must still pass through slot 7 as it probes to slot 8.
both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. the tombstone indicates that a record once
occupied the slot but does so no longer. if a tombstone is encountered when searching through a probe sequence, the search procedure is to continue with the search. when a tombstone is encountered during insertion, that slot can be used to store the new record. however, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. however, the new record would actually be inserted into the slot of the ﬁrst tombstone encountered. the use of tombstones allows searches to work correctly and allows reuse of deleted slots. however, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. this will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. a typical database application will ﬁrst load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. after the table is loaded with the initial collection of records, the ﬁrst few deletions will lengthen the average probe sequence distance for records (it will add tombstones). over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by ﬁlling in tombstone slots. for example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). after a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. this seems like a small increase, but it is three times longer on average beyond the home position than before deletions.
two possible solutions to this problem are 1. do a local reorganization upon deletion to try to shorten the average path length. for example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove a key from its probe sequence). this will not work for all collision resolution policies. 2. periodically rehash the table by reinserting all records into a new hash table. not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions.
for a comparison of the efﬁciencies for various self-organizing techniques, see bentley and mcgeoch, “amortized analysis of self-organizing sequential search heuristics” [bm85]. the text compression example of section 9.2 comes from
be sure to indicate how you are using h1 and h2 to do the hashing. function rev(k) reverses the decimal digits of k, for example, rev(37) = 73; rev(7) = 7. h1(k) = k mod 13. h2(k) = (rev(k + 1) mod 11). keys: 2, 8, 31, 20, 19, 18, 53, 27.
9.19 write an algorithm for a deletion function for hash tables that replaces the record with a special value indicating a tombstone. modify the functions hashinsert and hashsearch to work correctly with tombstones.
analyze what will happen if this permutation is used by an implementation of pseudo-random probing on a hash table of size seven. will this permutation solve the problem of primary clustering? what does this say about selecting a permutation for use when implementing pseudo-random probing?
9.1 implement a binary search and the quadratic binary search of section 9.1. run your implementations over a large range of problem sizes, timing the results for each algorithm. graph and compare these timing results.
9.2 implement the three self-organizing list heuristics count, move-to-front, and transpose. compare the cost for running the three heuristics on various input data. the cost metric should be the total number of comparisons required when searching the list. it is important to compare the heuristics using input data for which self-organizing lists are reasonable, that is, on frequency distributions that are uneven. one good approach is to read text ﬁles. the list should store individual words in the text ﬁle. begin with an empty list, as was done for the text compression example of section 9.2. each time a word is encountered in the text ﬁle, search for it in the self-organizing list. if the word is found, reorder the list as appropriate. if the word is not in the list, add it to the end of the list and then reorder as appropriate.
9.3 implement the text compression system described in section 9.2. 9.4 implement a system for managing document retrieval. your system should have the ability to insert (abstract references to) documents into the system, associate keywords with a given document, and to search for documents with speciﬁed keywords.
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
9.13 assume that you are hashing key k to a hash table of n slots (indexed from 0 to n − 1). for each of the following functions h(k), is the function acceptable as a hash function (i.e., would the hash program work correctly for both insertions and searches), and if so, is it a good hash function? function random(n) returns a random integer between 0 and n − 1, inclusive. (a) h(k) = k/n where k and n are integers. (b) h(k) = 1. (c) h(k) = (k + random(n)) mod n. (d) h(k) = k mod n where n is a prime number.
9.14 assume that you have a seven-slot closed hash table (the slots are numbered 0 through 6). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 7 and linear probing on this list of numbers: 3, 12, 9, 2. after inserting the record with key value 2, list for each empty slot the probability that it will be the next one ﬁlled.
9.15 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and quadratic probing on this list of numbers: 3, 12, 9, 2, 79, 46. after inserting the record with key value 46, list for each empty slot the probability that it will be the next one ﬁlled.
9.16 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and pseudo-random probing on this list of numbers: 3, 12, 9, 2, 79, 44. the permutation of offsets to be used by the pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. after inserting the record with key value 44, list for each empty slot the probability that it will be the next one ﬁlled.
9.18 using closed hashing, with double hashing to resolve collisions, insert the following keys into a hash table of thirteen slots (the slots are numbered 0 through 12). the hash functions to be used are h1 and h2, deﬁned below. you should show the hash table after all eight keys have been inserted.
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
are many approaches to hashing and it is easy to devise an inefﬁcient implementation. hashing is suitable for both in-memory and disk-based searching and is one of the two most widely used methods for organizing large databases stored on disk (the other is the b-tree, which is covered in chapter 10). as a simple (though unrealistic) example of hashing, consider storing n records, each with a unique key value in the range 0 to n − 1. in this simple case, a record with key k can be stored in ht[k], and the hash function is simply h(k) = k. to ﬁnd the record with key value k, simply look in ht[k].
typically, there are many more values in the key range than there are slots in the hash table. for a more realistic example, suppose that the key can take any value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that we expect to store approximately 1000 records at any given time. it is impractical in this situation to use a hash table with 65,536 slots, because most of the slots will be left empty. instead, we must devise a hash function that allows us to store the records in a much smaller table. because the possible key range is larger than the size of the table, at least some of the slots must be mapped to from multiple key values. given a hash function h and two keys k1 and k2, if h(k1) = β = h(k2) where β is a slot in the table, then we say that k1 and k2 have a collision at slot β under hash function h.
hashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. collisions occur when two records hash to the same slot in the table. if we are careful—or lucky—when selecting a hash function, then the actual number of collisions will be few. unfortunately, even under the best of circumstances, collisions are nearly unavoidable.1 for example, consider a classroom full of students. what is the
1the exception to this is perfect hashing. perfect hashing is a system in which records are hashed such that there are no collisions. a hash function is selected for the speciﬁc set of records being hashed, which requires that the entire collection of records be available before selecting the hash function. perfect hashing is efﬁcient because it always ﬁnds the record that we are looking for exactly where the hash function computes it to be, so only one access is required. selecting a perfect hash function can be expensive but might be worthwhile when extremely efﬁcient search
and the next four bytes (“bbbb”) will be interpreted as the integer value 1,650,614,882. their sum is 3,284,386,755 (when treated as an unsigned integer). if the table size is 101 then the modulus function will cause this key to hash to slot 75 in the table. note that for any sufﬁciently long string, the sum for the integer quantities will typically cause a 32-bit integer to overﬂow (thus losing some of the high-order bits) because the resulting values are so large. but this causes no problems when the goal is to compute a hash function.
while the goal of a hash function is to minimize collisions, collisions are normally unavoidable in practice. thus, hashing implementations must include some form of collision resolution policy. collision resolution techniques can be broken into two classes: open hashing (also called separate chaining) and closed hashing (also called open addressing).3 the difference between the two has to do with whether collisions are stored outside the table (open hashing), or whether collisions result in storing one of the records at another slot in the table (closed hashing). open hashing is treated in this section, and closed hashing in section 9.4.3.
the simplest form of open hashing deﬁnes each slot in the hash table to be the head of a linked list. all records that hash to a particular slot are placed on that slot’s linked list. figure 9.2 illustrates a hash table where each slot stores one record and a link pointer to the rest of the list.
records within a slot’s list can be ordered in several ways: by insertion order, by key value order, or by frequency-of-access order. ordering the list by key value provides an advantage in the case of an unsuccessful search, because we know to stop searching the list once we encounter a key that is greater than the one being searched for. if records on the list are unordered or ordered by frequency, then an unsuccessful search will need to visit every record on the list.
given a table of size m storing n records, the hash function will (ideally) spread the records evenly among the m positions in the table, yielding on average n/m records for each list. assuming that the table has more slots than there are records to be stored, we can hope that few slots will contain more than one record. in the case where a list is empty or has only one record, a search requires only one access to the list. thus, the average cost for hashing should be Θ(1). however, if clustering causes many records to hash to only a few of the slots, then the cost to
9.13 assume that you are hashing key k to a hash table of n slots (indexed from 0 to n − 1). for each of the following functions h(k), is the function acceptable as a hash function (i.e., would the hash program work correctly for both insertions and searches), and if so, is it a good hash function? function random(n) returns a random integer between 0 and n − 1, inclusive. (a) h(k) = k/n where k and n are integers. (b) h(k) = 1. (c) h(k) = (k + random(n)) mod n. (d) h(k) = k mod n where n is a prime number.
9.14 assume that you have a seven-slot closed hash table (the slots are numbered 0 through 6). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 7 and linear probing on this list of numbers: 3, 12, 9, 2. after inserting the record with key value 2, list for each empty slot the probability that it will be the next one ﬁlled.
9.15 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and quadratic probing on this list of numbers: 3, 12, 9, 2, 79, 46. after inserting the record with key value 46, list for each empty slot the probability that it will be the next one ﬁlled.
9.16 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and pseudo-random probing on this list of numbers: 3, 12, 9, 2, 79, 44. the permutation of offsets to be used by the pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. after inserting the record with key value 44, list for each empty slot the probability that it will be the next one ﬁlled.
9.18 using closed hashing, with double hashing to resolve collisions, insert the following keys into a hash table of thirteen slots (the slots are numbered 0 through 12). the hash functions to be used are h1 and h2, deﬁned below. you should show the hash table after all eight keys have been inserted.
figure 9.2 an illustration of open hashing for seven numbers stored in a ten-slot hash table using the hash function h(k) = k mod 10. the numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. two of the values hash to slot 0, one value hashes to slot 2, three of the values hash to slot 7, and one value hashes to slot 9.
open hashing is most appropriate when the hash table is kept in main memory, with the lists implemented by a standard in-memory linked list. storing an open hash table on disk in an efﬁcient way is difﬁcult, because members of a given linked list might be stored on different disk blocks. this would result in multiple disk accesses when searching for a particular key value, which defeats the purpose of using hashing.
there are similarities between open hashing and binsort. one way to view open hashing is that each record is simply placed in a bin. while multiple records may hash to the same bin, this initial binning should still greatly reduce the number of records accessed by a search operation. in a similar fashion, a simple binsort reduces the number of records in each bin to a small number that can be sorted in some other way.
closed hashing stores all records directly in the hash table. each record r with key value kr has a home position that is h(kr), the slot computed by the hash function. if r is to be inserted and another record already occupies r’s home position, then
the desired key value is not found and the bucket still has free slots, then the search is complete. if the bucket is full, then it is possible that the desired record is stored in the overﬂow bucket. in this case, the overﬂow bucket must be searched until the record is found or all records in the overﬂow bucket have been checked. if many records are in the overﬂow bucket, this will be an expensive process.
a simple variation on bucket hashing is to hash a key value to some slot in the hash table as though bucketing were not being used. if the home position is full, then the collision resolution process is to move down through the table toward the end of the bucket will searching for a free slot in which to store the record. if the bottom of the bucket is reached, then the collision resolution routine wraps around to the top of the bucket to continue the search for an open slot. for example, assume that buckets contain eight records, with the ﬁrst bucket consisting of slots 0 through 7. if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if all slots in this bucket are full, then the record is assigned to the overﬂow bucket. the advantage of this approach is that initial collisions are reduced, because any slot can be a home position rather than just the ﬁrst slot in the bucket.
bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. whenever search or insertion occurs, the entire bucket is read into memory. because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. if the bucket is full, then the overﬂow bucket must be retrieved from disk as well. naturally, overﬂow should be kept small to minimize unnecessary disk accesses.
we now turn to the most commonly used form of hashing: closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hash table.
during insertion, the goal of collision resolution is to ﬁnd a free slot in the hash table when the home position for the record is already occupied. we can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. the ﬁrst slot in the sequence will be the home position for the key. if the home position is occupied, then the collision resolution policy goes to the next slot in the sequence. if this is occupied as well, then another slot must be found, and so on. this sequence of slots is known as the probe sequence, and it is generated by some probe function that we will call p. the insert function is shown in figure 9.4. insertion works as follows:
the desired key value is not found and the bucket still has free slots, then the search is complete. if the bucket is full, then it is possible that the desired record is stored in the overﬂow bucket. in this case, the overﬂow bucket must be searched until the record is found or all records in the overﬂow bucket have been checked. if many records are in the overﬂow bucket, this will be an expensive process.
a simple variation on bucket hashing is to hash a key value to some slot in the hash table as though bucketing were not being used. if the home position is full, then the collision resolution process is to move down through the table toward the end of the bucket will searching for a free slot in which to store the record. if the bottom of the bucket is reached, then the collision resolution routine wraps around to the top of the bucket to continue the search for an open slot. for example, assume that buckets contain eight records, with the ﬁrst bucket consisting of slots 0 through 7. if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if all slots in this bucket are full, then the record is assigned to the overﬂow bucket. the advantage of this approach is that initial collisions are reduced, because any slot can be a home position rather than just the ﬁrst slot in the bucket.
bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. whenever search or insertion occurs, the entire bucket is read into memory. because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. if the bucket is full, then the overﬂow bucket must be retrieved from disk as well. naturally, overﬂow should be kept small to minimize unnecessary disk accesses.
we now turn to the most commonly used form of hashing: closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hash table.
during insertion, the goal of collision resolution is to ﬁnd a free slot in the hash table when the home position for the record is already occupied. we can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. the ﬁrst slot in the sequence will be the home position for the key. if the home position is occupied, then the collision resolution policy goes to the next slot in the sequence. if this is occupied as well, then another slot must be found, and so on. this sequence of slots is known as the probe sequence, and it is generated by some probe function that we will call p. the insert function is shown in figure 9.4. insertion works as follows:
figure 9.6 example of problems with linear probing. (a) four values are inserted in the order 1001, 9050, 9877, and 2037 using hash function h(k) = k mod 10. (b) the value 1059 is added to the hash table.
in slot 2 with probability 6/10. this is illustrated by figure 9.6(b). this tendency of linear probing to cluster items together is known as primary clustering. small clusters tend to merge into big clusters, making the problem worse. the objection to primary clustering is that it leads to long probe sequences.
how can we avoid primary clustering? one possible improvement might be to use linear probing, but to skip slots by a constant c other than 1. this would make the probe function
and so the ith slot in the probe sequence will be (h(k) + ic) mod m. in this way, records with adjacent home positions will not follow the same probe sequence. for example, if we were to skip by twos, then our offsets from the home slot would be 2, then 4, then 6, and so on.
one quality of a good probe sequence is that it will cycle through all slots in the hash table before returning to the home position. clearly linear probing (which “skips” slots by one each time) does this. unfortunately, not all values for c will make this happen. for example, if c = 2 and the table contains an even number of slots, then any key whose home position is in an even slot will have a probe
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
figure 9.8 growth of expected record accesses with α. the horizontal axis is the value for α, the vertical axis is the expected number of accesses to the hash table. solid lines show the cost for “random” probing (a theoretical lower bound on the cost), while dashed lines show the cost for linear probing (a relatively poor collision resolution strategy). the two leftmost lines show the cost for insertion (equivalently, unsuccessful search); the two rightmost lines show the cost for deletion (equivalently, successful search).
full. beyond that point performance will degrade rapidly. this requires that the implementor have some idea of how many records are likely to be in the table at maximum loading, and select the table size accordingly.
you might notice that a recommendation to never let a hash table become more than half full contradicts the disk-based space/time tradeoff principle, which strives to minimize disk space to increase information density. hashing represents an unusual situation in that there is no beneﬁt to be expected from locality of reference. in a sense, the hashing system implementor does everything possible to eliminate the effects of locality of reference! given the disk block containing the last record accessed, the chance of the next record access coming to the same disk block is no better than random chance in a well-designed hash system. this is because a good hashing implementation breaks up relationships between search keys. instead of improving performance by taking advantage of locality of reference, hashing trades increased hash table space for an improved chance that the record will be in its home position. thus, the more space available for the hash table, the more efﬁcient hashing should be.
9.13 assume that you are hashing key k to a hash table of n slots (indexed from 0 to n − 1). for each of the following functions h(k), is the function acceptable as a hash function (i.e., would the hash program work correctly for both insertions and searches), and if so, is it a good hash function? function random(n) returns a random integer between 0 and n − 1, inclusive. (a) h(k) = k/n where k and n are integers. (b) h(k) = 1. (c) h(k) = (k + random(n)) mod n. (d) h(k) = k mod n where n is a prime number.
9.14 assume that you have a seven-slot closed hash table (the slots are numbered 0 through 6). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 7 and linear probing on this list of numbers: 3, 12, 9, 2. after inserting the record with key value 2, list for each empty slot the probability that it will be the next one ﬁlled.
9.15 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and quadratic probing on this list of numbers: 3, 12, 9, 2, 79, 46. after inserting the record with key value 46, list for each empty slot the probability that it will be the next one ﬁlled.
9.16 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and pseudo-random probing on this list of numbers: 3, 12, 9, 2, 79, 44. the permutation of offsets to be used by the pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. after inserting the record with key value 44, list for each empty slot the probability that it will be the next one ﬁlled.
9.18 using closed hashing, with double hashing to resolve collisions, insert the following keys into a hash table of thirteen slots (the slots are numbered 0 through 12). the hash functions to be used are h1 and h2, deﬁned below. you should show the hash table after all eight keys have been inserted.
9.5 implement a database stored on disk using bucket hashing. deﬁne records to be 128 bytes long with a 4-byte key and 120 bytes of data. the remaining 4 bytes are available for you to store necessary information to support the hash table. a bucket in the hash table will be 1024 bytes long, so each bucket has space for 8 records. the hash table should consist of 27 buckets (total space for 216 records with slots indexed by positions 0 to 215) followed by the overﬂow bucket at record position 216 in the ﬁle. the hash function for key value k should be k mod 213. (note that this means the last three slots in the table will not be home positions for any record.) the collision resolution function should be linear probing with wrap-around within the bucket. for example, if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if a bucket is full, the record should be placed in the overﬂow section at the end of the ﬁle. your hash table should implement the dictionary adt of section 4.4. when you do your testing, assume that the system is meant to store about 100 or so records at a time.
9.6 implement the dictionary adt of section 4.4 by means of a hash table with linear probing as the collision resolution policy. you might wish to begin with the code of figure 9.7. using empirical simulation, determine the cost of insert and delete as α grows (i.e., reconstruct the dashed lines of figure 9.8). then, repeat the experiment using quadratic probing and pseudorandom probing. what can you say about the relative performance of these three collision resolution policies?
how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equivalent to the number required to successfully search for it. to insert a record, an empty slot along the record’s probe sequence must be found. this is equivalent to an unsuccessful search for the record (recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table).
when the hash table is empty, the ﬁrst record inserted will always ﬁnd its home position free. thus, it will require only one record access to ﬁnd a free slot. if all records are stored in their home positions, then successful searches will also require only one record access. as the table begins to ﬁll up, the probability that a record can be inserted in its home position decreases. if a record hashes to an occupied slot, then the collision resolution policy must locate another slot in which to store it. finding records not stored in their home position also requires additional record accesses as the record is searched for along its probe sequence. as the table ﬁlls up, more and more records are likely to be located ever further from their home positions.
from this discussion, we see that the expected cost of hashing is a function of how full the table is. deﬁne the load factor for the table as α = n/m, where n is the number of records currently in the table.
an estimate of the expected cost for an insertion (or an unsuccessful search) can be derived analytically as a function of α in the case where we assume that the probe sequence follows a random permutation of the slots in the hash table. assuming that every slot in the table has equal probability of being the home slot for the next record, the probability of ﬁnding the home position occupied is α. the probability of ﬁnding both the home position occupied and the next slot on the probe sequence occupied is n (n−1)
and the next four bytes (“bbbb”) will be interpreted as the integer value 1,650,614,882. their sum is 3,284,386,755 (when treated as an unsigned integer). if the table size is 101 then the modulus function will cause this key to hash to slot 75 in the table. note that for any sufﬁciently long string, the sum for the integer quantities will typically cause a 32-bit integer to overﬂow (thus losing some of the high-order bits) because the resulting values are so large. but this causes no problems when the goal is to compute a hash function.
while the goal of a hash function is to minimize collisions, collisions are normally unavoidable in practice. thus, hashing implementations must include some form of collision resolution policy. collision resolution techniques can be broken into two classes: open hashing (also called separate chaining) and closed hashing (also called open addressing).3 the difference between the two has to do with whether collisions are stored outside the table (open hashing), or whether collisions result in storing one of the records at another slot in the table (closed hashing). open hashing is treated in this section, and closed hashing in section 9.4.3.
the simplest form of open hashing deﬁnes each slot in the hash table to be the head of a linked list. all records that hash to a particular slot are placed on that slot’s linked list. figure 9.2 illustrates a hash table where each slot stores one record and a link pointer to the rest of the list.
records within a slot’s list can be ordered in several ways: by insertion order, by key value order, or by frequency-of-access order. ordering the list by key value provides an advantage in the case of an unsuccessful search, because we know to stop searching the list once we encounter a key that is greater than the one being searched for. if records on the list are unordered or ordered by frequency, then an unsuccessful search will need to visit every record on the list.
given a table of size m storing n records, the hash function will (ideally) spread the records evenly among the m positions in the table, yielding on average n/m records for each list. assuming that the table has more slots than there are records to be stored, we can hope that few slots will contain more than one record. in the case where a list is empty or has only one record, a search requires only one access to the list. thus, the average cost for hashing should be Θ(1). however, if clustering causes many records to hash to only a few of the slots, then the cost to
figure 9.3 an illustration of bucket hashing for seven numbers stored in a ﬁvebucket hash table using the hash function h(k) = k mod 5. each bucket contains two slots. the numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. two of the values hash to bucket 0, three values hash to bucket 2, one value hashes to bucket 3, and one value hashes to bucket 4. because bucket 2 cannot hold three values, the third one ends up in the overﬂow bucket.
r will be stored at some other slot in the table. it is the business of the collision resolution policy to determine which slot that will be. naturally, the same policy must be followed during search as during insertion, so that any record not found in its home position can be recovered by repeating the collision resolution process.
bucket hashing one implementation for closed hashing groups hash table slots into buckets. the m slots of the hash table are divided into b buckets, with each bucket consisting of m/b slots. the hash function assigns each record to the ﬁrst slot within one of the buckets. if this slot is already occupied, then the bucket slots are searched sequentially until an open slot is found. if a bucket is entirely full, then the record is stored in an overﬂow bucket of inﬁnite capacity at the end of the table. all buckets share the same overﬂow bucket. a good implementation will use a hash function that distributes the records evenly among the buckets so that as few records as possible go into the overﬂow bucket. figure 9.3 illustrates bucket hashing.
when searching for a record, the ﬁrst step is to hash the key to determine which bucket should contain the record. the records in this bucket are then searched. if
are many approaches to hashing and it is easy to devise an inefﬁcient implementation. hashing is suitable for both in-memory and disk-based searching and is one of the two most widely used methods for organizing large databases stored on disk (the other is the b-tree, which is covered in chapter 10). as a simple (though unrealistic) example of hashing, consider storing n records, each with a unique key value in the range 0 to n − 1. in this simple case, a record with key k can be stored in ht[k], and the hash function is simply h(k) = k. to ﬁnd the record with key value k, simply look in ht[k].
typically, there are many more values in the key range than there are slots in the hash table. for a more realistic example, suppose that the key can take any value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that we expect to store approximately 1000 records at any given time. it is impractical in this situation to use a hash table with 65,536 slots, because most of the slots will be left empty. instead, we must devise a hash function that allows us to store the records in a much smaller table. because the possible key range is larger than the size of the table, at least some of the slots must be mapped to from multiple key values. given a hash function h and two keys k1 and k2, if h(k1) = β = h(k2) where β is a slot in the table, then we say that k1 and k2 have a collision at slot β under hash function h.
hashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. collisions occur when two records hash to the same slot in the table. if we are careful—or lucky—when selecting a hash function, then the actual number of collisions will be few. unfortunately, even under the best of circumstances, collisions are nearly unavoidable.1 for example, consider a classroom full of students. what is the
1the exception to this is perfect hashing. perfect hashing is a system in which records are hashed such that there are no collisions. a hash function is selected for the speciﬁc set of records being hashed, which requires that the entire collection of records be available before selecting the hash function. perfect hashing is efﬁcient because it always ﬁnds the record that we are looking for exactly where the hash function computes it to be, so only one access is required. selecting a perfect hash function can be expensive but might be worthwhile when extremely efﬁcient search
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
figure 9.6 example of problems with linear probing. (a) four values are inserted in the order 1001, 9050, 9877, and 2037 using hash function h(k) = k mod 10. (b) the value 1059 is added to the hash table.
in slot 2 with probability 6/10. this is illustrated by figure 9.6(b). this tendency of linear probing to cluster items together is known as primary clustering. small clusters tend to merge into big clusters, making the problem worse. the objection to primary clustering is that it leads to long probe sequences.
how can we avoid primary clustering? one possible improvement might be to use linear probing, but to skip slots by a constant c other than 1. this would make the probe function
and so the ith slot in the probe sequence will be (h(k) + ic) mod m. in this way, records with adjacent home positions will not follow the same probe sequence. for example, if we were to skip by twos, then our offsets from the home slot would be 2, then 4, then 6, and so on.
one quality of a good probe sequence is that it will cycle through all slots in the hash table before returning to the home position. clearly linear probing (which “skips” slots by one each time) does this. unfortunately, not all values for c will make this happen. for example, if c = 2 and the table contains an even number of slots, then any key whose home position is in an even slot will have a probe
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
method hashinsert ﬁrst checks to see if the home slot for the key is empty. if the home slot is occupied, then we use the probe function, p(k, i) to locate a free slot in the table. function p has two parameters, the key k and a count i for where in the probe sequence we wish to be. that is, to get the ﬁrst position in the probe sequence after the home slot for key k, we call p(k, 1). for the next slot in the probe sequence, call p(k, 2). note that the probe function returns an offset from the original home position, rather than a slot in the hash table. thus, the for loop in hashinsert is computing positions in the table at each iteration by adding the value returned from the probe function to the home position. the ith call to p returns the ith offset to be used.
searching in a hash table follows the same probe sequence that was followed when inserting records. in this way, a record not in its home position can be recovered. a java implementation for the search procedure is shown in figure 9.5 both the insert and the search routines assume that at least one slot on the probe sequence of every key will be empty. otherwise, they will continue in an inﬁnite loop on unsuccessful searches. thus, the dictionary should keep a count of the
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
the desired key value is not found and the bucket still has free slots, then the search is complete. if the bucket is full, then it is possible that the desired record is stored in the overﬂow bucket. in this case, the overﬂow bucket must be searched until the record is found or all records in the overﬂow bucket have been checked. if many records are in the overﬂow bucket, this will be an expensive process.
a simple variation on bucket hashing is to hash a key value to some slot in the hash table as though bucketing were not being used. if the home position is full, then the collision resolution process is to move down through the table toward the end of the bucket will searching for a free slot in which to store the record. if the bottom of the bucket is reached, then the collision resolution routine wraps around to the top of the bucket to continue the search for an open slot. for example, assume that buckets contain eight records, with the ﬁrst bucket consisting of slots 0 through 7. if a record is hashed to slot 5, the collision resolution process will attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and ﬁnally 4. if all slots in this bucket are full, then the record is assigned to the overﬂow bucket. the advantage of this approach is that initial collisions are reduced, because any slot can be a home position rather than just the ﬁrst slot in the bucket.
bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. whenever search or insertion occurs, the entire bucket is read into memory. because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. if the bucket is full, then the overﬂow bucket must be retrieved from disk as well. naturally, overﬂow should be kept small to minimize unnecessary disk accesses.
we now turn to the most commonly used form of hashing: closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hash table.
during insertion, the goal of collision resolution is to ﬁnd a free slot in the hash table when the home position for the record is already occupied. we can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. the ﬁrst slot in the sequence will be the home position for the key. if the home position is occupied, then the collision resolution policy goes to the next slot in the sequence. if this is occupied as well, then another slot must be found, and so on. this sequence of slots is known as the probe sequence, and it is generated by some probe function that we will call p. the insert function is shown in figure 9.4. insertion works as follows:
the discussion on bucket hashing presented a simple method of collision resolution. if the home position for the record is occupied, then move down the bucket until a free slot is found. this is an example of a technique for collision resolution known as linear probing. the probe function for simple linear probing is
once the bottom of the table is reached, the probe sequence wraps around to the beginning of the table. linear probing has the virtue that all slots in the table will be candidates for inserting a new record before the probe sequence returns to the home position.
it is important to understand that, while linear probing is probably the ﬁrst idea that comes to mind when considering collision resolution policies, it is not the only one possible. probe function p allows us many options for how to do collision resolution. in fact, linear probing is one of the worst collision resolution methods. the main problem is illustrated by figure 9.6. here, we see a hash table of ten slots used to store four-digit numbers, with hash function h(k) = k mod 10. in figure 9.6(a), ﬁve numbers have been placed in the table, leaving ﬁve slots remaining.
the ideal behavior for a collision resolution mechanism is that each empty slot in the table will have equal probability of receiving the next record inserted (assuming that every slot in the table has equal probability of being hashed to initially). in this example, the hash function gives each slot (roughly) equal probability of being the home position for the next key. however, consider what happens to the next record if its key has its home position at slot 0. linear probing will send the record to slot 2. the same will happen to records whose home position is at slot 1. a record with home position at slot 2 will remain in slot 2. thus, the probability is 3/10 that the next record inserted will end up in slot 2. in a similar manner, records hashing to slots 7 or 8 will end up in slot 9. however, only records hashing to slot 3 will be stored in slot 3, yielding one chance in ten of this happening. likewise, there is only one chance in ten that the next record will be stored in slot 4, one chance in ten for slot 5, and one chance in ten for slot 6. thus, the resulting probabilities are not equal.
to make matters worse, if the next record ends up in slot 9 (which already has a higher than normal chance of happening), then the following record will end up
how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equivalent to the number required to successfully search for it. to insert a record, an empty slot along the record’s probe sequence must be found. this is equivalent to an unsuccessful search for the record (recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table).
when the hash table is empty, the ﬁrst record inserted will always ﬁnd its home position free. thus, it will require only one record access to ﬁnd a free slot. if all records are stored in their home positions, then successful searches will also require only one record access. as the table begins to ﬁll up, the probability that a record can be inserted in its home position decreases. if a record hashes to an occupied slot, then the collision resolution policy must locate another slot in which to store it. finding records not stored in their home position also requires additional record accesses as the record is searched for along its probe sequence. as the table ﬁlls up, more and more records are likely to be located ever further from their home positions.
from this discussion, we see that the expected cost of hashing is a function of how full the table is. deﬁne the load factor for the table as α = n/m, where n is the number of records currently in the table.
an estimate of the expected cost for an insertion (or an unsuccessful search) can be derived analytically as a function of α in the case where we assume that the probe sequence follows a random permutation of the slots in the hash table. assuming that every slot in the table has equal probability of being the home slot for the next record, the probability of ﬁnding the home position occupied is α. the probability of ﬁnding both the home position occupied and the next slot on the probe sequence occupied is n (n−1)
depending on the pattern of record accesses, it might be possible to reduce the expected cost of access even in the face of collisions. recall the 80/20 rule: 80% of the accesses will come to 20% of the data. in other words, some records are accessed more frequently. if two records hash to the same home position, which would be better placed in the home position, and which in a slot further down the probe sequence? the answer is that the record with higher frequency of access should be placed in the home position, because this will reduce the total number of record accesses. ideally, records along a probe sequence will be ordered by their frequency of access.
one approach to approximating this goal is to modify the order of records along the probe sequence whenever a record is accessed. if a search is made to a record that is not in its home position, a self-organizing list heuristic can be used. for example, if the linear probing collision resolution policy is used, then whenever a record is located that is not in its home position, it can be swapped with the record preceding it in the probe sequence. that other record will now be further from its home position, but hopefully it will be accessed less frequently. note that this approach will not work for the other collision resolution policies presented in this section, because swapping a pair of records to improve access to one might remove the other from its probe sequence.
another approach is to keep access counts for records and periodically rehash the entire table. the records should be inserted into the hash table in frequency order, ensuring that records that were frequently accessed during the last series of requests have the best chance of being near their home positions.
1. deleting a record must not hinder later searches. in other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. for example, in figure 9.6(a), keys 9877 and 2037 both hash to slot 7. key 2037 is placed in slot 8 by the collision resolution policy. if 9877 is deleted from the table, a search for 2037 must still pass through slot 7 as it probes to slot 8.
both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. the tombstone indicates that a record once
occupied the slot but does so no longer. if a tombstone is encountered when searching through a probe sequence, the search procedure is to continue with the search. when a tombstone is encountered during insertion, that slot can be used to store the new record. however, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. however, the new record would actually be inserted into the slot of the ﬁrst tombstone encountered. the use of tombstones allows searches to work correctly and allows reuse of deleted slots. however, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. this will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. a typical database application will ﬁrst load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. after the table is loaded with the initial collection of records, the ﬁrst few deletions will lengthen the average probe sequence distance for records (it will add tombstones). over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by ﬁlling in tombstone slots. for example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). after a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. this seems like a small increase, but it is three times longer on average beyond the home position than before deletions.
two possible solutions to this problem are 1. do a local reorganization upon deletion to try to shorten the average path length. for example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove a key from its probe sequence). this will not work for all collision resolution policies. 2. periodically rehash the table by reinserting all records into a new hash table. not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions.
for a comparison of the efﬁciencies for various self-organizing techniques, see bentley and mcgeoch, “amortized analysis of self-organizing sequential search heuristics” [bm85]. the text compression example of section 9.2 comes from
sequence that cycles through only the even slots. likewise, the probe sequence for a key whose home position is in an odd slot will cycle through the odd slots. thus, this combination of table size and linear probing constant effectively divides the records into two sets stored in two disjoint sections of the hash table. so long as both sections of the table contain the same number of records, this is not really important. however, just from chance it is likely that one section will become fuller than the other, leading to more collisions and poorer performance for those records. the other section would have fewer records, and thus better performance. but the overall system performance will be degraded, as the additional cost to the side that is more full outweighs the improved performance of the less-full side.
constant c must be relatively prime to m to generate a linear probing sequence that visits all slots in the table (that is, c and m must share no factors). for a hash table of size m = 10, if c is any one of 1, 3, 7, or 9, then the probe sequence will visit all slots for any key. when m = 11, any value for c between 1 and 10 generates a probe sequence that visits all slots for every key.
consider the situation where c = 2 and we wish to insert a record with key k1 such that h(k1) = 3. the probe sequence for k1 is 3, 5, 7, 9, and so on. if another key k2 has home position at slot 5, then its probe sequence will be 5, 7, 9, and so on. the probe sequences of k1 and k2 are linked together in a manner that contributes to clustering. in other words, linear probing with a value of c > 1 does not solve the problem of primary clustering. we would like to ﬁnd a probe function that does not link keys together in this way. we would prefer that the probe sequence for k1 after the ﬁrst step on the sequence should not be identical to the probe sequence of k2. instead, their probe sequences should diverge.
the ideal probe function would select the next position on the probe sequence at random from among the unvisited slots; that is, the probe sequence should be a random permutation of the hash table positions. unfortunately, we cannot actually select the next position in the probe sequence at random, because then we would not be able to duplicate this same probe sequence when searching for the key. however, we can do something similar called pseudo-random probing. in pseudo-random probing, the ith slot in the probe sequence is (h(k) + ri) mod m where ri is the ith value in a random permutation of the numbers from 1 to m − 1. all insertion and search operations must use the same random permutation. the probe function would be
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
example 9.9 consider a table of size m = 101, with perm[1] = 5, perm[2] = 2, and perm[3] = 32. assume that we have two keys k1 and k2 where h(k1) = 30 and h(k2) = 35. the probe sequence for k1 is 30, then 35, then 32, then 62. the probe sequence for k2 is 35, then 40, then 37, then 67. thus, while k2 will probe to k1’s home position as its second choice, the two keys’ probe sequences diverge immediately thereafter.
for some choice of constants c1, c2, and c3. the simplest variation is p(k, i) = i2 (i.e., c1 = 1, c2 = 0, and c3 = 0. then the ith value in the probe sequence would be (h(k) + i2) mod m. under quadratic probing, two keys with different home positions will have diverging probe sequences.
example 9.10 given a hash table of size m = 101, assume for keys k1 and k2 that h(k1) = 30 and h(k2) = 29. the probe sequence for k1 is 30, then 31, then 34, then 39. the probe sequence for k2 is 29, then 30, then 33, then 38. thus, while k2 will probe to k1’s home position as its second choice, the two keys’ probe sequences diverge immediately thereafter.
unfortunately, quadratic probing has the disadvantage that typically not all hash table slots will be on the probe sequence. using p(k, i) = i2 gives particularly inconsistent results. for many hash table sizes, this probe function will cycle through a relatively small number of slots. if all slots on that cycle happen to be full, then the record cannot be inserted at all! for example, if our hash table has three slots, then records that hash to slot 0 can probe only to slots 0 and 1 (that is, the probe sequence will never visit slot 2 in the table). thus, if slots 0 and 1 are full, then the record cannot be inserted even though the table is not full! a more realistic example is a table with 105 slots. the probe sequence starting from any given slot will only visit 23 other slots in the table. if all 24 of these slots should happen to be full, even if other slots in the table are empty, then the record cannot be inserted because the probe sequence will continually hit only those same 24 slots.
fortunately, it is possible to get good results from quadratic probing at low cost. the right combination of probe function and table size will visit many slots in the table. in particular, if the hash table size is a prime number and the probe function is p(k, i) = i2, then at least half the slots in the table will be visited. thus, if the table is less than half full, we can be certain that a free slot will be
found. alternatively, if the hash table size is a power of two and the probe function is p(k, i) = (i2 + i)/2, then every slot in the table will be visited by the probe function.
both pseudo-random probing and quadratic probing eliminate primary clustering, which is the problem of keys sharing substantial segments of a probe sequence. if two keys hash to the same home position, however, then they will always follow the same probe sequence for every collision resolution method that we have seen so far. the probe sequences generated by pseudo-random and quadratic probing (for example) are entirely a function of the home position, not the original key value. this is because function p ignores its input parameter k for these collision resolution methods. if the hash function generates a cluster at a particular home position, then the cluster remains under pseudo-random and quadratic probing. this problem is called secondary clustering.
to avoid secondary clustering, we need to have the probe sequence make use of the original key value in its decision-making process. a simple technique for doing this is to return to linear probing by a constant step size for the probe function, but to have that constant be determined by a second hash function, h2. thus, the probe sequence would be of the form p(k, i) = i∗ h2(k). this method is called double hashing.
example 9.11 assume a hash table has size m = 101, and that there are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30, h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. then, the probe sequence for k1 will be 30, 32, 34, 36, and so on. the probe sequence for k2 will be 28, 33, 38, 43, and so on. the probe sequence for k3 will be 30, 35, 40, 45, and so on. thus, none of the keys share substantial portions of the same probe sequence. of course, if a fourth key k4 has h(k4) = 28 and h2(k4) = 2, then it will follow the same probe sequence as k1. pseudorandom or quadratic probing can be combined with double hashing to solve this problem.
a good implementation of double hashing should ensure that all of the probe sequence constants are relatively prime to the table size m. this can be achieved easily. one way is to select m to be a prime number, and have h2 return a value in the range 1 ≤ h2(k) ≤ m − 1. another way is to set m = 2m for some value m and have h2 return an odd value between 1 and 2m.
figure 9.7 shows an implementation of the dictionary adt by means of a hash table. the simplest hash function is used, with collision resolution by linear probing, as the basis for the structure of a hash table implementation. a suggested
9.13 assume that you are hashing key k to a hash table of n slots (indexed from 0 to n − 1). for each of the following functions h(k), is the function acceptable as a hash function (i.e., would the hash program work correctly for both insertions and searches), and if so, is it a good hash function? function random(n) returns a random integer between 0 and n − 1, inclusive. (a) h(k) = k/n where k and n are integers. (b) h(k) = 1. (c) h(k) = (k + random(n)) mod n. (d) h(k) = k mod n where n is a prime number.
9.14 assume that you have a seven-slot closed hash table (the slots are numbered 0 through 6). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 7 and linear probing on this list of numbers: 3, 12, 9, 2. after inserting the record with key value 2, list for each empty slot the probability that it will be the next one ﬁlled.
9.15 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and quadratic probing on this list of numbers: 3, 12, 9, 2, 79, 46. after inserting the record with key value 46, list for each empty slot the probability that it will be the next one ﬁlled.
9.16 assume that you have a ten-slot closed hash table (the slots are numbered 0 through 9). show the ﬁnal hash table that would result if you used the hash function h(k) = k mod 10 and pseudo-random probing on this list of numbers: 3, 12, 9, 2, 79, 44. the permutation of offsets to be used by the pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. after inserting the record with key value 44, list for each empty slot the probability that it will be the next one ﬁlled.
9.18 using closed hashing, with double hashing to resolve collisions, insert the following keys into a hash table of thirteen slots (the slots are numbered 0 through 12). the hash functions to be used are h1 and h2, deﬁned below. you should show the hash table after all eight keys have been inserted.
method hashinsert ﬁrst checks to see if the home slot for the key is empty. if the home slot is occupied, then we use the probe function, p(k, i) to locate a free slot in the table. function p has two parameters, the key k and a count i for where in the probe sequence we wish to be. that is, to get the ﬁrst position in the probe sequence after the home slot for key k, we call p(k, 1). for the next slot in the probe sequence, call p(k, 2). note that the probe function returns an offset from the original home position, rather than a slot in the hash table. thus, the for loop in hashinsert is computing positions in the table at each iteration by adding the value returned from the probe function to the home position. the ith call to p returns the ith offset to be used.
searching in a hash table follows the same probe sequence that was followed when inserting records. in this way, a record not in its home position can be recovered. a java implementation for the search procedure is shown in figure 9.5 both the insert and the search routines assume that at least one slot on the probe sequence of every key will be empty. otherwise, they will continue in an inﬁnite loop on unsuccessful searches. thus, the dictionary should keep a count of the
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
depending on the pattern of record accesses, it might be possible to reduce the expected cost of access even in the face of collisions. recall the 80/20 rule: 80% of the accesses will come to 20% of the data. in other words, some records are accessed more frequently. if two records hash to the same home position, which would be better placed in the home position, and which in a slot further down the probe sequence? the answer is that the record with higher frequency of access should be placed in the home position, because this will reduce the total number of record accesses. ideally, records along a probe sequence will be ordered by their frequency of access.
one approach to approximating this goal is to modify the order of records along the probe sequence whenever a record is accessed. if a search is made to a record that is not in its home position, a self-organizing list heuristic can be used. for example, if the linear probing collision resolution policy is used, then whenever a record is located that is not in its home position, it can be swapped with the record preceding it in the probe sequence. that other record will now be further from its home position, but hopefully it will be accessed less frequently. note that this approach will not work for the other collision resolution policies presented in this section, because swapping a pair of records to improve access to one might remove the other from its probe sequence.
another approach is to keep access counts for records and periodically rehash the entire table. the records should be inserted into the hash table in frequency order, ensuring that records that were frequently accessed during the last series of requests have the best chance of being near their home positions.
1. deleting a record must not hinder later searches. in other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. for example, in figure 9.6(a), keys 9877 and 2037 both hash to slot 7. key 2037 is placed in slot 8 by the collision resolution policy. if 9877 is deleted from the table, a search for 2037 must still pass through slot 7 as it probes to slot 8.
both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. the tombstone indicates that a record once
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
figure 5.2 two different binary trees. (a) a binary tree whose root has a nonempty left child. (b) a binary tree whose root has a non-empty right child. (c) the binary tree of (a) with the missing right child made explicit. (d) the binary tree of (b) with the missing left child made explicit.
figure 5.3 illustrates the differences between full and complete binary trees.1 there is no particular relationship between these two tree shapes; that is, the tree of figure 5.3(a) is full but not complete while the tree of figure 5.3(b) is complete but not full. the heap data structure (section 5.5) is an example of a complete binary tree. the huffman coding tree (section 5.6) is an example of a full binary tree.
1while these deﬁnitions for full and complete binary tree are the ones most commonly used, they are not universal. some textbooks even reverse these deﬁnitions! because the common meaning of the words “full” and “complete” are quite similar, there is little that you can do to distinguish between them other than to memorize the deﬁnitions. here is a memory aid that you might ﬁnd useful: “complete” is a wider word than “full,” and complete binary trees tend to be wider than full binary trees because each level of a complete binary tree is as wide as possible.
this section presents a simple, compact implementation for complete binary trees. recall that complete binary trees have all levels except the bottom ﬁlled out completely, and the bottom level has all of its nodes ﬁlled in from left to right. thus, a complete binary tree of n nodes has only one possible shape. you might think that a complete binary tree is such an unusual occurrence that there is no reason to develop a special implementation for it. however, the complete binary tree has practical uses, the most important being the heap data structure discussed in section 5.5. heaps are often used to implement priority queues (section 5.5) and for external sorting algorithms (section 8.5.2).
we begin by assigning numbers to the node positions in the complete binary tree, level by level, from left to right as shown in figure 5.12(a). an array can store the tree’s data values efﬁciently, placing each data value in the array position corresponding to that node’s position within the tree. figure 5.12(b) lists the array indices for the children, parent, and siblings of each node in figure 5.12(a). from figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives within the array. simple formulae can be derived for calculating the array index for each relative of a node r from r’s index. no explicit pointers are necessary to reach a node’s left or right child. this means there is no overhead to the array implementation if the array is selected to be of size n for a tree of n nodes.
the formulae for calculating the array indices of the various relatives of a node are as follows. the total number of nodes in the tree is n. the index of the node in question is r, which must fall in the range 0 to n − 1.
• parent(r) = b(r − 1)/2c if r 6= 0. • left child(r) = 2r + 1 if 2r + 1 < n. • right child(r) = 2r + 2 if 2r + 2 < n. • left sibling(r) = r − 1 if r is even. • right sibling(r) = r + 1 if r is odd and r + 1 < n.
section 4.4 presented the dictionary adt, along with dictionary implementations based on sorted and unsorted lists. when implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. however, searching an unsorted list for a particular record requires Θ(n) time in the average case. for a large database, this is probably much too slow. alternatively, the records can be stored in a sorted list. if the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. on the other hand, if we use a sorted
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
removing the maximum (root) value from a heap containing n elements requires that we maintain the complete binary tree shape, and that the remaining n − 1 node values conform to the heap property. we can maintain the proper shape by moving the element in the last position in the heap (the current last element in the array) to the root position. we now consider the heap to be one element smaller. unfortunately, the new root value is probably not the maximum value in the new heap. this problem is easily solved by using siftdown to reorder the heap. because the heap is log n levels deep, the cost of deleting the maximum element is Θ(log n) in the average and worst cases.
the heap is a natural implementation for the priority queues discussed at the beginning of this section. jobs can be added to the heap (using their priority value as the ordering key) when needed. method removemax can be called whenever a new job is to be executed.
some applications of priority queues require the ability to change the priority of an object already stored in the queue. this might require that the object’s position in the heap representation be updated. unfortunately, a max-heap is not efﬁcient when searching for an arbitrary value; it is only good for ﬁnding the maximum value. however, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. the remove method takes as input the position of the node to be removed from the heap. a typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efﬁcient search for objects (such as a bst). records in the auxiliary data structure will store the object’s heap index, so that the object can be deleted from the heap and reinserted with its new priority (see project 5.5). sections 11.4.1 and 11.5.1 present applications for a priority queue with priority updating.
the space/time tradeoff principle from section 3.9 suggests that one can often gain an improvement in space requirements in exchange for a penalty in running time. there are many situations where this is a desirable tradeoff. a typical example is storing ﬁles on disk. if the ﬁles are not actively used, the owner might wish to compress them to save space. later, they can be uncompressed for use, which costs some time, but only once.
we often represent a set of items in a computer program by assigning a unique code to each item. for example, the standard ascii coding scheme assigns a unique eight-bit value to each character. it takes a certain minimum number of bits to provide unique codes for each character. for example, it takes dlog 128e or
techniques in common use today. the next section presents one such approach to assigning variable-length codes, called huffman coding. while it is not commonly used in its simplest form for ﬁle compression (there are better methods), huffman coding gives the ﬂavor of such coding schemes.
huffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character. thus, it is a variable-length code. if the estimated frequencies for letters match the actual frequency found in an encoded message, then the length of that message will typically be less than if a ﬁxed-length code had been used. the huffman code for each letter is derived from a full binary tree called the huffman coding tree, or simply the huffman tree. each leaf of the huffman tree corresponds to a letter, and we deﬁne the weight of the leaf node to be the weight (frequency) of its associated letter. the goal is to build a tree with the minimum external path weight. deﬁne the weighted path length of a leaf to be its weight times its depth. the binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. a letter with high weight should have low depth, so that it will count the least against the total path length. as a result, another letter might be pushed deeper in the tree if it has less weight.
the process of building the huffman tree for n letters is quite simple. first, create a collection of n initial huffman trees, each of which is a single leaf node containing one of the letters. put the n partial trees onto a min-heap (a priority queue) organized by weight (frequency). next, remove the ﬁrst two trees (the ones with lowest weight) from the heap. join these two trees together to create a new tree whose root has the two trees as children, and whose weight is the sum of the weights of the two trees. put this new tree back on the heap. this process is repeated until all of the partial huffman trees have been combined into one.
example 5.8 figure 5.25 illustrates part of the huffman tree construction process for the eight letters of figure 5.24. ranking d and l arbitrarily by alphabetical order, the letters are ordered by frequency as
5.14 why is the bst property deﬁned so that nodes with values equal to the value of the root appear only in the right subtree, rather than allow equal-valued nodes to appear in either subtree? (a) show the bst that results from inserting the values 15, 20, 25, 18, 16,
5.19 write a recursive function named smallcount that, given the pointer to the root of a bst and a key k, returns the number of nodes having key values less than or equal to k. function smallcount should visit as few nodes in the bst as possible.
5.20 write a recursive function named printrange that, given the pointer to the root to a bst, a low key value, and a high key value, prints in sorted order all records whose key values fall between the two given keys. function printrange should visit as few nodes in the bst as possible.
5.21 describe a simple modiﬁcation to the bst that will allow it to easily support ﬁnding the kth smallest value in Θ(log n) average case time. then write a pseudo-code function for ﬁnding the kth smallest value in your modiﬁed bst.
5.26 revise the heap deﬁnition of figure 5.19 to implement a min-heap. the member function removemax should be replaced by a new function called removemin.
5.28 what will the huffman coding tree look like for a set of sixteen characters all with equal weight? what is the average code length for a letter in this case? how does this differ from the smallest possible ﬁxed length code for sixteen characters?
of the characters is assigned code 001, then, (a) describe all codes that cannot have been assigned. (b) describe all codes that must have been assigned.
(a) for this alphabet, what is the worst-case number of bits required by the huffman code for a string of n letters? what string(s) have the worstcase performance?
(b) for this alphabet, what is the best-case number of bits required by the huffman code for a string of n letters? what string(s) have the bestcase performance?
(1) a linked-list maintained in sorted order. (2) a linked-list of unsorted records. (3) a binary search tree. (4) an array-based list maintained in sorted order. (5) an array-based list of unsorted records.
the bst should be organized by city name. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. collect running-time statistics for each operation. which operations can be implemented reasonably efﬁciently (i.e., in Θ(log n) time in the average case) using a bst? can the database system be made more efﬁcient by using one or more additional bsts to organize the records by location?
5.4 create a binary tree adt that includes generic traversal methods that take a visitor, as described in section 5.2. write functions count and bstcheck of section 5.2 as visitors to be used with the generic traversal method.
5.5 implement a priority queue class based on the max-heap class implementation of figure 5.19. the following methods should be supported for manipulating the priority queue: void enqueue(int objectid, int priority); int dequeue(); void changeweight(int objectid, int newpriority); method enqueue inserts a new object into the priority queue with id number objectid and priority priority. method dequeue removes the object with highest priority from the priority queue and returns its object id. method changeweight changes the priority of the object with id number objectid to be newpriority. the type for elem should be a class that stores the object id and the priority for that object. you will need a mechanism for ﬁnding the position of the desired object within the heap. use an array, storing the object with objectid i in position i. (be sure in your testing to keep the objectids within the array bounds.) you must also modify the heap implementation to store the object’s position in the auxiliary array so that updates to objects in the heap can be updated as well in the array.
5.6 the huffman coding tree function buildhuff of figure 5.29 manipulates a sorted list. this could result in a Θ(n2) algorithm, because placing an intermediate huffman tree on the list could take Θ(n) time. revise this algorithm to use a priority queue based on a min-heap instead of a list.
5.7 complete the implementation of the huffman coding tree, building on the code presented in section 5.6. include a function to compute and store in a table the codes for each letter, and functions to encode and decode messages. this project can be further extended to support ﬁle compression. to do so requires adding two steps: (1) read through the input ﬁle to generate actual
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
over the time required to ﬁnd the k largest elements using one of the other sorting methods described earlier. one situation where we are able to take advantage of this concept is in the implementation of kruskal’s minimum-cost spanning tree (mst) algorithm of section 11.5.2. that algorithm requires that edges be visited in ascending order (so, use a min-heap), but this process stops as soon as the mst is complete. thus, only a relatively small fraction of the edges need be sorted.
imagine that for the past year, as you paid your various bills, you then simply piled all the paperwork onto the top of a table somewhere. now the year has ended and its time to sort all of these papers by what the bill was for (phone, electricity, rent, etc.) and date. a pretty natural approach is to make some space on the ﬂoor, and as you go through the pile of papers, put the phone bills into one pile, the electric bills into another pile, and so on. once this initial assignment of bills to piles is done (in one pass), you can sort each pile by date relatively quickly because they are each fairly small. this is the basic idea behind a binsort. numbers 0 through n − 1:
here the key value is used to determine the position for a record in the ﬁnal sorted array. this is the most basic example of a binsort, where key values are used to assign records to bins. this algorithm is extremely efﬁcient, taking Θ(n) time regardless of the initial ordering of the keys. this is far better than the performance of any sorting algorithm that we have seen so far. the only problem is that this algorithm has limited use because it works only for a permutation of the numbers from 0 to n − 1.
we can extend this simple binsort algorithm to be more useful. because binsort must perform direct computation on the key value (as opposed to just asking which of two records comes ﬁrst as our previous sorting algorithms did), we will assume that the records use an integer key type.
the simplest extension is to allow for duplicate values among the keys. this can be done by turning array slots into arbitrary-length bins by turning b into an array of linked lists. in this way, all records with key value i can be placed in bin b[i]. a second extension allows for a key range greater than n. for example, a set of n records might have keys in the range 1 to 2n. the only requirement is
int v = minvertex(g, d); g.setmark(v, visited); if (d[v] == integer.max value) return; // unreachable for (int w = g.first(v); w < g.n(); w = g.next(v, w))
because this scan is done |v| times, and because each edge requires a constanttime update to d, the total cost for this approach is Θ(|v|2 + |e|) = Θ(|v|2), because |e| is in o(|v|2). the second method is to store unprocessed vertices in a min-heap ordered by distance values. the next-closest vertex can be found in the heap in Θ(log |v|) time. every time we modify d(x), we could reorder x in the heap by deleting and reinserting it. this is an example of a priority queue with priority update, as described in section 5.5. to implement true priority updating, we would need to store with each vertex its array index within the heap. a simpler approach is to add the new (smaller) distance value for a given vertex as a new record in the heap. the smallest value for a given vertex currently in the heap will be found ﬁrst, and greater distance values found later will be ignored because the vertex will already be marked as visited. the only disadvantage to repeatedly inserting distance values is that it will raise the number of elements in the heap from Θ(|v|) to Θ(|e|) in the worst case. the time complexity is Θ((|v| + |e|) log |e|), because for each edge we must reorder the heap. because the objects stored on the heap need to know both their vertex number and their distance, we create a simple class for the purpose called dijkelem, as follows. dijkelem is quite similar to the edge class used by the adjacency list representation.
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
method heapsize returns the current size of the heap. h.isleaf(pos) will return true if position pos is a leaf in heap h, and false otherwise. members leftchild, rightchild, and parent return the position (actually, the array index) for the left child, right child, and parent of the position passed, respectively.
one way to build a heap is to insert the elements one at a time. method insert will insert a new element v into the heap. you might expect the heap insertion process to be similar to the insert function for a bst, starting at the root and working down through the heap. however, this approach is not likely to work because the heap must maintain the shape of a complete binary tree. equivalently, if the heap takes up the ﬁrst n positions of its array prior to the call to insert, it must take up the ﬁrst n + 1 positions after. to accomplish this, insert ﬁrst places v at position n of the array. of course, v is unlikely to be in the correct position. to move v to the right place, it is compared to its parent’s value. if the value of v is less than or equal to the value of its parent, then it is in the correct place and the insert routine is ﬁnished. if the value of v is greater than that of its parent, then the two elements swap positions. from here, the process of comparing v to its (current) parent continues until v reaches its correct position.
each call to insert takes Θ(log n) time in the worst case, because the value being inserted can move at most the distance from the bottom of the tree to the top of the tree. thus, the time to insert n values into the heap, if we insert them one at a time, will be Θ(n log n) in the worst case.
if all n values are available at the beginning of the building process, we can build the heap faster than just inserting the values into the heap one by one. consider figure 5.20(a), which shows one series of exchanges that will result in a heap. note that this ﬁgure shows the input in its logical form as a complete binary tree, but you should realize that these values are physically stored in an array. all exchanges are between a node and one of its children. the heap is formed as a result of this exchange process. the array for the right-hand tree of figure 5.20(a) would appear as follows:
figure 5.20(b) shows an alternate series of exchanges that also forms a heap, but much more efﬁciently. from this example, it is clear that the heap for any given set of numbers is not unique, and we see that some rearrangements of the input values require fewer exchanges than others to build the heap. so, how do we pick the best rearrangement?
figure 5.21 final stage in the heap-building algorithm. both subtrees of node r are heaps. all that remains is to push r down to its proper level in the heap.
figure 5.22 the siftdown operation. the subtrees of the root are assumed to be heaps. (b) values 1 and 7 are swapped. (c) values 1 and 6 are swapped to form the ﬁnal heap.
array, with the ﬁrst internal node. the exchanges shown in figure 5.20(b) result from this process. method buildheap implements the building algorithm.
what is the cost of buildheap? clearly it is the sum of the costs for the calls to siftdown. each siftdown operation can cost at most the number of levels it takes for the node being sifted to reach the bottom of the tree. in any complete tree, approximately half of the nodes are leaves and so cannot be moved downward at all. one quarter of the nodes are one level above the leaves, and so their elements can move down at most one level. at each step up the tree we get half the number of nodes as were at the previous level, and an additional height of one. the maximum sum of total distances that elements can go is therefore i − 1 2i−1 .
from equation 2.9 we know that this summation has a closed-form solution of approximately 2, so this algorithm takes Θ(n) time in the worst case. this is far better than building the heap one element at a time, which would cost Θ(n log n) in the worst case. it is also faster than the Θ(n log n) average-case time and Θ(n2) worst-case time required to build the bst.
when all inserts and releases follow a simple pattern, such as last requested, ﬁrst released (stack order), or ﬁrst requested, ﬁrst released (queue order), memory management is fairly easy. we are concerned in this section with the general case where blocks of any size might be requested and released in any order. this is known as dynamic storage allocation. one example of dynamic storage allocation is managing free store for a compiler’s runtime environment, such as the systemlevel new operations in java. another example is managing main memory in a multitasking operating system. here, a program might require a certain amount of space, and the memory manager must keep track of which programs are using which parts of the main memory. yet another example is the ﬁle manager for a disk drive. when a disk ﬁle is created, expanded, or deleted, the ﬁle manager must allocate or deallocate disk space.
a block of memory or disk space managed in this way is sometimes referred to as a heap. the term “heap” is being used here in a different way than the heap data structure discussed in section 5.5. here “heap” refers to the memory controlled by a dynamic memory management scheme.
in the rest of this section, we ﬁrst study techniques for dynamic memory management. we then tackle the issue of what to do when no single block of memory in the memory pool is large enough to honor a given request.
12.3.1 dynamic storage allocation for the purpose of dynamic storage allocation, we view memory as a single array broken into a series of variable-size blocks, where some of the blocks are free and some are reserved or already allocated. the free blocks are linked together to form
method heapsize returns the current size of the heap. h.isleaf(pos) will return true if position pos is a leaf in heap h, and false otherwise. members leftchild, rightchild, and parent return the position (actually, the array index) for the left child, right child, and parent of the position passed, respectively.
one way to build a heap is to insert the elements one at a time. method insert will insert a new element v into the heap. you might expect the heap insertion process to be similar to the insert function for a bst, starting at the root and working down through the heap. however, this approach is not likely to work because the heap must maintain the shape of a complete binary tree. equivalently, if the heap takes up the ﬁrst n positions of its array prior to the call to insert, it must take up the ﬁrst n + 1 positions after. to accomplish this, insert ﬁrst places v at position n of the array. of course, v is unlikely to be in the correct position. to move v to the right place, it is compared to its parent’s value. if the value of v is less than or equal to the value of its parent, then it is in the correct place and the insert routine is ﬁnished. if the value of v is greater than that of its parent, then the two elements swap positions. from here, the process of comparing v to its (current) parent continues until v reaches its correct position.
each call to insert takes Θ(log n) time in the worst case, because the value being inserted can move at most the distance from the bottom of the tree to the top of the tree. thus, the time to insert n values into the heap, if we insert them one at a time, will be Θ(n log n) in the worst case.
if all n values are available at the beginning of the building process, we can build the heap faster than just inserting the values into the heap one by one. consider figure 5.20(a), which shows one series of exchanges that will result in a heap. note that this ﬁgure shows the input in its logical form as a complete binary tree, but you should realize that these values are physically stored in an array. all exchanges are between a node and one of its children. the heap is formed as a result of this exchange process. the array for the right-hand tree of figure 5.20(a) would appear as follows:
figure 5.20(b) shows an alternate series of exchanges that also forms a heap, but much more efﬁciently. from this example, it is clear that the heap for any given set of numbers is not unique, and we see that some rearrangements of the input values require fewer exchanges than others to build the heap. so, how do we pick the best rearrangement?
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
removing the maximum (root) value from a heap containing n elements requires that we maintain the complete binary tree shape, and that the remaining n − 1 node values conform to the heap property. we can maintain the proper shape by moving the element in the last position in the heap (the current last element in the array) to the root position. we now consider the heap to be one element smaller. unfortunately, the new root value is probably not the maximum value in the new heap. this problem is easily solved by using siftdown to reorder the heap. because the heap is log n levels deep, the cost of deleting the maximum element is Θ(log n) in the average and worst cases.
the heap is a natural implementation for the priority queues discussed at the beginning of this section. jobs can be added to the heap (using their priority value as the ordering key) when needed. method removemax can be called whenever a new job is to be executed.
some applications of priority queues require the ability to change the priority of an object already stored in the queue. this might require that the object’s position in the heap representation be updated. unfortunately, a max-heap is not efﬁcient when searching for an arbitrary value; it is only good for ﬁnding the maximum value. however, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. the remove method takes as input the position of the node to be removed from the heap. a typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efﬁcient search for objects (such as a bst). records in the auxiliary data structure will store the object’s heap index, so that the object can be deleted from the heap and reinserted with its new priority (see project 5.5). sections 11.4.1 and 11.5.1 present applications for a priority queue with priority updating.
the space/time tradeoff principle from section 3.9 suggests that one can often gain an improvement in space requirements in exchange for a penalty in running time. there are many situations where this is a desirable tradeoff. a typical example is storing ﬁles on disk. if the ﬁles are not actively used, the owner might wish to compress them to save space. later, they can be uncompressed for use, which costs some time, but only once.
we often represent a set of items in a computer program by assigning a unique code to each item. for example, the standard ascii coding scheme assigns a unique eight-bit value to each character. it takes a certain minimum number of bits to provide unique codes for each character. for example, it takes dlog 128e or
figure 5.20 two series of exchanges to build a max-heap. (a) this heap is built by a series of nine exchanges in the order (4-2), (4-1), (2-1), (5-2), (5-4), (6-3), (6-5), (7-5), (7-6). (b) this heap is built by a series of four exchanges in the order (5-2), (7-3), (7-1), (6-1).
one good algorithm stems from induction. suppose that the left and right subtrees of the root are already heaps, and r is the name of the element at the root. this situation is illustrated by figure 5.21. in this case there are two possibilities. (1) r has a value greater than or equal to its two children. in this case, construction is complete. (2) r has a value less than one or both of its children. in this case, r should be exchanged with the child that has greater value. the result will be a heap, except that r might still be less than one or both of its (new) children. in this case, we simply continue the process of “pushing down” r until it reaches a level where it is greater than its children, or is a leaf node. this process is implemented by the private method siftdown of the heap class. the siftdown operation is illustrated by figure 5.22.
this approach assumes that the subtrees are already heaps, suggesting that a complete algorithm can be obtained by visiting the nodes in some order such that the children of a node are visited before the node itself. one simple way to do this is simply to work from the high index of the array to the low index. actually, the build process need not visit the leaf nodes (they can never move down because they are already at the bottom), so the building algorithm can start in the middle of the
removing the maximum (root) value from a heap containing n elements requires that we maintain the complete binary tree shape, and that the remaining n − 1 node values conform to the heap property. we can maintain the proper shape by moving the element in the last position in the heap (the current last element in the array) to the root position. we now consider the heap to be one element smaller. unfortunately, the new root value is probably not the maximum value in the new heap. this problem is easily solved by using siftdown to reorder the heap. because the heap is log n levels deep, the cost of deleting the maximum element is Θ(log n) in the average and worst cases.
the heap is a natural implementation for the priority queues discussed at the beginning of this section. jobs can be added to the heap (using their priority value as the ordering key) when needed. method removemax can be called whenever a new job is to be executed.
some applications of priority queues require the ability to change the priority of an object already stored in the queue. this might require that the object’s position in the heap representation be updated. unfortunately, a max-heap is not efﬁcient when searching for an arbitrary value; it is only good for ﬁnding the maximum value. however, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. the remove method takes as input the position of the node to be removed from the heap. a typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efﬁcient search for objects (such as a bst). records in the auxiliary data structure will store the object’s heap index, so that the object can be deleted from the heap and reinserted with its new priority (see project 5.5). sections 11.4.1 and 11.5.1 present applications for a priority queue with priority updating.
the space/time tradeoff principle from section 3.9 suggests that one can often gain an improvement in space requirements in exchange for a penalty in running time. there are many situations where this is a desirable tradeoff. a typical example is storing ﬁles on disk. if the ﬁles are not actively used, the owner might wish to compress them to save space. later, they can be uncompressed for use, which costs some time, but only once.
we often represent a set of items in a computer program by assigning a unique code to each item. for example, the standard ascii coding scheme assigns a unique eight-bit value to each character. it takes a certain minimum number of bits to provide unique codes for each character. for example, it takes dlog 128e or
7.7 it has been proposed that heapsort can be optimized by altering the heap’s siftdown function. call the value being sifted down x. siftdown does two comparisons per level: first the children of x are compared, then the winner is compared to x. if x is too small, it is swapped with its larger child and the process repeated. the proposed optimization dispenses with the test against x. instead, the larger child automatically replaces x, until x reaches the bottom level of the heap. at this point, x might be too large to remain in that position. this is corrected by repeatedly comparing x with its parent and swapping as necessary to “bubble” it up to its proper level. the claim is that this process will save a number of comparisons because most nodes when sifted down end up near the bottom of the tree anyway. implement both versions of siftdown, and do an empirical study to compare their running times.
7.7 it has been proposed that heapsort can be optimized by altering the heap’s siftdown function. call the value being sifted down x. siftdown does two comparisons per level: first the children of x are compared, then the winner is compared to x. if x is too small, it is swapped with its larger child and the process repeated. the proposed optimization dispenses with the test against x. instead, the larger child automatically replaces x, until x reaches the bottom level of the heap. at this point, x might be too large to remain in that position. this is corrected by repeatedly comparing x with its parent and swapping as necessary to “bubble” it up to its proper level. the claim is that this process will save a number of comparisons because most nodes when sifted down end up near the bottom of the tree anyway. implement both versions of siftdown, and do an empirical study to compare their running times.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
over the time required to ﬁnd the k largest elements using one of the other sorting methods described earlier. one situation where we are able to take advantage of this concept is in the implementation of kruskal’s minimum-cost spanning tree (mst) algorithm of section 11.5.2. that algorithm requires that edges be visited in ascending order (so, use a min-heap), but this process stops as soon as the mst is complete. thus, only a relatively small fraction of the edges need be sorted.
imagine that for the past year, as you paid your various bills, you then simply piled all the paperwork onto the top of a table somewhere. now the year has ended and its time to sort all of these papers by what the bill was for (phone, electricity, rent, etc.) and date. a pretty natural approach is to make some space on the ﬂoor, and as you go through the pile of papers, put the phone bills into one pile, the electric bills into another pile, and so on. once this initial assignment of bills to piles is done (in one pass), you can sort each pile by date relatively quickly because they are each fairly small. this is the basic idea behind a binsort. numbers 0 through n − 1:
here the key value is used to determine the position for a record in the ﬁnal sorted array. this is the most basic example of a binsort, where key values are used to assign records to bins. this algorithm is extremely efﬁcient, taking Θ(n) time regardless of the initial ordering of the keys. this is far better than the performance of any sorting algorithm that we have seen so far. the only problem is that this algorithm has limited use because it works only for a permutation of the numbers from 0 to n − 1.
we can extend this simple binsort algorithm to be more useful. because binsort must perform direct computation on the key value (as opposed to just asking which of two records comes ﬁrst as our previous sorting algorithms did), we will assume that the records use an integer key type.
the simplest extension is to allow for duplicate values among the keys. this can be done by turning array slots into arbitrary-length bins by turning b into an array of linked lists. in this way, all records with key value i can be placed in bin b[i]. a second extension allows for a key range greater than n. for example, a set of n records might have keys in the range 1 to 2n. the only requirement is
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
figure 5.2 two different binary trees. (a) a binary tree whose root has a nonempty left child. (b) a binary tree whose root has a non-empty right child. (c) the binary tree of (a) with the missing right child made explicit. (d) the binary tree of (b) with the missing left child made explicit.
figure 5.3 illustrates the differences between full and complete binary trees.1 there is no particular relationship between these two tree shapes; that is, the tree of figure 5.3(a) is full but not complete while the tree of figure 5.3(b) is complete but not full. the heap data structure (section 5.5) is an example of a complete binary tree. the huffman coding tree (section 5.6) is an example of a full binary tree.
1while these deﬁnitions for full and complete binary tree are the ones most commonly used, they are not universal. some textbooks even reverse these deﬁnitions! because the common meaning of the words “full” and “complete” are quite similar, there is little that you can do to distinguish between them other than to memorize the deﬁnitions. here is a memory aid that you might ﬁnd useful: “complete” is a wider word than “full,” and complete binary trees tend to be wider than full binary trees because each level of a complete binary tree is as wide as possible.
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
5.26 revise the heap deﬁnition of figure 5.19 to implement a min-heap. the member function removemax should be replaced by a new function called removemin.
5.28 what will the huffman coding tree look like for a set of sixteen characters all with equal weight? what is the average code length for a letter in this case? how does this differ from the smallest possible ﬁxed length code for sixteen characters?
of the characters is assigned code 001, then, (a) describe all codes that cannot have been assigned. (b) describe all codes that must have been assigned.
(a) for this alphabet, what is the worst-case number of bits required by the huffman code for a string of n letters? what string(s) have the worstcase performance?
(b) for this alphabet, what is the best-case number of bits required by the huffman code for a string of n letters? what string(s) have the bestcase performance?
(1) a linked-list maintained in sorted order. (2) a linked-list of unsorted records. (3) a binary search tree. (4) an array-based list maintained in sorted order. (5) an array-based list of unsorted records.
frequencies for all letters in the ﬁle; and (2) store a representation for the huffman tree at the beginning of the encoded output ﬁle to be used by the decoding function. if you have trouble with devising such a representation, see section 6.5.
(a) write a function to decode the sequential representation for binary trees illustrated by example 6.5. the input should be the sequential representation and the output should be a pointer to the root of the resulting binary tree.
(b) write a function to decode the sequential representation for full binary trees illustrated by example 6.6. the input should be the sequential representation and the output should be a pointer to the root of the resulting binary tree.
(c) write a function to decode the sequential representation for general trees illustrated by example 6.8. the input should be the sequential representation and the output should be a pointer to the root of the resulting general tree.
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
a reverse process from that used to generate the codes. decoding a bit string begins at the root of the tree. we take branches depending on the bit value — left for ‘0’ and right for ‘1’ — until reaching a leaf node. this leaf contains the ﬁrst character in the message. we then process the next bit in the code restarting at the root to begin the next character.
example 5.10 to decode the bit string “1011001110111101” we begin at the root of the tree and take a right branch for the ﬁrst bit which is ‘1.’ because the next bit is a ‘0’ we take a left branch. we then take another right branch (for the third bit ‘1’), arriving at the leaf node corresponding to the letter d. thus, the ﬁrst letter of the coded word is d. we then begin again at the root of the tree to process the fourth bit, which is a ‘1.’ taking a right branch, then two left branches (for the next two bits which are ‘0’), we reach the leaf node corresponding to the letter u. thus, the second letter is u. in similar manner we complete the decoding process to ﬁnd that the last two letters are c and k, spelling the word “duck.”
a set of codes is said to meet the preﬁx property if no code in the set is the preﬁx of another. the preﬁx property guarantees that there will be no ambiguity in how a bit string is decoded. in other words, once we reach the last bit of a code during the decoding process, we know which letter it is the code for. huffman codes certainly have the preﬁx property because any preﬁx for a code would correspond to an internal node, while all codes correspond to leaf nodes. for example, the code for m is ‘11111.’ taking ﬁve right branches in the huffman tree of figure 5.26 brings us to the leaf node containing m. we can be sure that no letter can have code ‘111’ because this corresponds to an internal node of the tree, and the tree-building process places letters only at the leaf nodes.
how efﬁcient is huffman coding? in theory, it is an optimal coding method whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message. in practice, the frequencies of letters do change depending on context. for example, while e is the most commonly used letter of the alphabet in english documents, t is more common as the ﬁrst letter of a word. this is why most commercial compression utilities do not use huffman coding as their primary coding method, but instead use techniques that take advantage of the context for the letters.
another factor that affects the compression efﬁciency of huffman coding is the relative frequencies of the letters. some frequency patterns will save no space as compared to ﬁxed-length codes; others can result in great compression. in general,
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
example 1.8 consider the design for a relatively simple database system stored on disk. typically, records on disk in such a program are accessed through a buffer pool (see section 8.3) rather than directly. variable length records might use a memory manager (see section 12.3) to ﬁnd an appropriate location within the disk ﬁle to place the record. multiple index structures (see chapter 10) will typically be used to access records in various ways. thus, we have a chain of classes, each with its own responsibilities and access privileges. a database query from a user is implemented by searching an index structure. this index requests access to the record by means of a request to the buffer pool. if a record is being inserted or deleted, such a request goes through the memory manager, which in turn interacts with the buffer pool to gain access to the disk ﬁle. a program such as this is far too complex for nearly any human programmer to keep all of the details in his or her head at once. the only way to design and implement such a program is through proper use of abstraction and metaphors. in object-oriented programming, such abstraction is handled using classes.
data types have both a logical and a physical form. the deﬁnition of the data type in terms of an adt is its logical form. the implementation of the data type as a data structure is its physical form. figure 1.1 illustrates this relationship between logical and physical forms for data types. when you implement an adt, you are dealing with the physical form of the associated data type. when you use an adt elsewhere in your program, you are concerned with the associated data type’s logical form. some sections of this book focus on physical implementations for a given data structure. other sections use the logical adt for the data type in the context of a higher-level task.
example 1.9 a particular java environment might provide a library that includes a list class. the logical form of the list is deﬁned by the public functions, their inputs, and their outputs that deﬁne the class. this might be all that you know about the list class implementation, and this should be all you need to know. within the class, a variety of physical implementations for lists is possible. several are described in section 4.1.
at a higher level of abstraction than adts are abstractions for describing the design of programs — that is, the interactions of objects and classes. experienced software
7.7 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. we can make any algorithm stable if we alter the input keys so that (potentially) duplicate key values are made unique in a way that the ﬁrst occurance of the original duplicate value is less than the second occurance, which in turn is less than the third, and so on. in the worst case, it is possible that all n input records have the same key value. give an algorithm to modify the key values such that every modiﬁed key value is unique, the resulting key values give the same sort order as the original keys, the result is stable (in that the duplicate original key values remain in their original order), and the process of altering the keys is done in linear time using only a constant amount of additional space.
recursion to reduce the number of function calls made. (a) how deep can the stack get in the worst case? (b) quicksort makes two recursive calls. the algorithm could be changed to make these two calls in a speciﬁc order. in what order should the two calls be made, and how does this affect how deep the stack can become?
7.10 assume l is an array, length(l) returns the number of records in the array, and qsort(l, i, j) sorts the records of l from i to j (leaving the records sorted in l) using the quicksort algorithm. what is the averagecase time complexity for each of the following code fragments? (a) for (i=0; i<l.length; i++)
7.11 modify quicksort to ﬁnd the smallest k values in an array of records. your output should be the array modiﬁed so that the k smallest values are sorted in the ﬁrst k positions of the array. your algorithm should do the minimum amount of work necessary.
7.12 modify quicksort to sort a sequence of variable-length strings stored one after the other in a character array, with a second array (storing pointers to strings) used to index the strings. your function should modify the index array so that the ﬁrst pointer points to the beginning of the lowest valued string, and so on.
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
10.3 implement the dictionary adt of section 4.4 for a large ﬁle stored on disk by means of the b+-tree of section 10.5. assume that disk blocks are 1024 bytes, and thus both leaf nodes and internal nodes are also 1024 bytes. records should store a 4-byte (int) key value and a 60-byte data ﬁeld. internal nodes should store key value/pointer pairs where the “pointer” is actually the block number on disk for the child node. both internal nodes and leaf nodes will need room to store various information such as a count of the records stored on that node, and a pointer to the next node on that level. thus, leaf nodes will store 15 records, and internal nodes will have room to store about 120 to 125 children depending on how you implement them. use a buffer pool (section 8.3) to manage access to the nodes stored on disk.
of a single bank of read/write heads that move together over a stack of platters. if the sorting process involves reading from an input ﬁle, alternated with writing to an output ﬁle, then the i/o head will continuously seek between the input ﬁle and the output ﬁle. similarly, if two input ﬁles are being processed simultaneously (such as during a merge process), then the i/o head will continuously seek between these two ﬁles.
the moral is that, with a single disk drive, there often is no such thing as efﬁcient sequential processing of a data ﬁle. thus, a sorting algorithm might be more efﬁcient if it performs a smaller number of non-sequential disk operations rather than a larger number of logically sequential disk operations that require a large number of seeks in practice.
as mentioned previously, the record size might be quite large compared to the size of the key. for example, payroll entries for a large business might each store hundreds of bytes of information including the name, id, address, and job title for each employee. the sort key might be the id number, requiring only a few bytes. the simplest sorting algorithm might be to process such records as a whole, reading the entire record whenever it is processed. however, this will greatly increase the amount of i/o required, because only a relatively few records will ﬁt into a single disk block. another alternative is to do a key sort. under this method, the keys are all read and stored together in an index ﬁle, where each key is stored along with a pointer indicating the position of the corresponding record in the original data ﬁle. the key and pointer combination should be substantially smaller than the size of the original record; thus, the index ﬁle will be much smaller than the complete data ﬁle. the index ﬁle will then be sorted, requiring much less i/o because the index records are smaller than the complete records.
once the index ﬁle is sorted, it is possible to reorder the records in the original database ﬁle. this is typically not done for two reasons. first, reading the records in sorted order from the record ﬁle requires a random access for each record. this can take a substantial amount of time and is only of value if the complete collection of records needs to be viewed or processed in sorted order (as opposed to a search for selected records). second, database systems typically allow searches to be done on multiple keys. for example, today’s processing might be done in order of id numbers. tomorrow, the boss might want information sorted by salary. thus, there might be no single “sorted” order for the full record. instead, multiple index ﬁles are often maintained, one for each sort key. these ideas are explored further in chapter 10.
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
whose records have the indicated secondary key value. figure 10.3 illustrates this approach. now there is no duplication of secondary key values, possibly yielding a considerable space savings. the cost of insertion and deletion is reduced, because only one row of the table need be adjusted. note that a new row is added to the array when a new secondary key value is added. this might lead to moving many records, but this will happen infrequently in applications suited to using this arrangement. a drawback to this approach is that the array must be of ﬁxed size, which imposes an upper limit on the number of primary keys that might be associated with a particular secondary key. furthermore, those secondary keys with fewer records than the width of the array will waste the remainder of their row. a better approach is to have a one-dimensional array of secondary key values, where each secondary key is associated with a linked list. this works well if the index is stored in main memory, but not so well when it is stored on disk because the linked list for a given key might be scattered across several disk blocks.
consider a large database of employee records. if the primary key is the employee’s id number and the secondary key is the employee’s name, then each record in the name index associates a name with one or more id numbers. the id number index in turn associates an id number with a unique pointer to the full record on disk. the secondary key index in such an organization is also known as an inverted list or inverted ﬁle. it is inverted in that searches work backwards from the secondary key to the primary key to the actual data record. it is called a list because each secondary key value has (conceptually) a list of primary keys associated with it. figure 10.4 illustrates this arrangement. here, we have last names as the secondary key. the primary key is a four-character unique identiﬁer.
figure 10.5 shows a better approach to storing inverted lists. an array of secondary key values is shown as before. associated with each secondary key is a pointer to an array of primary keys. the primary key array uses a linked-list implementation. this approach combines the storage for all of the secondary key lists into a single array, probably saving space. each record in this array consists of a primary key value and a pointer to the next element on the list. it is easy to insert and delete secondary keys from this array, making this a good implementation for disk-based inverted ﬁles.
how do we handle large databases that require frequent update? the main problem with the linear index is that it is a single, large array that does not lend itself to updates because a single update can require changing the position of every key
10.3 modify the function binary of section 3.5 so as to support variable-length records with ﬁxed-length keys indexed by a simple linear index as illustrated by figure 10.1.
10.4 assume that a database stores records consisting of a 2-byte integer key and a variable-length data ﬁeld consisting of a string. show the linear index (as illustrated by figure 10.1) for the following collection of records:
10.5 each of the following series of records consists of a four-digit primary key (with no duplicates) and a four-character secondary key (with many duplicates).
10.9 you are given a series of records whose keys are letters. the records arrive in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the 2-3 tree that results from inserting these records.
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
figure 10.1 linear indexing for variable-length records. each record in the index ﬁle is of ﬁxed length and contains a pointer to the beginning of the corresponding record in the database ﬁle.
begins with a discussion of the variant normally referred to simply as a “b-tree.” section 10.5.1 presents the most widely implemented variant, the b+-tree.
a linear index is an index ﬁle organized as a sequence of key/pointer pairs where the keys are in sorted order and the pointers either (1) point to the position of the complete record on disk, (2) point to the position of the primary key in the primary index, or (3) are actually the value of the primary key. depending on its size, a linear index might be stored in main memory or on disk. a linear index provides a number of advantages. it provides convenient access to variable-length database records, because each entry in the index ﬁle contains a ﬁxed-length key ﬁeld and a ﬁxed-length pointer to the beginning of a (variable-length) record as shown in figure 10.1. a linear index also allows for efﬁcient search and random access to database records, becase it is amenable to binary search.
if the database contains enough records, the linear index might be too large to store in main memory. this makes binary search of the index more expensive because many disk accesses would typically be required by the search process. one solution to this problem is to store a second-level linear index in main memory that indicates which disk block in the index ﬁle stores a desired key. for example, the linear index on disk might reside in a series of 1024-byte blocks. if each key/pointer pair in the linear index requires 8 bytes, then 128 keys are stored per block. the second-level index, stored in main memory, consists of a simple table storing the value of the key in the ﬁrst position of each block in the linear index ﬁle. this arrangement is shown in figure 10.2. if the linear index requires 1024 disk blocks (1mb), the second-level index contains only 1024 entries, one per disk block. to ﬁnd which disk block contains a desired search key value, ﬁrst search through the
whose records have the indicated secondary key value. figure 10.3 illustrates this approach. now there is no duplication of secondary key values, possibly yielding a considerable space savings. the cost of insertion and deletion is reduced, because only one row of the table need be adjusted. note that a new row is added to the array when a new secondary key value is added. this might lead to moving many records, but this will happen infrequently in applications suited to using this arrangement. a drawback to this approach is that the array must be of ﬁxed size, which imposes an upper limit on the number of primary keys that might be associated with a particular secondary key. furthermore, those secondary keys with fewer records than the width of the array will waste the remainder of their row. a better approach is to have a one-dimensional array of secondary key values, where each secondary key is associated with a linked list. this works well if the index is stored in main memory, but not so well when it is stored on disk because the linked list for a given key might be scattered across several disk blocks.
consider a large database of employee records. if the primary key is the employee’s id number and the secondary key is the employee’s name, then each record in the name index associates a name with one or more id numbers. the id number index in turn associates an id number with a unique pointer to the full record on disk. the secondary key index in such an organization is also known as an inverted list or inverted ﬁle. it is inverted in that searches work backwards from the secondary key to the primary key to the actual data record. it is called a list because each secondary key value has (conceptually) a list of primary keys associated with it. figure 10.4 illustrates this arrangement. here, we have last names as the secondary key. the primary key is a four-character unique identiﬁer.
figure 10.5 shows a better approach to storing inverted lists. an array of secondary key values is shown as before. associated with each secondary key is a pointer to an array of primary keys. the primary key array uses a linked-list implementation. this approach combines the storage for all of the secondary key lists into a single array, probably saving space. each record in this array consists of a primary key value and a pointer to the next element on the list. it is easy to insert and delete secondary keys from this array, making this a good implementation for disk-based inverted ﬁles.
how do we handle large databases that require frequent update? the main problem with the linear index is that it is a single, large array that does not lend itself to updates because a single update can require changing the position of every key
10.3 modify the function binary of section 3.5 so as to support variable-length records with ﬁxed-length keys indexed by a simple linear index as illustrated by figure 10.1.
10.4 assume that a database stores records consisting of a 2-byte integer key and a variable-length data ﬁeld consisting of a string. show the linear index (as illustrated by figure 10.1) for the following collection of records:
10.5 each of the following series of records consists of a four-digit primary key (with no duplicates) and a four-character secondary key (with many duplicates).
10.9 you are given a series of records whose keys are letters. the records arrive in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the 2-3 tree that results from inserting these records.
10.10 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the tree that results from inserting these records when the 2-3 tree is modiﬁed to be a 2-3+ tree, that is, the internal nodes act only as placeholders. assume that the leaf nodes are capable of holding up to two records.
10.11 show the result of inserting the value 55 into the b-tree of figure 10.16. 10.12 show the result of inserting the values 1, 2, 3, 4, 5, and 6 (in that order) into
10.14 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the b+-tree of order four that results from inserting these records. assume that the leaf nodes are capable of storing up to three records.
10.15 assume that you have a b+-tree whose internal nodes can store up to 100 children and whose leaf nodes can store up to 15 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.16 assume that you have a b+-tree whose internal nodes can store up to 50 children and whose leaf nodes can store up to 50 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.1 implement a two-level linear index for variable-length records as illustrated by figures 10.1 and 10.2. assume that disk blocks are 1024 bytes in length. records in the database ﬁle should typically range between 20 and 200 bytes, including a 4-byte key value. each record of the index ﬁle should store a key value and the byte offset in the database ﬁle for the ﬁrst byte of the corresponding record. the top-level index (stored in memory) should be a simple array storing the lowest key value on the corresponding block in the index ﬁle.
10.2 implement the 2-3+ tree, that is, a 2-3 tree where the internal nodes act only as placeholders. your 2-3+ tree should implement the dictionary interface of section 4.4.
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
key value for each block in that cylinder, called the cylinder index. when new records are inserted, they are placed in the correct cylinder’s overﬂow area (in effect, a cylinder acts as a bucket). if a cylinder’s overﬂow area ﬁlls completely, then a system-wide overﬂow area is used. search proceeds by determining the proper cylinder from the system-wide table kept in main memory. the cylinder’s block table is brought in from disk and consulted to determine the correct block. if the record is found in that block, then the search is complete. otherwise, the cylinder’s overﬂow area is searched. if that is full, and the record is not found, then the system-wide overﬂow is searched.
after initial construction of the database, so long as no new records are inserted or deleted, access is efﬁcient because it rquires only two disk fetches. the ﬁrst disk fetch recovers the block table for the desired cylinder. the second disk fetch recovers the block that, under good conditions, contains the record. after many inserts, the overﬂow list becomes too long, resulting in signiﬁcant search time as the cylinder overﬂow area ﬁlls up. under extreme conditions, many searches might eventually lead to the system overﬂow area. the “solution” to this problem is to periodically reorganize the entire database. this means rebalancing the records among the cylinders, sorting the records within each cylinder, and updating both the system index table and the within-cylinder block table. such reorganization was typical of database systems during the 1960s and would normally be done each night or weekly.
linear indexing is efﬁcient when the database is static, that is, when records are inserted and deleted rarely or never. isam is adequate for a limited number of updates, but not for frequent changes. because it has essentially two levels of indexing, isam will also break down for a truly large database where the number of cylinders is too great for the top-level index to ﬁt in main memory.
1. large sets of records are frequently updated. 2. search is by one or a combination of several keys. 3. key range queries or min/max queries are used. for such databases, a better organization must be found. one approach would be to use the binary search tree (bst) to store primary and secondary key indices. bsts can store duplicate key values, they provide efﬁcient insertion and deletion as well as efﬁcient search, and they can perform efﬁcient range queries. when there
the asymptotic cost of search, insertion, and deletion of records from b-trees, b+-trees, and b∗-trees is Θ(log n) where n is the total number of records in the tree. however, the base of the log is the (average) branching factor of the tree. typical database applications use extremely high branching factors, perhaps 100 or more. thus, in practice the b-tree and its variants are extremely shallow.
as an illustration, consider a b+-tree of order 100 and leaf nodes that contain up to 100 records. a one-level b+-tree can have at most 100 records. a two-level b+-tree must have at least 100 records (2 leaves with 50 records each). it has at most 10,000 records (100 leaves with 100 records each). a three-level b+-tree must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes with 100 full children each). a four-level b+-tree must have at least 250,000 records and at most 100 million records. thus, it would require an extremely large database to generate a b+-tree of more than four levels.
we can reduce the number of disk fetches required for the b-tree even more by using the following methods. first, the upper levels of the tree can be stored in main memory at all times. because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. if the b-tree is only four levels deep, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record.
as mentioned earlier, a buffer pool should be used to manage nodes of the b-tree. several nodes of the tree would typically be in main memory at one time. the most straightforward approach is to use a standard method such as lru to do node replacement. however, sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool. in general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently.
2this concept can be extended further if higher space utilization is required. however, the update routines become much more complicated. i once worked on a project where we implemented 3-for-4 node split and merge routines. this gave better performance than the 2-for-3 node split and merge routines of the b∗-tree. however, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed!
some binary tree implementations store data only at the leaf nodes, using the internal nodes to provide structure to the tree. more generally, binary tree implementations might require some amount of space for internal nodes, and a different amount for leaf nodes. thus, to analyze the space required by such implementations, it is useful to know the minimum and maximum fraction of the nodes that are leaves in a tree containing n internal nodes.
unfortunately, this fraction is not ﬁxed. a binary tree of n internal nodes might have only one leaf. this occurs when the internal nodes are arranged in a chain ending in a single leaf as shown in figure 5.4. in this case, the number of leaves is low because each internal node has only one non-empty child. to ﬁnd an upper bound on the number of leaves for a tree of n internal nodes, ﬁrst note that the upper bound will occur when each internal node has two non-empty children, that is, when the tree is full. however, this observation does not tell what shape of tree will yield the highest percentage of non-empty leaves. it turns out not to matter, because all full binary trees with n internal nodes have the same number of leaves. this fact allows us to compute the space requirements for a full binary tree implementation whose leaves require a different amount of space from its internal nodes.
proof: the proof is by mathematical induction on n, the number of internal nodes. this is an example of an induction proof where we reduce from an arbitrary instance of size n to an instance of size n − 1 that meets the induction hypothesis.
• base cases: the non-empty tree with zero internal nodes has one leaf node. a full binary tree with one internal node has two leaf nodes. thus, the base cases for n = 0 and n = 1 conform to the theorem.
• induction step: given tree t with n internal nodes, select an internal node i whose children are both leaf nodes. remove both of i’s children, making i a leaf node. call the new tree t0. t0 has n − 1 internal nodes. from the induction hypothesis, t0 has n leaves. now, restore i’s two children. we once again have tree t with n internal nodes. how many leaves does t have? because t0 has n leaves, adding the two children yields n+2. however, node i counted as one of the leaves in t0 and has now become an internal node. thus, tree t has n + 1 leaf nodes and n internal nodes.
by mathematical induction the theorem holds for all values of n ≥ 0. 2 when analyzing the space requirements for a binary tree implementation, it is useful to know how many empty subtrees a tree contains. a simple extension of the full binary tree theorem tells us exactly how many empty subtrees there are in any binary tree, whether full or not. here are two approaches to proving the following theorem, and each suggests a useful way of thinking about binary trees.
proof 1: take an arbitrary binary tree t and replace every empty subtree with a leaf node. call the new tree t0. all nodes originally in t will be internal nodes in t0 (because even the leaf nodes of t have children in t0). t0 is a full binary tree, because every internal node of t now must have two children in t0, and each leaf node in t must have two children in t0 (the leaves just added). the full binary tree theorem tells us that the number of leaves in a full binary tree is one more than the number of internal nodes. thus, the number of new leaves that were added to create t0 is one more than the number of nodes in t. each leaf node in t0 corresponds to an empty subtree in t. thus, the number of empty subtrees in t is one more than the number of nodes in t. 2
proof 2: by deﬁnition, every node in binary tree t has two children, for a total of 2n children in a tree of n nodes. every node except the root node has one parent, for a total of n − 1 nodes with parents. in other words, there are n − 1 non-empty children. because the total number of children is 2n, the remaining n + 1 children must be empty. 2
just as we developed a generic list adt on which to build specialized list implementations, we would like to deﬁne a generic binary tree adt based on those
figure 5.20 two series of exchanges to build a max-heap. (a) this heap is built by a series of nine exchanges in the order (4-2), (4-1), (2-1), (5-2), (5-4), (6-3), (6-5), (7-5), (7-6). (b) this heap is built by a series of four exchanges in the order (5-2), (7-3), (7-1), (6-1).
one good algorithm stems from induction. suppose that the left and right subtrees of the root are already heaps, and r is the name of the element at the root. this situation is illustrated by figure 5.21. in this case there are two possibilities. (1) r has a value greater than or equal to its two children. in this case, construction is complete. (2) r has a value less than one or both of its children. in this case, r should be exchanged with the child that has greater value. the result will be a heap, except that r might still be less than one or both of its (new) children. in this case, we simply continue the process of “pushing down” r until it reaches a level where it is greater than its children, or is a leaf node. this process is implemented by the private method siftdown of the heap class. the siftdown operation is illustrated by figure 5.22.
this approach assumes that the subtrees are already heaps, suggesting that a complete algorithm can be obtained by visiting the nodes in some order such that the children of a node are visited before the node itself. one simple way to do this is simply to work from the high index of the array to the low index. actually, the build process need not visit the leaf nodes (they can never move down because they are already at the bottom), so the building algorithm can start in the middle of the
proof: call the two letters with least frequency l1 and l2. they must be siblings because buildhuff selects them in the ﬁrst step of the construction process. assume that l1 and l2 are not the deepest nodes in the tree. in this case, the huffman tree must either look as shown in figure 5.30, or in some sense be symmetrical to this. for this situation to occur, the parent of l1 and l2, labeled v, must have greater weight than the node labeled x. otherwise, function buildhuff would have selected node v in place of node x as the child of node u. however, this is impossible because l1 and l2 are the letters with least frequency. 2
• base case: for n = 2, the huffman tree must have the minimum external path weight because there are only two possible trees, each with identical weighted path lengths for the two leaves. • induction hypothesis: assume that any tree created by buildhuff that contains n − 1 leaves has minimum external path length. • induction step: given a huffman tree t built by buildhuff with n leaves, n ≥ 2, suppose that w1 ≤ w2 ≤ ··· ≤ wn where w1 to wn are the weights of the letters. call v the parent of the letters with frequencies w1 and w2. from the lemma, we know that the leaf nodes containing the letters with frequencies w1 and w2 are as deep as any nodes in t. if any other leaf nodes in the tree were deeper, we could reduce their weighted path length by swapping them with w1 or w2. but the lemma tells us that no such deeper nodes exist. call t0 the huffman tree that is identical to t except that node
v is replaced with a leaf node v 0 whose weight is w1 + w2. by the induction hypothesis, t0 has minimum external path length. returning the children to v 0 restores tree t, which must also have minimum external path length.
once the huffman tree has been constructed, it is an easy matter to assign codes to individual letters. beginning at the root, we assign either a ‘0’ or a ‘1’ to each edge in the tree. ‘0’ is assigned to edges connecting a node with its left child, and ‘1’ to edges connecting a node with its right child. this process is illustrated by figure 5.26. the huffman code for a letter is simply a binary number determined by the path from the root to the leaf corresponding to that letter. thus, the code for e is ‘0’ because the path from the root to the leaf node for e takes a single left branch. the code for k is ‘111101’ because the path to the node for k takes four right branches, then a left, and ﬁnally one last right. figure 5.31 lists the codes for all eight letters.
given codes for the letters, it is a simple matter to use these codes to encode a text message. we simply replace each letter in the string with its binary code. a lookup table can be used for this purpose.
example 5.9 using the code generated by our example huffman tree, the word “deed” is represented by the bit string “10100101” and the word “muck” is represented by the bit string “111111001110111101.”
decoding the message is done by looking at the bits in the coded string from left to right until a letter is decoded. this can be done by using the huffman tree in
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
recurrence relations are often used to model the cost of recursive functions. for example, the standard mergesort (section 7.4) takes a list of size n, splits it in half, performs mergesort on each half, and ﬁnally merges the two sublists in n steps. the cost for this can be modeled as
in other words, the cost of the algorithm on input of size n is two times the cost for input of size n/2 (due to the two recursive calls to mergesort) plus n (the time to merge the sublists together again).
there are many approaches to solving recurrence relations, and we brieﬂy consider three here. the ﬁrst is an estimation technique: guess the upper and lower bounds for the recurrence, use induction to prove the bounds, and tighten as required. the second approach is to expand the recurrence to convert it to a summation and then use summation techniques. the third approach is to take advantage of already proven theorems when the recurrence is of a suitable form. in particular, typical divide and conquer algorithms such as mergesort yield recurrences of a form that ﬁts a pattern for which we have a ready solution.
14.2.1 estimating upper and lower bounds the ﬁrst approach to solving recurrences is to guess the answer and then attempt to prove it correct. if a correct upper or lower bound estimate is given, an easy induction proof will verify this fact. if the proof is successful, then try to tighten the bound. if the induction proof fails, then loosen the bound and try again. once the upper and lower bounds match, you are ﬁnished. this is a useful technique when you are only looking for asymptotic complexities. when seeking a precise closed-form solution (i.e., you seek the constants for the expression), this method will not be appropriate.
we prove this guess is correct by induction. in this proof, we assume that n is a power of two, to make the calculations easy. for the base case, t(2) = 1 ≤ 22. for the induction step, we need to show that t(n) ≤ n2 implies that t(2n) ≤ (2n)2 for n = 2n , n ≥ 1. the induction hypothesis is
is o(n2) a good estimate? in the next-to-last step we went from n2+2n to the much larger 4n2. this suggests that o(n2) is a high estimate. if we guess something smaller, such as t(n) ≤ cn for some constant c, it should be clear that this cannot work because c2n = 2cn and there is no room for the extra n cost to join the two pieces together. thus, the true cost must be somewhere between cn and n2. let us now try t(n) ≤ n log n. for the base case, the deﬁnition of the recurrence sets t(2) = 1 ≤ (2·log 2) = 2. assume (induction hypothesis) that t(n) ≤ n log n. then, t(2n) = 2t(n) + 2n ≤ 2n log n + 2n ≤ 2n(log n + 1) ≤ 2n log 2n
example 14.5 we know that the factorial function grows exponentially. how does it compare to 2n? to nn? do they all grow “equally fast” (in an asymptotic sense)? we can begin by looking at a few initial terms.
the next step is to deﬁne the adt for a list object in terms of a set of operations on that object. we will use the java notation of an interface to formally deﬁne the list adt. interface list deﬁnes the member functions that any list implementation inheriting from it must support, along with their parameters and return types. we increase the ﬂexibility of the list adt by making it a template.
true to the notion of an adt, an abstract class does not specify how operations are implemented. two complete implementations are presented later in this section, both of which use the same list adt to deﬁne their operations, but they are considerably different in approaches and in their space/time tradeoffs.
figure 4.1 presents our list adt. class list is a generic of one parameter, named e. e serves as a placeholder for whatever element type the user would like to store in a list. the comments given in figure 4.1 describe precisely what each member function is intended to do. however, some explanation of the basic design is in order. given that we wish to support the concept of a sequence, with access to any position in the list, the need for many of the member functions such as insert and movetopos is clear. the key design decision embodied in this adt is support for the concept of a current position. for example, member movetostart sets the current position to be the ﬁrst element on the list, while methods next and prev move the current position to the next and previous elements, respectively. the intention is that any implementation for this adt support the concept of a current position.
given that our adt deﬁnes lists to have a current position, it is helpful to modify our list display notation to indicate this position. i will use a vertical bar, such as h20, 23 | 12, 15i to indicate the list of four elements, with the current position immediately to the right of the bar. given this conﬁguration, calling insert with value 10 will change the list to be h20, 23 | 10, 12, 15i.
if you examine figure 4.1, you should ﬁnd that the list member functions provided allow you to build a list with elements in any desired order, and to access any desired position in the list. you might have noticed that the clear method is not necessary, in that it could be implemented by means of the other member functions in the same asymptotic time. it is included merely for convenience.
method getvalue returns a reference to the current element. it is considered a violation of getvalue’s preconditions to ask for the value of a non-existent element (i.e., there must be an element at the current position). in our concrete list implementations, the java’s assert mechanism will be used to enforce such preconditions. in a commercial implementation, such violations would be best implemented by the java’s exception mechanism.
@param size max number of elements list can contain. */ @suppresswarnings("unchecked") // generic array allocation alist(int size) { maxsize = size; listsize = curr = 0; listarray = (e[])new object[size];
figure 4.2 shows the array-based list implementation, named alist. alist inherits from abstract class list and so must implement all of the member functions of list.
a key design decision for the linked list implementation is how to represent the current position. the most reasonable choices appear to be a pointer to the current element. but there is a big advantage to making curr point to the element preceding the current element.
figure 4.5(a) shows the list’s curr pointer pointing to the current element. the vertical line between the nodes containing 23 and 12 indicates the logical position of the current element. consider what happens if we wish to insert a new node with value 10 into the list. the result should be as shown in figure 4.5(b). however, there is a problem. to “splice” the list node containing the new element into the list, the list node storing 23 must have its next pointer changed to point to the new node. unfortunately, there is no convenient access to the node preceding the one pointed to by curr. however, if curr points directlyto the preceding element, there is no difﬁculty in adding a new element after curr. see exercise 4.5 for further discussion of why making curr point directly to the current element fails. unfortunately, we encounter a number of problems when the list is empty, or when the current position is at an end of the list. in particular, when the list is empty we have no element for head, tail, and curr to point to. one solution is to implement a number of special cases in the implementations for insert and remove. this increases code complexity, making it harder to understand, and thus increases the chance of introducing a programming bug.
these special cases can be eliminated by implementing linked lists with a special header node as the ﬁrst node of the list. this header node is a link node like any other, but its value is ignored and it is not considered to be an actual element of the list. the header node saves coding effort because we no longer need to consider special cases for empty lists or when the current position is at one end of the list. the cost of this simpliﬁcation is the space for the header node. however, there are space savings due to smaller code size, because statements to handle the special cases are omitted. in practice, this reduction in code size typically saves more space than that required for the header node, depending on the number of lists created. figure 4.6 shows the state of an initialized or empty list when using a header node. figure 4.7 shows the insertion example of figure 4.5 using a header node and the convention that curr points to the node preceding the current node.
figure 4.8 shows the deﬁnition for the linked list class, named llist. class llist inherits from the abstract list class and thus must implement all of class list’s member functions.
examining class ualdict (ual stands for “unsorted array-based list), we can easily see that insert is a constant time operation, because it simply inserts the new record at the end of the list. however, find, and remove both require Θ(n) time in the average and worst cases, because we need to do a sequential search. method remove in particular must touch every record in the list, because once the desired record is found, the remaining records must be shifted down in the list to ﬁll the gap. method removeany removes the last record from the list, so this is a constant-time operation.
as an alternative, we could implement the dictionary using a linked list. the implementation would be quite similar to that shown in figure 4.31, and the cost of the functions should be the same asymptotically.
another alternative would be to implement the dictionary with a sorted list. the advantage of this approach would be that we might be able to speed up the find operation by using a binary search. to do so, ﬁrst we must deﬁne a variation on the list adt to support sorted lists. a sorted list is somewhat different from an unsorted list in that it cannot permit the user to control where elements get inserted. thus, the insert method must be quite different in a sorted list than in an unsorted list. likewise, the user cannot be permitted to append elements onto the list. for these reasons, a sorted list cannot be implemented with straightforward inheritance from the list adt.
the cost for find in a sorted list is Θ(log n) for a list of length n. this is a great improvement over the cost of find in an unsorted list. unfortunately, the cost of insert changes from constant time in the unsorted list to Θ(n) time in the sorted list. whether the sorted list implementation for the dictionary adt is more or less efﬁcient than the unsorted list implementation depends on the relative number of insert and find operations to be performed. if many more find operations than insert operations are used, then it might be worth using a sorted list to implement the dictionary. in both cases, remove requires Θ(n) time in the worst and average cases. even if we used binary search to cut down on the time to ﬁnd the record prior to removal, we would still need to shift down the remaining records in the list to ﬁll the gap left by the remove operation.
java allows us to differentiate leaf from internal nodes through the use of class inheritance. as we have seen with lists and with class binnode, a base class provides a general deﬁnition for an object, and a subclass modiﬁes a base class to add more detail. a base class can be declared for nodes in general, with subclasses deﬁned for the internal and leaf nodes. the base class of figure 5.10 is named varbinnode. it includes a virtual member function named isleaf, which indicates the node type. subclasses for the internal and leaf node types each implement isleaf. internal nodes store child pointers of the base class type; they do not distinguish their children’s actual subclass. whenever a node is examined, its version of isleaf indicates the node’s subclass.
{ operator = op; left = l; right = r; } public boolean isleaf() { return false; } public varbinnode leftchild() { return left; } public varbinnode rightchild() { return right; } public character value() { return operator; }
figure 5.11 a second implementation for separate internal and leaf node representations using java class inheritance and virtual functions using the composite design pattern. here, the functionality of traverse is embedded into the node subclasses.
if p = d, the overhead drops to about one half of the total space. however, if only leaf nodes store useful information, the overhead fraction for this implementation is actually three quarters of the total space, because half of the “data” space is unused. if a full binary tree needs to store data only at the leaf nodes, a better implementation would have the internal nodes store two pointers and no data ﬁeld while the leaf nodes store only a data ﬁeld. this implementation requires 2p n + d(n + 1) units of space. if p = d, then the overhead is about 2p/(2p +d) = 2/3. it might seem counter-intuitive that the overhead ratio has gone up while the total amount of space has gone down. the reason is because we have changed our deﬁnition of “data” to refer only to what is stored in the leaf nodes, so while the overhead fraction is higher, it is from a total storage requirement that is lower.
there is one serious ﬂaw with this analysis. when using separate implementations for internal and leaf nodes, there must be a way to distinguish between the node types. when separate node types are implemented via java subclasses, the runtime environment stores information with each object allowing it to determine, for example, the correct subclass to use when the isleaf virtual function is called. thus, each node requires additional space. only one bit is truly necessary to distinguish the two possibilities. in rare applications where space is a critical resource, implementors can often ﬁnd a spare bit within the node’s value ﬁeld in which to store the node type indicator. an alternative is to use a spare bit within a node pointer to indicate node type. for example, this is often possible when the compiler requires that structures and objects start on word boundaries, leaving the last bit of a pointer value always zero. thus, this bit can be used to store the nodetype ﬂag and is reset to zero before the pointer is dereferenced. another alternative when the leaf value ﬁeld is smaller than a pointer is to replace the pointer to a leaf with that leaf’s value. when space is limited, such techniques can make the difference between success and failure. in any other situation, such “bit packing” tricks should be avoided because they are difﬁcult to debug and understand at best, and are often machine dependent at worst.2
the previous section points out that a large fraction of the space in a typical binary tree node implementation is devoted to structural overhead, not to storing data.
2in the early to mid 1980s, i worked on a geographic information system that stored spatial data in quadtrees (see section 13.3). at the time space was a critical resource, so we used a bit-packing approach where we stored the nodetype ﬂag as the last bit in the parent node’s pointer. this worked perfectly on various 32-bit workstations. unfortunately, in those days ibm pc-compatibles used 16-bit pointers. we never did ﬁgure out how to port our code to the 16-bit machine.
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
properly used to describe our understanding of a problem or an algorithm, it is best to consider an example where you do not already know a lot about the problem.
let us look ahead to analyzing the problem of sorting to see how this process works. what is the least possible cost for any sorting algorithm in the worst case? the algorithm must at least look at every element in the input, just to determine that the input is truly sorted. it is also possible that each of the n values must be moved to another location in the sorted output. thus, any sorting algorithm must take at least cn time. for many problems, this observation that each of the n inputs must be looked at leads to an easy Ω(n) lower bound.
in your previous study of computer science, you have probably seen an example of a sorting algorithm whose running time is in o(n2) in the worst case. the simple bubble sort and insertion sort algorithms typically given as examples in a ﬁrst year programming course have worst case running times in o(n2). thus, the problem of sorting can be said to have an upper bound in o(n2). how do we close the gap between Ω(n) and o(n2)? can there be a better sorting algorithm? if you can think of no algorithm whose worst-case growth rate is better than o(n2), and if you have discovered no analysis technique to show that the least cost for the problem of sorting in the worst case is greater than Ω(n), then you cannot know for sure whether or not there is a better algorithm.
chapter 7 presents sorting algorithms whose running time is in o(n log n) for the worst case. this greatly narrows the gap. witht his new knowledge, we now have a lower bound in Ω(n) and an upper bound in o(n log n). should we search for a faster algorithm? many have tried, without success. fortunately (or perhaps unfortunately?), chapter 7 also includes a proof that any sorting algorithm must have running time in Ω(n log n) in the worst case.2 this proof is one of the most important results in the ﬁeld of algorithm analysis, and it means that no sorting algorithm can possibly run faster than cn log n for the worst-case input of size n. thus, we can conclude that the problem of sorting is Θ(n log n) in the worst case, because the upper and lower bounds have met.
knowing the lower bound for a problem does not give you a good algorithm. but it does help you to know when to stop looking. if the lower bound for the problem matches the upper bound for the algorithm (within a constant factor), then we know that we can ﬁnd an algorithm that is better only by a constant factor.
figure 7.1 an illustration of insertion sort. each column shows the array after the iteration with the indicated value of i in the outer for loop. values above the line in each column have been sorted. arrows indicate the upward motions of records through the array.
imagine that you have a stack of phone bills from the past two years and that you wish to organize them by date. a fairly natural way to do this might be to look at the ﬁrst two bills and put them in order. then take the third bill and put it into the right order with respect to the ﬁrst two, and so on. as you take each bill, you would add it to the sorted pile that you have already made. this naturally intuitive process is the inspiration for our ﬁrst sorting algorithm, called insertion sort. insertion sort iterates through a list of records. each record is inserted in turn at the correct position within a sorted list composed of those records already processed. the following is a java implementation. the input is an array of n records stored in array a.
consider the case where inssort is processing the ith record, which has key value x. the record is moved upward in the array as long as x is less than the key value immediately above it. as soon as a key value less than or equal to x is encountered, inssort is done with that record because all records above it in the array must have smaller keys. figure 7.1 illustrates how insertion sort works. the body of inssort is made up of two nested for loops. the outer for loop is executed n − 1 times. the inner for loop is harder to analyze because the number of times it executes depends on how many keys in positions 1 to i − 1 have a value less than that of the key in position i. in the worst case, each record
must make its way to the top of the array. this would occur if the keys are initially arranged from highest to lowest, in the reverse of sorted order. in this case, the number of comparisons will be one the ﬁrst time through the for loop, two the second time, and so on. thus, the total number of comparisons will be
in contrast, consider the best-case cost. this occurs when the keys begin in sorted order from lowest to highest. in this case, every pass through the inner for loop will fail immediately, and no values will be moved. the total number of comparisons will be n − 1, which is the number of times the outer for loop executes. thus, the cost for insertion sort in the best case is Θ(n).
while the best case is signiﬁcantly faster than the worst case, the worst case is usually a more reliable indication of the “typical” running time. however, there are situations where we can expect the input to be in sorted or nearly sorted order. one example is when an already sorted list is slightly disordered; restoring sorted order using insertion sort might be a good idea if we know that the disordering is slight. examples of algorithms that take advantage of insertion sort’s best-case running time are the shellsort algorithm of section 7.3 and the quicksort algorithm of section 7.5.
what is the average-case cost of insertion sort? when record i is processed, the number of times through the inner for loop depends on how far “out of order” the record is. in particular, the inner for loop is executed once for each key greater than the key of record i that appears in array positions 0 through i−1. for example, in the leftmost column of figure 7.1 the value 15 is preceded by ﬁve values greater than 15. each such occurrence is called an inversion. the number of inversions (i.e., the number of values greater than a given value that occur prior to it in the array) will determine the number of comparisons and swaps that must take place. we need to determine what the average number of inversions will be for the record in position i. we expect on average that half of the keys in the ﬁrst i − 1 array positions will have a value greater than that of the key at position i. thus, the average case should be about half the cost of the worst case, which is still Θ(n2). so, the average case is no better than the worst case in asymptotic complexity.
counting comparisons or swaps yields similar results because each time through the inner for loop yields both a comparison and a swap, except the last (i.e., the comparison that fails the inner for loop’s test), which has no swap. thus, the number of swaps for the entire sort operation is n − 1 less than the number of comparisons. this is 0 in the best case, and Θ(n2) in the average and worst cases.
determining bubble sort’s number of comparisons is easy. regardless of the arrangement of the values in the array, the number of comparisons made by the inner for loop is always i, leading to a total cost of
the number of swaps required depends on how often a value is less than the one immediately preceding it in the array. we can expect this to occur for about half the comparisons in the average case, leading to Θ(n2) for the expected number of swaps. the actual number of swaps performed by bubble sort will be identical to that performed by insertion sort.
7.2.3 selection sort consider again the problem of sorting a pile of phone bills for the past year. another intuitive approach might be to look through the pile until you ﬁnd the bill for january, and pull that out. then look through the remaining pile until you ﬁnd the bill for february, and add that behind january. proceed through the ever-shrinking pile of bills to select the next one in order until you are done. this is the inspiration for our last Θ(n2) sort, called selection sort. the ith pass of selection sort “selects” the ith smallest key in the array, placing that record into position i. in other words, selection sort ﬁrst ﬁnds the smallest key in an unsorted list, then the second smallest, and so on. its unique feature is that there are few record swaps. to ﬁnd the next smallest key value requires searching through the entire unsorted portion of the array, but only one swap is required to put the record in place. thus, the total number of swaps required will be n − 1 (we get the last record in place “for free”).
selection sort is essentially a bubble sort, except that rather than repeatedly swapping adjacent values to get the next smallest record into place, we instead
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
compared with x). again, there are two possibilities. if z is less than x, then these items should be swapped (the left branch). if z is not less than x, then insertion sort is complete (the right branch).
note that the right branch reaches a leaf node, and that this leaf node contains only permutation yxz. this means that only permutation yxz can be the outcome based on the results of the decisions taken to reach this node. in other words, insertion sort has “found” the single permutation of the original input that yields a sorted list. likewise, if the second decision resulted in taking the left branch, a third comparison, regardless of the outcome, yields nodes in the decision tree with only single permutations. again, insertion sort has “found” the correct permutation that yields a sorted list.
any sorting algorithm based on comparisons can be modeled by a decision tree in this way, regardless of the size of the input. thus, all sorting algorithms can be viewed as algorithms to “ﬁnd” the correct permutation of the input that yields a sorted list. each algorithm based on comparisons can be viewed as proceeding by making branches in the tree based on the results of key comparisons, and each algorithm can terminate once a node with a single permutation has been reached.
how is the worst-case cost of an algorithm expressed by the decision tree? the decision tree shows the decisions made by an algorithm for all possible inputs of a given size. each path through the tree from the root to a leaf is one possible series of decisions taken by the algorithm. the depth of the deepest node represents the longest series of decisions required by the algorithm to reach an answer.
there are many comparison-based sorting algorithms, and each will be modeled by a different decision tree. some decision trees might be well-balanced, others might be unbalanced. some trees will have more nodes than others (those with more nodes might be making “unnecessary” comparisons). in fact, a poor sorting algorithm might have an arbitrarily large number of nodes in its decision tree, with leaves of arbitrary depth. there is no limit to how slow the “worst” possible sorting algorithm could be. however, we are interested here in knowing what the best sorting algorithm could have as its minimum cost in the worst case. in other words, we would like to know what is the smallest depth possible for the deepest node in the tree for any sorting algorithm.
the smallest depth of the deepest node will depend on the number of nodes in the tree. clearly we would like to “push up” the nodes in the tree, but there is limited room at the top. a tree of height 1 can only store one node (the root); the tree of height 2 can store three nodes; the tree of height 3 can store seven nodes, and so on.
7.13 graph f1(n) = n log n, f2(n) = n1.5, and f3(n) = n2 in the range 1 ≤ n ≤ 1000 to visually compare their growth rates. typically, the constant factor in the running-time expression for an implementation of insertion sort will be less than the constant factors for shellsort or quicksort. how many times greater can the constant factor be for shellsort to be faster than insertion sort when n = 1000? how many times greater can the constant factor be for quicksort to be faster than insertion sort when n = 1000?
7.14 imagine that there exists an algorithm splitk that can split a list l of n elements into k sublists, each containing one or more elements, such that sublist i contains only elements whose values are less than all elements in sublist j for i < j <= k. if n < k, then k− n sublists are empty, and the rest are of length 1. assume that splitk has time complexity o(length of l). furthermore, assume that the k lists can be concatenated again in constant time. consider the following algorithm: list sortk(list l) {
7.15 here is a variation on sorting. the problem is to sort a collection of n nuts and n bolts by size. it is assumed that for each bolt in the collection, there is a corresponding nut of the same size, but initially we do not know which nut goes with which bolt. the differences in size between two nuts or two bolts can be too small to see by eye, so you cannot rely on comparing the sizes of two nuts or two bolts directly. instead, you can only compare the sizes of a nut and a bolt by attempting to screw one into the other (assume this comparison to be a constant time operation). this operation tells you that either the nut is bigger than the bolt, the bolt is bigger than the nut, or they are the same size. what is the minimum number of comparisons needed to sort the nuts and bolts in the worst case?
can stop early. this makes the best case performance become o(n) (because if the list is already sorted, then no iterations will take place on the ﬁrst pass, and the sort will stop right there). modify the bubble sort implementation to add this ﬂag and test. compare the modiﬁed implementation on a range of inputs to determine if it does or does not improve performance in practice.
7.2 starting with the java code for quicksort given in this chapter, write a series of quicksort implementations to test the following optimizations on a wide range of input data sizes. try these optimizations in various combinations to try and develop the fastest possible quicksort implementation that you can. (a) look at more values when selecting a pivot. (b) do not make a recursive call to qsort when the list size falls below a given threshold, and use insertion sort to complete the sorting process. test various values for the threshold size.
7.3 write your own collection of sorting programs to implement the algorithms described in this chapter, and compare their running times. be sure to implement optimized versions, trying to make each program as fast as possible. do you get the same relative timings as shown in figure 7.13? if not, why do you think this happened? how do your results compare with those of your classmates? what does this say about the difﬁculty of doing empirical timing studies?
7.4 perform a study of shellsort, using different increments. compare the version shown in section 7.3, where each increment is half the previous one, with others. in particular, try implementing “division by 3” where the increments on a list of length n will be n/3, n/9, etc. do other increment schemes work as well?
7.5 the implementation for mergesort given in section 7.4 takes an array as input and sorts that array. at the beginning of section 7.4 there is a simple pseudocode implementation for sorting a linked list using mergesort. implement both a linked list-based version of mergesort and the array-based version of mergesort, and compare their running times.
7.6 radix sort is typically implemented to support only a radix that is a power of two. this allows for a direct conversion from the radix to some number of bits in an integer key value. for example, if the radix is 16, then a 32-bit key will be processed in 8 steps of 4 bits each. this can lead to a more efﬁcient implementation because bit shifting can replace the division operations shown in the implementation of section 7.7. reimplement the radix sort
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
if you want to be a successful java programmer, you need good reference manuals close at hand. david flanagan’s java in a nutshell [fla05] provides a good reference for those familiar with the basics of the language.
after gaining proﬁciency in the mechanics of program writing, the next step is to become proﬁcient in program design. good design is difﬁcult to learn in any discipline, and good design for object-oriented software is one of the most difﬁcult of arts. the novice designer can jump-start the learning process by studying wellknown and well-used design patterns. the classic reference on design patterns is design patterns: elements of reusable object-oriented software by gamma, helm, johnson, and vlissides [ghjv95] (this is commonly referred to as the “gang of four” book). unfortunately, this is an extremely difﬁcult book to understand, in part because the concepts are inherently difﬁcult. a number of web sites are available that discuss design patterns, and which provide study guides for the design patterns book. two other books that discuss object-oriented software design are object-oriented software design and construction with c++ by dennis kafura [kaf98], and object-oriented design heuristics by arthur j. riel [rie96].
the exercises for this chapter are different from those in the rest of the book. most of these exercises are answered in the following chapters. however, you should not look up the answers in other parts of the book. these exercises are intended to make you think about some of the issues to be covered later on. answer them to the best of your ability with your current knowledge.
1.1 think of a program you have used that is unacceptably slow. identify the speciﬁc operations that make the program slow. identify other basic operations that the program performs quickly enough.
1.2 most programming languages have a built-in integer data type. normally this representation has a ﬁxed size, thus placing a limit on how large a value can be stored in an integer variable. describe a representation for integers that has no size restriction (other than the limits of the computer’s available main memory), and thus no practical limit on how large an integer can be stored. brieﬂy show how your representation can be used to implement the operations of addition, multiplication, and exponentiation.
1.3 deﬁne an adt for character strings. your adt should consist of typical functions that can be performed on strings, with each function deﬁned in
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
must make its way to the top of the array. this would occur if the keys are initially arranged from highest to lowest, in the reverse of sorted order. in this case, the number of comparisons will be one the ﬁrst time through the for loop, two the second time, and so on. thus, the total number of comparisons will be
in contrast, consider the best-case cost. this occurs when the keys begin in sorted order from lowest to highest. in this case, every pass through the inner for loop will fail immediately, and no values will be moved. the total number of comparisons will be n − 1, which is the number of times the outer for loop executes. thus, the cost for insertion sort in the best case is Θ(n).
while the best case is signiﬁcantly faster than the worst case, the worst case is usually a more reliable indication of the “typical” running time. however, there are situations where we can expect the input to be in sorted or nearly sorted order. one example is when an already sorted list is slightly disordered; restoring sorted order using insertion sort might be a good idea if we know that the disordering is slight. examples of algorithms that take advantage of insertion sort’s best-case running time are the shellsort algorithm of section 7.3 and the quicksort algorithm of section 7.5.
what is the average-case cost of insertion sort? when record i is processed, the number of times through the inner for loop depends on how far “out of order” the record is. in particular, the inner for loop is executed once for each key greater than the key of record i that appears in array positions 0 through i−1. for example, in the leftmost column of figure 7.1 the value 15 is preceded by ﬁve values greater than 15. each such occurrence is called an inversion. the number of inversions (i.e., the number of values greater than a given value that occur prior to it in the array) will determine the number of comparisons and swaps that must take place. we need to determine what the average number of inversions will be for the record in position i. we expect on average that half of the keys in the ﬁrst i − 1 array positions will have a value greater than that of the key at position i. thus, the average case should be about half the cost of the worst case, which is still Θ(n2). so, the average case is no better than the worst case in asymptotic complexity.
counting comparisons or swaps yields similar results because each time through the inner for loop yields both a comparison and a swap, except the last (i.e., the comparison that fails the inner for loop’s test), which has no swap. thus, the number of swaps for the entire sort operation is n − 1 less than the number of comparisons. this is 0 in the best case, and Θ(n2) in the average and worst cases.
figure 7.5 summarizes the cost of insertion, bubble, and selection sort in terms of their required number of comparisons and swaps1 in the best, average, and worst cases. the running time for each of these sorts is Θ(n2) in the average and worst cases.
the remaining sorting algorithms presented in this chapter are signiﬁcantly better than these three under typical conditions. but before continuing on, it is instructive to investigate what makes these three sorts so slow. the crucial bottleneck is that only adjacent records are compared. thus, comparisons and moves (in all but selection sort) are by single steps. swapping adjacent records is called an exchange. thus, these sorts are sometimes referred to as exchange sorts. the cost of any exchange sort can be at best the total number of steps that the records in the array must move to reach their “correct” location (i.e., the number of inversions for each record). what is the average number of inversions? consider a list l containing n values. deﬁne lr to be l in reverse. l has n(n−1)/2 distinct pairs of values, each of which could potentially be an inversion. each such pair must either be an inversion in l or in lr. thus, the total number of inversions in l and lr together is exactly n(n− 1)/2 for an average of n(n− 1)/4 per list. we therefore know with certainty that any sorting algorithm which limits comparisons to adjacent items will cost at least n(n − 1)/4 = Ω(n2) in the average case.
1there is a slight anomaly with selection sort. the supposed advantage for selection sort is its low number of swaps required, yet selection sort’s best-case number of swaps is worse than that for insertion sort or bubble sort. this is because the implementation given for selection sort does not avoid a swap in the case where record i is already in position i. the reason is that it usually takes more time to repeatedly check for this situation than would be saved by avoiding such swaps.
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
whose records have the indicated secondary key value. figure 10.3 illustrates this approach. now there is no duplication of secondary key values, possibly yielding a considerable space savings. the cost of insertion and deletion is reduced, because only one row of the table need be adjusted. note that a new row is added to the array when a new secondary key value is added. this might lead to moving many records, but this will happen infrequently in applications suited to using this arrangement. a drawback to this approach is that the array must be of ﬁxed size, which imposes an upper limit on the number of primary keys that might be associated with a particular secondary key. furthermore, those secondary keys with fewer records than the width of the array will waste the remainder of their row. a better approach is to have a one-dimensional array of secondary key values, where each secondary key is associated with a linked list. this works well if the index is stored in main memory, but not so well when it is stored on disk because the linked list for a given key might be scattered across several disk blocks.
consider a large database of employee records. if the primary key is the employee’s id number and the secondary key is the employee’s name, then each record in the name index associates a name with one or more id numbers. the id number index in turn associates an id number with a unique pointer to the full record on disk. the secondary key index in such an organization is also known as an inverted list or inverted ﬁle. it is inverted in that searches work backwards from the secondary key to the primary key to the actual data record. it is called a list because each secondary key value has (conceptually) a list of primary keys associated with it. figure 10.4 illustrates this arrangement. here, we have last names as the secondary key. the primary key is a four-character unique identiﬁer.
figure 10.5 shows a better approach to storing inverted lists. an array of secondary key values is shown as before. associated with each secondary key is a pointer to an array of primary keys. the primary key array uses a linked-list implementation. this approach combines the storage for all of the secondary key lists into a single array, probably saving space. each record in this array consists of a primary key value and a pointer to the next element on the list. it is easy to insert and delete secondary keys from this array, making this a good implementation for disk-based inverted ﬁles.
how do we handle large databases that require frequent update? the main problem with the linear index is that it is a single, large array that does not lend itself to updates because a single update can require changing the position of every key
key value for each block in that cylinder, called the cylinder index. when new records are inserted, they are placed in the correct cylinder’s overﬂow area (in effect, a cylinder acts as a bucket). if a cylinder’s overﬂow area ﬁlls completely, then a system-wide overﬂow area is used. search proceeds by determining the proper cylinder from the system-wide table kept in main memory. the cylinder’s block table is brought in from disk and consulted to determine the correct block. if the record is found in that block, then the search is complete. otherwise, the cylinder’s overﬂow area is searched. if that is full, and the record is not found, then the system-wide overﬂow is searched.
after initial construction of the database, so long as no new records are inserted or deleted, access is efﬁcient because it rquires only two disk fetches. the ﬁrst disk fetch recovers the block table for the desired cylinder. the second disk fetch recovers the block that, under good conditions, contains the record. after many inserts, the overﬂow list becomes too long, resulting in signiﬁcant search time as the cylinder overﬂow area ﬁlls up. under extreme conditions, many searches might eventually lead to the system overﬂow area. the “solution” to this problem is to periodically reorganize the entire database. this means rebalancing the records among the cylinders, sorting the records within each cylinder, and updating both the system index table and the within-cylinder block table. such reorganization was typical of database systems during the 1960s and would normally be done each night or weekly.
linear indexing is efﬁcient when the database is static, that is, when records are inserted and deleted rarely or never. isam is adequate for a limited number of updates, but not for frequent changes. because it has essentially two levels of indexing, isam will also break down for a truly large database where the number of cylinders is too great for the top-level index to ﬁt in main memory.
1. large sets of records are frequently updated. 2. search is by one or a combination of several keys. 3. key range queries or min/max queries are used. for such databases, a better organization must be found. one approach would be to use the binary search tree (bst) to store primary and secondary key indices. bsts can store duplicate key values, they provide efﬁcient insertion and deletion as well as efﬁcient search, and they can perform efﬁcient range queries. when there
10.3 modify the function binary of section 3.5 so as to support variable-length records with ﬁxed-length keys indexed by a simple linear index as illustrated by figure 10.1.
10.4 assume that a database stores records consisting of a 2-byte integer key and a variable-length data ﬁeld consisting of a string. show the linear index (as illustrated by figure 10.1) for the following collection of records:
10.5 each of the following series of records consists of a four-digit primary key (with no duplicates) and a four-character secondary key (with many duplicates).
10.9 you are given a series of records whose keys are letters. the records arrive in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the 2-3 tree that results from inserting these records.
figure 1.1 the relationship between data items, abstract data types, and data structures. the adt deﬁnes the logical form of the data type. the data structure implements the physical form of the data type.
a design pattern embodies and generalizes important design concepts for a recurring problem. a primary goal of design patterns is to quickly transfer the knowledge gained by expert designers to newer programmers. another goal is to allow for efﬁcient communication between programmers. its much easier to discuss a design issue when you share a vocabulary relevant to the topic.
speciﬁc design patterns emerge from the discovery that a particular design problem appears repeatedly in many contexts. they are meant to solve real problems. design patterns are a bit like generics: they describe the structure for a design solution, with the details ﬁlled in for any given problem. design patterns are a bit like data structures: each one provides costs and beneﬁts, which implies that tradeoffs are possible. therefore, a given design pattern might have variations on its application to match the various tradeoffs inherent in a given situation.
the flyweight design pattern is meant to solve the following problem. you have an application with many objects. some of these objects are identical in the information that they contain, and the role that they play. but they must be reached from various places, and conceptually they really are distinct objects. because so much information is shared, we would like to take advantage of the opportunity to reduce memory cost by sharing space. an example comes from representing the
where disk i/o is the bottleneck for the program, even the time to copy lots of information between the buffer pool user and the buffer might be inconsequential. another advantage to buffer passing is the reduction in unnecessary read operations for data that will be overwritten anyway.
you should note that the implementations for class bufferpool above are not generic. instead, the space parameter and the buffer pointer are declared to be byte[]. when a class is generic, that means that the record type is arbitrary, in contrast, using a byte[] but that the class knows what the record type is. pointer for the space means that not only is the record type arbitrary, but also the buffer pool does not even know what the user’s record type is. in fact, a given buffer pool might have many users who store many types of records.
in a buffer pool, the user decides where a given record will be stored but has no control over the precise mechanism by which data are transfered to the backing storage. this is in contrast to the memory manager described in section 12.3 in which the user passes a record to the manager and has no control at all over where the record is stored.
the java programmer’s logical view of a random access ﬁle is a single stream of bytes. interaction with a ﬁle can be viewed as a communications channel for issuing one of three instructions: read bytes from the current position in the ﬁle, write bytes to the current position in the ﬁle, and move the current position within the ﬁle. you do not normally see how the bytes are stored in sectors, clusters, and so forth. the mapping from logical to physical addresses is done by the ﬁle system, and sector-level buffering is done automatically by the disk controller.
when processing records in a disk ﬁle, the order of access can have a great effect on i/o time. a random access procedure processes records in an order independent of their logical order within the ﬁle. sequential access processes records in order of their logical appearance within the ﬁle. sequential processing requires less seek time if the physical layout of the disk ﬁle matches its logical layout, as would be expected if the ﬁle were created on a disk with a high percentage of free space.
• write(byte[] b): write some bytes at the current position in the ﬁle (overwriting the bytes already at that position). the current position moves forward as the bytes are written.
we now consider the problem of sorting collections of records too large to ﬁt in main memory. because the records must reside in peripheral or external memory, such sorting methods are called external sorts. this is in contrast to the internal sorts discussed in chapter 7 which assume that the records to be sorted are stored in main memory. sorting large collections of records is central to many applications, such as processing payrolls and other large business databases. as a consequence, many external sorting algorithms have been devised. years ago, sorting algorithm designers sought to optimize the use of speciﬁc hardware conﬁgurations, such as multiple tape or disk drives. most computing today is done on personal computers and low-end workstations with relatively powerful cpus, but only one or at most two disk drives. the techniques presented here are geared toward optimized processing on a single disk drive. this approach allows us to cover the most important issues in external sorting while skipping many less important machine-dependent details. readers who have a need to implement efﬁcient external sorting algorithms that take advantage of more sophisticated hardware conﬁgurations should consult the references in section 8.6.
when a collection of records is too large to ﬁt in main memory, the only practical way to sort it is to read some records from disk, do some rearranging, then write them back to disk. this process is repeated until the ﬁle is sorted, with each record read perhaps many times. given the high cost of disk i/o, it should come as no surprise that the primary goal of an external sorting algorithm is to minimize the amount of information that must be read from or written to disk. a certain amount of additional cpu processing can proﬁtably be traded for reduced disk access.
before discussing external sorting techniques, consider again the basic model for accessing information from disk. the ﬁle to be sorted is viewed by the programmer as a sequential series of ﬁxed-size blocks. assume (for simplicity) that each
figure 4.10 the linked list removal process. (a) the linked list before removing the node with value 10. (b) the linked list after removal. 1 marks the list node being removed. it is set to point to the element. 2 marks the next ﬁeld of the preceding list node, which is set to point to the node following the one being deleted.
the new operator is relatively expensive to use. section 12.3 discusses how a general-purpose memory manager can be implemented. the problem is that freestore routines must be capable of handling requests to and from free store with no particular pattern, as well as requests of vastly different sizes. garbage collection is also expensive.
most compilers today provide reasonable implementations for their free-store operators. however, the requirement that free-store managers be able to handle any pattern of new operations, combined with unpredictable freeing of space by the garbage collector, makes them inefﬁcient compared to what might be implemented for more controlled patterns of memory access.
the conditions under which list nodes are created and deleted in a linked list implementation allow the link class programmer to provide simple but efﬁcient memory management routines in place of the system-level free-store operators. instead of making repeated calls to new, the link class can handle its own freelist. a freelist holds those list nodes that are not currently being used. when a node is deleted from a linked list, it is placed at the head of the freelist. when a new element is to be added to a linked list, the freelist is checked to see if a list node is available. if so, the node is taken from the freelist. if the freelist is empty, the standard new operator must then be called.
then updates the prev ﬁeld of the node that the new node’s next ﬁeld points to. the existance of the header and tailer nodes mean that there are no special cases to be worried about when inserting into an empty list.
again, the link class constructor sets the element, prev, and next ﬁelds of the node when the new operator is executed, and it also sets the appropriate pointers in the previous last node and the list tailer node.
when all inserts and releases follow a simple pattern, such as last requested, ﬁrst released (stack order), or ﬁrst requested, ﬁrst released (queue order), memory management is fairly easy. we are concerned in this section with the general case where blocks of any size might be requested and released in any order. this is known as dynamic storage allocation. one example of dynamic storage allocation is managing free store for a compiler’s runtime environment, such as the systemlevel new operations in java. another example is managing main memory in a multitasking operating system. here, a program might require a certain amount of space, and the memory manager must keep track of which programs are using which parts of the main memory. yet another example is the ﬁle manager for a disk drive. when a disk ﬁle is created, expanded, or deleted, the ﬁle manager must allocate or deallocate disk space.
a block of memory or disk space managed in this way is sometimes referred to as a heap. the term “heap” is being used here in a different way than the heap data structure discussed in section 5.5. here “heap” refers to the memory controlled by a dynamic memory management scheme.
in the rest of this section, we ﬁrst study techniques for dynamic memory management. we then tackle the issue of what to do when no single block of memory in the memory pool is large enough to honor a given request.
12.3.1 dynamic storage allocation for the purpose of dynamic storage allocation, we view memory as a single array broken into a series of variable-size blocks, where some of the blocks are free and some are reserved or already allocated. the free blocks are linked together to form
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
natural extension of the bst to multiple dimensions. it is a binary tree whose splitting decisions alternate among the key dimensions. like the bst, the k-d tree uses object space decomposition. the pr quadtree uses key space decomposition and so is a form of trie. it is a binary tree only for one-dimensional keys (in which case it is a trie with a binary alphabet). for d dimensions it has 2d branches. thus, in two dimensions, the pr quadtree has four branches (hence the name “quadtree”), splitting space into four equal-sized quadrants at each branch. section 13.3.3 brieﬂy mentions two other variations on these data structures, the bintree and the point quadtree. these four structures cover all four combinations of object versus key space decomposition on the one hand, and multi-level binary versus 2d-way branching on the other. section 13.3.4 brieﬂy discusses spatial data structures for storing other types of spatial data.
the k-d tree is a modiﬁcation to the bst that allows for efﬁcient processing of multidimensional keys. the k-d tree differs from the bst in that each level of the k-d tree makes branching decisions based on a particular search key associated with that level, called the discriminator. we deﬁne the discriminator at level i to be i mod k for k dimensions. for example, assume that we store data organized by xy-coordinates. in this case, k is 2 (there are two coordinates), with the xcoordinate ﬁeld arbitrarily designated key 0, and the y-coordinate ﬁeld designated key 1. at each level, the discriminator alternates between x and y. thus, a node n at level 0 (the root) would have in its left subtree only nodes whose x values are less than nx (because x is search key 0, and 0 mod 2 = 0). the right subtree would contain nodes whose x values are greater than nx. a node m at level 1 would have in its left subtree only nodes whose y values are less than my. there is no restriction on the relative values of mx and the x values of m’s descendants, because branching decisions made at m are based solely on the y coordinate. figure 13.11 shows an example of how a collection of two-dimensional points would be stored in a k-d tree. in figure 13.11 the region containing the points is (arbitrarily) restricted to a 128 × 128 square, and each internal node splits the search space. each split is shown by a line, vertical for nodes with x discriminators and horizontal for nodes with y discriminators. the root node splits the space into two parts; its children further subdivide the space into smaller parts. the children’s split lines do not cross the root’s split line. thus, each node in the k-d tree helps to decompose the space into rectangles that show the extent of where nodes can fall in the various subtrees.
figure 13.14 searching in the k-d treeof figure 13.11. (a) the k-d tree decomposition for a 128× 128-unit region containing seven data points. (b) the k-d tree for the region of (a).
the circle. thus, search proceeds to the node containing record d. again, d is outside the search circle. because 25 + 25 < 69, no record in d’s right subtree could be within the search circle. thus, only d’s left subtree need be searched. this leads to comparing record e’s coordinates against the search circle. record e falls outside the search circle, and processing is complete. so we see that we only search subtrees whose rectangles fall within the search circle.
figure 13.15 shows an implementation for the region search method. when a node is visited, function incircle is used to check the euclidean distance between the node’s record and the query point. it is not enough to simply check that the differences between the x- and y-coordinates are each less than the query distances because the the record could still be outside the search circle, as illustrated by figure 13.13.
in the point-region quadtree (hereafter referred to as the pr quadtree) each node either has exactly four children or is a leaf. that is, the pr quadtree is a full fourway branching (4-ary) tree in shape. the pr quadtree represents a collection of data points in two dimensions by decomposing the region containing the data points into four equal quadrants, subquadrants, and so on, until no leaf node contains more than a single point. in other words, if a region contains zero or one data points, then it is represented by a pr quadtree consisting of a single leaf node. if the region con-
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
figure 6.14 converting from a forest of general trees to a single binary tree. each node stores pointers to its left child and right sibling. the tree roots are assumed to be siblings for the purpose of converting.
compared with the implementation illustrated by figure 6.13 which requires overhead of three pointers/node, the implementation of figure 6.15 only requires two pointers per node.
because each node of the general tree now contains a ﬁxed number of pointers, and because each function of the general tree adt can now be implemented efﬁciently, the dynamic “left-child/right-sibling” implementation is preferred to the other general tree implementations described in sections 6.3.1 to 6.3.3.
k-ary trees are trees whose internal nodes all have exactly k children. thus, a full binary tree is a 2-ary tree. the pr quadtree discussed in section 13.3 is an
next we consider a fundamentally different approach to implementing trees. the goal is to store a series of node values with the minimum information needed to reconstruct the tree structure. this approach, known as a sequential tree implementation, has the advantage of saving space because no pointers are stored. it has the disadvantage that accessing any node in the tree requires sequentially processing all nodes that appear before it in the node list. in other words, node access must start at the beginning of the node list, processing nodes sequentially in whatever order they are stored until the desired node is reached. thus, one primary virtue of the other implementations discussed in this section is lost: efﬁcient access (typically Θ(log n) time) to arbitrary nodes in the tree. sequential tree implementations are ideal for archiving trees on disk for later use because they save space, and the tree structure can be reconstructed as needed for later processing.
seqential tree implementations can also be used to serialize a tree structure. serialization is the process of storing an object as a series of bytes, typically so that the data structure can be transmitted between computers. this capability is important when using data structures in a distributed processing environment.
a sequential tree implementation stores the node values as they would be enumerated by a preorder traversal, along with sufﬁcient information to describe the tree’s shape. if the tree has restricted form, for example if it is a full binary tree, then less information about structure typically needs to be stored. a general tree, because it has the most ﬂexible shape, tends to require the most additional shape information. there are many possible sequential tree implementation schemes. we will begin by describing methods appropriate to binary trees, then generalize to an implementation appropriate to a general tree structure.
because every node of a binary tree is either a leaf or has two (possibly empty) children, we can take advantage of this fact to implicitly represent the tree’s structure. the most straightforward sequential tree implementation lists every node value as it would be enumerated by a preorder traversal. unfortunately, the node values alone do not provide enough information to recover the shape of the tree. in particular, as we read the series of node values, we do not know when a leaf node has been reached. however, we can treat all non-empty nodes as internal nodes with two (possibly empty) children. only null values will be interpreted as leaf nodes, and these can be listed explicitly. such an augmented node list provides enough information to recover the tree structure.
will use the “)” symbol) to indicate the end of a child list. all leaf nodes are followed by a “)” symbol because they have no children. a leaf node that is also the last child for its parent would indicate this by two or more successive “)” symbols.
note that f is followed by three “)” marks, because it is a leaf, the last node of b’s rightmost subtree, and the last node of r’s rightmost subtree.
note that this representation for serializing general trees cannot be used for binary trees. this is because a binary tree is not merely a restricted form of general tree with at most two children. every binary tree node has a left and a right child, though either or both might be empty. for example, the representation of example 6.8 cannot let us distinguish whether node d in figure 6.17 is the left or right child of node b.
6.6 further reading the expression log∗ n cited in section 6.2 is closely related to the inverse of ackermann’s function. for more information about ackermann’s function and the cost of path compression, see robert e. tarjan’s paper “on the efﬁciency of a good but not linear set merging algorithm” [tar75]. the article “data structures and algorithms for disjoint set union problems” by galil and italiano [gi91] covers many aspects of the equivalence class problem.
foundations of multidimensional and metric data structures by hanan samet [sam06] treats various implementations of tree structures in detail within the context of k-ary trees. samet covers sequential implementations as well as the linked and array implementations such as those described in this chapter and chapter 5. while these books are ostensibly concerned with spatial data structures, many of the concepts treated are relevant to anyone who must implement tree structures.
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
figure 13.14 searching in the k-d treeof figure 13.11. (a) the k-d tree decomposition for a 128× 128-unit region containing seven data points. (b) the k-d tree for the region of (a).
the circle. thus, search proceeds to the node containing record d. again, d is outside the search circle. because 25 + 25 < 69, no record in d’s right subtree could be within the search circle. thus, only d’s left subtree need be searched. this leads to comparing record e’s coordinates against the search circle. record e falls outside the search circle, and processing is complete. so we see that we only search subtrees whose rectangles fall within the search circle.
figure 13.15 shows an implementation for the region search method. when a node is visited, function incircle is used to check the euclidean distance between the node’s record and the query point. it is not enough to simply check that the differences between the x- and y-coordinates are each less than the query distances because the the record could still be outside the search circle, as illustrated by figure 13.13.
in the point-region quadtree (hereafter referred to as the pr quadtree) each node either has exactly four children or is a leaf. that is, the pr quadtree is a full fourway branching (4-ary) tree in shape. the pr quadtree represents a collection of data points in two dimensions by decomposing the region containing the data points into four equal quadrants, subquadrants, and so on, until no leaf node contains more than a single point. in other words, if a region contains zero or one data points, then it is represented by a pr quadtree consisting of a single leaf node. if the region con-
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
once again, if there are multiple records in the dictionary that match the desired key, there is no requirement as to which one actually is removed and returned. method size returns the number of elements in the dictionary.
the remaining method is removeany. this is similar to remove, except that it does not take a key value. instead, it removes an arbitrary record from the dictionary, if one exists. the purpose of this method is to allow a user the ability to iterate over all elements in the dictionary (of course, the dictionary will become empty in the process). without the removeany method, a dictionary user could not get at a record of the dictionary that he didn’t already know the key value for. with the removeany method, the user can process all records in the dictionary as shown in the following code fragment.
there are other approaches that might seem more natural for iterating though a dictionary, such as using a “ﬁrst” and a “next” function. but not all data structures that we want to use to implement a dictionary are able to do “ﬁrst” efﬁciently. for example, a hash table implementation cannot efﬁciently locate the record in the table with the smallest key value. by using removeany, we have a mechanism that provides generic access.
given a database storing records of a particular type, we might want to search for records in multiple ways. for example, we might want to store payroll records in one dictionary that allows us to search by id, and also store those same records in a second dictionary that allows us to search by name.
figure 4.28 shows the deﬁnition for a payroll record. the payroll class has multiple ﬁelds, each of which might be used as a search key. simply by varying the type for the key, and looking at the appropriate ﬁeld in each record, we can deﬁne a dictionary whose search key is the id ﬁeld, another whose search key is the name ﬁeld, and a third whose search key is the address ﬁeld of the payroll class. figure 4.29 shows an example where payroll objects are stored in two separate dictionaries, one using the id ﬁeld as the key and the other using the name ﬁeld as the key.
a fundamental operation on a dictionary is to ﬁnd a record that matches a given key. this raises the issue of how to extract the key from a record. this could be done by providing to the dictionary some class that knows how to extract the key from a record. unfortunately, this solution does not work in all situations, because there are record types for which it is not possible to write the key extraction
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
figure 17.4 the graph generated from boolean expression b = (x1+x2)·(x1+ x2 + x3)· (x1 + x3). literals from the ﬁrst clause are labeled c1, and literals from the second clause are labeled c2. there is an edge between every two pairs of vertices except when both vertices represent instances of literals from the same clause, or a negation of the same variable. thus, the vertex labeled c1 : y1 does not connect to the vertex labeled c1 : y2 (because they are literals in the same clause) or the vertex labeled c2 : y1 (because they are opposite values for the same variable).
there are several techniques to try. one approach is to run only small instances of the problem. for some problems, this is not acceptable. for example, traveling salesman grows so quickly that it cannot be run on modern computers for problem sizes much over 20 cities, which is not an unreasonable problem size for real-life situations. however, some other problems in np, while requiring exponential time, still grow slowly enough that they allow solutions for problems of a useful size.
consider the knapsack problem from section 16.2.1. we have a dynamic programming algorithm whose cost is Θ(nk) for n objects being ﬁt into a knapsack of size k. but it turns out that knapsack is np-complete. isn’t this a contradiction? not when we consider the relationship between n and k. how big is k? input size is typically o(n lg k) because the item sizes are smaller than k. thus, Θ(nk) is exponential on input size.
this dynamic programming algorithm is tractable if the numbers are “reasonable.” that is, we can successfully ﬁnd solutions to the problem when nk is in the thousands. such an algorithm is called a pseudo-polynomial time algorithm. this is different from traveling salesman which cannot possibly be solved when n = 100 given current algorithms. a second approach to handling np-complete problems is to solve a special instance of the problem that is not so hard. for example, many problems on graphs
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
17.21 consider a program named comp that takes two strings as input. it returns true if the strings are the same. it returns false if the strings are different. why doesn’t the argument that we used to prove that a program to solve the halting problem does not exist work to prove that comp does not exist?
17.1 implement vertex cover; that is, given graph g and integer k, answer the question of whether or not there is a vertex cover of size k or less. begin by using a brute-force algorithm that checks all possible sets of vertices of size k to ﬁnd an acceptable vertex cover, and measure the running time on a number of input graphs. then try to reduce the running time through the use of any heuristics you can think of. next, try to ﬁnd approximate solutions to the problem in the sense of ﬁnding the smallest set of vertices that forms a vertex cover.
17.3 implement an approximation of traveling salesman; that is, given a graph g with costs for all edges, ﬁnd the cheapest cycle that visits all vertices in g. try various heuristics to ﬁnd the best approximations for a wide variety of input graphs.
17.4 write a program that, given a positive integer n as input, prints out the collatz sequence for that number. what can you say about the types of integers that have long collatz sequences? what can you say about the length of the collatz sequence for various types of integers?
11.20 when can prim’s and kruskal’s algorithms yield different msts? 11.21 prove that, if the costs for the edges of graph g are distinct, then only one
11.23 consider the collection of edges selected by dijkstra’s algorithm as the shortest paths to the graph’s vertices from the start vertex. do these edges form a spanning tree (not necessarily of minimum cost)? do these edges form an mst? explain why or why not.
11.24 prove that a tree is a bipartite graph. 11.25 prove that any tree can be two-colored. 11.26 write an algorithm that deterimines if an arbitrary undirected graph is a bipartite graph. if the graph is bipartite, then your algorithm should also identify the vertices as to which of the two partitions each belongs to.
11.1 design a format for storing graphs in ﬁles. then implement two functions: one to read a graph from a ﬁle and the other to write a graph to a ﬁle. test your functions by implementing a complete mst program that reads an undirected graph in from a ﬁle, constructs the mst, and then writes to a second ﬁle the graph representing the mst.
11.2 an undirected graph need not explicitly store two separate directed edges to represent a single undirected edge. an alternative would be to store only a single undirected edge (i, j) to connect vertices i and j. however, what if the user asks for edge (j, i)? we can solve this problem by consistently storing the edge such that the lesser of i and j always comes ﬁrst. thus, if we have an edge connecting vertices 5 and 3, requests for edge (5, 3) and (3, 5) both map to (3, 5) because 3 < 5. looking at the adacency matrix, we notice that only the lower triangle of the array is used. thus we could cut the space required by the adjacency matrix from |v|2 positions to |v|(|v|− 1)/2 positions. read section 12.2 on triangular matrices. the reimplement the adjacency matrix representation of figure 11.6 to implement undirected graphs using a triangular array.
11.3 while the underlying implementation (whether adjacency matrix or adjacency list) is hidden behind the graph adt, these two implementations can have an impact on the efﬁciency of the resulting program. for dijkstra’s shortest paths algorithm, two different implementations were given in section 11.4.1 that provide diffent ways for determining the next closest vertex
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; long curr, prev, past; if ((n == 1) || (n == 2)) return 1; curr = prev = 1; for (int i=3; i<=n; i++) { // compute next value
2.12 write a recursive function to solve a generalization of the towers of hanoi problem where each ring may begin on any pole so long as no ring sits on top of a smaller ring.
this function makes progress towards the base case on every recursive call. in theory (that is, if double variables acted like true real numbers), would this function ever terminate for input val a nonzero number? in practice (an actual computer implementation), will it terminate?
2.16 the largest common factor (lcf) for two positive integers n and m is the largest integer that divides both n and m evenly. lcf(n, m) is at least one, and at most m, assuming that n ≥ m. over two thousand years ago, euclid provided an efﬁcient algorithm based on the observation that, when n mod m 6= 0, lcf(n, m) = gcd(m, n mod m). use this fact to write two algorithms to ﬁnd lcf for two positive integers. the ﬁrst version should compute the value iteratively. the second version should compute the value using recursion.
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
figure 8.4 an illustration of sector gaps within a track. each sector begins with a sector header containing the sector address and an error detection code for the contents of that sector. the sector header is followed by a small intrasector gap, followed in turn by the sector data. each sector is separated from the next sector by a larger intersector gap.
the track where the head is moving to, we will consider only two numbers. one is the track-to-track cost, or the minimum time necessary to move from a track to an adjacent track. this is appropriate when you want to analyze access times for ﬁles that are well placed on the disk. the second number is the average seek time for a random access. these two numbers are often provided by disk manufacturers. a typical example is the 120gb western digital wd caviar se serial ata drive. the manufacturer’s speciﬁcations indicate that the track-to-track time is 2.0 ms and the average seek time is 9.0 ms.
for many years, typical rotation speed for disk drives was 3600 rpm, or one rotation every 16.7 ms. most disk drives today have a rotation speed of 7200 rpm, or 8.3 ms per rotation. when reading a sector at random, you can expect that the disk will need to rotate halfway around to bring the desired sector under the i/o head, or 4.2 ms for a 7200-rpm disk drive.
once under the i/o head, a sector of data can be transferred as fast as that sector rotates under the head. if an entire track is to be read, then it will require one rotation (8.3 ms at 7200 rpm) to move the full track under the head. if only part of the track is to be read, then proportionately less time will be required. for example, if there are 16k sectors on the track and one sector is to be read, this will require a trivial amount of time (1/16k of a rotation).
example 8.1 assume that an older disk drive has a total (nominal) capacity of 16.8gb spread among 10 platters, yielding 1.68gb/platter. each platter contains 13,085 tracks and each track contains (after formating) 256 sectors of 512 bytes/sector. track-to-track seek time is 2.2 ms and average seek time for random access is 9.5 ms. assume the operating system maintains a cluster size of 8 sectors per cluster (4kb), yielding 32 clusters per track. the disk rotation rate is 5400 rpm (11.1 ms per rotation). based on this information we can estimate the cost for various ﬁle processing operations.
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
tion was ﬁrst accessed. typically it is more important to know how many times the information has been accessed, or how recently the information was last accessed. another approach is called “least frequently used” (lfu). lfu tracks the number of accesses to each buffer in the buffer pool. when a buffer must be reused, the buffer that has been accessed the fewest number of times is considered to contain the “least important” information, and so it is used next. lfu, while it seems intuitively reasonable, has many drawbacks. first, it is necessary to store and update access counts for each buffer. second, what was referenced many times in the past might now be irrelevant. thus, some time mechanism where counts “expire” is often desirable. this also avoids the problem of buffers that slowly build up big counts because they get used just often enough to avoid being replaced. an alternative is to maintain counts for all sectors ever read, not just the sectors currently in the buffer pool.
the third approach is called “least recently used” (lru). lru simply keeps the buffers in a list. whenever information in a buffer is accessed, this buffer is brought to the front of the list. when new information must be read, the buffer at the back of the list (the one least recently used) is taken and its “old” information is either discarded or written to disk, as appropriate. this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless special knowledge about information access patterns for an application suggests a special-purpose buffer management scheme.
the main purpose of a buffer pool is to minimize disk i/o. when the contents of a block are modiﬁed, we could write the updated information to disk immediately. but what if the block is changed again? if we write the block’s contents after every change, that might be a lot of disk write operations that can be avoided. it is more efﬁcient to wait until either the ﬁle is to be closed, or the buffer containing that block is ﬂushed from the buffer pool.
when a buffer’s contents are to be replaced in the buffer pool, we only want to write the contents to disk if it is necessary. that would be necessary only if the contents have changed since the block was read in originally from the ﬁle. the way to insure that the block is written when necessary, but only when necessary, is to maintain a boolean variable with the buffer (often referred to as the dirty bit) that is turned on when the buffer’s contents are modiﬁed by the client. at the time when the block is ﬂushed from the buffer pool, it is written to disk if and only if the dirty bit has been turned on.
modern operating systems support virtual memory. virtual memory is a technique that allows the programmer to pretend that there is more of the faster main memory (such as ram) than actually exists. this is done by means of a buffer pool
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
tion was ﬁrst accessed. typically it is more important to know how many times the information has been accessed, or how recently the information was last accessed. another approach is called “least frequently used” (lfu). lfu tracks the number of accesses to each buffer in the buffer pool. when a buffer must be reused, the buffer that has been accessed the fewest number of times is considered to contain the “least important” information, and so it is used next. lfu, while it seems intuitively reasonable, has many drawbacks. first, it is necessary to store and update access counts for each buffer. second, what was referenced many times in the past might now be irrelevant. thus, some time mechanism where counts “expire” is often desirable. this also avoids the problem of buffers that slowly build up big counts because they get used just often enough to avoid being replaced. an alternative is to maintain counts for all sectors ever read, not just the sectors currently in the buffer pool.
the third approach is called “least recently used” (lru). lru simply keeps the buffers in a list. whenever information in a buffer is accessed, this buffer is brought to the front of the list. when new information must be read, the buffer at the back of the list (the one least recently used) is taken and its “old” information is either discarded or written to disk, as appropriate. this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless special knowledge about information access patterns for an application suggests a special-purpose buffer management scheme.
the main purpose of a buffer pool is to minimize disk i/o. when the contents of a block are modiﬁed, we could write the updated information to disk immediately. but what if the block is changed again? if we write the block’s contents after every change, that might be a lot of disk write operations that can be avoided. it is more efﬁcient to wait until either the ﬁle is to be closed, or the buffer containing that block is ﬂushed from the buffer pool.
when a buffer’s contents are to be replaced in the buffer pool, we only want to write the contents to disk if it is necessary. that would be necessary only if the contents have changed since the block was read in originally from the ﬁle. the way to insure that the block is written when necessary, but only when necessary, is to maintain a boolean variable with the buffer (often referred to as the dirty bit) that is turned on when the buffer’s contents are modiﬁed by the client. at the time when the block is ﬂushed from the buffer pool, it is written to disk if and only if the dirty bit has been turned on.
modern operating systems support virtual memory. virtual memory is a technique that allows the programmer to pretend that there is more of the faster main memory (such as ram) than actually exists. this is done by means of a buffer pool
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
in most applications, we have no means of knowing in advance the frequencies of access for the data records. to complicate matters further, certain records might be accessed frequently for a brief period of time, and then rarely thereafter. thus, the probability of access for records might change over time (in most database systems, this is to be expected). self-organizing lists seek to solve both of these problems.
self-organizing lists modify the order of records within the list based on the actual pattern of record access. self-organizing lists use a heuristic for deciding how to to reorder the list. these heuristics are similar to the rules for managing buffer pools (see section 8.3). in fact, a buffer pool is a form of self-organizing list. ordering the buffer pool by expected frequency of access is a good strategy, because typically we must search the contents of the buffers to determine if the desired information is already in main memory. when ordered by frequency of access, the buffer at the end of the list will be the one most appropriate for reuse when a new page of information must be read. below are three traditional heuristics for managing self-organizing lists:
1. the most obvious way to keep a list ordered by frequency would be to store a count of accesses to each record and always maintain records in this order. this method will be referred to as count. count is similar to the least frequently used buffer replacement strategy. whenever a record is accessed, it might move toward the front of the list if its number of accesses becomes greater than a record preceding it. thus, count will store the records in the order of frequency that has actually occurred so far. besides requiring space for the access counts, count does not react well to changing frequency of access over time. once a record has been accessed a large number of times under the frequency count system, it will remain near the front of the list regardless of further access history.
2. bring a record to the front of the list when it is found, pushing all the other records back one position. this is analogous to the least recently used buffer replacement strategy and is called move-to-front. this heuristic is easy to implement if the records are stored using a linked list. when records are stored in an array, bringing a record forward from near the end of the array will result in a large number of records changing position. move-to-front’s cost is bounded in the sense that it requires at most twice the number of accesses required by the optimal static ordering for n records when at least
4. b-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. this improves space efﬁciency while reducing the typical number of disk fetches necessary during a search or update operation.
a b-tree of order m is deﬁned to have the following shape properties: • the root is either a leaf or has at least two children. • each internal node, except for the root, has between dm/2e and m children. • all leaves are at the same level in the tree, so the tree is always height bal-
the b-tree is a generalization of the 2-3 tree. put another way, a 2-3 tree is a b-tree of order three. normally, the size of a node in the b-tree is chosen to ﬁll a disk block. a b-tree node implementation typically allows 100 or more children. thus, a b-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk ﬁle). in a typical application, b-tree block i/o will be managed using a buffer pool and a block-replacement scheme such as lru (see section 8.3).
1. perform a binary search on the records in the current node. if a record with the search key is found, then return that record. if the current node is a leaf node and the key is not found, then report an unsuccessful search.
for example, consider a search for the record with key value 47 in the tree of figure 10.16. the root node is examined and the second (right) branch taken. after
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
typical car is driven about 12,000 miles per year. if gasoline costs $2/gallon, then the yearly gas bill is $1200 for the less efﬁcient car and $800 for the more efﬁcient car. if we ignore issues such as the payback that would be received if we invested $2000 in a bank, it would take 5 years to make up the difference in price. at this point, the buyer must decide if price is the only criterion and if a 5-year payback time is acceptable. naturally, a person who drives more will make up the difference more quickly, and changes in gasoline prices will also greatly affect the outcome.
example 2.20 when at the supermarket doing the week’s shopping, can you estimate about how much you will have to pay at the checkout? one simple way is to round the price of each item to the nearest dollar, and add this value to a mental running total as you put the item in your shopping cart. this will likely give an answer within a couple of dollars of the true total.
most of the topics covered in this chapter are considered part of discrete mathematics. an introduction to this ﬁeld is discrete mathematics with applications by susanna s. epp [epp04]. an advanced treatment of many mathematical topics useful to computer scientists is concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
see “technically speaking” from the february 1995 issue of ieee spectrum [sel95] for a discussion on the standard for indicating units of computer storage used in this book.
for more information on recursion, see thinking recursively by eric s. roberts [rob86]. to learn recursion properly, it is worth your while to learn the programming language lisp, even if you never intend to write a lisp program. in particular, friedman and felleisen’s the little lisper [ff89] is designed to teach you how to think recursively as well as teach you lisp. this book is entertaining reading as well.
a good book on writing mathematical proofs is daniel solow’s how to read and do proofs [sol90]. to improve your general mathematical problem-solving abilities, see the art and craft of problem solving by paul zeitz [zei07]. zeitz
figure 12.4 linked representation for the pure list of figure 12.1. the ﬁrst ﬁeld in each link node stores a tag bit. if the tag bit stores “+,” then the data ﬁeld stores an atom. if the tag bit stores “−,” then the data ﬁeld stores a pointer to a sublist.
figure 12.5 lisp-like linked representation for the cyclic multilist of figure 12.3. each link node stores two pointers. a pointer either points to an atom, or to another link node. link nodes are represented by two boxes, and atoms by circles.
represented by linked lists. pure lists can be represented as linked lists with an additional tag ﬁeld to indicate whether the node is an atom or a sublist. if it is a sublist, the data ﬁeld points to the ﬁrst element on the sublist. this is illustrated by figure 12.4.
another approach is to represent all list elements with link nodes storing two pointer ﬁelds, except for atoms. atoms just contain data. this is the system used by the programming language lisp. figure 12.5 illustrates this representation. either the pointer contains a tag bit to identify what it points to, or the object being pointed to stores a tag bit to identify itself. tags distinguish atoms from list nodes. this implementation can easily support reentrant and cyclic lists, because non-atoms can point to any other node.
while in java this would be no problem (due to auotmatic garbage collection), in some languages such as c++, this would be considered bad form because the original space allocated to p is lost as a result of the third assignment. this space cannot be used again by the program. such lost memory is referred to as garbage, also known as a memory leak. when no program variable points to a block of space, no future access to that space is possible. of course, if another variable had ﬁrst been assigned to point to p’s space, then reassigning p would not create garbage.
some programming languages take a different view towards garbage. in particular, the lisp programming language uses the multilist representation of figure 12.5, and all storage is in the form either of internal nodes with two pointers or atoms. figure 12.16 shows a typical collection of lisp structures, headed by variables named a, b, and c, along with a freelist.
in lisp, list objects are constantly being put together in various ways as temporary variables, and then all reference to them is lost when the object is no longer needed. thus, garbage is normal in lisp, and in fact cannot be avoided during normal processing. when lisp runs out of memory, it resorts to a garbage collection process to recover the space tied up in garbage. garbage collection consists of
examining the managed memory pool to determine which parts are still being used and which parts are garbage. in particular, a list is kept of all program variables, and any memory locations not reachable from one of these variables are considered to be garbage. when the garbage collector executes, all unused memory locations are placed in free store for future access. this approach has the advantage that it allows for easy collection of garbage. it has the disadvantage, from a user’s point of view, that every so often the system must halt while it performs garbage collection. for example, garbage collection is noticeable in the emacs text editor, which is normally implemented in lisp. occasionally the user must wait for a moment while the memory management system performs garbage collection.
the java programming language also makes use of garbage collection. as in lisp, it is common practice in java to allocate dynamic memory as needed, and to later drop all references to that memory. the garbage collector is responsible for reclaiming such unused space as necessary. this might require extra time when running the program, but it makes life considerably easier for the programmer. in contrast, many large applications written in c++ (even commonly used commercial software) contain memory leaks that will in time cause the program to fail.
several algorithms have been used for garbage collection. one is the reference count algorithm. here, every dynamically allocated memory block includes space for a count ﬁeld. whenever a pointer is directed to a memory block, the reference count is increased. whenever a pointer is directed away from a memory block, the reference count is decreased. if the count ever becomes zero, then the memory block is considered garbage and is immediately placed in free store. this approach has the advantage that it does not require an explicit garbage collection phase, because information is put in free store immediately when it becomes garbage.
the reference count algorithm is used by the unix ﬁle system. files can have multiple names, called links. the ﬁle system keeps a count of the number of links to each ﬁle. whenever a ﬁle is “deleted,” in actuality its link ﬁeld is simply reduced by one. if there is another link to the ﬁle, then no space is recovered by the ﬁle system. whenever the number of links goes to zero, the ﬁle’s space becomes available for reuse.
reference counts have several major disadvantages. first, a reference count must be maintained for each memory object. this works well when the objects are large, such as a ﬁle. however, it will not work well in a system such as lisp where the memory objects typically consist of two pointers or a value (an atom). another major problem occurs when garbage contains cycles. consider figure 12.17. here each memory object is pointed to once, but the collection of objects is still garbage because no pointer points to the collection. thus, reference counts only work when
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
1.18 imagine that you are a programmer who must write a function to sort an array of about 1000 integers from lowest value to highest value. write down at least ﬁve approaches to sorting the array. do not write algorithms in java or pseudocode. just write a sentence or two for each approach to describe how it would work.
1.19 think of an algorithm to ﬁnd the maximum value in an (unsorted) array. now, think of an algorithm to ﬁnd the second largest value in the array. which is harder to implement? which takes more time to run (as measured by the number of comparisons performed)? now, think of an algorithm to ﬁnd the third largest value. finally, think of an algorithm to ﬁnd the middle value. which is the most difﬁcult of these problems to solve?
1.20 an unsorted list of integers allows for constant-time insert simply by adding a new integer at the end of the list. unfortunately, searching for the integer with key value x requires a sequential search through the unsorted list until you ﬁnd x, which on average requires looking at half the list. on the other hand, a sorted array-based list of n integers can be searched in log n time by using a binary search. unfortunately, inserting a new integer requires a lot of time because many integers might be shifted in the array if we want to keep it sorted. how might data be organized to support both insertion and search in log n time?
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
techniques in common use today. the next section presents one such approach to assigning variable-length codes, called huffman coding. while it is not commonly used in its simplest form for ﬁle compression (there are better methods), huffman coding gives the ﬂavor of such coding schemes.
huffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character. thus, it is a variable-length code. if the estimated frequencies for letters match the actual frequency found in an encoded message, then the length of that message will typically be less than if a ﬁxed-length code had been used. the huffman code for each letter is derived from a full binary tree called the huffman coding tree, or simply the huffman tree. each leaf of the huffman tree corresponds to a letter, and we deﬁne the weight of the leaf node to be the weight (frequency) of its associated letter. the goal is to build a tree with the minimum external path weight. deﬁne the weighted path length of a leaf to be its weight times its depth. the binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. a letter with high weight should have low depth, so that it will count the least against the total path length. as a result, another letter might be pushed deeper in the tree if it has less weight.
the process of building the huffman tree for n letters is quite simple. first, create a collection of n initial huffman trees, each of which is a single leaf node containing one of the letters. put the n partial trees onto a min-heap (a priority queue) organized by weight (frequency). next, remove the ﬁrst two trees (the ones with lowest weight) from the heap. join these two trees together to create a new tree whose root has the two trees as children, and whose weight is the sum of the weights of the two trees. put this new tree back on the heap. this process is repeated until all of the partial huffman trees have been combined into one.
example 5.8 figure 5.25 illustrates part of the huffman tree construction process for the eight letters of figure 5.24. ranking d and l arbitrarily by alphabetical order, the letters are ordered by frequency as
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
simple lists and arrays are the right tool for the many applications. other situations require support for operations that cannot be implemented efﬁciently by the standard list representations of chapter 4. this chapter presents advanced implementations for lists and arrays that overcome some of the problems of simple linked list and contiguous array representations. a wide range of topics are covered, whose unifying thread is that the data structures are all list- or array-like. this chapter should also serve to reinforce the concept of logical representation versus physical implementation, as some of the “list” implementations have quite different organizations internally.
section 12.1 describes a series of representations for multilists, which are lists that may contain sublists. section 12.2 discusses representations for implementing sparse matrices, large matrices where most of the elements have zero values. section 12.3 discusses memory management techniques, which are essentially a way of allocating variable-length sections from a large array.
recall from chapter 4 that a list is a ﬁnite, ordered sequence of items of the form hx0, x1, ..., xn−1i where n ≥ 0. we can represent the empty list by null or hi. in chapter 4 we assumed that all list elements had the same data type. in this section, we extend the deﬁnition of lists to allow elements to be arbitrary in nature. in general, list elements are one of two types.
for convenience, we will adopt a convention of allowing sublists and atoms to be labeled, such as “l1:”. whenever a label is repeated, the element corresponding to that label will be substituted when we write out the list. thus, the bracket notation for the list of figure 12.2 could be written
a cyclic list is a list structure whose graph corresponds to any directed graph, possibly containing cycles. figure 12.3 illustrates such a list. labels are required to write this in bracket notation. here is the notation for the list of figure 12.3.
multilists can be implemented in a number of ways. most of these should be familiar from implementations suggested earlier in the book for list, tree, and graph data structures.
one simple approach is to use an array representation. this works well for chains with ﬁxed-length elements, equivalent to the simple array-based list of chapter 4. we can view nested sublists as variable-length elements. to use this approach, we require some indication of the beginning and end of each sublist. in essence, we are using a sequential tree implementation as discussed in section 6.5. this should be no surprise, because the pure list is equivalent to a general tree structure. unfortunately, as with any sequential representation, access to the nth sublist must be done sequentially from the beginning of the list.
because pure lists are equivalent to trees, we can also use linked allocation methods to support direct access to the list of children. simple linear lists are
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
in this example, each element of the list is stored in it, and passed in turn to the dosomething function. the loop terminates when the current position reaches the position equal to the length of the list.
the list class declaration presented here is just one of many possible interpretations for lists. figure 4.1 provides most of the operations that one naturally expects to perform on lists and serves to illustrate the issues relevant to implementing the list data structure. as an example of using the list adt, we can create a function to return true if there is an occurrence of a given integer in the list, and false otherwise. the find method needs no knowledge about the speciﬁc list implementation, just the list adt.
while this implementation for find is generic with respect to the list implementation, it is limited in its ability to handle different data types stored on the list. in particular, it only works when the description for the object being searched for (k in the function) is of the same type as the objects themselves. a more typical situation is that we are searching for a record that contains a key ﬁeld who’s value matches k. similar functions to ﬁnd and return a composite element based on a key value can be created using the list implementation, but to do so requires some agreement between the list adt and the find function on the concept of a key, and on how keys may be compared. this topic will be discussed in section 4.4.
4.1.1 array-based list implementation there are two standard approaches to implementing lists, the array-based list, and the linked list. this section discusses the array-based approach. the linked list is presented in section 4.1.2. time and space efﬁciency comparisons for the two are discussed in section 4.1.3.
for more discussion on choice of functions used to deﬁne the list adt, see the work of the reusable software research group from ohio state. their deﬁnition for the list adt can be found in [swh93]. more information about designing such classes can be found in [sw94].
4.2 show the list conﬁguration resulting from each series of list operations using the list adt of figure 4.1. assume that lists l1 and l2 are empty at the beginning of each series. show where the current position is in the list.
4.3 write a series of java statements that uses the list adt of figure 4.1 to create a list capable of holding twenty elements and which actually stores the list with the following conﬁguration:
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
class to be used with doubly linked lists. this code is a little longer than that for the singly linked list node implementation. not only do the doubly linked list nodes have an extra data member, but the constructors are a little more intelligent. when a new node is being added to a doubly linked list, the neighboring nodes in the list must also point back to the newly added node. thus, the constructors will check to see if the next or prev ﬁelds are non-null. when they are, the node being pointed to will have its appropriate link ﬁeld modiﬁed to point back to the new node being added to the list. this simpliﬁes the doubly linked list insert method.
figure 4.14 shows the implementation for the insert, append, remove, and prev doubly linked list methods. the class declaration and the remaining member functions for the doubly linked list class are nearly identical to those of figures 4.8.
the insert method is especially simple for our doubly linked list implementation, because most of the work is done by the node’s constructor. figure 4.15 shows the list before and after insertion of a node with value 10. the following line of code from figure 4.14 does the actual work. curr.setnext(new dlink<e>(it, curr.next(), curr));
the three parameters to the new operator allow the list node class constructor to set the element, prev, and next ﬁelds, respectively, for the new link node. the new operator returns a pointer to the newly created node. the node constructor also updates the next ﬁeld of the node that the new node’s prev ﬁeld points to. it
then updates the prev ﬁeld of the node that the new node’s next ﬁeld points to. the existance of the header and tailer nodes mean that there are no special cases to be worried about when inserting into an empty list.
again, the link class constructor sets the element, prev, and next ﬁelds of the node when the new operator is executed, and it also sets the appropriate pointers in the previous last node and the list tailer node.
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
in this example, each element of the list is stored in it, and passed in turn to the dosomething function. the loop terminates when the current position reaches the position equal to the length of the list.
the list class declaration presented here is just one of many possible interpretations for lists. figure 4.1 provides most of the operations that one naturally expects to perform on lists and serves to illustrate the issues relevant to implementing the list data structure. as an example of using the list adt, we can create a function to return true if there is an occurrence of a given integer in the list, and false otherwise. the find method needs no knowledge about the speciﬁc list implementation, just the list adt.
while this implementation for find is generic with respect to the list implementation, it is limited in its ability to handle different data types stored on the list. in particular, it only works when the description for the object being searched for (k in the function) is of the same type as the objects themselves. a more typical situation is that we are searching for a record that contains a key ﬁeld who’s value matches k. similar functions to ﬁnd and return a composite element based on a key value can be created using the list implementation, but to do so requires some agreement between the list adt and the find function on the concept of a key, and on how keys may be compared. this topic will be discussed in section 4.4.
4.1.1 array-based list implementation there are two standard approaches to implementing lists, the array-based list, and the linked list. this section discusses the array-based approach. the linked list is presented in section 4.1.2. time and space efﬁciency comparisons for the two are discussed in section 4.1.3.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
4.19 a common problem for compilers and text editors is to determine if the parentheses (or other brackets) in a string are balanced and properly nested. for example, the string “((())())()” contains properly nested pairs of parentheses, but the string “)()(” does not, and the string “())” does not contain properly matching parentheses. (a) give an algorithm that returns true if a string contains properly nested and balanced parentheses, and false otherwise. use a stack to keep track of the number of left parentheses seen so far. hint: at no time while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses.
(b) give an algorithm that returns the position in the string of the ﬁrst offending parenthesis if the string is not properly nested and balanced. that is, if an excess right parenthesis is found, return its position; if there are too many left parentheses, return the position of the ﬁrst excess left parenthesis. return −1 if the string is properly balanced and nested. use a stack to keep track of the number and positions of left parentheses seen so far.
4.20 imagine that you are designing an application where you need to perform the operations insert, delete maximum, and delete minimum. for this application, the cost of inserting is not important, because it can be done off-line prior to startup of the time-critical section, but the performance of the two deletion operations are critical. repeated deletions of either kind must work as fast as possible. suggest a data structure that can support this application, and justify your suggestion. what is the time complexity for each of the three key operations?
4.1 a deque (pronounced “deck”) is like a queue, except that items may be added and removed from both the front and the rear. write either an array-based or linked implementation for the deque.
4.2 one solution to the problem of running out of space for an array-based list implementation is to replace the array with a larger array whenever the original array overﬂows. a good rule that leads to an implementation that is both space and time efﬁcient is to double the current size of the array when there is an overﬂow. reimplement the array-based list class of figure 4.2 to support this array-doubling rule.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
4.5 in the linked list implementation presented in section 4.1.2, the current position is implemented using a pointer to the element ahead of the logical current node. the more “natural” approach might seem to be to have curr point directly to the node containing the current element. however, if this was done, then the pointer of the node preceeding the current one cannot be updated properly because there is no access to this node from curr. an alternative is to add a new node after the current element, copy the value of the current element to this new node, and then insert the new value into the old current node. (a) what happens if curr is at the end of the list already? is there still a way to make this work? is the resulting code simpler or more complex than the implementation of section 4.1.2?
4.6 add to the llist class implementation a member function to reverse the order of the elements on the list. your algorithm should run in Θ(n) time for a list of n elements.
4.7 write a function to merge two linked lists. the input lists have their elements in sorted order, from smallest to highest. the output list should also be sorted from highest to lowest. your algorithm should run in linear time on the length of the output list.
4.8 a circular linked list is one in which the next ﬁeld for the last link node of the list points to the ﬁrst link node of the list. this can be useful when you wish to have a relative positioning for elements, but no concept of an absolute ﬁrst or last position. (a) modify the code of figure 4.8 to implement circular singly linked lists. (b) modify the code of figure 4.14 to implement circular doubly linked
4.10 section 4.1.3 presents an equation for determining the break-even point for the space requirements of two implementations of lists. the variables are d, e, p , and n. what are the dimensional units for each variable? show that both sides of the equation balance in terms of their dimensional units.
4.11 use the space equation of section 4.1.3 to determine the break-even point for an array-based list and linked list implementation for lists when the sizes for the data ﬁeld, a pointer, and the array-based list’s array are as speciﬁed.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
the next step is to deﬁne the adt for a list object in terms of a set of operations on that object. we will use the java notation of an interface to formally deﬁne the list adt. interface list deﬁnes the member functions that any list implementation inheriting from it must support, along with their parameters and return types. we increase the ﬂexibility of the list adt by making it a template.
true to the notion of an adt, an abstract class does not specify how operations are implemented. two complete implementations are presented later in this section, both of which use the same list adt to deﬁne their operations, but they are considerably different in approaches and in their space/time tradeoffs.
figure 4.1 presents our list adt. class list is a generic of one parameter, named e. e serves as a placeholder for whatever element type the user would like to store in a list. the comments given in figure 4.1 describe precisely what each member function is intended to do. however, some explanation of the basic design is in order. given that we wish to support the concept of a sequence, with access to any position in the list, the need for many of the member functions such as insert and movetopos is clear. the key design decision embodied in this adt is support for the concept of a current position. for example, member movetostart sets the current position to be the ﬁrst element on the list, while methods next and prev move the current position to the next and previous elements, respectively. the intention is that any implementation for this adt support the concept of a current position.
given that our adt deﬁnes lists to have a current position, it is helpful to modify our list display notation to indicate this position. i will use a vertical bar, such as h20, 23 | 12, 15i to indicate the list of four elements, with the current position immediately to the right of the bar. given this conﬁguration, calling insert with value 10 will change the list to be h20, 23 | 10, 12, 15i.
if you examine figure 4.1, you should ﬁnd that the list member functions provided allow you to build a list with elements in any desired order, and to access any desired position in the list. you might have noticed that the clear method is not necessary, in that it could be implemented by means of the other member functions in the same asymptotic time. it is included merely for convenience.
method getvalue returns a reference to the current element. it is considered a violation of getvalue’s preconditions to ask for the value of a non-existent element (i.e., there must be an element at the current position). in our concrete list implementations, the java’s assert mechanism will be used to enforce such preconditions. in a commercial implementation, such violations would be best implemented by the java’s exception mechanism.
figure 4.9 the linked list insertion process. (a) the linked list before insertion. (b) the linked list after insertion. 1 marks the element ﬁeld of the new link node. 2 marks the next ﬁeld of the new link node, which is set to point to what used to be the current node (the node with value 12). 3 marks the next ﬁeld of the node preceding the current position. it used to point to the node containing 12; now it points to the new node containing 10.
method next simply moves curr one position toward the tail of the list, which takes Θ(1) time. method prev moves curr one position toward the head of the list, but its implementation is more difﬁcult. in a singly linked list, there is no pointer to the previous node. thus, the only alternative is to march down the list from the beginning until we reach the current node (being sure always to remember the node before it, because that is what we really want). this takes Θ(n) time in the average and worst cases. implementation of method movetopos is similar in that ﬁnding the ith position requires marching down i positions from the head of the list, taking Θ(i) time.
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
the third issue that users of the list implementations must face is primarily of concern when programming in languages that do not support automatic garbage collection. that is how to deal with the memory of the objects stored on the list when the list is deleted or the clear method is called. the list destructor and the clear method are problematic in that there is a potential that they will be misused, thus causing a memory leak. deleting listarray in the array-based implementation, or deleting a link node in the linked list implementation, might remove the only reference to an object, leaving its memory space inaccessible. unfortunately, there is no way for the list implementation to know whether a given object is pointed to in another part of the program or not. thus, the user of the list must be responsible for deleting these objects when that is appropriate.
the singly linked list presented in section 4.1.2 allows for direct access from a list node only to the next node in the list. a doubly linked list allows convenient access from a list node to the next node and also to the preceding node on the list. the doubly linked list node accomplishes this in the obvious way by storing two pointers: one to the node following it (as in the singly linked list), and a second pointer to the node preceding it. the most common reason to use a doubly linked list is because it is easier to implement than a singly linked list. while the code for the doubly linked implementation is a little longer than for the singly linked version, it tends to be a bit more “obvious” in its intention, and so easier to implement and debug. figure 4.12 illustrates the doubly linked list concept.
like our singly linked list implementation, the doubly linked list implementation makes use of a header node. we also add a tailer node to the end of the list. the tailer is similar to the header, in that it is a node that contains no value, and it always exists. when the doubly linked list is initialized, the header and tailer nodes are created. data member head points to the header node, and tail points to the tailer node. the purpose of these nodes is to simplify the insert, append, and remove methods by eliminating all need for special-case code when the list is empty.
whether a list implementation is doubly or singly linked should be hidden from the list class user. figure 4.13 shows the complete implementation for a link
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
4.5 in the linked list implementation presented in section 4.1.2, the current position is implemented using a pointer to the element ahead of the logical current node. the more “natural” approach might seem to be to have curr point directly to the node containing the current element. however, if this was done, then the pointer of the node preceeding the current one cannot be updated properly because there is no access to this node from curr. an alternative is to add a new node after the current element, copy the value of the current element to this new node, and then insert the new value into the old current node. (a) what happens if curr is at the end of the list already? is there still a way to make this work? is the resulting code simpler or more complex than the implementation of section 4.1.2?
4.6 add to the llist class implementation a member function to reverse the order of the elements on the list. your algorithm should run in Θ(n) time for a list of n elements.
4.7 write a function to merge two linked lists. the input lists have their elements in sorted order, from smallest to highest. the output list should also be sorted from highest to lowest. your algorithm should run in linear time on the length of the output list.
4.8 a circular linked list is one in which the next ﬁeld for the last link node of the list points to the ﬁrst link node of the list. this can be useful when you wish to have a relative positioning for elements, but no concept of an absolute ﬁrst or last position. (a) modify the code of figure 4.8 to implement circular singly linked lists. (b) modify the code of figure 4.14 to implement circular doubly linked
4.10 section 4.1.3 presents an equation for determining the break-even point for the space requirements of two implementations of lists. the variables are d, e, p , and n. what are the dimensional units for each variable? show that both sides of the equation balance in terms of their dimensional units.
4.11 use the space equation of section 4.1.3 to determine the break-even point for an array-based list and linked list implementation for lists when the sizes for the data ﬁeld, a pointer, and the array-based list’s array are as speciﬁed.
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
method remove (illustrated by figure 4.16) is straightforward, though the code is somewhat longer. first, the variable it is assigned the value being removed, note that we must separate the element, which is returned to the caller, from the link object. the following lines then adjust the pointers. e it = curr.next().element(); // remember value if (curr.next().next() != null) curr.next().next().setprev(curr); else tail = curr; // removed last object: curr.setnext(curr.next().next()); // remove from list
the ﬁrst line stores the value of the node being removed. the second line makes the next node’s prev pointer point to the left of the node being removed. if necessary, tail is updated. finally, the next ﬁeld of the node preceding the one being deleted is adjusted. the ﬁnal steps of method remove are to update the list length and to return the value of the deleted element.
the only disadvantage of the doubly linked list as compared to the singly linked list is the additional space used. the doubly linked list requires two pointers per node, and so in the implementation presented it requires twice as much overhead as the singly linked list.
example 4.1 there is a space-saving technique that can be employed to eliminate the additional space requirement, though it will complicate the implementation and be somewhat slower. thus, the technique is an example of a space/time tradeoff. it is based on observing that, if we store the sum of two values, then we can get either value back by subtracting the other. that is, if we store a + b in variable c, then b = c − a and a = c − b. of course, to recover one of the values out of the stored summation, the other value must be supplied. a pointer to the ﬁrst node in the list, along with the value of one of its two link ﬁelds, will allow access to all of the remaining nodes of the list in order. this is because the pointer to the node must be the same as the value of the following node’s prev pointer, as well as the previous node’s next pointer. it is possible to move down the list breaking apart the summed link ﬁelds as though you were opening a zipper. details for implementing this variation are left as an exercise.
the principle behind this technique is worth remembering, because it has many applications. the following code fragment will swap the contents of two variables without using a temporary variable (at the cost of three arithmetic operations). a = a + b; b = a - b; // now b contains original value of a a = a - b; // now a contains original value of b
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
cost for a series of insert/delete operations relatively inexpensive, even though an occasional insert/delete operation might be expensive. to analyze the cost of dynamic array operations, we need to use a technique known as amortized analysis, which is discussed in section 14.3.
list users must decide whether they wish to store a copy of any given element on each list that contains it. for small elements such as an integer, this makes sense. if the elements are payroll records, it might be desirable for the list node to store a pointer to the record rather than store a copy of the record itself. this change would allow multiple list nodes (or other data structures) to point to the same record, rather than make repeated copies of the record. not only might this save space, but it also means that a modiﬁcation to an element’s value is automatically reﬂected at all locations where it is referenced. the disadvantage of storing a pointer to each element is that the pointer requires space of its own. if elements are never duplicated, then this additional space adds unnecessary overhead. java most naturally stores references to objects, meaning that only a single copy of an object such as a payroll record will be maintained, even if it is on multiple lists.
whether it is more advantageous to use references to shared elements or separate copies depends on the intended application. in general, the larger the elements and the more they are duplicated, the more likely that references to shared elements is the better approach.
a second issue faced by implementors of a list class (or any other data structure that stores a collection of user-deﬁned data elements) is whether the elements stored are all required to be of the same type. this is known as homogeneity in a data structure. in some applications, the user would like to deﬁne the class of the data element that is stored on a given list, and then never permit objects of a different class to be stored on that same list. in other applications, the user would like to permit the objects stored on a single list to be of differing types.
for the list implementations presented in this section, the compiler requires that all objects stored on the list be of the same type. besides java generics, there are other techniques that implementors of a list class can use to ensure that the element type for a given list remains ﬁxed, while still permitting different lists to store different element types. one approach is to store an object of the appropriate type in the header node of the list (perhaps an object of the appropriate type is supplied as a parameter to the list constructor), and then check that all insert operations on that list use the same element type.
the third issue that users of the list implementations must face is primarily of concern when programming in languages that do not support automatic garbage collection. that is how to deal with the memory of the objects stored on the list when the list is deleted or the clear method is called. the list destructor and the clear method are problematic in that there is a potential that they will be misused, thus causing a memory leak. deleting listarray in the array-based implementation, or deleting a link node in the linked list implementation, might remove the only reference to an object, leaving its memory space inaccessible. unfortunately, there is no way for the list implementation to know whether a given object is pointed to in another part of the program or not. thus, the user of the list must be responsible for deleting these objects when that is appropriate.
the singly linked list presented in section 4.1.2 allows for direct access from a list node only to the next node in the list. a doubly linked list allows convenient access from a list node to the next node and also to the preceding node on the list. the doubly linked list node accomplishes this in the obvious way by storing two pointers: one to the node following it (as in the singly linked list), and a second pointer to the node preceding it. the most common reason to use a doubly linked list is because it is easier to implement than a singly linked list. while the code for the doubly linked implementation is a little longer than for the singly linked version, it tends to be a bit more “obvious” in its intention, and so easier to implement and debug. figure 4.12 illustrates the doubly linked list concept.
like our singly linked list implementation, the doubly linked list implementation makes use of a header node. we also add a tailer node to the end of the list. the tailer is similar to the header, in that it is a node that contains no value, and it always exists. when the doubly linked list is initialized, the header and tailer nodes are created. data member head points to the header node, and tail points to the tailer node. the purpose of these nodes is to simplify the insert, append, and remove methods by eliminating all need for special-case code when the list is empty.
whether a list implementation is doubly or singly linked should be hidden from the list class user. figure 4.13 shows the complete implementation for a link
figure 4.10 the linked list removal process. (a) the linked list before removing the node with value 10. (b) the linked list after removal. 1 marks the list node being removed. it is set to point to the element. 2 marks the next ﬁeld of the preceding list node, which is set to point to the node following the one being deleted.
the new operator is relatively expensive to use. section 12.3 discusses how a general-purpose memory manager can be implemented. the problem is that freestore routines must be capable of handling requests to and from free store with no particular pattern, as well as requests of vastly different sizes. garbage collection is also expensive.
most compilers today provide reasonable implementations for their free-store operators. however, the requirement that free-store managers be able to handle any pattern of new operations, combined with unpredictable freeing of space by the garbage collector, makes them inefﬁcient compared to what might be implemented for more controlled patterns of memory access.
the conditions under which list nodes are created and deleted in a linked list implementation allow the link class programmer to provide simple but efﬁcient memory management routines in place of the system-level free-store operators. instead of making repeated calls to new, the link class can handle its own freelist. a freelist holds those list nodes that are not currently being used. when a node is deleted from a linked list, it is placed at the head of the freelist. when a new element is to be added to a linked list, the freelist is checked to see if a list node is available. if so, the node is taken from the freelist. if the freelist is empty, the standard new operator must then be called.
we do not want to allocate freelists in advance for each potential node length. on the other hand, we need to make sure no more than one copy of the freelist for a given size is created.
we can modify our freelist free-store methods to request the appropriate freelist. this access function will search for the proper freelist. if it exists, that freelist is used. if not, that freelist is created.
now that you have seen two substantially different implementations for lists, it is natural to ask which is better. in particular, if you must implement a list for some task, which implementation should you choose?
array-based lists have the disadvantage that their size must be predetermined before the array can be allocated. array-based lists cannot grow beyond their predetermined size. whenever the list contains only a few elements, a substantial amount of space might be tied up in a largely empty array. linked lists have the advantage that they only need space for the objects actually on the list. there is no limit to the number of elements on a linked list, as long as there is free-store memory available. the amount of space required by a linked list is Θ(n), while the space required by the array-based list implementation is Ω(n), but can be greater. array-based lists have the advantage that there is no wasted space for an individual element. linked lists require that a pointer be added to every list node. if the element size is small, then the overhead for links can be a signiﬁcant fraction of the total storage. when the array for the array-based list is completely ﬁlled, there is no storage overhead. the array-based list will then be more space efﬁcient, by a constant factor, than the linked implementation.
a simple formula can be used to determine whether the array-based list or linked list implementation will be more space efﬁcient in a particular situation. call n the number of elements currently in the list, p the size of a pointer in storage units (typically four bytes), e the size of a data element in storage units (this could be anything, from one bit for a boolean variable on up to thousands of bytes or more for complex records), and d the maximum number of list elements that can be stored in the array. the amount of space required for the array-based list is de, regardless of the number of elements actually stored in the list at any given time. the amount of space required for the linked list is n(p + e). the smaller of these expressions for a given value n determines the more space-efﬁcient implementation for n elements. in general, the linked implementation requires less space than the array-based implementation when relatively few elements are in the list. conversely, the array-based implementation becomes more space efﬁcient when
when all inserts and releases follow a simple pattern, such as last requested, ﬁrst released (stack order), or ﬁrst requested, ﬁrst released (queue order), memory management is fairly easy. we are concerned in this section with the general case where blocks of any size might be requested and released in any order. this is known as dynamic storage allocation. one example of dynamic storage allocation is managing free store for a compiler’s runtime environment, such as the systemlevel new operations in java. another example is managing main memory in a multitasking operating system. here, a program might require a certain amount of space, and the memory manager must keep track of which programs are using which parts of the main memory. yet another example is the ﬁle manager for a disk drive. when a disk ﬁle is created, expanded, or deleted, the ﬁle manager must allocate or deallocate disk space.
a block of memory or disk space managed in this way is sometimes referred to as a heap. the term “heap” is being used here in a different way than the heap data structure discussed in section 5.5. here “heap” refers to the memory controlled by a dynamic memory management scheme.
in the rest of this section, we ﬁrst study techniques for dynamic memory management. we then tackle the issue of what to do when no single block of memory in the memory pool is large enough to honor a given request.
12.3.1 dynamic storage allocation for the purpose of dynamic storage allocation, we view memory as a single array broken into a series of variable-size blocks, where some of the blocks are free and some are reserved or already allocated. the free blocks are linked together to form
figure 12.17 garbage cycle example. all memory elements in the cycle have non-zero reference counts because each element has one pointer to it, even though the entire cycle is garbage.
another approach to garbage collection is the mark/sweep strategy. here, each memory object needs only a single mark bit rather than a reference counter ﬁeld. when free store is exhausted, a separate garbage collection phase takes place as follows.
1. clear all mark bits. 2. perform depth-ﬁrst search (dfs) following pointers from each variable on the system’s list of variables. each memory element encountered during the dfs has its mark bit turned on.
the advantages of the mark/sweep approach are that it needs less space than is necessary for reference counts, and it works for cycles. however, there is a major disadvantage. this is a “hidden” space requirement needed to do the processing. dfs is a recursive algorithm: either it must be implemented recursively, in which case the compiler’s runtime system maintains a stack, or else the memory manager can maintain its own stack. what happens if all memory is contained in a single linked list? then the depth of the recursion (or the size of the stack) is the number of memory cells! unfortunately, the space for the dfs stack must be available at the worst conceivable time, that is, when free memory has been exhausted.
fortunately, a clever technique allows dfs to be performed without requiring additional space for a stack. instead, the structure being traversed is used to hold the stack. at each step deeper into the traversal, instead of storing a pointer on the stack, we “borrow” the pointer being followed. this pointer is set to point back to the node we just came from in the previous step, as illustrated by figure 12.18. each borrowed pointer stores an additional bit to tell us whether we came down the left branch or the right branch of the link node being pointed to. at any given instant we have passed down only one path from the root, and we can follow the trail of pointers back up. as we return (equivalent to popping the recursion stack), we set the pointer back to its original position so as to return the structure to its
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
a key design decision for the linked list implementation is how to represent the current position. the most reasonable choices appear to be a pointer to the current element. but there is a big advantage to making curr point to the element preceding the current element.
figure 4.5(a) shows the list’s curr pointer pointing to the current element. the vertical line between the nodes containing 23 and 12 indicates the logical position of the current element. consider what happens if we wish to insert a new node with value 10 into the list. the result should be as shown in figure 4.5(b). however, there is a problem. to “splice” the list node containing the new element into the list, the list node storing 23 must have its next pointer changed to point to the new node. unfortunately, there is no convenient access to the node preceding the one pointed to by curr. however, if curr points directlyto the preceding element, there is no difﬁculty in adding a new element after curr. see exercise 4.5 for further discussion of why making curr point directly to the current element fails. unfortunately, we encounter a number of problems when the list is empty, or when the current position is at an end of the list. in particular, when the list is empty we have no element for head, tail, and curr to point to. one solution is to implement a number of special cases in the implementations for insert and remove. this increases code complexity, making it harder to understand, and thus increases the chance of introducing a programming bug.
these special cases can be eliminated by implementing linked lists with a special header node as the ﬁrst node of the list. this header node is a link node like any other, but its value is ignored and it is not considered to be an actual element of the list. the header node saves coding effort because we no longer need to consider special cases for empty lists or when the current position is at one end of the list. the cost of this simpliﬁcation is the space for the header node. however, there are space savings due to smaller code size, because statements to handle the special cases are omitted. in practice, this reduction in code size typically saves more space than that required for the header node, depending on the number of lists created. figure 4.6 shows the state of an initialized or empty list when using a header node. figure 4.7 shows the insertion example of figure 4.5 using a header node and the convention that curr points to the node preceding the current node.
figure 4.8 shows the deﬁnition for the linked list class, named llist. class llist inherits from the abstract list class and thus must implement all of class list’s member functions.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
@param size max number of elements list can contain. */ @suppresswarnings("unchecked") // generic array allocation alist(int size) { maxsize = size; listsize = curr = 0; listarray = (e[])new object[size];
figure 4.2 shows the array-based list implementation, named alist. alist inherits from abstract class list and so must implement all of the member functions of list.
// reset public void movetoend() { curr = listsize; } public void prev() { if (curr != 0) curr--; } // back up public void next() { if (curr < listsize) curr++; } // next
class alist’s private portion contains the data members for the array-based list. these include listarray, the array which holds the list elements. because listarray must be allocated at some ﬁxed size, the size of the array must be known when the list object is created. note that an optional parameter is declared for the alist constructor. with this parameter, the user can indicate the maximum number of elements permitted in the list. if no parameter is given, then it takes the value defaultlistsize, which is assumed to be a suitably deﬁned constant value.
because each list can have a differently sized array, each list must remember its maximum permitted size. data member maxsize serves this purpose. at any
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
figure 4.7 insertion using a header node, with curr pointing one node head of the current element. (a) linked list before insertion. the current node contains 12. (b) linked list after inserting the node containing 10.
inserting a new element is a three-step process. first, the new list node is created and the new element is stored into it. second, the next ﬁeld of the new list node is assigned to point to the current node (the one (after) the node that curr points to). third, the next ﬁeld of node pointed to by curr is assigned to point to the newly inserted node. the following line in the insert method of figure 4.8 actually does all three of these steps. curr.setnext(new link<e>(it, curr.next())); operator new creates the new link node and calls the constructor for the link class, which takes two parameters. the ﬁrst is the element. the second is the value to be placed in the list node’s next ﬁeld, in this case “curr.next().” figure 4.9 illustrates this three-step process. once the new node is added, tail is pushed forward if the new element was added to the end of the list. insertion requires Θ(1) time.
removing a node from the linked list requires only that the appropriate pointer be redirected around the node to be deleted. this memory eventually be reclaimed by the garbage collector. the following lines from the remove method of figure 4.8 do precisely this.
figure 4.9 the linked list insertion process. (a) the linked list before insertion. (b) the linked list after insertion. 1 marks the element ﬁeld of the new link node. 2 marks the next ﬁeld of the new link node, which is set to point to what used to be the current node (the node with value 12). 3 marks the next ﬁeld of the node preceding the current position. it used to point to the node containing 12; now it points to the new node containing 10.
method next simply moves curr one position toward the tail of the list, which takes Θ(1) time. method prev moves curr one position toward the head of the list, but its implementation is more difﬁcult. in a singly linked list, there is no pointer to the previous node. thus, the only alternative is to march down the list from the beginning until we reach the current node (being sure always to remember the node before it, because that is what we really want). this takes Θ(n) time in the average and worst cases. implementation of method movetopos is similar in that ﬁnding the ith position requires marching down i positions from the head of the list, taking Θ(i) time.
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
in this example, each element of the list is stored in it, and passed in turn to the dosomething function. the loop terminates when the current position reaches the position equal to the length of the list.
the list class declaration presented here is just one of many possible interpretations for lists. figure 4.1 provides most of the operations that one naturally expects to perform on lists and serves to illustrate the issues relevant to implementing the list data structure. as an example of using the list adt, we can create a function to return true if there is an occurrence of a given integer in the list, and false otherwise. the find method needs no knowledge about the speciﬁc list implementation, just the list adt.
while this implementation for find is generic with respect to the list implementation, it is limited in its ability to handle different data types stored on the list. in particular, it only works when the description for the object being searched for (k in the function) is of the same type as the objects themselves. a more typical situation is that we are searching for a record that contains a key ﬁeld who’s value matches k. similar functions to ﬁnd and return a composite element based on a key value can be created using the list implementation, but to do so requires some agreement between the list adt and the find function on the concept of a key, and on how keys may be compared. this topic will be discussed in section 4.4.
4.1.1 array-based list implementation there are two standard approaches to implementing lists, the array-based list, and the linked list. this section discusses the array-based approach. the linked list is presented in section 4.1.2. time and space efﬁciency comparisons for the two are discussed in section 4.1.3.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
figure 4.9 the linked list insertion process. (a) the linked list before insertion. (b) the linked list after insertion. 1 marks the element ﬁeld of the new link node. 2 marks the next ﬁeld of the new link node, which is set to point to what used to be the current node (the node with value 12). 3 marks the next ﬁeld of the node preceding the current position. it used to point to the node containing 12; now it points to the new node containing 10.
method next simply moves curr one position toward the tail of the list, which takes Θ(1) time. method prev moves curr one position toward the head of the list, but its implementation is more difﬁcult. in a singly linked list, there is no pointer to the previous node. thus, the only alternative is to march down the list from the beginning until we reach the current node (being sure always to remember the node before it, because that is what we really want). this takes Θ(n) time in the average and worst cases. implementation of method movetopos is similar in that ﬁnding the ith position requires marching down i positions from the head of the list, taking Θ(i) time.
whose records have the indicated secondary key value. figure 10.3 illustrates this approach. now there is no duplication of secondary key values, possibly yielding a considerable space savings. the cost of insertion and deletion is reduced, because only one row of the table need be adjusted. note that a new row is added to the array when a new secondary key value is added. this might lead to moving many records, but this will happen infrequently in applications suited to using this arrangement. a drawback to this approach is that the array must be of ﬁxed size, which imposes an upper limit on the number of primary keys that might be associated with a particular secondary key. furthermore, those secondary keys with fewer records than the width of the array will waste the remainder of their row. a better approach is to have a one-dimensional array of secondary key values, where each secondary key is associated with a linked list. this works well if the index is stored in main memory, but not so well when it is stored on disk because the linked list for a given key might be scattered across several disk blocks.
consider a large database of employee records. if the primary key is the employee’s id number and the secondary key is the employee’s name, then each record in the name index associates a name with one or more id numbers. the id number index in turn associates an id number with a unique pointer to the full record on disk. the secondary key index in such an organization is also known as an inverted list or inverted ﬁle. it is inverted in that searches work backwards from the secondary key to the primary key to the actual data record. it is called a list because each secondary key value has (conceptually) a list of primary keys associated with it. figure 10.4 illustrates this arrangement. here, we have last names as the secondary key. the primary key is a four-character unique identiﬁer.
figure 10.5 shows a better approach to storing inverted lists. an array of secondary key values is shown as before. associated with each secondary key is a pointer to an array of primary keys. the primary key array uses a linked-list implementation. this approach combines the storage for all of the secondary key lists into a single array, probably saving space. each record in this array consists of a primary key value and a pointer to the next element on the list. it is easy to insert and delete secondary keys from this array, making this a good implementation for disk-based inverted ﬁles.
how do we handle large databases that require frequent update? the main problem with the linear index is that it is a single, large array that does not lend itself to updates because a single update can require changing the position of every key
simple lists and arrays are the right tool for the many applications. other situations require support for operations that cannot be implemented efﬁciently by the standard list representations of chapter 4. this chapter presents advanced implementations for lists and arrays that overcome some of the problems of simple linked list and contiguous array representations. a wide range of topics are covered, whose unifying thread is that the data structures are all list- or array-like. this chapter should also serve to reinforce the concept of logical representation versus physical implementation, as some of the “list” implementations have quite different organizations internally.
section 12.1 describes a series of representations for multilists, which are lists that may contain sublists. section 12.2 discusses representations for implementing sparse matrices, large matrices where most of the elements have zero values. section 12.3 discusses memory management techniques, which are essentially a way of allocating variable-length sections from a large array.
recall from chapter 4 that a list is a ﬁnite, ordered sequence of items of the form hx0, x1, ..., xn−1i where n ≥ 0. we can represent the empty list by null or hi. in chapter 4 we assumed that all list elements had the same data type. in this section, we extend the deﬁnition of lists to allow elements to be arbitrary in nature. in general, list elements are one of two types.
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
a key design decision for the linked list implementation is how to represent the current position. the most reasonable choices appear to be a pointer to the current element. but there is a big advantage to making curr point to the element preceding the current element.
figure 4.5(a) shows the list’s curr pointer pointing to the current element. the vertical line between the nodes containing 23 and 12 indicates the logical position of the current element. consider what happens if we wish to insert a new node with value 10 into the list. the result should be as shown in figure 4.5(b). however, there is a problem. to “splice” the list node containing the new element into the list, the list node storing 23 must have its next pointer changed to point to the new node. unfortunately, there is no convenient access to the node preceding the one pointed to by curr. however, if curr points directlyto the preceding element, there is no difﬁculty in adding a new element after curr. see exercise 4.5 for further discussion of why making curr point directly to the current element fails. unfortunately, we encounter a number of problems when the list is empty, or when the current position is at an end of the list. in particular, when the list is empty we have no element for head, tail, and curr to point to. one solution is to implement a number of special cases in the implementations for insert and remove. this increases code complexity, making it harder to understand, and thus increases the chance of introducing a programming bug.
these special cases can be eliminated by implementing linked lists with a special header node as the ﬁrst node of the list. this header node is a link node like any other, but its value is ignored and it is not considered to be an actual element of the list. the header node saves coding effort because we no longer need to consider special cases for empty lists or when the current position is at one end of the list. the cost of this simpliﬁcation is the space for the header node. however, there are space savings due to smaller code size, because statements to handle the special cases are omitted. in practice, this reduction in code size typically saves more space than that required for the header node, depending on the number of lists created. figure 4.6 shows the state of an initialized or empty list when using a header node. figure 4.7 shows the insertion example of figure 4.5 using a header node and the convention that curr points to the node preceding the current node.
figure 4.8 shows the deﬁnition for the linked list class, named llist. class llist inherits from the abstract list class and thus must implement all of class list’s member functions.
class to be used with doubly linked lists. this code is a little longer than that for the singly linked list node implementation. not only do the doubly linked list nodes have an extra data member, but the constructors are a little more intelligent. when a new node is being added to a doubly linked list, the neighboring nodes in the list must also point back to the newly added node. thus, the constructors will check to see if the next or prev ﬁelds are non-null. when they are, the node being pointed to will have its appropriate link ﬁeld modiﬁed to point back to the new node being added to the list. this simpliﬁes the doubly linked list insert method.
figure 4.14 shows the implementation for the insert, append, remove, and prev doubly linked list methods. the class declaration and the remaining member functions for the doubly linked list class are nearly identical to those of figures 4.8.
the insert method is especially simple for our doubly linked list implementation, because most of the work is done by the node’s constructor. figure 4.15 shows the list before and after insertion of a node with value 10. the following line of code from figure 4.14 does the actual work. curr.setnext(new dlink<e>(it, curr.next(), curr));
the three parameters to the new operator allow the list node class constructor to set the element, prev, and next ﬁelds, respectively, for the new link node. the new operator returns a pointer to the newly created node. the node constructor also updates the next ﬁeld of the node that the new node’s prev ﬁeld points to. it
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
while lists are most commonly ordered by key value, this is not the only viable option. another approach to organizing lists to speed search is to order the records by expected frequency of access. while the beneﬁts might not be as great as when oganized by key value, the cost to organize by frequency of access is much cheaper, and thus is of use in some situations.
assume that we know, for each key ki, the probability pi that the record with key ki will be requested. assume also that the list is ordered so that the most frequently requested record is ﬁrst, then the next most frequently requested record, and so on. search in the list will be done sequentially, beginning with the ﬁrst position. over the course of many searches, the expected number of comparisons required for one search is
in other words, the cost to access the ﬁrst record is one (because one key value is looked at), and the probability of this occurring is p1. the cost to access the second record is two (because we must look at the ﬁrst and the second records’ key values), with probability p2, and so on. for n records, assuming that all searches are for records that actually exist, the probabilities p1 through pn must sum to one.
example 9.1 calculate the expected cost to search a list when each record has equal chance of being accessed (the classic sequential search through an unsorted list). setting pi = 1/n yields
this result matches our expectation that half the records will be accessed on average by normal sequential search. if the records truly have equal access probabilities, then ordering records by frequency yields no beneﬁt. we saw in section 9.1 the more general case where we must conside the probability (labeled p0) that the search key does not match that for any record in the array. in that case, in accordance with our general formula, we get
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
one approach to representing a sparse matrix is to concatenate (or otherwise combine) the row and column coordinates into a single value and use this as a key in a hash table. thus, if we want to know the value of a particular position in the matrix, we search the hash table for the appropriate key. if a value for this position is not found, it is assumed to be zero. this is an ideal approach when all queries to the matrix are in terms of access by speciﬁed position. however, if we wish to ﬁnd the ﬁrst non-zero element in a given row, or the next non-zero element below the current one in a given column, then the hash table requires us to check sequentially through all possible positions in some row or column.
another approach is to implement the matrix as an orthogonal list, as illustrated in figure 12.7. here we have a list of row headers, each of which contains a pointer to a list of matrix records. a second list of column headers also contains pointers to matrix records. each non-zero matrix element stores pointers to its non-zero neighbors in the row, both following and preceding it. each non-zero
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
given time the list actually holds some number of elements that can be less than the maximum allowed by the array. this value is stored in listsize. data member curr stores the current position. because listarray, maxsize, listsize, and curr are all declared to be private, they may only be accessed by methods of class alist.
class alist stores the list elements in contiguous array positions. array positions correspond to list positions. in other words, the element at position i in the list is stored at array cell i. the head of the list is always at position 0. this makes random access to any element in the list quite easy. given some position in the list, the value of the element in that position can be accessed directly. thus, access to any element using the movetopos method followed by the getvalue method takes Θ(1) time.
because the array-based list implementation is deﬁned to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. inserting or removing elements at the tail of the list is easy, and the append operation takes Θ(1) time. however, if we wish to insert an element at the head of the list, all elements currently in the list must shift one position toward the tail to make room, as illustrated by figure 4.3. this process takes Θ(n) time if there are n elements already in the list. if we wish to insert at position i within a list of n elements, then n − i elements must shift toward the tail. removing an element from the head of the list is similar in that all remaining elements in the array must shift toward the head by one position to ﬁll in the gap. to remove the element at position i, n − i − 1 elements must shift toward the head. in the average case, insertion or removal requires moving half of the elements, which is Θ(n).
most of the other member functions for class alist simply access the current list element or move the current position. such operations all require Θ(1) time. aside from insert and remove, the only other operations that might require more than constant time are the constructor, the destructor, and clear. these three member functions each make use of the system free-store operation new. as discussed further in section 4.1.2, system free-store operations can be expensive.
the second traditional approach to implementing lists makes use of pointers and is usually called a linked list. the linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed.
a linked list is made up of a series of objects, called the nodes of the list. because a list node is a distinct object (as opposed to simply a cell in an array), it is
figure 4.7 insertion using a header node, with curr pointing one node head of the current element. (a) linked list before insertion. the current node contains 12. (b) linked list after inserting the node containing 10.
inserting a new element is a three-step process. first, the new list node is created and the new element is stored into it. second, the next ﬁeld of the new list node is assigned to point to the current node (the one (after) the node that curr points to). third, the next ﬁeld of node pointed to by curr is assigned to point to the newly inserted node. the following line in the insert method of figure 4.8 actually does all three of these steps. curr.setnext(new link<e>(it, curr.next())); operator new creates the new link node and calls the constructor for the link class, which takes two parameters. the ﬁrst is the element. the second is the value to be placed in the list node’s next ﬁeld, in this case “curr.next().” figure 4.9 illustrates this three-step process. once the new node is added, tail is pushed forward if the new element was added to the end of the list. insertion requires Θ(1) time.
removing a node from the linked list requires only that the appropriate pointer be redirected around the node to be deleted. this memory eventually be reclaimed by the garbage collector. the following lines from the remove method of figure 4.8 do precisely this.
figure 4.9 the linked list insertion process. (a) the linked list before insertion. (b) the linked list after insertion. 1 marks the element ﬁeld of the new link node. 2 marks the next ﬁeld of the new link node, which is set to point to what used to be the current node (the node with value 12). 3 marks the next ﬁeld of the node preceding the current position. it used to point to the node containing 12; now it points to the new node containing 10.
method next simply moves curr one position toward the tail of the list, which takes Θ(1) time. method prev moves curr one position toward the head of the list, but its implementation is more difﬁcult. in a singly linked list, there is no pointer to the previous node. thus, the only alternative is to march down the list from the beginning until we reach the current node (being sure always to remember the node before it, because that is what we really want). this takes Θ(n) time in the average and worst cases. implementation of method movetopos is similar in that ﬁnding the ith position requires marching down i positions from the head of the list, taking Θ(i) time.
figure 4.10 the linked list removal process. (a) the linked list before removing the node with value 10. (b) the linked list after removal. 1 marks the list node being removed. it is set to point to the element. 2 marks the next ﬁeld of the preceding list node, which is set to point to the node following the one being deleted.
the new operator is relatively expensive to use. section 12.3 discusses how a general-purpose memory manager can be implemented. the problem is that freestore routines must be capable of handling requests to and from free store with no particular pattern, as well as requests of vastly different sizes. garbage collection is also expensive.
most compilers today provide reasonable implementations for their free-store operators. however, the requirement that free-store managers be able to handle any pattern of new operations, combined with unpredictable freeing of space by the garbage collector, makes them inefﬁcient compared to what might be implemented for more controlled patterns of memory access.
the conditions under which list nodes are created and deleted in a linked list implementation allow the link class programmer to provide simple but efﬁcient memory management routines in place of the system-level free-store operators. instead of making repeated calls to new, the link class can handle its own freelist. a freelist holds those list nodes that are not currently being used. when a node is deleted from a linked list, it is placed at the head of the freelist. when a new element is to be added to a linked list, the freelist is checked to see if a list node is available. if so, the node is taken from the freelist. if the freelist is empty, the standard new operator must then be called.
class to be used with doubly linked lists. this code is a little longer than that for the singly linked list node implementation. not only do the doubly linked list nodes have an extra data member, but the constructors are a little more intelligent. when a new node is being added to a doubly linked list, the neighboring nodes in the list must also point back to the newly added node. thus, the constructors will check to see if the next or prev ﬁelds are non-null. when they are, the node being pointed to will have its appropriate link ﬁeld modiﬁed to point back to the new node being added to the list. this simpliﬁes the doubly linked list insert method.
figure 4.14 shows the implementation for the insert, append, remove, and prev doubly linked list methods. the class declaration and the remaining member functions for the doubly linked list class are nearly identical to those of figures 4.8.
the insert method is especially simple for our doubly linked list implementation, because most of the work is done by the node’s constructor. figure 4.15 shows the list before and after insertion of a node with value 10. the following line of code from figure 4.14 does the actual work. curr.setnext(new dlink<e>(it, curr.next(), curr));
the three parameters to the new operator allow the list node class constructor to set the element, prev, and next ﬁelds, respectively, for the new link node. the new operator returns a pointer to the newly created node. the node constructor also updates the next ﬁeld of the node that the new node’s prev ﬁeld points to. it
figure 4.15 insertion for doubly linked lists. the labels 1 , 2 , and 3 correspond to assignments done by the linked list node constructor. 4 marks the assignment to curr->next. 5 marks the assignment to the prev pointer of the node following the newly inserted node.
figure 4.16 doubly linked list removal. pointer ltemp is set to point to the current node. then the nodes to either side of the node being removed have their pointers adjusted.
method remove (illustrated by figure 4.16) is straightforward, though the code is somewhat longer. first, the variable it is assigned the value being removed, note that we must separate the element, which is returned to the caller, from the link object. the following lines then adjust the pointers. e it = curr.next().element(); // remember value if (curr.next().next() != null) curr.next().next().setprev(curr); else tail = curr; // removed last object: curr.setnext(curr.next().next()); // remove from list
the ﬁrst line stores the value of the node being removed. the second line makes the next node’s prev pointer point to the left of the node being removed. if necessary, tail is updated. finally, the next ﬁeld of the node preceding the one being deleted is adjusted. the ﬁnal steps of method remove are to update the list length and to return the value of the deleted element.
the only disadvantage of the doubly linked list as compared to the singly linked list is the additional space used. the doubly linked list requires two pointers per node, and so in the implementation presented it requires twice as much overhead as the singly linked list.
example 4.1 there is a space-saving technique that can be employed to eliminate the additional space requirement, though it will complicate the implementation and be somewhat slower. thus, the technique is an example of a space/time tradeoff. it is based on observing that, if we store the sum of two values, then we can get either value back by subtracting the other. that is, if we store a + b in variable c, then b = c − a and a = c − b. of course, to recover one of the values out of the stored summation, the other value must be supplied. a pointer to the ﬁrst node in the list, along with the value of one of its two link ﬁelds, will allow access to all of the remaining nodes of the list in order. this is because the pointer to the node must be the same as the value of the following node’s prev pointer, as well as the previous node’s next pointer. it is possible to move down the list breaking apart the summed link ﬁelds as though you were opening a zipper. details for implementing this variation are left as an exercise.
the principle behind this technique is worth remembering, because it has many applications. the following code fragment will swap the contents of two variables without using a temporary variable (at the cost of three arithmetic operations). a = a + b; b = a - b; // now b contains original value of a a = a - b; // now a contains original value of b
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
occupied the slot but does so no longer. if a tombstone is encountered when searching through a probe sequence, the search procedure is to continue with the search. when a tombstone is encountered during insertion, that slot can be used to store the new record. however, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. however, the new record would actually be inserted into the slot of the ﬁrst tombstone encountered. the use of tombstones allows searches to work correctly and allows reuse of deleted slots. however, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. this will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. a typical database application will ﬁrst load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. after the table is loaded with the initial collection of records, the ﬁrst few deletions will lengthen the average probe sequence distance for records (it will add tombstones). over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by ﬁlling in tombstone slots. for example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). after a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. this seems like a small increase, but it is three times longer on average beyond the home position than before deletions.
two possible solutions to this problem are 1. do a local reorganization upon deletion to try to shorten the average path length. for example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove a key from its probe sequence). this will not work for all collision resolution policies. 2. periodically rehash the table by reinserting all records into a new hash table. not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions.
for a comparison of the efﬁciencies for various self-organizing techniques, see bentley and mcgeoch, “amortized analysis of self-organizing sequential search heuristics” [bm85]. the text compression example of section 9.2 comes from
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
be sure to indicate how you are using h1 and h2 to do the hashing. function rev(k) reverses the decimal digits of k, for example, rev(37) = 73; rev(7) = 7. h1(k) = k mod 13. h2(k) = (rev(k + 1) mod 11). keys: 2, 8, 31, 20, 19, 18, 53, 27.
9.19 write an algorithm for a deletion function for hash tables that replaces the record with a special value indicating a tombstone. modify the functions hashinsert and hashsearch to work correctly with tombstones.
analyze what will happen if this permutation is used by an implementation of pseudo-random probing on a hash table of size seven. will this permutation solve the problem of primary clustering? what does this say about selecting a permutation for use when implementing pseudo-random probing?
9.1 implement a binary search and the quadratic binary search of section 9.1. run your implementations over a large range of problem sizes, timing the results for each algorithm. graph and compare these timing results.
9.2 implement the three self-organizing list heuristics count, move-to-front, and transpose. compare the cost for running the three heuristics on various input data. the cost metric should be the total number of comparisons required when searching the list. it is important to compare the heuristics using input data for which self-organizing lists are reasonable, that is, on frequency distributions that are uneven. one good approach is to read text ﬁles. the list should store individual words in the text ﬁle. begin with an empty list, as was done for the text compression example of section 9.2. each time a word is encountered in the text ﬁle, search for it in the self-organizing list. if the word is found, reorder the list as appropriate. if the word is not in the list, add it to the end of the list and then reorder as appropriate.
9.3 implement the text compression system described in section 9.2. 9.4 implement a system for managing document retrieval. your system should have the ability to insert (abstract references to) documents into the system, associate keywords with a given document, and to search for documents with speciﬁed keywords.
o(m log n) time for a tree of n nodes whenever m ≥ n. thus, a single insert or search operation could take o(n) time. however, m such operations are guaranteed to require a total of o(m log n) time, for an average cost of o(log n) per access operation. this is a desirable performance guarantee for any search-tree structure. unlike the avl tree, the splay tree is not guaranteed to be height balanced. what is guaranteed is that the total cost of the entire series of accesses will be cheap. ultimately, it is the cost of the series of operations that matters, not whether the tree is balanced. maintaining balance is really done only for the sake of reaching this time efﬁciency goal.
the splay tree access functions operate in a manner reminiscent of the moveto-front rule for self-organizing lists from section 9.2, and of the path compression technique for managing parent-pointer trees from section 6.2. these access functions tend to make the tree more balanced, but an individual access will not necessarily result in a more balanced tree.
whenever a node s is accessed (e.g., when s is inserted, deleted, or is the goal of a search), the splay tree performs a process called splaying. splaying moves s to the root of the bst. when s is being deleted, splaying moves the parent of s to the root. as in the avl tree, a splay of node s consists of a series of rotations. a rotation moves s higher in the tree by adjusting its position with respect to its parent and grandparent. a side effect of the rotations is a tendency to balance the tree. there are three types of rotation.
a single rotation is performed only if s is a child of the root node. the single rotation is illustrated by figure 13.7. it basically switches s with its parent in a way that retains the bst property. while figure 13.7 is slightly different from figure 13.5, in fact the splay tree single rotation is identical to the avl tree single rotation.
unlike the avl tree, the splay tree requires two types of double rotation. double rotations involve s, its parent (call it p), and s’s grandparent (call it g). the effect of a double rotation is to move s up two levels in the tree.
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
the third issue that users of the list implementations must face is primarily of concern when programming in languages that do not support automatic garbage collection. that is how to deal with the memory of the objects stored on the list when the list is deleted or the clear method is called. the list destructor and the clear method are problematic in that there is a potential that they will be misused, thus causing a memory leak. deleting listarray in the array-based implementation, or deleting a link node in the linked list implementation, might remove the only reference to an object, leaving its memory space inaccessible. unfortunately, there is no way for the list implementation to know whether a given object is pointed to in another part of the program or not. thus, the user of the list must be responsible for deleting these objects when that is appropriate.
the singly linked list presented in section 4.1.2 allows for direct access from a list node only to the next node in the list. a doubly linked list allows convenient access from a list node to the next node and also to the preceding node on the list. the doubly linked list node accomplishes this in the obvious way by storing two pointers: one to the node following it (as in the singly linked list), and a second pointer to the node preceding it. the most common reason to use a doubly linked list is because it is easier to implement than a singly linked list. while the code for the doubly linked implementation is a little longer than for the singly linked version, it tends to be a bit more “obvious” in its intention, and so easier to implement and debug. figure 4.12 illustrates the doubly linked list concept.
like our singly linked list implementation, the doubly linked list implementation makes use of a header node. we also add a tailer node to the end of the list. the tailer is similar to the header, in that it is a node that contains no value, and it always exists. when the doubly linked list is initialized, the header and tailer nodes are created. data member head points to the header node, and tail points to the tailer node. the purpose of these nodes is to simplify the insert, append, and remove methods by eliminating all need for special-case code when the list is empty.
whether a list implementation is doubly or singly linked should be hidden from the list class user. figure 4.13 shows the complete implementation for a link
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
examining class ualdict (ual stands for “unsorted array-based list), we can easily see that insert is a constant time operation, because it simply inserts the new record at the end of the list. however, find, and remove both require Θ(n) time in the average and worst cases, because we need to do a sequential search. method remove in particular must touch every record in the list, because once the desired record is found, the remaining records must be shifted down in the list to ﬁll the gap. method removeany removes the last record from the list, so this is a constant-time operation.
as an alternative, we could implement the dictionary using a linked list. the implementation would be quite similar to that shown in figure 4.31, and the cost of the functions should be the same asymptotically.
another alternative would be to implement the dictionary with a sorted list. the advantage of this approach would be that we might be able to speed up the find operation by using a binary search. to do so, ﬁrst we must deﬁne a variation on the list adt to support sorted lists. a sorted list is somewhat different from an unsorted list in that it cannot permit the user to control where elements get inserted. thus, the insert method must be quite different in a sorted list than in an unsorted list. likewise, the user cannot be permitted to append elements onto the list. for these reasons, a sorted list cannot be implemented with straightforward inheritance from the list adt.
the cost for find in a sorted list is Θ(log n) for a list of length n. this is a great improvement over the cost of find in an unsorted list. unfortunately, the cost of insert changes from constant time in the unsorted list to Θ(n) time in the sorted list. whether the sorted list implementation for the dictionary adt is more or less efﬁcient than the unsorted list implementation depends on the relative number of insert and find operations to be performed. if many more find operations than insert operations are used, then it might be worth using a sorted list to implement the dictionary. in both cases, remove requires Θ(n) time in the worst and average cases. even if we used binary search to cut down on the time to ﬁnd the record prior to removal, we would still need to shift down the remaining records in the list to ﬁll the gap left by the remove operation.
we do not want to allocate freelists in advance for each potential node length. on the other hand, we need to make sure no more than one copy of the freelist for a given size is created.
we can modify our freelist free-store methods to request the appropriate freelist. this access function will search for the proper freelist. if it exists, that freelist is used. if not, that freelist is created.
now that you have seen two substantially different implementations for lists, it is natural to ask which is better. in particular, if you must implement a list for some task, which implementation should you choose?
array-based lists have the disadvantage that their size must be predetermined before the array can be allocated. array-based lists cannot grow beyond their predetermined size. whenever the list contains only a few elements, a substantial amount of space might be tied up in a largely empty array. linked lists have the advantage that they only need space for the objects actually on the list. there is no limit to the number of elements on a linked list, as long as there is free-store memory available. the amount of space required by a linked list is Θ(n), while the space required by the array-based list implementation is Ω(n), but can be greater. array-based lists have the advantage that there is no wasted space for an individual element. linked lists require that a pointer be added to every list node. if the element size is small, then the overhead for links can be a signiﬁcant fraction of the total storage. when the array for the array-based list is completely ﬁlled, there is no storage overhead. the array-based list will then be more space efﬁcient, by a constant factor, than the linked implementation.
a simple formula can be used to determine whether the array-based list or linked list implementation will be more space efﬁcient in a particular situation. call n the number of elements currently in the list, p the size of a pointer in storage units (typically four bytes), e the size of a data element in storage units (this could be anything, from one bit for a boolean variable on up to thousands of bytes or more for complex records), and d the maximum number of list elements that can be stored in the array. the amount of space required for the array-based list is de, regardless of the number of elements actually stored in the list at any given time. the amount of space required for the linked list is n(p + e). the smaller of these expressions for a given value n determines the more space-efﬁcient implementation for n elements. in general, the linked implementation requires less space than the array-based implementation when relatively few elements are in the list. conversely, the array-based implementation becomes more space efﬁcient when
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
4.5 in the linked list implementation presented in section 4.1.2, the current position is implemented using a pointer to the element ahead of the logical current node. the more “natural” approach might seem to be to have curr point directly to the node containing the current element. however, if this was done, then the pointer of the node preceeding the current one cannot be updated properly because there is no access to this node from curr. an alternative is to add a new node after the current element, copy the value of the current element to this new node, and then insert the new value into the old current node. (a) what happens if curr is at the end of the list already? is there still a way to make this work? is the resulting code simpler or more complex than the implementation of section 4.1.2?
4.6 add to the llist class implementation a member function to reverse the order of the elements on the list. your algorithm should run in Θ(n) time for a list of n elements.
4.7 write a function to merge two linked lists. the input lists have their elements in sorted order, from smallest to highest. the output list should also be sorted from highest to lowest. your algorithm should run in linear time on the length of the output list.
4.8 a circular linked list is one in which the next ﬁeld for the last link node of the list points to the ﬁrst link node of the list. this can be useful when you wish to have a relative positioning for elements, but no concept of an absolute ﬁrst or last position. (a) modify the code of figure 4.8 to implement circular singly linked lists. (b) modify the code of figure 4.14 to implement circular doubly linked
4.10 section 4.1.3 presents an equation for determining the break-even point for the space requirements of two implementations of lists. the variables are d, e, p , and n. what are the dimensional units for each variable? show that both sides of the equation balance in terms of their dimensional units.
4.11 use the space equation of section 4.1.3 to determine the break-even point for an array-based list and linked list implementation for lists when the sizes for the data ﬁeld, a pointer, and the array-based list’s array are as speciﬁed.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
given the speciﬁcations of the disk drive from example 8.1, we ﬁnd that it takes about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. it takes about 9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data. this is a good savings (slightly over half the time), but less than 1% of the data on the track are read. if we want to read only a single byte, it would save us effectively no time over that required to read an entire sector. for this reason, nearly all disk drives automatically read or write an entire sector’s worth of information whenever the disk is accessed, even when only one byte of information is requested.
once a sector is read, its information is stored in main memory. this is known as buffering or caching the information. if the next disk request is to that same sector, then it is not necessary to read from disk again because the information is already stored in main memory. buffering is an example of one method for minimizing disk accesses mentioned at the beginning of the chapter: bring off additional information from disk to satisfy future requests. if information from ﬁles were accessed at random, then the chance that two consecutive disk requests are to the same sector would be low. however, in practice most disk requests are close to the location (in the logical ﬁle at least) of the previous request. this means that the probability of the next request “hitting the cache” is much higher than chance would indicate.
this principle explains one reason why average access times for new disk drives are lower than in the past. not only is the hardware faster, but information is also now stored using better algorithms and larger caches that minimize the number of times information needs to be fetched from disk. this same concept is also used to store parts of programs in faster memory within the cpu, using the cpu cache that is prevalent in modern microprocessors.
sector-level buffering is normally provided by the operating system and is often built directly into the disk drive controller hardware. most operating systems maintain at least two buffers, one for input and one for output. consider what would happen if there were only one buffer during a byte-by-byte copy operation. the sector containing the ﬁrst byte would be read into the i/o buffer. the output operation would need to destroy the contents of the single i/o buffer to write this byte. then the buffer would need to be ﬁlled again from disk for the second byte, only to be destroyed during output. the simple solution to this problem is to keep one buffer for input, and a second for output.
most disk drive controllers operate independently from the cpu once an i/o request is received. this is useful because the cpu can typically execute millions of instructions during the time required for a single i/o operation. a technique that
figure 9.8 growth of expected record accesses with α. the horizontal axis is the value for α, the vertical axis is the expected number of accesses to the hash table. solid lines show the cost for “random” probing (a theoretical lower bound on the cost), while dashed lines show the cost for linear probing (a relatively poor collision resolution strategy). the two leftmost lines show the cost for insertion (equivalently, unsuccessful search); the two rightmost lines show the cost for deletion (equivalently, successful search).
full. beyond that point performance will degrade rapidly. this requires that the implementor have some idea of how many records are likely to be in the table at maximum loading, and select the table size accordingly.
you might notice that a recommendation to never let a hash table become more than half full contradicts the disk-based space/time tradeoff principle, which strives to minimize disk space to increase information density. hashing represents an unusual situation in that there is no beneﬁt to be expected from locality of reference. in a sense, the hashing system implementor does everything possible to eliminate the effects of locality of reference! given the disk block containing the last record accessed, the chance of the next record access coming to the same disk block is no better than random chance in a well-designed hash system. this is because a good hashing implementation breaks up relationships between search keys. instead of improving performance by taking advantage of locality of reference, hashing trades increased hash table space for an improved chance that the record will be in its home position. thus, the more space available for the hash table, the more efﬁcient hashing should be.
figure 8.9 the snowplow analogy showing the action during one revolution of the snowplow. a circular track is laid out straight for purposes of illustration, and is shown in cross section. at any time t , the most snow is directly in front of the snowplow. as the plow moves around the track, the same amount of snow is always in front of the plow. as the plow moves forward, less of this is snow that was in the track at time t ; more is snow that has fallen since.
this is the place least recently plowed. at any instant, there is a certain amount of snow s on the track. snow is constantly falling throughout the track at a steady rate, with some snow falling “in front” of the plow and some “behind” the plow. (on a circular track, everything is actually “in front” of the plow, but figure 8.9 illustrates the idea.) during the next revolution of the plow, all snow s on the track is removed, plus half of what falls. because everything is assumed to be in steady state, after one revolution s snow is still on the track, so 2s snow must fall during a revolution, and 2s snow is removed during a revolution (leaving s snow behind). at the beginning of replacement selection, nearly all values coming from the input ﬁle are greater (i.e., “in front of the plow”) than the latest key value output for this run, because the run’s initial key values should be small. as the run progresses, the latest key value output becomes greater and so new key values coming from the input ﬁle are more likely to be too small (i.e., “after the plow”); such records go to the bottom of the array. the total length of the run is expected to be twice the size of the array. of course, this assumes that incoming key values are evenly distributed within the key range (in terms of the snowplow analogy, we assume that snow falls evenly throughout the track). sorted and reverse sorted inputs do not meet this expectation and so change the length of the run.
the second stage of a typical external sorting algorithm merges the runs created by the ﬁrst stage. assume that we have r runs to merge. if a simple two-way merge is used, then r runs (regardless of their sizes) will require log r passes through the ﬁle. while r should be much less than the total number of records (because
figure 8.11 a comparison of three external sorts on a collection of small records for ﬁles of various sizes. each entry in the table shows time in seconds and total number of blocks read and written by the program. file sizes are in megabytes. for the third sorting algorithm, on ﬁle size of 4mb, the time and blocks shown in the last column are for a 32-way merge. 32 is used instead of 16 because 32 is a root of the number of blocks in the ﬁle (while 16 is not), thus allowing the same number of runs to be merged at every pass.
merges for r beyond about 4 or 8 runs does not help much because a lot of time is spent determining which is the next smallest element among the r runs.
we see from this experiment that building large initial runs reduces the running time to slightly more than one third that of standard mergesort, depending on ﬁle and memory sizes. using a multiway merge further cuts the time nearly in half. in summary, a good external sorting algorithm will seek to do the following: • make the initial runs as long as possible. • at all stages, overlap input, processing, and output as much as possible. • use as much working memory as possible. applying more memory usually speeds processing. in fact, more memory will have a greater effect than a faster disk. a faster cpu is unlikely to yield much improvement in running time for external sorting, because disk i/o speed is the limiting factor.
a good general text on ﬁle processing is folk and zoellig’s file structures: a conceptual toolkit [fz98]. a somewhat more advanced discussion on key issues in ﬁle processing is betty salzberg’s file structures: an analytical approach [sal88].
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
this section presents the b-tree. b-trees are usually attributed to r. bayer and e. mccreight who described the b-tree in a 1972 paper. by 1979, b-trees had replaced virtually all large-ﬁle access methods other than hashing. b-trees, or some variant of b-trees, are the standard ﬁle organization for applications requiring insertion, deletion, and key range searches. b-trees address effectively all of the major problems encountered when implementing disk-based search trees:
1. b-trees are always height balanced, with all leaf nodes at the same level. 2. update and search operations affect only a few disk blocks. the fewer the
3. b-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk i/o on searches due to locality of reference.
unfortunately, there is more than one way to assign values to q and r, depending on how integer division is interpreted. the most common mathematical deﬁnition computes the mod function as n mod m = n − mbn/mc. in this case, −3 mod 5 = 2. however, java and c++ compilers typically use the underlying processor’s machine instruction for computing integer arithmetic. on many computers this is done by truncating the resulting fraction, meaning n mod m = n − m(trunc(n/m)). under this deﬁnition, −3 mod 5 = −3.
unfortunately, for many applications this is not what the user wants or expects. for example, many hash systems will perform some computation on a record’s key value and then take the result modulo the hash table size. the expectation here would be that the result is a legal index into the hash table, not a negative number. implementors of hash functions must either insure that the result of the computation is always postive, or else add the hash table size to the result of the modulo function when that result is negative.
a logarithm of base b for value y is the power to which b is raised to get y. normally, this is written as logb y = x. thus, if logb y = x then bx = y, and blogby = y.
example 2.6 many programs require an encoding for a collection of objects. what is the minimum number of bits needed to represent n distinct code values? the answer is dlog2 ne bits. for example, if you have 1000 codes to store, you will require at least dlog2 1000e = 10 bits to have 1000 different codes (10 bits provide 1024 distinct code values).
example 2.7 consider the binary search algorithm for ﬁnding a given value within an array sorted by value from lowest to highest. binary search ﬁrst looks at the middle element and determines if the value being searched for is in the upper half or the lower half of the array. the algorithm then continues splitting the appropriate subarray in half until the desired value is found. (binary search is described in more detail in section 3.5.) how many times can an array of size n be split in half until only one element remains in the ﬁnal subarray? the answer is dlog2 ne times.
in this book, nearly all logarithms used have a base of two. this is because data structures and algorithms most often divide things in half, or store codes with binary bits. whenever you see the notation log n in this book, either log2 n is meant or else the term is being used asymptotically and the actual base does not matter. if any base for the logarithm other than two is intended, then the base will be shown explicitly.
1. log(nm) = log n + log m. 2. log(n/m) = log n − log m. 3. log(nr) = r log n. 4. loga n = logb n/ logb a. the ﬁrst two properties state that the logarithm of two numbers multiplied (or divided) can be found by adding (or subtracting) the logarithms of the two numbers.4 property (3) is simply an extension of property (1). property (4) tells us that, for variable n and any two integer constants a and b, loga n and logb n differ by the constant factor logb a, regardless of the value of n. most runtime analyses in this book are of a type that ignores constant factors in costs. property (4) says that such analyses need not be concerned with the base of the logarithm, because this can change the total cost only by a constant factor. note that 2log n = n.
when discussing logarithms, exponents often lead to confusion. property (3) tells us that log n2 = 2 log n. how do we indicate the square of the logarithm (as opposed to the logarithm of n2)? this could be written as (log n)2, but it is traditional to use log2 n. on the other hand, we might want to take the logarithm of the logarithm of n. this is written log log n.
a special notation is used in the rare case where we would like to know how many times we must take the log of a number before we reach a value ≤ 1. this quantity is written log∗ n. for example, log∗ 1024 = 4 because log 1024 = 10, log 10 ≈ 3.33, log 3.33 ≈ 1.74, and log 1.74 < 1, which is a total of 4 log operations.
4these properties are the idea behind the slide rule. adding two numbers can be viewed as joining two lengths together and measuring their combined length. multiplication is not so easily done. however, if the numbers are ﬁrst converted to the lengths of their logarithms, then those lengths can be added and the inverse logarithm of the resulting length gives the answer for the multiplication (this is simply logarithm property (1)). a slide rule measures the length of the logarithm for the numbers, lets you slide bars representing these lengths to add up the total length, and ﬁnally converts this total length to the correct numeric answer by taking the inverse of the logarithm for the result.
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
union rule for joining sets) is Θ(n log∗ n). the notation “log∗ n” means the number of times that the log of n must be taken before n ≤ 1. for example, log∗ 65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and ﬁnally log 2 = 1. thus, log∗ n grows very slowly, so the cost for a series of n find operations is very close to n.
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth Θ(log∗ n). one can devise a series of equivalence operations that yields Θ(log n) depth for the resulting tree. however, many of the equivalences in such a series will look only at the roots of the trees being merged, requiring little processing time. the total amount of processing time required for n operations will be Θ(n log∗ n), yielding nearly constant time for each equivalence operation. this is an example of the technique of amortized analysis, discussed further in section 14.3.
we now tackle the problem of devising an implementation for general trees that allows efﬁcient processing of all member functions of the adt shown in figure 6.2. this section presents several approaches to implementing general trees. each implementation yields advantages and disadvantages in the amount of space required to store a node and the relative ease with which key operations can be performed. general tree implementations should place no restriction on how many children a node may have. in some applications, once a node is created the number of children never changes. in such cases, a ﬁxed amount of space can be allocated for the node when it is created, based on the number of children for the node. matters become more complicated if children can be added to or deleted from a node, requiring that the node’s space allocation be adjusted accordingly.
will use the “)” symbol) to indicate the end of a child list. all leaf nodes are followed by a “)” symbol because they have no children. a leaf node that is also the last child for its parent would indicate this by two or more successive “)” symbols.
note that f is followed by three “)” marks, because it is a leaf, the last node of b’s rightmost subtree, and the last node of r’s rightmost subtree.
note that this representation for serializing general trees cannot be used for binary trees. this is because a binary tree is not merely a restricted form of general tree with at most two children. every binary tree node has a left and a right child, though either or both might be empty. for example, the representation of example 6.8 cannot let us distinguish whether node d in figure 6.17 is the left or right child of node b.
6.6 further reading the expression log∗ n cited in section 6.2 is closely related to the inverse of ackermann’s function. for more information about ackermann’s function and the cost of path compression, see robert e. tarjan’s paper “on the efﬁciency of a good but not linear set merging algorithm” [tar75]. the article “data structures and algorithms for disjoint set union problems” by galil and italiano [gi91] covers many aspects of the equivalence class problem.
foundations of multidimensional and metric data structures by hanan samet [sam06] treats various implementations of tree structures in detail within the context of k-ary trees. samet covers sequential implementations as well as the linked and array implementations such as those described in this chapter and chapter 5. while these books are ostensibly concerned with spatial data structures, many of the concepts treated are relevant to anyone who must implement tree structures.
one important aspect of algorithm design is referred to as the space/time tradeoff principle. the space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacriﬁce space or vice versa. many programs can be modiﬁed to reduce storage requirements by “packing” or encoding information. “unpacking” or decoding the information requires additional time. thus, the resulting program uses less space but runs slower. conversely, many programs can be modiﬁed to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. typically, such changes in time and space are both by a constant factor.
a classic example of a space/time tradeoff is the lookup table. a lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. for example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. if you are writing a program that often computes factorials, it is likely to be much more time efﬁcient to simply pre-compute the 12 storable values in a table. whenever the program needs the value of n! for n ≤ 12, it can simply check the lookup table. (if n > 12, the value is too large to store as an int variable anyway.) compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table.
lookup tables can also store approximations for an expensive function such as sine or cosine. if you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. note that initially building the lookup table requires a certain amount of time. your application must use the lookup table often enough to make this initialization worthwhile.
another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. here is a simple code fragment for sorting an array of integers. we assume that this is a special case where there are n integers whose values are a permutation of the integers from 0 to n − 1. this is an example of a binsort, which is discussed in section 7.7. binsort assigns each value to an array position corresponding to its value.
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
sequential search is practical for large n, in a way that is not true for some other algorithms in o(n2). we always seek to deﬁne the running time of an algorithm with the tightest (lowest) possible upper bound. thus, we prefer to say that sequential search is in o(n). this also explains why the phrase “is in o(f(n))” or the notation “∈ o(f(n))” is used instead of “is o(f(n))” or “= o(f(n)).” there is no strict equality to the use of big-oh notation. o(n) is in o(n2), but o(n2) is not in o(n).
big-oh notation describes an upper bound. in other words, big-oh notation states a claim about the greatest amount of some resource (usually time) that is required by an algorithm for some class of inputs of size n (typically the worst such input, the average of all possible inputs, or the best such input).
similar notation is used to describe the least amount of a resource that an algorithm needs for some class of input. like big-oh notation, this is a measure of the algorithm’s growth rate. like big-oh notation, it works for any resource, but we most often measure the least amount of time required. and again, like big-oh notation, we are measuring the resource required for some particular class of inputs: the worst-, average-, or best-case input of size n.
the lower bound for an algorithm (or a problem, as explained later) is denoted by the symbol Ω, pronounced “big-omega” or just “omega.” the following deﬁnition for Ω is symmetric with the deﬁnition of big-oh.
for t(n) a non-negatively valued function, t(n) is in set Ω(g(n)) if there exist two positive constants c and n0 such that t(n) ≥ cg(n) for all n > n0.1
this deﬁnition says that for an “interesting” number of cases, the algorithm takes at least cg(n) time. note that this deﬁnition is not symmetric with the deﬁnition of big-oh. for g(n) to be a lower bound, this deﬁnition does not require that t(n) ≥ cg(n) for all values of n greater than some constant. it only requires that this happen often enough, in particular that it happen for an inﬁnite number of values for n. motivation for this alternate deﬁnition can be found in the following example.
many algorithms (or their instantiations as programs), it is easy to come up with the equation that deﬁnes their runtime behavior. most algorithms presented in this book are well understood and we can almost always give a Θ analysis for them. however, chapter 17 discusses a whole class of algorithms for which we have no Θ analysis, just some unsatisfying big-oh and Ω analyses. exercise 3.14 presents a short, simple program fragment for which nobody currently knows the true upper or lower bounds.
while some textbooks and programmers will casually say that an algorithm is “order of” or “big-oh” of some cost function, it is generally better to use Θ notation rather than big-oh notation whenever we have sufﬁcient knowledge about an algorithm to be sure that the upper and lower bounds indeed match. throughout this book, Θ notation will be used in preference to big-oh notation whenever our state of knowledge makes that possible. limitations on our ability to analyze certain algorithms may require use of big-oh or Ω notations. in rare occasions when the discussion is explicitly about the upper or lower bound of a problem or algorithm, the corresponding notation will be used in preference to Θ notation.
once you determine the running-time equation for an algorithm, it really is a simple matter to derive the big-oh, Ω, and Θ expressions from the equation. you do not need to resort to the formal deﬁnitions of asymptotic analysis. instead, you can use the following rules to determine the simplest form.
1. if f(n) is in o(g(n)) and g(n) is in o(h(n)), then f(n) is in o(h(n)). 2. if f(n) is in o(kg(n)) for any constant k > 0, then f(n) is in o(g(n)). 3. if f1(n) is in o(g1(n)) and f2(n) is in o(g2(n)), then f1(n) + f2(n) is in
the ﬁrst rule says that if some function g(n) is an upper bound for your cost function, then any upper bound for g(n) is also an upper bound for your cost function. a similar property holds true for Ω notation: if g(n) is a lower bound for your cost function, then any lower bound for g(n) is also a lower bound for your cost function. likewise for Θ notation.
the signiﬁcance of rule (2) is that you can ignore any multiplicative constants in your equations when using big-oh notation. this rule also holds true for Ω and Θ notations.
if n and m are large, then this is approximately (n/m)i. the expected number of probes is one plus the sum over i ≥ 1 of the probability of i collisions, which is approximately
the cost for a successful search (or a deletion) has the same cost as originally inserting that record. however, the expected value for the insertion cost depends on the value of α not at the time of deletion, but rather at the time of the original insertion. we can derive an estimate of this cost (essentially an average over all the insertion costs) by integrating from 0 to the current value of α, yielding a result of
it is important to realize that these equations represent the expected cost for operations using the unrealistic assumption that the probe sequence is based on a random permutation of the slots in the hash table (thus avoiding all expense resulting from clustering). thus, these costs are lower-bound estimates in the average 2(1 + 1/(1− α)2) for insertions case. the true average cost under linear probing is 1 2(1 + 1/(1− α)) for deletions or successful searches. or unsuccessful searches and 1 proofs for these results can be found in the references cited in section 9.5. figure 9.8 shows the graphs of these four equations to help you visualize the expected performance of hashing based on the load factor. the two solid lines show the costs in the case of a “random” probe sequence for (1) insertion or unsuccessful search and (2) deletion or successful search. as expected, the cost for insertion or unsuccessful search grows faster, because these operations typically search further down the probe sequence. the two dashed lines show equivalent costs for linear probing. as expected, the cost of linear probing grows faster than the cost for “random” probing.
from figure 9.8 we see that the cost for hashing when the table is not too full is typically close to one record access. this is extraordinarily efﬁcient, much better than binary search which requires log n record accesses. as α increases, so does the expected cost. for small values of α, the expected cost is low. it remains below two until the hash table is about half full. when the table is nearly empty, adding a new record to the table does not increase the cost of future search operations by much. however, the additional search cost caused by each additional insertion increases rapidly once the table becomes half full. based on this analysis, the rule of thumb is to design a hashing system so that the hash table never gets above half
arrays, optimized quicksort performs well because it does one partition step before calling insertion sort. compared to the other o(n log n) sorts, unoptimized heapsort is quite slow due to the overhead of the class structure. when all of this is stripped away and the algorithm is implemented to manipulate an array directly, it is still somewhat slower than mergesort. in general, optimizating the various algorithms makes a noticible improvement for larger array sizes.
overall, radix sort is a surprisingly poor performer. if the code had been tuned to use bit shifting of the key value, it would likely improve substantially; but this would seriously limit the range of element types that the sort could support.
this book contains many analyses for algorithms. these analyses generally deﬁne the upper and lower bounds for algorithms in their worst and average cases. for most of the algorithms presented so far, analysis is easy. this section considers a more difﬁcult task — an analysis for the cost of a problem as opposed to an algorithm. the upper bound for a problem can be deﬁned as the asymptotic cost of the fastest known algorithm. the lower bound deﬁnes the best possible efﬁciency for any algorithm that solves the problem, including algorithms not yet invented. once the upper and lower bounds for the problem meet, we know that no future algorithm can possibly be (asymptotically) more efﬁcient.
a simple estimate for a problem’s lower bound can be obtained by measuring the size of the input that must be read and the output that must be written. certainly no algorithm can be more efﬁcient than the problem’s i/o time. from this we see that the sorting problem cannot be solved by any algorithm in less than Ω(n) time because it takes at least n steps to read and write the n values to be sorted. based on our current knowledge of sorting algorithms and the size of the input, we know that the problem of sorting is bounded by Ω(n) and o(n log n).
computer scientists have spent much time devising efﬁcient general-purpose sorting algorithms, but no one has ever found one that is faster than o(n log n) in the worst or average cases. should we keep searching for a faster sorting algorithm? or can we prove that there is no faster sorting algorithm by ﬁnding a tighter lower bound?
this section presents one of the most important and most useful proofs in computer science: no sorting algorithm based on key comparisons can possibly be faster than Ω(n log n) in the worst case. this proof is important for three reasons. first, knowing that widely used sorting algorithms are asymptotically optimal is reassuring. in particular, it means that you need not bang your head against the wall searching for an o(n) sorting algorithm (or at least not one in any way based on key
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
ﬁnd one method to convert the inputs to pairing into inputs to sorting “fast enough,” and a second method to convert the result of sorting back to the correct result for pairing “fast enough,” then the asymptotic cost of pairing cannot be more than the cost of sorting. in this case, there is little work to be done to convert from pairing to sorting, or to convert the answer from sorting back to the answer for pairing, so the dominant cost of this solution is performing the sort operation. thus, an upper bound for pairing is in o(n log n).
it is important to note that the pairing problem does not require that elements of the two sequences be sorted. this is merely one possible way to solve the problem. pairing only requires that the elements of the sequences be paired correctly. perhaps there is another way to do it? certainly if we use sorting to solve pairing, the algorithms will require Ω(n log n) time. but, another approach might conceivably be faster.
there is another use of reductions aside from applying an old algorithm to solve a new problem (and thereby establishing an upper bound for the new problem). that is to prove a lower bound on the cost of a new problem by showing that it could be used as a solution for an old problem with a known lower bound.
assume we can go the other way and convert sorting to pairing “fast enough.” what does this say about the minimum cost of pairing? we know from section 7.9 that the cost of sorting in the worst and average cases is in Ω(n log n). in other words, the best possible algorithm for sorting requires at least n log n time.
assume that pairing could be done in o(n) time. then, one way to create a sorting algorithm would be to convert sorting into pairing, run the algorithm for pairing, and ﬁnally convert the answer back to the answer for sorting. provided that we can convert sorting to/from pairing “fast enough,” this process would yield an o(n) algorithm for sorting! because this contradicts what we know about the lower bound for sorting, and the only ﬂaw in the reasoning is the initial assumption that pairing can be done in o(n) time, we can conclude that there is no o(n) time algorithm for pairing. this reduction process tells us that pairing must be at least as expensive as sorting and so must itself have a lower bound in Ω(n log n).
to complete this proof regarding the lower bound for pairing, we need now to ﬁnd a way to reduce sorting to pairing. this is easily done. take an instance of sorting (i.e., an array a of n elements). a second array b is generated that simply stores i in position i for 0 ≤ i < n. pass the two arrays to pairing. take the resulting set of pairs, and use the value from the b half of the pair to tell which position in the sorted array the a half should take; that is, we can now reorder
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
on a road map, a road connecting two towns is typically labeled with its distance. we can model a road network as a directed graph whose edges are labeled with real numbers. these numbers represent the distance (or other cost metric, such as travel time) between two vertices. these labels may be called weights, costs, or distances, depending on the application. given such a graph, a typical problem is to ﬁnd the total length of the shortest path between two speciﬁed vertices. this is not a trivial problem, because the shortest path may not be along the edge (if any) connecting two vertices, but rather may be along a path involving one or more intermediate vertices. for example, in figure 11.15, the cost of the path from a to b to d is 15. the cost of the edge directly from a to d is 20. the cost of the path from a to c to b to d is 10. thus, the shortest path from a to d is 10 (not along the edge connecting a to d). we use the notation d(a, d) = 10 to indicate that the shortest distance from a to d is 10. in figure 11.15, there is no path from e to b, so we set d(e, b) = ∞. we deﬁne w(a, d) = 20 to be the weight of edge (a, d), that is, the weight of the direct connection from a to d. because there is no edge from e to b, w(e, b) = ∞. note that w(d, a) = ∞ because the graph of figure 11.15 is directed. we assume that all weights are positive.
11.4.1 single-source shortest paths this section presents an algorithm to solve the single-source shortest-paths problem. given vertex s in graph g, ﬁnd a shortest path from s to every other vertex in g. we might want only the shortest path between two vertices, s and t. however in the worst case, while ﬁnding the shortest path from s to t, we might ﬁnd the shortest paths from s to every other vertex as well. so there is no better algorithm (in the worst case) for ﬁnding the shortest path to a single vertex than to ﬁnd shortest paths to all vertices. the algorithm described here will only compute the
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
some applications must represent a large, two-dimensional matrix where many of the elements have a value of zero. one example is the lower triangular matrix that results from solving systems of simultaneous equations. a lower triangular matrix stores zero values at positions [r, c] such that r < c, as shown in figure 12.6(a). thus, the upper-right triangle of the matrix is always zero. another example is the representation of undirected graphs in an adjacency matrix (see project 11.2). because all edges between vertices i and j go in both directions, there is no need to store both. instead we can just store one edge going from the higher-indexed vertex to the lower-indexed vertex. in this case, only the lower triangle of the matrix can have non-zero values. we can take advantage of this fact to save space. instead of storing n(n + 1)/2 pieces of information in an n × n array, it would save space to use a list of length n(n + 1)/2. this is only practical if some means can be found to locate within the list the element that would correspond to position [r, c] in the original matrix.
to derive an equation to do this computation, note that row 0 of the matrix has one non-zero value, row 1 has two non-zero values, and so on. thus, row r k=1 k = (r2 + r)/2 non-zero elements. adding c to reach the cth position in the rth row yields the following equation to convert position [r, c] in the original matrix to the correct position in the list.
a similar equation can be used to store an upper triangular matrix, that is, a matrix with zero values at positions [r, c] such that r > c, as shown in figure 12.6(b). for an n × n upper triangular matrix, the equation would be
a more difﬁcult situation arises when the vast majority of values stored in an n × n matrix are zero, but there is no restriction on which positions are zero and which are non-zero. this is known as a sparse matrix.
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
will allow for a faster algorithm than is required by the more general multiplication problem. however, a simple reduction proof serves to show that squaring is “as hard” as multiplying.
the signiﬁcance of this formula is that it allows us to convert an arbitrary instance of multiplication to a series of operations involving three addition/subtractions (each of which can be done in linear time), two squarings, and a division by 4. note that the division by 4 can be done in linear time (simply convert to binary, shift right by two digits, and convert back).
this reduction shows that if a linear time algorithm for squaring can be found, our next example of reduction concerns the multiplication of two n × n matrices. for this problem, we will assume that the values stored in the matrices are simple integers and that multiplying two simple integers takes constant time (because multiplication of two int variables takes a ﬁxed number of machine instructions). the standard algorithm for multiplying two matrices is to multiply each element of the ﬁrst matrix’s ﬁrst row by the corresponding element of the second matrix’s ﬁrst column, then adding the numbers. this takes Θ(n) time. each of the n2 elements of the solution are computed in similar fashion, requiring a total of Θ(n3) time. faster algorithms are known (see the discussion of strassen’s algorithm in section 16.4.3), but none are so fast as to be in o(n2).
now, consider the case of multiplying two symmetric matrices. a symmetric matrix is one in which entry ij is equal to entry ji; that is, the upper-right triangle of the matrix is a mirror image of the lower-left triangle. is there something about this restricted case that allows us to multiply two symmetric matrices faster than in the general case? the answer is no, as can be seen by the following reduction. assume that we have been given two n × n matrices a and b. we can construct a 2n × 2n symmetric matrix from an arbitrary matrix a as follows:
here 0 stands for an n× n matrix composed of zero values, a is the original matrix, and at stands for the transpose of matrix a.1 note that the resulting matrix is now
symmetric. we can convert matrix b to a symmetric matrix in a similar manner. if symmetric matrices could be multiplied “quickly” (in particular, if they could be multiplied together in Θ(n2) time), then we could ﬁnd the result of multiplying two arbitrary n × n matrices in Θ(n2) time by taking advantage of the following observation:
there are several ways that a problem could be considered hard. for example, we might have trouble understanding the deﬁnition of the problem itself. at the beginning of a large data collection and analysis project, developers and their clients might have only a hazy notion of what their goals actually are, and need to work that out over time. for other types of problems, we might have trouble ﬁnding or understanding an algorithm to solve the problem. understanding spoken engish and translating it to written text is an example of a problem whose goals are easy to deﬁne, but whose solution is not easy to discover. but even though a natural language processing algorithm might be difﬁcult to write, the program’s running time might be fairly fast. there are many practical systems today that solve aspects of this problem in reasonable time.
none of these is what is commonly meant when a computer theoretician uses the word “hard.” throughout this section, “hard” means that the best-known algorithm for the problem is expensive in its running time. one example of a hard problem is towers of hanoi. it is easy to understand this problem and its solution. it is also easy to write a program to solve this problem. but, it takes an extremely long time to run for any “reasonably” large value of n. try running a program to solve towers of hanoi for only 30 disks!
the towers of hanoi problem takes exponential time, that is, its running time is Θ(2n). this is radically different from an algorithm that takes Θ(n log n) time or Θ(n2) time. it is even radically different from a problem that takes Θ(n4) time. these are all examples of polynomial running time, because the exponents for all terms of these equations are constants. recall from chapter 3 that if we buy a new computer that runs twice as fast, the size of problem with complexity Θ(n4) that we can solve in a certain amount of time is increased by the fourth root of two. in other words, there is a multiplicative factor increase, even if it is a rather small one. this is true for any algorithm whose running time can be represented by a polynomial.
some applications must represent a large, two-dimensional matrix where many of the elements have a value of zero. one example is the lower triangular matrix that results from solving systems of simultaneous equations. a lower triangular matrix stores zero values at positions [r, c] such that r < c, as shown in figure 12.6(a). thus, the upper-right triangle of the matrix is always zero. another example is the representation of undirected graphs in an adjacency matrix (see project 11.2). because all edges between vertices i and j go in both directions, there is no need to store both. instead we can just store one edge going from the higher-indexed vertex to the lower-indexed vertex. in this case, only the lower triangle of the matrix can have non-zero values. we can take advantage of this fact to save space. instead of storing n(n + 1)/2 pieces of information in an n × n array, it would save space to use a list of length n(n + 1)/2. this is only practical if some means can be found to locate within the list the element that would correspond to position [r, c] in the original matrix.
to derive an equation to do this computation, note that row 0 of the matrix has one non-zero value, row 1 has two non-zero values, and so on. thus, row r k=1 k = (r2 + r)/2 non-zero elements. adding c to reach the cth position in the rth row yields the following equation to convert position [r, c] in the original matrix to the correct position in the list.
a similar equation can be used to store an upper triangular matrix, that is, a matrix with zero values at positions [r, c] such that r > c, as shown in figure 12.6(b). for an n × n upper triangular matrix, the equation would be
a more difﬁcult situation arises when the vast majority of values stored in an n × n matrix are zero, but there is no restriction on which positions are zero and which are non-zero. this is known as a sparse matrix.
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
example 1.8 consider the design for a relatively simple database system stored on disk. typically, records on disk in such a program are accessed through a buffer pool (see section 8.3) rather than directly. variable length records might use a memory manager (see section 12.3) to ﬁnd an appropriate location within the disk ﬁle to place the record. multiple index structures (see chapter 10) will typically be used to access records in various ways. thus, we have a chain of classes, each with its own responsibilities and access privileges. a database query from a user is implemented by searching an index structure. this index requests access to the record by means of a request to the buffer pool. if a record is being inserted or deleted, such a request goes through the memory manager, which in turn interacts with the buffer pool to gain access to the disk ﬁle. a program such as this is far too complex for nearly any human programmer to keep all of the details in his or her head at once. the only way to design and implement such a program is through proper use of abstraction and metaphors. in object-oriented programming, such abstraction is handled using classes.
data types have both a logical and a physical form. the deﬁnition of the data type in terms of an adt is its logical form. the implementation of the data type as a data structure is its physical form. figure 1.1 illustrates this relationship between logical and physical forms for data types. when you implement an adt, you are dealing with the physical form of the associated data type. when you use an adt elsewhere in your program, you are concerned with the associated data type’s logical form. some sections of this book focus on physical implementations for a given data structure. other sections use the logical adt for the data type in the context of a higher-level task.
example 1.9 a particular java environment might provide a library that includes a list class. the logical form of the list is deﬁned by the public functions, their inputs, and their outputs that deﬁne the class. this might be all that you know about the list class implementation, and this should be all you need to know. within the class, a variety of physical implementations for lists is possible. several are described in section 4.1.
at a higher level of abstraction than adts are abstractions for describing the design of programs — that is, the interactions of objects and classes. experienced software
simple lists and arrays are the right tool for the many applications. other situations require support for operations that cannot be implemented efﬁciently by the standard list representations of chapter 4. this chapter presents advanced implementations for lists and arrays that overcome some of the problems of simple linked list and contiguous array representations. a wide range of topics are covered, whose unifying thread is that the data structures are all list- or array-like. this chapter should also serve to reinforce the concept of logical representation versus physical implementation, as some of the “list” implementations have quite different organizations internally.
section 12.1 describes a series of representations for multilists, which are lists that may contain sublists. section 12.2 discusses representations for implementing sparse matrices, large matrices where most of the elements have zero values. section 12.3 discusses memory management techniques, which are essentially a way of allocating variable-length sections from a large array.
recall from chapter 4 that a list is a ﬁnite, ordered sequence of items of the form hx0, x1, ..., xn−1i where n ≥ 0. we can represent the empty list by null or hi. in chapter 4 we assumed that all list elements had the same data type. in this section, we extend the deﬁnition of lists to allow elements to be arbitrary in nature. in general, list elements are one of two types.
12.9 write memory manager allocation and deallocation routines for the situation where all requests and releases follow a last-requested, ﬁrst-released (stack) order.
12.10 write memory manager allocation and deallocation routines for the situation where all requests and releases follow a last-requested, last-released (queue) order.
12.11 show the result of allocating the following blocks from a memory pool of size 1000 using ﬁrst ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.12 show the result of allocating the following blocks from a memory pool of size 1000 using best ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.13 show the result of allocating the following blocks from a memory pool of size 1000 using worst ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.14 assume that the memory pool contains three blocks of free storage. their sizes are 1300, 2000, and 1000. give examples of storage requests for which (a) ﬁrst-ﬁt allocation will work, but not best ﬁt or worst ﬁt. (b) best-ﬁt allocation will work, but not ﬁrst ﬁt or worst ﬁt. (c) worst-ﬁt allocation will work, but not ﬁrst ﬁt or best ﬁt.
• insert an element at a given position, • delete an element from a given position, • return the value of the element at a given position, • take the transpose of a matrix, and
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
12.2 implement the memmanager adt shown at the beginning of section 12.3. use a separate linked list to implement the freelist. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.3 implement the memmanager adt shown at the beginning of section 12.3. do not use separate memory for the free list, but instead embed the free list into the memory pool as shown in figure 12.12. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.4 implement the memmanager adt shown at the beginning of section 12.3 using the buddy method of section 12.3.1. your system should support requests for blocks of a speciﬁed size and release of previously requested blocks.
figure 12.13 adding block f to the freelist. the word immediately preceding the start of f in the memory pool stores the tag bit of the preceding block p. if p is free, merge f into p. we ﬁnd the end of f by using f’s size ﬁeld. the word following the end of f is the tag ﬁeld for block s. if s is free, merge it into f.
then simply have its size extended to include block f. if block p is not free, then we just add block f to the freelist. finally, we also check the bit following the end of block f. if this bit indicates that the following block (call it s) is free, then s is removed from the freelist and the size of f is extended appropriately.
we now consider how a “suitable” free block is selected to service a memory request. to illustrate the process, assume there are four blocks on the freelist of sizes 500, 700, 650, and 900 (in that order). assume that a request is made for 600 units of storage. for our examples, we ignore the overhead imposed for the tag, link, and size ﬁelds discussed above.
the simplest method for selecting a block would be to move down the free block list until a block of size at least 600 is found. any remaining space in this block is left on the freelist. if we begin at the beginning of the list and work down to the ﬁrst free block at least as large as 600, we select the block of size 700. because this approach selects the ﬁrst block with enough space, it is called ﬁrst ﬁt. a simple variation that will improve performance is, instead of always beginning at the head of the freelist, remember the last position reached in the previous search and start from there. when the end of the freelist is reached, search begins again at the head of the freelist. this modiﬁcation reduces the number of unnecessary searches through small blocks that were passed over by previous requests.
there is a potential disadvantage to ﬁrst ﬁt: it might “waste” larger blocks by breaking them up, and so they will not be available for large requests later. a strategy that avoids using large blocks unnecessarily is called best ﬁt. best ﬁt looks at the entire list and picks the smallest block that is at least as large as the request (i.e., the “best” or closest ﬁt to the request). continuing with the preceding example, the best ﬁt for a request of 600 units is the block of size 650, leaving a
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
that allocate ﬁle space in clusters. another example of sacriﬁcing space to internal fragmentation so as to simplify memory management is the buddy method described later in this section.
the process of searching the memory pool for a block large enough to service the request, possibly reserving the remaining space as a free block, is referred to as a sequential ﬁt method.
sequential-ﬁt methods attempt to ﬁnd a “good” block to service a storage request. the three sequential-ﬁt methods described here assume that the free blocks are organized into a doubly linked list, as illustrated by figure 12.11.
there are two basic approaches to implementing the freelist. the simpler approach is to store the freelist separately from the memory pool. in other words, a simple linked-list implementation such as described in chapter 4 can be used, where each node of the linked list contains a pointer to a single free block in the memory pool. this is ﬁne if there is space available for the linked list itself, separate from the memory pool.
the second approach to storing the freelist is more complicated but saves space. because the free space is free, it can be used by the memory manager to help it do its job; that is, the memory manager temporarily “borrows” space within the free blocks to maintain its doubly linked list. to do so, each unallocated block must be large enough to hold these pointers. in addition, it is usually worthwhile to let the memory manager add a few bytes of space to each reserved block for its own purposes. in other words, a request for m bytes of space might result in slightly more than m bytes being allocated by the memory manager, with the extra bytes used by the memory manager itself rather than the requester. we will assume that all memory blocks are organized as shown in figure 12.12, with space for tags and linked list pointers. here, free and reserved blocks are distinguished by a tag bit at both the beginning and the end of the block, for reasons that will be explained. in addition, both free and reserved blocks have a size indicator immediately after the tag bit at the beginning of the block to indicate how large the block is. free blocks have a second size indicator immediately preceding the tag bit at the end of the block. finally, free blocks have left and right pointers to their neighbors in the free block list.
the information ﬁelds associated with each block permit the memory manager to allocate and deallocate blocks as needed. when a request comes in for m words of storage, the memory manager searches the linked list of free blocks until it ﬁnds a “suitable” block for allocation. how it determines which block is suitable will
remainder of size 50. best ﬁt has the disadvantage that it requires that the entire list be searched. another problem is that the remaining portion of the best-ﬁt block is likely to be small, and thus useless for future requests. in other words, best ﬁt tends to maximize problems of external fragmentation while it minimizes the chance of not being able to service an occasional large request.
a strategy contrary to best ﬁt might make sense because it tends to minimize the effects of external fragmentation. this is called worst ﬁt, which always allocates the largest block on the list hoping that the remainder of the block will be useful for servicing a future request. in our example, the worst ﬁt is the block of size 900, leaving a remainder of size 300. if there are a few unusually large requests, this approach will have less chance of servicing them. if requests generally tend to be of the same size, then this might be an effective strategy. like best ﬁt, worst ﬁt requires searching the entire freelist at each memory request to ﬁnd the largest block. alternatively, the freelist can be ordered from largest to smallest free block, possibly by using a priority queue implementation.
which strategy is best? it depends on the expected types of memory requests. if the requests are of widely ranging size, best ﬁt might work well. if the requests tend to be of similar size, with rare large and small requests, ﬁrst or worst ﬁt might work well. unfortunately, there are always request patterns that one of the three sequential ﬁt methods will service, but which the other two will not be able to service. for example, if the series of requests 600, 650, 900, 500, 100 is made to a freelist containing blocks 500, 700, 650, 900 (in that order), the requests can all be serviced by ﬁrst ﬁt, but not by best ﬁt. alternatively, the series of requests 600, 500, 700, 900 can be serviced by best ﬁt but not by ﬁrst ﬁt on this same freelist.
sequential-ﬁt methods rely on a linked list of free blocks, which must be searched for a suitable block at each memory request. thus, the time to ﬁnd a suitable free block would be Θ(n) in the worst case for a freelist containing n blocks. merging adjacent free blocks is somewhat complicated. finally, we must either use additional space for the linked list, or use space within the memory pool to support the memory manager operations. in the second option, both free and reserved blocks require tag and size ﬁelds. fields in free blocks do not cost any space (because they are stored in memory that is not otherwise being used), but ﬁelds in reserved blocks create additional overhead.
the buddy system solves most of these problems. searching for a block of the proper size is efﬁcient, merging adjacent free blocks is simple, and no tag or other information ﬁelds need be stored within reserved blocks. the buddy system
assumes that memory is of size 2n for some integer n. both free and reserved blocks will always be of size 2k for k ≤ n. at any given time, there might be both free and reserved blocks of various sizes. the buddy system keeps a separate list for free blocks of each size. there can be at most n such lists, because there can only be n distinct block sizes. when a request comes in for m words, we ﬁrst determine the smallest value of k such that 2k ≥ m. a block of size 2k is selected from the free list for that block size if one exists. the buddy system does not worry about internal fragmentation: the entire block of size 2k is allocated.
if no block of size 2k exists, the next larger block is located. this block is split in half (repeatedly if necessary) until the desired block of size 2k is created. any other blocks generated as a by-product of this splitting process are placed on the appropriate freelists.
the disadvantage of the buddy system is that it allows internal fragmentation. for example, a request for 257 words will require a block of size 512. the primary advantages of the buddy system are (1) there is less external fragmentation; (2) search for a block of the right size is cheaper than, say, best ﬁt because we need only ﬁnd the ﬁrst available block on the block list for blocks of size 2k; and (3) merging adjacent free blocks is easy.
the reason why this method is called the buddy system is because of the way that merging takes place. the buddy for any block of size 2k is another block of the same size, and with the same address (i.e., the byte position in memory, read as a binary value) except that the kth bit is reversed. for example, the block of size 8 with beginning address 0000 in figure 12.14(a) has buddy with address 1000. likewise, in figure 12.14(b), the block of size 4 with address 0000 has buddy 0100. if free blocks are sorted by address value, the buddy can be found by searching the correct block size list. merging simply requires that the address for the combined buddies be moved to the freelist for the next larger block size.
in addition to sequential-ﬁt and buddy methods, there are many ad hoc approaches to memory management. if the application is sufﬁciently complex, it might be desirable to break available memory into several memory zones, each with a different memory management scheme. for example, some zones might have a simple memory access pattern of ﬁrst-in, ﬁrst-out. this zone can therefore be managed efﬁciently by using a simple stack. another zone might allocate only records of ﬁxed size, and so can be managed with a simple freelist as described in section 4.1.2. other zones might need one of the general-purpose memory allocation methods
12.2 implement the memmanager adt shown at the beginning of section 12.3. use a separate linked list to implement the freelist. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.3 implement the memmanager adt shown at the beginning of section 12.3. do not use separate memory for the free list, but instead embed the free list into the memory pool as shown in figure 12.12. your implementation should work for any of the three sequential-ﬁt methods: ﬁrst ﬁt, best ﬁt, and worst ﬁt. test your system empirically to determine under what conditions each method performs well.
12.4 implement the memmanager adt shown at the beginning of section 12.3 using the buddy method of section 12.3.1. your system should support requests for blocks of a speciﬁed size and release of previously requested blocks.
figure 12.9 dynamic storage allocation model. memory is made up of a series of variable-size blocks, some allocated and some free. in this example, shaded areas represent memory currently allocated and unshaded areas represent unused memory available for future allocation.
a freelist used for servicing future memory requests. figure 12.9 illustrates the situation that can arise after a series of memory allocations and deallocations.
when a memory request is received by the memory manager, some block on the freelist must be found that is large enough to service the request. if no such block is found, then the memory manager must resort to a failure policy such as discussed in section 12.3.2.
if there is a request for m words, and no block exists of exactly size m, then a larger block must be used instead. one possibility in this case is that the entire block is given away to the memory allocation request. this might be desirable when the size of the block is only slightly larger than the request. this is because saving a tiny block that is too small to be useful for a future memory request might not be worthwhile. alternatively, for a free block of size k, with k > m, up to k − m space may be retained by the memory manager to form a new free block, while the rest is used to service the request.
memory managers can suffer from two types of fragmentation. external fragmentation occurs when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. internal fragmentation occurs when more than m words are allocated to a request for m words, wasting free storage. this is equivalent to the internal fragmentation that occurs when ﬁles are allocated in multiples of the cluster size. the difference between internal and external fragmentation is illustrated by figure 12.10.
some memory management schemes sacriﬁce space to internal fragmentation to make memory management easier (and perhaps reduce external fragmentation). for example, external fragmentation does not happen in ﬁle management systems
discussed in this section. the advantage of zones is that some portions of memory can be managed more efﬁciently. the disadvantage is that one zone might ﬁll up while other zones have excess memory if the zone sizes are chosen poorly.
another approach to memory management is to impose a standard size on all memory requests. we have seen an example of this concept already in disk ﬁle management, where all ﬁles are allocated in multiples of the cluster size. this approach leads to internal fragmentation, but managing ﬁles composed of clusters is easier than managing arbitrarily sized ﬁles. the cluster scheme also allows us to relax the restriction that the memory request be serviced by a contiguous block of memory. most disk ﬁle managers and operating system main memory managers work on a cluster or page system. block management is usually done with a buffer pool to allocate available blocks in main memory efﬁciently.
at some point during processing, a memory manager could encounter a request for memory that it cannot satisfy. in some situations, there might be nothing that can be done: there simply might not be enough free memory to service the request, and the application may require that the request be serviced immediately. in this case, the memory manager has no option but to return an error, which could in turn lead to a failure of the application program. however, in many cases there are alternatives to simply returning an error. the possible options are referred to collectively as failure policies.
in some cases, there might be sufﬁcient free memory to satisfy the request, but it is scattered among small blocks. this can happen when using a sequential-
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
figure 12.13 adding block f to the freelist. the word immediately preceding the start of f in the memory pool stores the tag bit of the preceding block p. if p is free, merge f into p. we ﬁnd the end of f by using f’s size ﬁeld. the word following the end of f is the tag ﬁeld for block s. if s is free, merge it into f.
then simply have its size extended to include block f. if block p is not free, then we just add block f to the freelist. finally, we also check the bit following the end of block f. if this bit indicates that the following block (call it s) is free, then s is removed from the freelist and the size of f is extended appropriately.
we now consider how a “suitable” free block is selected to service a memory request. to illustrate the process, assume there are four blocks on the freelist of sizes 500, 700, 650, and 900 (in that order). assume that a request is made for 600 units of storage. for our examples, we ignore the overhead imposed for the tag, link, and size ﬁelds discussed above.
the simplest method for selecting a block would be to move down the free block list until a block of size at least 600 is found. any remaining space in this block is left on the freelist. if we begin at the beginning of the list and work down to the ﬁrst free block at least as large as 600, we select the block of size 700. because this approach selects the ﬁrst block with enough space, it is called ﬁrst ﬁt. a simple variation that will improve performance is, instead of always beginning at the head of the freelist, remember the last position reached in the previous search and start from there. when the end of the freelist is reached, search begins again at the head of the freelist. this modiﬁcation reduces the number of unnecessary searches through small blocks that were passed over by previous requests.
there is a potential disadvantage to ﬁrst ﬁt: it might “waste” larger blocks by breaking them up, and so they will not be available for large requests later. a strategy that avoids using large blocks unnecessarily is called best ﬁt. best ﬁt looks at the entire list and picks the smallest block that is at least as large as the request (i.e., the “best” or closest ﬁt to the request). continuing with the preceding example, the best ﬁt for a request of 600 units is the block of size 650, leaving a
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
figure 12.15 using handles for dynamic memory management. the memory manager returns the address of the handle in response to a memory request. the handle stores the address of the actual memory block. in this way, the memory block might be moved (with its address updated in the handle) without disrupting the application program.
ﬁt memory allocation method, where external fragmentation has led to a series of small blocks that collectively could service the request. in this case, it might be possible to compact memory by moving the reserved blocks around so that the free space is collected into a single block. a problem with this approach is that the application must somehow be able to deal with the fact that all of its data have now been moved to different locations. if the application program relies on the absolute positions of the data in any way, this would be disastrous. one approach for dealing with this problem is the use of handles. a handle is a second level of indirection to a memory location. the memory allocation routine does not return a pointer to the block of storage, but rather a pointer to a variable that in turn points to the storage. this variable is the handle. the handle never moves its position, but the position of the block might be moved and the value of the handle updated. figure 12.15 illustrates the concept.
another failure policy that might work in some applications is to defer the memory request until sufﬁcient memory becomes available. for example, a multitasking operating system could adopt the strategy of not allowing a process to run until there is sufﬁcient memory available. while such a delay might be annoying to the user, it is better than halting the entire system. the assumption here is that other processes will eventually terminate, freeing memory.
another option might be to allocate more memory to the memory manager. in a zoned memory allocation system where the memory manager is part of a larger system, this might be a viable option. in a java program that implements its own memory manager, it might be possible to get more memory from the system-level new operator, such as is done by the freelist of section 4.1.2.
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
that allocate ﬁle space in clusters. another example of sacriﬁcing space to internal fragmentation so as to simplify memory management is the buddy method described later in this section.
the process of searching the memory pool for a block large enough to service the request, possibly reserving the remaining space as a free block, is referred to as a sequential ﬁt method.
sequential-ﬁt methods attempt to ﬁnd a “good” block to service a storage request. the three sequential-ﬁt methods described here assume that the free blocks are organized into a doubly linked list, as illustrated by figure 12.11.
there are two basic approaches to implementing the freelist. the simpler approach is to store the freelist separately from the memory pool. in other words, a simple linked-list implementation such as described in chapter 4 can be used, where each node of the linked list contains a pointer to a single free block in the memory pool. this is ﬁne if there is space available for the linked list itself, separate from the memory pool.
the second approach to storing the freelist is more complicated but saves space. because the free space is free, it can be used by the memory manager to help it do its job; that is, the memory manager temporarily “borrows” space within the free blocks to maintain its doubly linked list. to do so, each unallocated block must be large enough to hold these pointers. in addition, it is usually worthwhile to let the memory manager add a few bytes of space to each reserved block for its own purposes. in other words, a request for m bytes of space might result in slightly more than m bytes being allocated by the memory manager, with the extra bytes used by the memory manager itself rather than the requester. we will assume that all memory blocks are organized as shown in figure 12.12, with space for tags and linked list pointers. here, free and reserved blocks are distinguished by a tag bit at both the beginning and the end of the block, for reasons that will be explained. in addition, both free and reserved blocks have a size indicator immediately after the tag bit at the beginning of the block to indicate how large the block is. free blocks have a second size indicator immediately preceding the tag bit at the end of the block. finally, free blocks have left and right pointers to their neighbors in the free block list.
the information ﬁelds associated with each block permit the memory manager to allocate and deallocate blocks as needed. when a request comes in for m words of storage, the memory manager searches the linked list of free blocks until it ﬁnds a “suitable” block for allocation. how it determines which block is suitable will
remainder of size 50. best ﬁt has the disadvantage that it requires that the entire list be searched. another problem is that the remaining portion of the best-ﬁt block is likely to be small, and thus useless for future requests. in other words, best ﬁt tends to maximize problems of external fragmentation while it minimizes the chance of not being able to service an occasional large request.
a strategy contrary to best ﬁt might make sense because it tends to minimize the effects of external fragmentation. this is called worst ﬁt, which always allocates the largest block on the list hoping that the remainder of the block will be useful for servicing a future request. in our example, the worst ﬁt is the block of size 900, leaving a remainder of size 300. if there are a few unusually large requests, this approach will have less chance of servicing them. if requests generally tend to be of the same size, then this might be an effective strategy. like best ﬁt, worst ﬁt requires searching the entire freelist at each memory request to ﬁnd the largest block. alternatively, the freelist can be ordered from largest to smallest free block, possibly by using a priority queue implementation.
which strategy is best? it depends on the expected types of memory requests. if the requests are of widely ranging size, best ﬁt might work well. if the requests tend to be of similar size, with rare large and small requests, ﬁrst or worst ﬁt might work well. unfortunately, there are always request patterns that one of the three sequential ﬁt methods will service, but which the other two will not be able to service. for example, if the series of requests 600, 650, 900, 500, 100 is made to a freelist containing blocks 500, 700, 650, 900 (in that order), the requests can all be serviced by ﬁrst ﬁt, but not by best ﬁt. alternatively, the series of requests 600, 500, 700, 900 can be serviced by best ﬁt but not by ﬁrst ﬁt on this same freelist.
sequential-ﬁt methods rely on a linked list of free blocks, which must be searched for a suitable block at each memory request. thus, the time to ﬁnd a suitable free block would be Θ(n) in the worst case for a freelist containing n blocks. merging adjacent free blocks is somewhat complicated. finally, we must either use additional space for the linked list, or use space within the memory pool to support the memory manager operations. in the second option, both free and reserved blocks require tag and size ﬁelds. fields in free blocks do not cost any space (because they are stored in memory that is not otherwise being used), but ﬁelds in reserved blocks create additional overhead.
the buddy system solves most of these problems. searching for a block of the proper size is efﬁcient, merging adjacent free blocks is simple, and no tag or other information ﬁelds need be stored within reserved blocks. the buddy system
12.9 write memory manager allocation and deallocation routines for the situation where all requests and releases follow a last-requested, ﬁrst-released (stack) order.
12.10 write memory manager allocation and deallocation routines for the situation where all requests and releases follow a last-requested, last-released (queue) order.
12.11 show the result of allocating the following blocks from a memory pool of size 1000 using ﬁrst ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.12 show the result of allocating the following blocks from a memory pool of size 1000 using best ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.13 show the result of allocating the following blocks from a memory pool of size 1000 using worst ﬁt for each series of block requests. state if a given request cannot be satisﬁed. (a) take 300 (call this block a), take 500, release a, take 200, take 300. (b) take 200 (call this block a), take 500, release a, take 200, take 300. (c) take 500 (call this block a), take 300, release a, take 300, take 200.
12.14 assume that the memory pool contains three blocks of free storage. their sizes are 1300, 2000, and 1000. give examples of storage requests for which (a) ﬁrst-ﬁt allocation will work, but not best ﬁt or worst ﬁt. (b) best-ﬁt allocation will work, but not ﬁrst ﬁt or worst ﬁt. (c) worst-ﬁt allocation will work, but not ﬁrst ﬁt or best ﬁt.
• insert an element at a given position, • delete an element from a given position, • return the value of the element at a given position, • take the transpose of a matrix, and
remainder of size 50. best ﬁt has the disadvantage that it requires that the entire list be searched. another problem is that the remaining portion of the best-ﬁt block is likely to be small, and thus useless for future requests. in other words, best ﬁt tends to maximize problems of external fragmentation while it minimizes the chance of not being able to service an occasional large request.
a strategy contrary to best ﬁt might make sense because it tends to minimize the effects of external fragmentation. this is called worst ﬁt, which always allocates the largest block on the list hoping that the remainder of the block will be useful for servicing a future request. in our example, the worst ﬁt is the block of size 900, leaving a remainder of size 300. if there are a few unusually large requests, this approach will have less chance of servicing them. if requests generally tend to be of the same size, then this might be an effective strategy. like best ﬁt, worst ﬁt requires searching the entire freelist at each memory request to ﬁnd the largest block. alternatively, the freelist can be ordered from largest to smallest free block, possibly by using a priority queue implementation.
which strategy is best? it depends on the expected types of memory requests. if the requests are of widely ranging size, best ﬁt might work well. if the requests tend to be of similar size, with rare large and small requests, ﬁrst or worst ﬁt might work well. unfortunately, there are always request patterns that one of the three sequential ﬁt methods will service, but which the other two will not be able to service. for example, if the series of requests 600, 650, 900, 500, 100 is made to a freelist containing blocks 500, 700, 650, 900 (in that order), the requests can all be serviced by ﬁrst ﬁt, but not by best ﬁt. alternatively, the series of requests 600, 500, 700, 900 can be serviced by best ﬁt but not by ﬁrst ﬁt on this same freelist.
sequential-ﬁt methods rely on a linked list of free blocks, which must be searched for a suitable block at each memory request. thus, the time to ﬁnd a suitable free block would be Θ(n) in the worst case for a freelist containing n blocks. merging adjacent free blocks is somewhat complicated. finally, we must either use additional space for the linked list, or use space within the memory pool to support the memory manager operations. in the second option, both free and reserved blocks require tag and size ﬁelds. fields in free blocks do not cost any space (because they are stored in memory that is not otherwise being used), but ﬁelds in reserved blocks create additional overhead.
the buddy system solves most of these problems. searching for a block of the proper size is efﬁcient, merging adjacent free blocks is simple, and no tag or other information ﬁelds need be stored within reserved blocks. the buddy system
example 1.6 when operating a car, the primary activities are steering, accelerating, and braking. on nearly all passenger cars, you steer by turning the steering wheel, accelerate by pushing the gas pedal, and brake by pushing the brake pedal. this design for cars can be viewed as an adt with operations “steer,” “accelerate,” and “brake.” two cars might implement these operations in radically different ways, say with different types of engine, or front- versus rear-wheel drive. yet, most drivers can operate many different cars because the adt presents a uniform method of operation that does not require the driver to understand the speciﬁcs of any particular engine or drive design. these differences are deliberately hidden.
the concept of an adt is one instance of an important principle that must be understood by any successful computer scientist: managing complexity through abstraction. a central theme of computer science is complexity and techniques for handling it. humans deal with complexity by assigning a label to an assembly of objects or concepts and then manipulating the label in place of the assembly. cognitive psychologists call such a label a metaphor. a particular label might be related to other pieces of information or other labels. this collection can in turn be given a label, forming a hierarchy of concepts and labels. this hierarchy of labels allows us to focus on important issues while ignoring unnecessary details.
example 1.7 we apply the label “hard drive” to a collection of hardware that manipulates data on a particular type of storage device, and we apply the label “cpu” to the hardware that controls execution of computer instructions. these and other labels are gathered together under the label “computer.” because even small home computers have millions of components, some form of abstraction is necessary to comprehend how a computer operates.
consider how you might go about the process of designing a complex computer program that implements and manipulates an adt. the adt is implemented in one part of the program by a particular data structure. while designing those parts of the program that use the adt, you can think in terms of operations on the data type without concern for the data structure’s implementation. without this ability to simplify your thinking about a complex program, you would have no hope of understanding or implementing it.
to understand once you have mastered this book is algorithms by robert sedgewick [sed03]. for an excellent and highly readable (but more advanced) teaching introduction to algorithms, their design, and their analysis, see introduction to algorithms: a creative approach by udi manber [man89]. for an advanced, encyclopedic approach, see introduction to algorithms by cormen, leiserson, and rivest [clrs01]. steven s. skiena’s the algorithm design manual [ski98] provides pointers to many implementations for data structures and algorithms that are available on the web.
for a gentle introduction to adts and program speciﬁcation, see abstract data types: their speciﬁcation, representation, and use by thomas, robinson, and emms [tre88].
the claim that all modern programming languages can implement the same algorithms (stated more precisely, any function that is computable by one programming language is computable by any programming language with certain standard capabilities) is a key result from computability theory. for an easy introduction to this ﬁeld see james l. hein, discrete structures, logic, and computability [hei03]. much of computer science is devoted to problem solving. indeed, this is what attracts many people to the ﬁeld. how to solve it by george p´olya [p´ol57] is considered to be the classic work on how to improve your problem-solving abilities. if you want to be a better student (as well as a better problem solver in general), see strategies for creative problem solving by folger and leblanc [fl95], effective problem solving by marvin levine [lev94], and problem solving & comprehension by arthur whimbey and jack lochhead [wl99].
see the origin of consciousness in the breakdown of the bicameral mind by julian jaynes [jay90] for a good discussion on how humans use the concept of metaphor to handle complexity. more directly related to computer science education and programming, see “cogito, ergo sum! cognitive processes of students dealing with data structures” by dan aharoni [aha00] for a discussion on moving from programming-context thinking to higher-level (and more design-oriented) programming-free thinking.
on a more pragmatic level, most people study data structures to write better programs. if you expect your program to work correctly and efﬁciently, it must ﬁrst be understandable to yourself and your co-workers. kernighan and pike’s the practice of programming [kp99] discusses a number of practical issues related to programming, including good coding and documentation style. for an excellent (and entertaining!) introduction to the difﬁculties involved with writing large programs, read the classic the mythical man-month: essays on software engineering by frederick p. brooks [bro95].
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
a great discussion on external sorting methods can be found in salzberg’s book. the presentation in this chapter is similar in spirit to salzberg’s.
for details on disk drive modeling and measurement, see the article by ruemmler and wilkes, “an introduction to disk drive modeling” [rw94]. see andrew s. tanenbaum’s structured computer organization [tan06] for an introduction to computer hardware and organization. an excellent, detailed description of memory and hard disk drives can be found online at “the pc guide,” by charles m. kozierok [koz05] (www.pcguide.com). the pc guide also gives detailed descriptions of the microsoft windows and unix (linux) ﬁle systems.
see “outperforming lru with an adaptive replacement cache algorithm” by megiddo and modha for an example of a more sophisticated algorithm than lru for managing buffer pools.
8.1 computer memory and storage prices change rapidly. find out what the current prices are for the media listed in figure 8.1. does your information change any of the basic conclusions regarding disk processing?
8.2 assume a disk drive from the late 1990s is conﬁgured as follows. the total storage is approximately 675mb divided among 15 surfaces. each surface has 612 tracks; there are 144 sectors/track, 512 bytes/sector, and 8 sectors/cluster. the disk turns at 3600 rpm. the track-to-track seek time is 20 ms, and the average seek time is 80 ms. now assume that there is a 360kb ﬁle on the disk. on average, how long does it take to read all of the data in the ﬁle? assume that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on adjacent tracks, and that the ﬁle completely ﬁlls each track on which it is found. a seek must be performed each time the i/o head moves to a new track. show your calculations.
8.3 using the speciﬁcations for the disk drive given in exercise 8.2, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
we sort many things in our everyday lives: a handful of cards when playing bridge; bills and other piles of paper; jars of spices; and so on. and we have many intuitive strategies that we can use to do the sorting, depending on how many objects we have to sort and how hard they are to move around. sorting is also one of the most frequently performed computing tasks. we might sort the records in a database so that we can search the collection efﬁciently. we might sort the records by zip code so that we can print and mail them more cheaply. we might use sorting as an intrinsic part of an algorithm to solve some other problem, such as when computing the minimum-cost spanning tree (see section 11.5).
because sorting is so important, naturally it has been studied intensively and many algorithms have been devised. some of these algorithms are straightforward adaptations of schemes we use in everyday life. others are totally alien to how humans do things, having been invented to sort thousands or even millions of records stored on the computer. after years of study, there are still unsolved problems related to sorting. new algorithms are still being developed and reﬁned for specialpurpose applications.
while introducing this central problem in computer science, this chapter has a secondary purpose of illustrating many important issues in algorithm design and analysis. the collection of sorting algorithms presented will illustate that divideand-conquer is a powerful approach to solving a problem, and that there are multiple ways to do the dividing. mergesort divides a list in half. quicksort divides a list into big values and small values. and radix sort divides the problem by working on one digit of the key at a time.
sorting algorithms will be used to illustrate a wide variety of analysis techniques in this chapter. we’ll ﬁnd that it is possible for an algorithm to have an average case whose growth rate is signiﬁcantly smaller than its worse case (quicksort). we’ll see how it is possible to speed up sorting algorithms (both shellsort
over the time required to ﬁnd the k largest elements using one of the other sorting methods described earlier. one situation where we are able to take advantage of this concept is in the implementation of kruskal’s minimum-cost spanning tree (mst) algorithm of section 11.5.2. that algorithm requires that edges be visited in ascending order (so, use a min-heap), but this process stops as soon as the mst is complete. thus, only a relatively small fraction of the edges need be sorted.
imagine that for the past year, as you paid your various bills, you then simply piled all the paperwork onto the top of a table somewhere. now the year has ended and its time to sort all of these papers by what the bill was for (phone, electricity, rent, etc.) and date. a pretty natural approach is to make some space on the ﬂoor, and as you go through the pile of papers, put the phone bills into one pile, the electric bills into another pile, and so on. once this initial assignment of bills to piles is done (in one pass), you can sort each pile by date relatively quickly because they are each fairly small. this is the basic idea behind a binsort. numbers 0 through n − 1:
here the key value is used to determine the position for a record in the ﬁnal sorted array. this is the most basic example of a binsort, where key values are used to assign records to bins. this algorithm is extremely efﬁcient, taking Θ(n) time regardless of the initial ordering of the keys. this is far better than the performance of any sorting algorithm that we have seen so far. the only problem is that this algorithm has limited use because it works only for a permutation of the numbers from 0 to n − 1.
we can extend this simple binsort algorithm to be more useful. because binsort must perform direct computation on the key value (as opposed to just asking which of two records comes ﬁrst as our previous sorting algorithms did), we will assume that the records use an integer key type.
the simplest extension is to allow for duplicate values among the keys. this can be done by turning array slots into arbitrary-length bins by turning b into an array of linked lists. in this way, all records with key value i can be placed in bin b[i]. a second extension allows for a key range greater than n. for example, a set of n records might have keys in the range 1 to 2n. the only requirement is
figure 11.18 illustrates dijkstra’s algorithm. the start vertex is a. all vertices except a have an initial value of ∞. after processing vertex a, its neighbors have their d estimates updated to be the direct distance from a. after processing c (the closest vertex to a), vertices b and e are updated to reﬂect the shortest path through c. the remaining vertices are processed in order b, d, and e.
this section presents two algorithms for determining the minimum-cost spanning tree (mst) for a graph. the mst problem takes as input a connected, undirected graph g, where each edge has a distance or weight measure attached. the mst is the graph containing the vertices of g along with the subset of g’s edges that (1) has minimum total cost as measured by summing the values for all of the edges in the subset, and (2) keeps the vertices connected. applications where a solution to this problem is useful include soldering the shortest set of wires needed to connect a set of terminals on a circuit board, and connecting a set of cities by telephone lines in such a way as to require the least amount of cable.
the mst contains no cycles. if a proposed set of edges did have a cycle, a cheaper mst could be had by removing any one of the edges in the cycle. thus, the mst is a free tree with |v|−1 edges. the name “minimum-cost spanning tree” comes from the fact that the required set of edges forms a tree, it spans the vertices (i.e., it connects them together), and it has minimum cost. figure 11.19 shows the mst for an example graph.
mst. the third edge we process is (c, f), which causes the mst containing vertices c and d to merge with mst containing vertices e and f. the next edge to process is (d, f). but because vertices d and f are currently in the same mst, this edge is rejected. the algorithm will continue on to accept edges (b, c) and (a, c) into the mst.
the edges can be processed in order of weight by using a min-heap. this is generally faster than sorting the edges ﬁrst, because in practice we need only visit a small fraction of the edges before completing the mst. this is an example of ﬁnding only a few smallest elements in a list, as discussed in section 7.6.
the only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. fortunately, the ideal algorithm is available for the purpose — the union/find algorithm based on the parent pointer representation for trees described in section 6.2. figure 11.24 shows an implementation for the algorithm. class kruskalelem is used to store the edges on the min-heap.
kruskal’s algorithm is dominated by the time required to process the edges. the differ and union functions are nearly constant in time if path compression and weighted union is used. thus, the total cost of the algorithm is Θ(|e| log |e|) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. more often the edges of the spanning tree are the shorter ones,and only about |v| edges must be processed. if so, the cost is often close to Θ(|v| log |e|) in the average case.
many interesting properties of graphs can be investigated by playing with the programs in the stanford graphbase. this is a collection of benchmark databases and graph processing programs. the stanford graphbase is documented in [knu94].
11.7 exercises 11.1 prove by induction that a graph with n vertices has at most n(n−1)/2 edges. 11.2 prove the following implications regarding free trees.
11.10 show the shortest paths generated by running dijkstra’s shortest-paths algorithm on the graph of figure 11.25, beginning at vertex 4. show the d values as each vertex is processed, as in figure 11.18.
11.12 the root of a dag is a vertex r such that every vertex of the dag can be reached by a directed path from r. write an algorithm that takes a directed graph as input and determines the root (if there is one) for the graph. the running time of your algorithm should be Θ(|v| + |e|).
11.13 write an algorithm to ﬁnd the longest path in a dag, where the length of the path is measured by the number of edges that it contains. what is the asymptotic complexity of your algorithm? 11.14 write an algorithm to determine whether a directed graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v| + |e|) time. 11.15 write an algorithm to determine whether an undirected graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v|) time.
11.16 the single-destination shortest-paths problem for a directed graph is to ﬁnd the shortest path from every vertex to a speciﬁed vertex v. write an algorithm to solve the single-destination shortest-paths problem.
11.17 list the order in which the edges of the graph in figure 11.25 are visited when running prim’s mst algorithm starting at vertex 3. show the ﬁnal mst.
11.18 list the order in which the edges of the graph in figure 11.25 are visited when running kruskal’s mst algorithm. each time an edge is added to the mst, show the result on the equivalence array, (e.g., show the array as in figure 6.7).
11.20 when can prim’s and kruskal’s algorithms yield different msts? 11.21 prove that, if the costs for the edges of graph g are distinct, then only one
11.23 consider the collection of edges selected by dijkstra’s algorithm as the shortest paths to the graph’s vertices from the start vertex. do these edges form a spanning tree (not necessarily of minimum cost)? do these edges form an mst? explain why or why not.
11.24 prove that a tree is a bipartite graph. 11.25 prove that any tree can be two-colored. 11.26 write an algorithm that deterimines if an arbitrary undirected graph is a bipartite graph. if the graph is bipartite, then your algorithm should also identify the vertices as to which of the two partitions each belongs to.
11.1 design a format for storing graphs in ﬁles. then implement two functions: one to read a graph from a ﬁle and the other to write a graph to a ﬁle. test your functions by implementing a complete mst program that reads an undirected graph in from a ﬁle, constructs the mst, and then writes to a second ﬁle the graph representing the mst.
11.2 an undirected graph need not explicitly store two separate directed edges to represent a single undirected edge. an alternative would be to store only a single undirected edge (i, j) to connect vertices i and j. however, what if the user asks for edge (j, i)? we can solve this problem by consistently storing the edge such that the lesser of i and j always comes ﬁrst. thus, if we have an edge connecting vertices 5 and 3, requests for edge (5, 3) and (3, 5) both map to (3, 5) because 3 < 5. looking at the adacency matrix, we notice that only the lower triangle of the array is used. thus we could cut the space required by the adjacency matrix from |v|2 positions to |v|(|v|− 1)/2 positions. read section 12.2 on triangular matrices. the reimplement the adjacency matrix representation of figure 11.6 to implement undirected graphs using a triangular array.
11.3 while the underlying implementation (whether adjacency matrix or adjacency list) is hidden behind the graph adt, these two implementations can have an impact on the efﬁciency of the resulting program. for dijkstra’s shortest paths algorithm, two different implementations were given in section 11.4.1 that provide diffent ways for determining the next closest vertex
this book describes many data structures that can be used in a wide variety of problems. there are also many examples of efﬁcient algorithms. in general, our search algorithms strive to be at worst in o(log n) to ﬁnd a record, while our sorting algorithms strive to be in o(n log n). a few algorithms have higher asymptotic complexity, such as floyd’s all-pairs shortest-paths algorithm, whose running time is Θ(n3).
we can solve many problems efﬁciently because we have available (and choose to use) efﬁcient algorithms. given any problem for which you know some algorithm, it is always possible to write an inefﬁcient algorithm to “solve” the problem. for example, consider a sorting algorithm that tests every possible permutation of its input until it ﬁnds the correct permutation that provides a sorted list. the running time for this algorithm would be unacceptably high, because it is proportional to the number of permutations which is n! for n inputs. when solving the minimumcost spanning tree problem, if we were to test every possible subset of edges to see which forms the shortest minimum spanning tree, the amount of work would be proportional to 2|e| for a graph with |e| edges. fortunately, for both of these problems we have more clever algorithms that allow us to ﬁnd answers (relatively) quickly without explicitly testing every possible solution.
unfortunately, there are many computing problems for which the best possible algorithm takes a long time to run. a simple example is the towers of hanoi problem, which requires 2n moves to “solve” a tower with n disks. it is not possible for any computer program that solves the towers of hanoi problem to run in less than Ω(2n) time, because that many moves must be printed out.
besides those problems whose solutions must take a long time to run, there are also many problems for which we simply do not know if there are efﬁcient algorithms or not. the best algorithms that we know for such problems are very
example 2.1 for the integers, = is an equivalence relation that partitions each element into a distinct subset. in other words, for any integer a, three things are true.
1. a = a, 2. if a = b then b = a, and 3. if a = b and b = c, then a = c. of course, for distinct integers a, b, and c there are never cases where a = b, b = a, or b = c. so the claims that = is symmetric and transitive are vacuously true (there are never examples in the relation where these events occur). but becausethe requirements for symmetry and transitivity are not violated, the relation is symmetric and transitive.
example 2.2 if we clarify the deﬁnition of sibling to mean that a person is a sibling of him- or herself, then the sibling relation is an equivalence relation that partitions the set of people.
example 2.3 we can use the modulus function (deﬁned in the next section) to deﬁne an equivalence relation. for the set of integers, use the modulus function to deﬁne a binary relation such that two numbers x and y are in the relation if and only if x mod m = y mod m. thus, for m = 4, h1, 5i is in the relation because 1 mod 4 = 5 mod 4. we see that modulus used in this way deﬁnes an equivalence relation on the integers, and this relation can be used to partition the integers into m equivalence classes. this relation is an equivalence relation because
a binary relation is called a partial order if it is antisymmetric and transitive.2 the set on which the partial order is deﬁned is called a partially ordered set or a poset. elements x and y of a set are comparable under a given relation if either
2not all authors use this deﬁnition for partial order. i have seen at least three signiﬁcantly different deﬁnitions in the literature. i have selected the one that lets < and ≤ both deﬁne partial orders on the integers, becausethis seems the most natural to me.
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
in most applications, we have no means of knowing in advance the frequencies of access for the data records. to complicate matters further, certain records might be accessed frequently for a brief period of time, and then rarely thereafter. thus, the probability of access for records might change over time (in most database systems, this is to be expected). self-organizing lists seek to solve both of these problems.
self-organizing lists modify the order of records within the list based on the actual pattern of record access. self-organizing lists use a heuristic for deciding how to to reorder the list. these heuristics are similar to the rules for managing buffer pools (see section 8.3). in fact, a buffer pool is a form of self-organizing list. ordering the buffer pool by expected frequency of access is a good strategy, because typically we must search the contents of the buffers to determine if the desired information is already in main memory. when ordered by frequency of access, the buffer at the end of the list will be the one most appropriate for reuse when a new page of information must be read. below are three traditional heuristics for managing self-organizing lists:
1. the most obvious way to keep a list ordered by frequency would be to store a count of accesses to each record and always maintain records in this order. this method will be referred to as count. count is similar to the least frequently used buffer replacement strategy. whenever a record is accessed, it might move toward the front of the list if its number of accesses becomes greater than a record preceding it. thus, count will store the records in the order of frequency that has actually occurred so far. besides requiring space for the access counts, count does not react well to changing frequency of access over time. once a record has been accessed a large number of times under the frequency count system, it will remain near the front of the list regardless of further access history.
2. bring a record to the front of the list when it is found, pushing all the other records back one position. this is analogous to the least recently used buffer replacement strategy and is called move-to-front. this heuristic is easy to implement if the records are stored using a linked list. when records are stored in an array, bringing a record forward from near the end of the array will result in a large number of records changing position. move-to-front’s cost is bounded in the sense that it requires at most twice the number of accesses required by the optimal static ordering for n records when at least
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
this analysis is unreasonably pessimistic. clearly it is not really possible to pop m1 elements each time multipop is called. analysis that focuses on single operations cannot deal with this global limit, and so we turn to amortized analysis to model the entire series of operations.
the key to an amortized analysis of this problem lies in the concept of potential. at any given time, a certain number of items may be on the stack. the cost for multipop can be no more than this number of items. each call to push places another item on the stack, which can be removed by only a single multipop operation. thus, each call to push raises the potential of the stack by one item. the sum of costs for all calls to multipop can never be more than the total potential of the stack (aside from a constant time cost associated with each call to multipop itself).
the amortized cost for any series of push and multipop operations is the sum of three costs. first, each of the push operations takes constant time. second, each multipop operation takes a constant time in overhead, regardless of the number of items popped on that call. finally, we count the sum of the potentials expended by all multipop operations, which is at most m1, the number of push operations. this total cost can therefore be expressed as
a similar argument was used in our analysis for the partition function in the quicksort algorithm (section 7.5). while on any given pass through the while loop the left or right pointers might move all the way through the remainder of the partition, doing so would reduce the number of times that the while loop can be further executed.
our ﬁnal example uses amortized analysis to prove a relationship between the cost of the move-to-front self-organizing list heuristic from section 9.2 and the cost for the optimal static ordering of the list.
recall that, for a series of search operations, the minimum cost for a static list results when the list is sorted by frequency of access to its records. this is the optimal ordering for the records if we never allow the positions of records to change, because the most frequently accessed record is ﬁrst (and thus has least cost), followed by the next most frequently accessed record, and so on.
theorem 14.2 the total number of comparisons required by any series s of n or more searches on a self-organizing list of length n using the move-to-front heuristic is never more than twice the total number of comparisons required when series s is applied to the list stored in its optimal static order.
proof: each comparison of the search key with a record in the list is either successful or unsuccessful. for m searches, there must be exactly m successful comparisons for both the self-organizing list and the static list. the total number of unsuccessful comparisons in the self-organizing list is the sum, over all pairs of distinct keys, of the number of unsuccessful comparisons made between that pair. consider a particular pair of keys a and b. for any sequence of searches s, the total number of (unsuccessful) comparisons between a and b is identical to the number of comparisons between a and b required for the subsequence of s made up only of searches for a or b. call this subsequence sab. in other words, including searches for other keys does not affect the relative position of a and b and so does not affect the relative contribution to the total cost of the unsuccessful comparisons between a and b.
the number of unsuccessful comparisons between a and b made by the moveto-front heuristic on subsequence sab is at most twice the number of unsuccessful comparisons between a and b required when sab is applied to the optimal static ordering for the list. to see this, assume that sab contains i as and j bs, with i ≤ j. under the optimal static ordering, i unsuccessful comparisons are required because b must appear before a in the list (because its access frequency is higher). move-tofront will yield an unsuccessful comparison whenever the request sequence changes from a to b or from b to a. the total number of such changes possible is 2i because each change involves an a and each a can be part of at most two changes.
because the total number of unsuccessful comparisons required by move-tofront for any given pair of keys is at most twice that required by the optimal static ordering, the total number of unsuccessful comparisons required by move-to-front for all pairs of keys is also at most twice as high. because the number of successful comparisons is the same for both methods, the total number of comparisons required by move-to-front is less than twice the number of comparisons required by the optimal static ordering. 2
a good introduction to solving recurrence relations appears in applied combinatorics by fred s. roberts [rob84]. for a more advanced treatment, see concrete mathematics by graham, knuth, and patashnik [gkp94].
cormen, leiserson, and rivest provide a good discussion on various methods for performing amortized analysis in introduction to algorithms [clrs01]. for an amortized analysis that the splay tree requires m log n time to perform a series of m operations on n nodes when m > n, see “self-adjusting binary search trees” by sleator and tarjan [st85]. the proof for theorem 14.2 comes from
|p| = 3 (because p has three members) and |q| = 2 (because q has two members). the union of p and q, written p ∪ q, is the set of elements in either p or q, which is {2, 3, 5, 10}. the intersection of p and q, written p ∩ q, is the set of elements that appear in both p and q, which is {5}. the set difference of p and q, written p − q, is the set of elements that occur in p but not in q, which is {2, 3}. note that p ∪ q = q ∪ p and that p ∩ q = q ∩ p, but in general p − q 6= q − p. in this example, q − p = {10}. note that the set {4, 3, 5} is indistinguishable from set p, because sets have no concept of order. likewise, set {4, 3, 4, 5} is also indistinguishable from p, because sets have no concept of duplicate elements. s = {a, b, c}. the powerset of s is
sometimes we wish to deﬁne a collection of elements with no order (like a set), but with duplicate-valued elements. such a collection is called a bag.1 to distinguish bags from sets, i use square brackets [] around a bag’s elements. for
simple lists and arrays are the right tool for the many applications. other situations require support for operations that cannot be implemented efﬁciently by the standard list representations of chapter 4. this chapter presents advanced implementations for lists and arrays that overcome some of the problems of simple linked list and contiguous array representations. a wide range of topics are covered, whose unifying thread is that the data structures are all list- or array-like. this chapter should also serve to reinforce the concept of logical representation versus physical implementation, as some of the “list” implementations have quite different organizations internally.
section 12.1 describes a series of representations for multilists, which are lists that may contain sublists. section 12.2 discusses representations for implementing sparse matrices, large matrices where most of the elements have zero values. section 12.3 discusses memory management techniques, which are essentially a way of allocating variable-length sections from a large array.
recall from chapter 4 that a list is a ﬁnite, ordered sequence of items of the form hx0, x1, ..., xn−1i where n ≥ 0. we can represent the empty list by null or hi. in chapter 4 we assumed that all list elements had the same data type. in this section, we extend the deﬁnition of lists to allow elements to be arbitrary in nature. in general, list elements are one of two types.
figure 12.4 linked representation for the pure list of figure 12.1. the ﬁrst ﬁeld in each link node stores a tag bit. if the tag bit stores “+,” then the data ﬁeld stores an atom. if the tag bit stores “−,” then the data ﬁeld stores a pointer to a sublist.
figure 12.5 lisp-like linked representation for the cyclic multilist of figure 12.3. each link node stores two pointers. a pointer either points to an atom, or to another link node. link nodes are represented by two boxes, and atoms by circles.
represented by linked lists. pure lists can be represented as linked lists with an additional tag ﬁeld to indicate whether the node is an atom or a sublist. if it is a sublist, the data ﬁeld points to the ﬁrst element on the sublist. this is illustrated by figure 12.4.
another approach is to represent all list elements with link nodes storing two pointer ﬁelds, except for atoms. atoms just contain data. this is the system used by the programming language lisp. figure 12.5 illustrates this representation. either the pointer contains a tag bit to identify what it points to, or the object being pointed to stores a tag bit to identify itself. tags distinguish atoms from list nodes. this implementation can easily support reentrant and cyclic lists, because non-atoms can point to any other node.
multilist in graphical form such as shown in figure 12.2. (a) ha, b,hc, d, ei,hf,hgi, hii (b) ha, b,hc, d, l1: ei, l1i (c) hl1: a, l1,hl2: bi, l2,hl1ii (a) show the bracket notation for the list of figure 12.19(a). (b) show the bracket notation for the list of figure 12.19(b). (c) show the bracket notation for the list of figure 12.19(c).
write an in-place reversal algorithm to reverse the sublists at all levels including the topmost level. for this example, the result would be a linked representation corresponding to
12.4 what fraction of the values in a matrix must be zero for the sparse matrix representation of section 12.2 to be more space efﬁcient than the standard two-dimensional matrix representation when data values require eight bytes, array indices require two bytes, and pointers require four bytes?
12.7 write a function to transpose a sparse matrix as represented in section 12.2. 12.8 write a function to add two sparse matrices as represented in section 12.2.
1.12 imagine that you have been hired to design a database service containing information about cities and towns in the united states, as described in example 1.2. suggest two possible implementations for the database.
1.13 imagine that you are given an array of records that is sorted with respect to some key ﬁeld contained in each record. give two different algorithms for searching the array to ﬁnd the record with a speciﬁed key value. which one do you consider “better” and why?
1.15 a common problem for compilers and text editors is to determine if the parentheses (or other brackets) in a string are balanced and properly nested. for example, the string “((())())()” contains properly nested pairs of parentheses, but the string “)()(” does not; and the string “())” does not contain properly matching parentheses. (a) give an algorithm that returns true if a string contains properly nested and balanced parentheses, and false if otherwise. hint: at no time while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses.
(b) give an algorithm that returns the position in the string of the ﬁrst offending parenthesis if the string is not properly nested and balanced. that is, if an excess right parenthesis is found, return its position; if there are too many left parentheses, return the position of the ﬁrst excess left parenthesis. return −1 if the string is properly balanced and nested.
1.16 a graph consists of a set of objects (called vertices) and a set of edges, where each edge connects two vertices. any given pair of vertices can be connected by only one edge. describe at least two different ways to represent the connections deﬁned by the vertices and edges of a graph.
1.17 imagine that you are a shipping clerk for a large company. you have just been handed about 1000 invoices, each of which is a single sheet of paper with a large number in the upper right corner. the invoices must be sorted by this number, in order from lowest to highest. write down as many different approaches to sorting the invoices as you can think of.
4.19 a common problem for compilers and text editors is to determine if the parentheses (or other brackets) in a string are balanced and properly nested. for example, the string “((())())()” contains properly nested pairs of parentheses, but the string “)()(” does not, and the string “())” does not contain properly matching parentheses. (a) give an algorithm that returns true if a string contains properly nested and balanced parentheses, and false otherwise. use a stack to keep track of the number of left parentheses seen so far. hint: at no time while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses.
(b) give an algorithm that returns the position in the string of the ﬁrst offending parenthesis if the string is not properly nested and balanced. that is, if an excess right parenthesis is found, return its position; if there are too many left parentheses, return the position of the ﬁrst excess left parenthesis. return −1 if the string is properly balanced and nested. use a stack to keep track of the number and positions of left parentheses seen so far.
4.20 imagine that you are designing an application where you need to perform the operations insert, delete maximum, and delete minimum. for this application, the cost of inserting is not important, because it can be done off-line prior to startup of the time-critical section, but the performance of the two deletion operations are critical. repeated deletions of either kind must work as fast as possible. suggest a data structure that can support this application, and justify your suggestion. what is the time complexity for each of the three key operations?
4.1 a deque (pronounced “deck”) is like a queue, except that items may be added and removed from both the front and the rear. write either an array-based or linked implementation for the deque.
4.2 one solution to the problem of running out of space for an array-based list implementation is to replace the array with a larger array whenever the original array overﬂows. a good rule that leads to an implementation that is both space and time efﬁcient is to double the current size of the array when there is an overﬂow. reimplement the array-based list class of figure 4.2 to support this array-doubling rule.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
distance to every such vertex, rather than recording the actual path. recording the path requires modiﬁcations to the algorithm that are left as an exercise.
computer networks provide an application for the single-source shortest-paths problem. the goal is to ﬁnd the cheapest way for one computer to broadcast a message to all other computers on the network. the network can be modeled by a graph with edge weights indicating time or cost to send a message to a neighboring computer.
for unweighted graphs (or whenever all edges have the same cost), the singlesource shortest paths can be found using a simple breadth-ﬁrst search. when weights are added, bfs will not give the correct answer.
one approach to solving this problem when the edges have differing weights might be to process the vertices in a ﬁxed order. label the vertices v0 to vn−1, with s = v0. when processing vertex v1, we take the edge connecting v0 and v1. when processing v2, we consider the shortest distance from v0 to v2 and compare that to the shortest distance from v0 to v1 to v2. when processing vertex vi, we consider the shortest path for vertices v0 through vi−1 that have already been processed. unfortunately, the true shortest path to vi might go through vertex vj for j > i. such a path will not be considered by this algorithm. however, the problem would not occur if we process the vertices in order of distance from s. assume that we have processed in order of distance from s to the ﬁrst i − 1 vertices that are closest to s; call this set of vertices s. we are now about to process the ith closest vertex; call it x. a shortest path from s to x must have its next-to-last vertex in s. thus,
in other words, the shortest path from s to x is the minimum over all paths that go from s to u, then have an edge from u to x, where u is some vertex in s.
this solution is usually referred to as dijkstra’s algorithm. it works by maintaining a distance estimate d(x) for all vertices x in v. the elements of d are initialized to the value infinite. vertices are processed in order of distance from s. whenever a vertex v is processed, d(x) is updated for every neighbor x of v. figure 11.16 shows an implementation for dijkstra’s algorithm. at the end, array d will contain the shortest distance values.
there are two reasonable solutions to the key issue of ﬁnding the unvisited vertex with minimum distance value during each pass through the main for loop. the ﬁrst method is simply to scan through the list of |v| vertices searching for the minimum value, as follows:
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
when we want an estimate of the running time or other resource requirements of an algorithm. this simpliﬁes the analysis and keeps us thinking about the most important aspect: the growth rate. this is called asymptotic algorithm analysis. to be precise, asymptotic analysis refers to the study of an algorithm as the input size “gets big” or reaches a limit (in the calculus sense). however, it has proved to be so useful to ignore all constant factors that asymptotic analysis is used for most algorithm comparisons.
it is not always reasonable to ignore the constants. when comparing algorithms meant to run on small values of n, the constant can have a large effect. for example, if the problem is to sort a collection of exactly ﬁve records, then an algorithm designed for sorting thousands of records is probably not appropriate, even if its asymptotic analysis indicates good performance. there are rare cases where the constants for two algorithms under comparison can differ by a factor of 1000 or more, making the one with lower growth rate impractical for most purposes due to its large constant. asymptotic analysis is a form of “back of the envelope” estimation for algorithm resource consumption. it provides a simpliﬁed model of the running time or other resource needs of an algorithm. this simpliﬁcation usually helps you understand the behavior of your algorithms. just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important.
several terms are used to describe the running-time equation for an algorithm. these terms — and their associated symbols — indicate precisely what aspect of the algorithm’s behavior is being described. one is the upper bound for the growth of the algorithm’s running time. it indicates the upper or highest growth rate that the algorithm can have.
to make any statement about the upper bound of an algorithm, we must be making it about some class of inputs of size n. we measure this upper bound nearly always on the best-case, average-case, or worst-case inputs. thus, we cannot say, “this algorithm has an upper bound to its growth rate of n2.” we must say something like, “this algorithm has an upper bound to its growth rate of n2 in the average case.”
because the phrase “has an upper bound to its growth rate of f(n)” is long and often used when discussing algorithms, we adopt a special notation, called big-oh notation. if the upper bound for an algorithm’s growth rate (for, say, the worst case) is f(n), then we would write that this algorithm is “in the set o(f(n))in the worst case” (or just “in o(f(n))in the worst case”). for example, if n2 grows as
rule (3) says that given two parts of a program run in sequence (whether two statements or two sections of code), you need consider only the more expensive part. this rule applies to Ω and Θ notations as well: for both, you need consider only the more expensive part.
rule (4) is used to analyze simple loops in programs. if some action is repeated some number of times, and each repetition has the same cost, then the total cost is the cost of the action multiplied by the number of times that the action takes place. this rule applies to Ω and Θ notations as well.
taking the ﬁrst three rules collectively, you can ignore all constants and all lower-order terms to determine the asymptotic growth rate for any cost function. the advantages and dangers of ignoring constants were discussed near the beginning of this section. ignoring lower-order terms is reasonable when performing an asymptotic analysis. the higher-order terms soon swamp the lower-order terms in their contribution to the total cost as n becomes larger. thus, if t(n) = 3n4 + 5n2, then t(n) is in o(n4). the n2 term contributes relatively little to the total cost.
3.4.5 classifying functions given functions f(n) and g(n) whose growth rates are expressed as algebraic equations, we might like to determine if one grows faster than the other. the best way to do this is to take the limit of the two functions as n grows towards inﬁnity,
if the limit goes to ∞, then f(n) is in Ω(g(n)) because f(n) grows faster. if the limit goes to zero, then f(n) is in o(g(n)) because g(n) grows faster. if the limit goes to some constant other than zero, then f(n) = Θ(g(n)) because both grow at the same rate.
3.7 using the deﬁnition of big-oh, show that 1 is in o(1) and that 1 is in o(n). 3.8 using the deﬁnitions of big-oh and Ω, ﬁnd the upper and lower bounds for the following expressions. be sure to state appropriate values for c and n0. (a) c1n (b) c2n3 + c3 (c) c4n log n + c5n (d) c62n + c7n6 (a) what is the smallest integer k such that (b) what is the smallest integer k such that n log n = o(nk)? (a) is 2n = Θ(3n)? explain why or why not. (b) is 2n = Θ(3n)? explain why or why not.
3.11 for each of the following pairs of functions, either f(n) is in o(g(n)), f(n) is in Ω(g(n)), or f(n) = Θ(g(n)). for each pair, determine which relationship is correct. justify your answer, using the method of limits discussed in section 3.4.5. (a) f(n) = log n2; (b) f(n) = (c) f(n) = log2 n; (d) f(n) = n; (e) f(n) = n log n + n; (f) f(n) = log n2; (g) f(n) = 10; (h) f(n) = 2n; (i) f(n) = 2n; (j) f(n) = 2n; (k) f(n) = 2n;
if you want to be a successful java programmer, you need good reference manuals close at hand. david flanagan’s java in a nutshell [fla05] provides a good reference for those familiar with the basics of the language.
after gaining proﬁciency in the mechanics of program writing, the next step is to become proﬁcient in program design. good design is difﬁcult to learn in any discipline, and good design for object-oriented software is one of the most difﬁcult of arts. the novice designer can jump-start the learning process by studying wellknown and well-used design patterns. the classic reference on design patterns is design patterns: elements of reusable object-oriented software by gamma, helm, johnson, and vlissides [ghjv95] (this is commonly referred to as the “gang of four” book). unfortunately, this is an extremely difﬁcult book to understand, in part because the concepts are inherently difﬁcult. a number of web sites are available that discuss design patterns, and which provide study guides for the design patterns book. two other books that discuss object-oriented software design are object-oriented software design and construction with c++ by dennis kafura [kaf98], and object-oriented design heuristics by arthur j. riel [rie96].
the exercises for this chapter are different from those in the rest of the book. most of these exercises are answered in the following chapters. however, you should not look up the answers in other parts of the book. these exercises are intended to make you think about some of the issues to be covered later on. answer them to the best of your ability with your current knowledge.
1.1 think of a program you have used that is unacceptably slow. identify the speciﬁc operations that make the program slow. identify other basic operations that the program performs quickly enough.
1.2 most programming languages have a built-in integer data type. normally this representation has a ﬁxed size, thus placing a limit on how large a value can be stored in an integer variable. describe a representation for integers that has no size restriction (other than the limits of the computer’s available main memory), and thus no practical limit on how large an integer can be stored. brieﬂy show how your representation can be used to implement the operations of addition, multiplication, and exponentiation.
1.3 deﬁne an adt for character strings. your adt should consist of typical functions that can be performed on strings, with each function deﬁned in
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
there are two fundamental approaches to dealing with the relationship between a collection of actions and a hierarchy of object types. first consider the typical procedural approach. say we have a base class for page layout entities, with a subclass hierarchy to deﬁne speciﬁc subtypes (page, columns, rows, ﬁgures, characters, etc.). and say there are actions to be performed on a collection of such objects (such as rendering the objects to the screen). the procedural design approach is for each action to be implemented as a method that takes as a parameter a pointer to the base class type. each such method will traverse through the collection of objects, visiting each object in turn. each method contains something like a case statement that deﬁnes the details of the action for each subclass in the collection (e.g., page, column, row, character). we can cut the code down some by using the visitor design pattern so that we only need to write the traversal once, and then write a visitor subroutine for each action that might be applied to the collection of objects. but each such visitor subroutine must still contain logic for dealing with each of the possible subclasses.
in our page composition application, there are only a few activities that we would like to perform on the page representation. we might render the objects in full detail. or we might want a “rough draft” rendering that prints only the bounding boxes of the objects. if we come up with a new activity to apply to the collection of objects, we do not need to change any of the code that implements the existing activities. but adding new activities won’t happen often for this application. in contrast, there could be many object types, and we might frequently add new object types to our implementation. unfortunately, adding a new object type requires that we modify each activity, and the subroutines implementing the activities get rather long case statements to distinguish the behavior of the many subclasses.
an alternative design is to have each object subclass in the hierarchy embody the action for each of the various activities that might be performed. each subclass will have code to perform each activity (such as full rendering or bounding box rendering). then, if we wish to apply the activity to the collection, we simply call the ﬁrst object in the collection and specify the action (as a method call on that object). in the case of our page layout and its hierarchical collection of objects, those objects that contain other objects (such as a row objects that contains letters) will call the appropriate method for each child. if we want to add a new activity with this organization, we have to change the code for every subclass. but this is relatively rare for our text compositing application. in contrast, adding a new object into the subclass hierarchy (which for this application is far more likely than adding a new rendering function) is easy. adding a new subclass does not require changing
this second design approach of burying the functional activity in the subclasses is called the composite design pattern. a detailed example for using the composite design pattern is presented in section 5.3.1.
our ﬁnal example of a design pattern lets us encapsulate and make interchangeable a set of alternative actions that might be performed as part of some larger activity. again continuing our text compositing example, each output device that we wish to render to will require its own function for doing the actual rendering. that is, the objects will be broken down into constituent pixels or strokes, but the actual mechanics of rendering a pixel or stroke will depend on the output device. we don’t want to build this rendering functionality into the object subclasses. instead, we want to pass to the subroutine performing the rendering action a method or class that does the appropriate rendering details for that output device. that is, we wish to hand to the object the appropriate “strategy” for accomplishing the details of the rendering task. thus, we call this approach the strategy design pattern.
the strategy design pattern will be discussed further in chapter 7. there, a sorting function is given a class (called a comparator) that understands how to extract and compare the key values for records to be sorted. in this way, the sorting function does not need to know any details of how its record type is implemented. one of the biggest challenges to understanding design patterns is that many of them appear to be pretty much the same. for example, you might be confused about the difference between the composite pattern and the visitor pattern. the distinction is that the composite design pattern is about whether to give control of the traversal process to the nodes of the tree or to the tree itself. both approaches can make use of the visitor design pattern to avoid rewriting the traversal function many times, by encapsulating the activity performed at each node.
but isn’t the strategy design pattern doing the same thing? the difference between the visitor pattern and the strategy pattern is more subtle. here the difference is primarily one of intent and focus. in both the strategy design pattern and the visitor design pattern, an activity is being passed in as a parameter. the strategy design pattern is focused on encapsulating an activity that is part of a larger process, so that different ways of performing that activity can be substituted. the visitor design pattern is focused on encapsulating an activity that will be performed on all members of a collection so that completely different activities can be substituted within a generic method that accesses all of the collection members.
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
hidden from users of that tree class. on the other hand, if the nodes are objects that have meaning to users of the tree separate from their existence as nodes in the tree, then the version of figure 5.11 might be preferred because hiding the internal behavior of the nodes becomes more important.
this section presents techniques for calculating the amount of overhead required by a binary tree implementation. recall that overhead is the amount of space necessary to maintain the data structure. in other words, it is any space not used to store data records. the amount of overhead depends on several factors including which nodes store data values (all nodes, or just the leaves), whether the leaves store child pointers, and whether the tree is a full binary tree.
in a simple pointer-based implementation for the binary tree such as that of figure 5.7, every node has two pointers to its children (even when the children are null). this implementation requires total space amounting to n(2p + d) for a tree of n nodes. here, p stands for the amount of space required by a pointer, and d stands for the amount of space required by a data value. the total overhead space will be 2p n for the entire tree. thus, the overhead fraction will be 2p/(2p + d). the actual value for this expression depends on the relative size of pointers versus data ﬁelds. if we arbitrarily assume that p = d, then a full tree has about two thirds of its total space taken up in overhead. worse yet, theorem 5.2 tells us that about half of the pointers are “wasted” null values that serve only to indicate tree structure, but which do not provide access to new data.
if only leaves store data values, then the fraction of total space devoted to overhead depends on whether the tree is full. if the tree is not full, then conceivably there might only be one leaf node at the end of a series of internal nodes. thus, the overhead can be an arbitrarily high percentage for non-full binary trees. the overhead fraction drops as the tree becomes closer to full, being lowest when the tree is truly full. in this case, about one half of the nodes are internal.
great savings can be had by eliminating the pointers from leaf nodes in full binary trees. because about half of the nodes are leaves and half internal nodes, and because only internal nodes now have overhead, the overhead fraction in this case will be approximately
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
a data type is a type together with a collection of operations to manipulate the type. for example, an integer variable is a member of the integer data type. addition is an example of an operation on the integer data type.
a distinction should be made between the logical concept of a data type and its physical implementation in a computer program. for example, there are two traditional implementations for the list data type: the linked list and the array-based list. the list data type can therefore be implemented using a linked list or an array. even the term “array” is ambiguous in that it can refer either to a data type or an implementation. “array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one ﬁxed-length data item. by this meaning, an array is a physical data structure. however, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identiﬁed by an index number. it is possible to implement arrays in many different ways. for example, section 12.2 describes the data structure used to implement a sparse matrix, a large two-dimensional array that stores only a relatively few non-zero values. this implementation is quite different from the physical representation of an array as contiguous memory locations.
an abstract data type (adt) is the realization of a data type as a software component. the interface of the adt is deﬁned in terms of a type and a set of operations on that type. the behavior of each operation is determined by its inputs and outputs. an adt does not specify how the data type is implemented. these implementation details are hidden from the user of the adt and protected from outside access, a concept referred to as encapsulation.
a data structure is the implementation for an adt. in an object-oriented language such as java, an adt and its implementation together make up a class. each operation associated with the adt is implemented by a member function or method. the variables that deﬁne the space required by a data item are referred to as data members. an object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program.
the term “data structure” often refers to data stored in a computer’s main memory. the related term ﬁle structure often refers to the organization of data on peripheral storage, such as a disk drive or cd-rom.
example 1.3 the mathematical concept of an integer, along with operations that manipulate integers, form a data type. the java int variable type is a physical representation of the abstract integer. the int variable type, along with the operations that act on an int variable, form an adt. un-
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
sequential search is practical for large n, in a way that is not true for some other algorithms in o(n2). we always seek to deﬁne the running time of an algorithm with the tightest (lowest) possible upper bound. thus, we prefer to say that sequential search is in o(n). this also explains why the phrase “is in o(f(n))” or the notation “∈ o(f(n))” is used instead of “is o(f(n))” or “= o(f(n)).” there is no strict equality to the use of big-oh notation. o(n) is in o(n2), but o(n2) is not in o(n).
big-oh notation describes an upper bound. in other words, big-oh notation states a claim about the greatest amount of some resource (usually time) that is required by an algorithm for some class of inputs of size n (typically the worst such input, the average of all possible inputs, or the best such input).
similar notation is used to describe the least amount of a resource that an algorithm needs for some class of input. like big-oh notation, this is a measure of the algorithm’s growth rate. like big-oh notation, it works for any resource, but we most often measure the least amount of time required. and again, like big-oh notation, we are measuring the resource required for some particular class of inputs: the worst-, average-, or best-case input of size n.
the lower bound for an algorithm (or a problem, as explained later) is denoted by the symbol Ω, pronounced “big-omega” or just “omega.” the following deﬁnition for Ω is symmetric with the deﬁnition of big-oh.
for t(n) a non-negatively valued function, t(n) is in set Ω(g(n)) if there exist two positive constants c and n0 such that t(n) ≥ cg(n) for all n > n0.1
this deﬁnition says that for an “interesting” number of cases, the algorithm takes at least cg(n) time. note that this deﬁnition is not symmetric with the deﬁnition of big-oh. for g(n) to be a lower bound, this deﬁnition does not require that t(n) ≥ cg(n) for all values of n greater than some constant. it only requires that this happen often enough, in particular that it happen for an inﬁnite number of values for n. motivation for this alternate deﬁnition can be found in the following example.
rule (3) says that given two parts of a program run in sequence (whether two statements or two sections of code), you need consider only the more expensive part. this rule applies to Ω and Θ notations as well: for both, you need consider only the more expensive part.
rule (4) is used to analyze simple loops in programs. if some action is repeated some number of times, and each repetition has the same cost, then the total cost is the cost of the action multiplied by the number of times that the action takes place. this rule applies to Ω and Θ notations as well.
taking the ﬁrst three rules collectively, you can ignore all constants and all lower-order terms to determine the asymptotic growth rate for any cost function. the advantages and dangers of ignoring constants were discussed near the beginning of this section. ignoring lower-order terms is reasonable when performing an asymptotic analysis. the higher-order terms soon swamp the lower-order terms in their contribution to the total cost as n becomes larger. thus, if t(n) = 3n4 + 5n2, then t(n) is in o(n4). the n2 term contributes relatively little to the total cost.
3.4.5 classifying functions given functions f(n) and g(n) whose growth rates are expressed as algebraic equations, we might like to determine if one grows faster than the other. the best way to do this is to take the limit of the two functions as n grows towards inﬁnity,
if the limit goes to ∞, then f(n) is in Ω(g(n)) because f(n) grows faster. if the limit goes to zero, then f(n) is in o(g(n)) because g(n) grows faster. if the limit goes to some constant other than zero, then f(n) = Θ(g(n)) because both grow at the same rate.
3.7 using the deﬁnition of big-oh, show that 1 is in o(1) and that 1 is in o(n). 3.8 using the deﬁnitions of big-oh and Ω, ﬁnd the upper and lower bounds for the following expressions. be sure to state appropriate values for c and n0. (a) c1n (b) c2n3 + c3 (c) c4n log n + c5n (d) c62n + c7n6 (a) what is the smallest integer k such that (b) what is the smallest integer k such that n log n = o(nk)? (a) is 2n = Θ(3n)? explain why or why not. (b) is 2n = Θ(3n)? explain why or why not.
3.11 for each of the following pairs of functions, either f(n) is in o(g(n)), f(n) is in Ω(g(n)), or f(n) = Θ(g(n)). for each pair, determine which relationship is correct. justify your answer, using the method of limits discussed in section 3.4.5. (a) f(n) = log n2; (b) f(n) = (c) f(n) = log2 n; (d) f(n) = n; (e) f(n) = n log n + n; (f) f(n) = log n2; (g) f(n) = 10; (h) f(n) = 2n; (i) f(n) = 2n; (j) f(n) = 2n; (k) f(n) = 2n;
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
4. it must be composed of a ﬁnite number of steps. if the description for the algorithm were made up of an inﬁnite number of steps, we could never hope to write it down, nor implement it as a computer program. most languages for describing algorithms (including english and “pseudocode”) provide some way to perform repeated actions, known as iteration. examples of iteration in programming languages include the while and for loop constructs of java. iteration allows for short descriptions, with the number of steps actually performed controlled by the input.
programs: we often think of a computer program as an instance, or concrete representation, of an algorithm in some programming language. in this book, nearly all of the algorithms are presented in terms of programs, or parts of programs. naturally, there are many programs that are instances of the same algorithm, because any modern computer programming language can be used to implement the same collection of algorithms (although some programming languages can make life easier for the programmer). to simplify presentation throughout the remainder of the text, i often use the terms “algorithm” and “program” interchangeably, despite the fact that they are really separate concepts. by deﬁnition, an algorithm must provide sufﬁcient detail that it can be converted into a program when needed.
the requirement that an algorithm must terminate means that not all computer programs meet the technical deﬁnition of an algorithm. your operating system is one such program. however, you can think of the various tasks for an operating system (each with associated inputs and outputs) as individual problems, each solved by speciﬁc algorithms implemented by a part of the operating system program, and each one of which terminates once its output is produced.
to summarize: a problem is a function or a mapping of inputs to outputs. an algorithm is a recipe for solving a problem whose steps are concrete and unambiguous. the algorithm must be correct, of ﬁnite length, and must terminate for all inputs. a program is an instantiation of an algorithm in a computer programming language.
the ﬁrst authoritative work on data structures and algorithms was the series of books the art of computer programming by donald e. knuth, with volumes 1 and 3 being most relevant to the study of data structures [knu97, knu98]. a modern encyclopedic approach to data structures and algorithms that should be easy
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
given the speciﬁcations of the disk drive from example 8.1, we ﬁnd that it takes about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. it takes about 9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data. this is a good savings (slightly over half the time), but less than 1% of the data on the track are read. if we want to read only a single byte, it would save us effectively no time over that required to read an entire sector. for this reason, nearly all disk drives automatically read or write an entire sector’s worth of information whenever the disk is accessed, even when only one byte of information is requested.
once a sector is read, its information is stored in main memory. this is known as buffering or caching the information. if the next disk request is to that same sector, then it is not necessary to read from disk again because the information is already stored in main memory. buffering is an example of one method for minimizing disk accesses mentioned at the beginning of the chapter: bring off additional information from disk to satisfy future requests. if information from ﬁles were accessed at random, then the chance that two consecutive disk requests are to the same sector would be low. however, in practice most disk requests are close to the location (in the logical ﬁle at least) of the previous request. this means that the probability of the next request “hitting the cache” is much higher than chance would indicate.
this principle explains one reason why average access times for new disk drives are lower than in the past. not only is the hardware faster, but information is also now stored using better algorithms and larger caches that minimize the number of times information needs to be fetched from disk. this same concept is also used to store parts of programs in faster memory within the cpu, using the cpu cache that is prevalent in modern microprocessors.
sector-level buffering is normally provided by the operating system and is often built directly into the disk drive controller hardware. most operating systems maintain at least two buffers, one for input and one for output. consider what would happen if there were only one buffer during a byte-by-byte copy operation. the sector containing the ﬁrst byte would be read into the i/o buffer. the output operation would need to destroy the contents of the single i/o buffer to write this byte. then the buffer would need to be ﬁlled again from disk for the second byte, only to be destroyed during output. the simple solution to this problem is to keep one buffer for input, and a second for output.
most disk drive controllers operate independently from the cpu once an i/o request is received. this is useful because the cpu can typically execute millions of instructions during the time required for a single i/o operation. a technique that
tion was ﬁrst accessed. typically it is more important to know how many times the information has been accessed, or how recently the information was last accessed. another approach is called “least frequently used” (lfu). lfu tracks the number of accesses to each buffer in the buffer pool. when a buffer must be reused, the buffer that has been accessed the fewest number of times is considered to contain the “least important” information, and so it is used next. lfu, while it seems intuitively reasonable, has many drawbacks. first, it is necessary to store and update access counts for each buffer. second, what was referenced many times in the past might now be irrelevant. thus, some time mechanism where counts “expire” is often desirable. this also avoids the problem of buffers that slowly build up big counts because they get used just often enough to avoid being replaced. an alternative is to maintain counts for all sectors ever read, not just the sectors currently in the buffer pool.
the third approach is called “least recently used” (lru). lru simply keeps the buffers in a list. whenever information in a buffer is accessed, this buffer is brought to the front of the list. when new information must be read, the buffer at the back of the list (the one least recently used) is taken and its “old” information is either discarded or written to disk, as appropriate. this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless special knowledge about information access patterns for an application suggests a special-purpose buffer management scheme.
the main purpose of a buffer pool is to minimize disk i/o. when the contents of a block are modiﬁed, we could write the updated information to disk immediately. but what if the block is changed again? if we write the block’s contents after every change, that might be a lot of disk write operations that can be avoided. it is more efﬁcient to wait until either the ﬁle is to be closed, or the buffer containing that block is ﬂushed from the buffer pool.
when a buffer’s contents are to be replaced in the buffer pool, we only want to write the contents to disk if it is necessary. that would be necessary only if the contents have changed since the block was read in originally from the ﬁle. the way to insure that the block is written when necessary, but only when necessary, is to maintain a boolean variable with the buffer (often referred to as the dirty bit) that is turned on when the buffer’s contents are modiﬁed by the client. at the time when the block is ﬂushed from the buffer pool, it is written to disk if and only if the dirty bit has been turned on.
modern operating systems support virtual memory. virtual memory is a technique that allows the programmer to pretend that there is more of the faster main memory (such as ram) than actually exists. this is done by means of a buffer pool
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
when all inserts and releases follow a simple pattern, such as last requested, ﬁrst released (stack order), or ﬁrst requested, ﬁrst released (queue order), memory management is fairly easy. we are concerned in this section with the general case where blocks of any size might be requested and released in any order. this is known as dynamic storage allocation. one example of dynamic storage allocation is managing free store for a compiler’s runtime environment, such as the systemlevel new operations in java. another example is managing main memory in a multitasking operating system. here, a program might require a certain amount of space, and the memory manager must keep track of which programs are using which parts of the main memory. yet another example is the ﬁle manager for a disk drive. when a disk ﬁle is created, expanded, or deleted, the ﬁle manager must allocate or deallocate disk space.
a block of memory or disk space managed in this way is sometimes referred to as a heap. the term “heap” is being used here in a different way than the heap data structure discussed in section 5.5. here “heap” refers to the memory controlled by a dynamic memory management scheme.
in the rest of this section, we ﬁrst study techniques for dynamic memory management. we then tackle the issue of what to do when no single block of memory in the memory pool is large enough to honor a given request.
12.3.1 dynamic storage allocation for the purpose of dynamic storage allocation, we view memory as a single array broken into a series of variable-size blocks, where some of the blocks are free and some are reserved or already allocated. the free blocks are linked together to form
figure 12.15 using handles for dynamic memory management. the memory manager returns the address of the handle in response to a memory request. the handle stores the address of the actual memory block. in this way, the memory block might be moved (with its address updated in the handle) without disrupting the application program.
ﬁt memory allocation method, where external fragmentation has led to a series of small blocks that collectively could service the request. in this case, it might be possible to compact memory by moving the reserved blocks around so that the free space is collected into a single block. a problem with this approach is that the application must somehow be able to deal with the fact that all of its data have now been moved to different locations. if the application program relies on the absolute positions of the data in any way, this would be disastrous. one approach for dealing with this problem is the use of handles. a handle is a second level of indirection to a memory location. the memory allocation routine does not return a pointer to the block of storage, but rather a pointer to a variable that in turn points to the storage. this variable is the handle. the handle never moves its position, but the position of the block might be moved and the value of the handle updated. figure 12.15 illustrates the concept.
another failure policy that might work in some applications is to defer the memory request until sufﬁcient memory becomes available. for example, a multitasking operating system could adopt the strategy of not allowing a process to run until there is sufﬁcient memory available. while such a delay might be annoying to the user, it is better than halting the entire system. the assumption here is that other processes will eventually terminate, freeing memory.
another option might be to allocate more memory to the memory manager. in a zoned memory allocation system where the memory manager is part of a larger system, this might be a viable option. in a java program that implements its own memory manager, it might be possible to get more memory from the system-level new operator, such as is done by the freelist of section 4.1.2.
figure 12.18 example of the deutsch-schorr-waite garbage collection algorithm. (b) the multilist structure of (a) at the instant when link node 5 is being processed by the garbage collection algorithm. a chain of pointers stretching from variable prev to the head node of the structure has been (temporarily) created by the garbage collection algorithm.
an introductory text on operating systems covers many topics relating to memory management issues, including layout of ﬁles on disk and caching of information in main memory. all of the topics covered here on memory management, buffer pools, and paging are relevant to operating system implementation. for example, see operating systems by william stallings[sta05].
for information on lisp, see the little lisper by friedman and felleisen [ff89]. another good lisp reference is common lisp: the language by guy l. steele [ste84]. for information on emacs, which is both an excellent text editor and a fully developed programming environment, see the gnu emacs manual by richard m. stallman [sta07]. you can get more information about java’s garbage collection system from the java programming language by ken arnold and james gosling [ag06].
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
some choices for increments will make shellsort run more efﬁciently than others. in particular, the choice of increments described above (2k, 2k−1, ..., 2, 1) turns out to be relatively inefﬁcient. a better choice is the following series based on division by three: (..., 121, 40, 13, 4, 1).
the analysis of shellsort is difﬁcult, so we must accept without proof that the average-case performance of shellsort (for “divisions by three” increments) is o(n1.5). other choices for the increment series can reduce this upper bound somewhat. thus, shellsort is substantially better than insertion sort, or any of the Θ(n2) sorts presented in section 7.2. in fact, shellsort is competitive with the asymptotically better sorts to be presented whenever n is of medium size. shellsort illustrates how we can sometimes exploit the special properties of an algorithm (in this case insertion sort) even if in general that algorithm is unacceptably slow.
a natural approach to problem solving is divide and conquer. in terms of sorting, we might consider breaking the list to be sorted into pieces, process the pieces, and then put them back together somehow. a simple way to do this would be to split the list in half, sort the halves, and then merge the sorted halves together. this is the idea behind mergesort.
mergesort is one of the simplest sorting algorithms conceptually, and has good performance both in the asymptotic sense and in empirical running time. suprisingly, even though it is based on a simple concept, it is relatively difﬁcult to implement in practice. figure 7.7 illustrates mergesort. a pseudocode sketch of mergesort is as follows: list mergesort(list inlist) {
if (inlist.length() <= 1) return inlist;; list l1 = half of the items from inlist; list l2 = other half of the items from inlist; return merge(mergesort(l1), mergesort(l2));
before discussing how to implement mergesort, we will ﬁrst examine the merge function. merging two sorted sublists is quite simple. function merge examines the ﬁrst element of each sublist and picks the smaller value as the smallest element overall. this smaller value is removed from its sublist and placed into the output list. merging continues in this way, comparing the front elements of the sublists and continually appending the smaller to the output list until no more input elements remain.
int i, j, k, mid = (l+r)/2; if (l == r) return; if ((mid-l) >= threshold) mergesort(a, temp, l, mid); else inssort(a, l, mid-l+1); if ((r-mid) > threshold) mergesort(a, temp, mid+1, r); else inssort(a, mid+1, r-mid); // do the merge operation. for (i=l; i<=mid; i++) temp[i] = a[i]; for (j=1; j<=r-mid; j++) temp[r-j+1] = a[j+mid]; // merge sublists back to array for (i=l,j=r,k=l; k<=r; k++)
analysis of mergesort is straightforward, despite the fact that it is a recursive algorithm. the merging part takes time Θ(i) where i is the total length of the two subarrays being merged. the array to be sorted is repeatedly split in half until subarrays of size 1 are reached, at which time they are merged to be of size 2, these merged to subarrays of size 4, and so on as shown in figure 7.7. thus, the depth of the recursion is log n for n elements (assume for simplicity that n is a power of two). the ﬁrst level of recursion can be thought of as working on one array of size n, the next level working on two arrays of size n/2, the next on four arrays of size n/4, and so on. the bottom of the recursion has n arrays of size 1. thus, n arrays of size 1 are merged (requiring n total steps), n/2 arrays of size 2 (again requiring n total steps), n/4 arrays of size 4, and so on. at each of the log n levels of recursion, Θ(n) work is done, for a total cost of Θ(n log n). this cost is unaffected by the relative order of the values being sorted, thus this analysis holds for the best, average, and worst cases.
while mergesort uses the most obvious form of divide and conquer (split the list in half then sort the halves), it is not the only way that we can break down the sorting problem. and we saw that doing the merge step for mergesort when using an array implementation is not so easy. so prehaps a different divide and conquer strategy might turn out to be more efﬁcient?
quicksort is aptly named because, when properly implemented, it is the fastest known general-purpose in-memory sorting algorithm in the average case. it does not require the extra array needed by mergesort, so it is space efﬁcent as well. quicksort is widely used, and is typically the algorithm implemented in a library
as with mergesort, an efﬁcient implementation of radix sort is somewhat difﬁcult to achieve. in particular, we would prefer to sort an array of values and avoid processing linked lists. if we know how many values will be in each bin, then an auxiliary array of size r can be used to hold the bins. for example, if during the ﬁrst pass the 0 bin will receive three records and the 1 bin will receive ﬁve records, then we could simply reserve the ﬁrst three array positions for the 0 bin and the next ﬁve array positions for the 1 bin. exactly this approach is taken by the following java implementation. at the end of each pass, the records are copied back to the original array.
the ﬁrst inner for loop initializes array cnt. the second loop counts the number of records to be assigned to each bin. the third loop sets the values in cnt to their proper indices within array b. note that the index stored in cnt[j] is the last index for bin j; bins are ﬁlled from the bottom. the fourth loop assigns the records to the bins (within array b). the ﬁnal loop simply copies the records back to array a to be ready for the next pass. variable rtoi stores ri for use in bin computation on the i’th iteration. figure 7.12 shows how this algorithm processes the input shown in figure 7.11.
this algorithm requires k passes over the list of n numbers in base r, with Θ(n + r) work done at each pass. thus the total work is Θ(nk + rk). what is this in terms of n? because r is the size of the base, it might be rather small.
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
why can we not simply use the value of c for input size and say that the cost of the algorithm is Θ(c log c)? because, c is typically much less than p . for example, a picture might have 1000 × 1000 pixels and a range of 256 possible colors. so, p is one million, which is much larger than c log c. but, if p is smaller, or c larger (even if it is still less than p ), then c log c can become the larger quantity. thus, neither variable should be ignored.
besides time, space is the other computing resource that is commonly of concern to programmers. just as computers have become much faster over the years, they have also received greater allotments of memory. even so, the amount of available disk space or main memory can be signiﬁcant constraints for algorithm designers. the analysis techniques used to measure space requirements are similar to those used to measure time requirements. however, while time requirements are normally measured for an algorithm that manipulates a particular data structure, space requirements are normally determined for the data structure itself. the concepts of asymptotic analysis for growth rates on input size apply completely to measuring space requirements.
example 3.16 what are the space requirements for an array of n integers? if each integer requires c bytes, then the array requires cn bytes, which is Θ(n).
example 3.17 imagine that we want to keep track of friendships between n people. we can do this with an array of size n× n. each row of the array represents the friends of an individual, with the columns indicating who has that individual as a friend. for example, if person j is a friend of person i, then we place a mark in column j of row i in the array. likewise, we should also place a mark in column i of row j if we assume that friendship works both ways. for n people, the total size of the array is Θ(n2).
a data structure’s primary purpose is to store data in a way that allows efﬁcient access to those data. to provide efﬁcient access, it may be necessary to store additional information about where the data are within the data structure. for example, each node of a linked list must store a pointer to the next value on the list. all such information stored in addition to the actual data values is referred to as overhead. ideally, overhead should be kept to a minimum while allowing maximum access.
we do not want to allocate freelists in advance for each potential node length. on the other hand, we need to make sure no more than one copy of the freelist for a given size is created.
we can modify our freelist free-store methods to request the appropriate freelist. this access function will search for the proper freelist. if it exists, that freelist is used. if not, that freelist is created.
now that you have seen two substantially different implementations for lists, it is natural to ask which is better. in particular, if you must implement a list for some task, which implementation should you choose?
array-based lists have the disadvantage that their size must be predetermined before the array can be allocated. array-based lists cannot grow beyond their predetermined size. whenever the list contains only a few elements, a substantial amount of space might be tied up in a largely empty array. linked lists have the advantage that they only need space for the objects actually on the list. there is no limit to the number of elements on a linked list, as long as there is free-store memory available. the amount of space required by a linked list is Θ(n), while the space required by the array-based list implementation is Ω(n), but can be greater. array-based lists have the advantage that there is no wasted space for an individual element. linked lists require that a pointer be added to every list node. if the element size is small, then the overhead for links can be a signiﬁcant fraction of the total storage. when the array for the array-based list is completely ﬁlled, there is no storage overhead. the array-based list will then be more space efﬁcient, by a constant factor, than the linked implementation.
a simple formula can be used to determine whether the array-based list or linked list implementation will be more space efﬁcient in a particular situation. call n the number of elements currently in the list, p the size of a pointer in storage units (typically four bytes), e the size of a data element in storage units (this could be anything, from one bit for a boolean variable on up to thousands of bytes or more for complex records), and d the maximum number of list elements that can be stored in the array. the amount of space required for the array-based list is de, regardless of the number of elements actually stored in the list at any given time. the amount of space required for the linked list is n(p + e). the smaller of these expressions for a given value n determines the more space-efﬁcient implementation for n elements. in general, the linked implementation requires less space than the array-based implementation when relatively few elements are in the list. conversely, the array-based implementation becomes more space efﬁcient when
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
recurrence relations are often used to model the cost of recursive functions. for example, the standard mergesort (section 7.4) takes a list of size n, splits it in half, performs mergesort on each half, and ﬁnally merges the two sublists in n steps. the cost for this can be modeled as
in other words, the cost of the algorithm on input of size n is two times the cost for input of size n/2 (due to the two recursive calls to mergesort) plus n (the time to merge the sublists together again).
there are many approaches to solving recurrence relations, and we brieﬂy consider three here. the ﬁrst is an estimation technique: guess the upper and lower bounds for the recurrence, use induction to prove the bounds, and tighten as required. the second approach is to expand the recurrence to convert it to a summation and then use summation techniques. the third approach is to take advantage of already proven theorems when the recurrence is of a suitable form. in particular, typical divide and conquer algorithms such as mergesort yield recurrences of a form that ﬁts a pattern for which we have a ready solution.
14.2.1 estimating upper and lower bounds the ﬁrst approach to solving recurrences is to guess the answer and then attempt to prove it correct. if a correct upper or lower bound estimate is given, an easy induction proof will verify this fact. if the proof is successful, then try to tighten the bound. if the induction proof fails, then loosen the bound and try again. once the upper and lower bounds match, you are ﬁnished. this is a useful technique when you are only looking for asymptotic complexities. when seeking a precise closed-form solution (i.e., you seek the constants for the expression), this method will not be appropriate.
where a, b, c, and k are constants. in general, this recurrence describes a problem of size n divided into a subproblems of size n/b, while cnk is the amount of work necessary to combine the partial solutions. mergesort is an example of a divide and conquer algorithm, and its recurrence ﬁts this form. so does binary search. we use the method of expanding recurrences to derive the general solution for any divide and conquer recurrence, assuming that n = bm.
2. r = 1. because r = bk/a, we know that a = bk. from the deﬁnition of logarithms it follows immediately that k = logb a. we also note from equation 14.1 that m = logb n. thus,
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
leaf nodes in the tree. prove by induction that if tree t is a full binary tree with n internal nodes, i is t’s internal path length, and e is t’s external path length, then e = i + 2n for n ≥ 0.
5.4 explain why function preorder2 from section 5.2 makes half as many recursive calls as function preorder. explain why it makes twice as many accesses to left and right children. (a) modify the preorder traversal of section 5.2 to perform an inorder
5.6 write a recursive function named search that takes as input the pointer to the root of a binary tree (not a bst!) and a value k, and returns true if value k appears in the tree and false otherwise.
5.7 write an algorithm that takes as input the pointer to the root of a binary tree and prints the node values of the tree in level order. level order ﬁrst prints the root, then all nodes of level 1, then all nodes of level 2, and so on. hint: preorder traversals make use of a stack through recursive calls. consider making use of another data structure to help implement the levelorder traversal.
5.8 write a recursive function that returns the height of a binary tree. 5.9 write a recursive function that returns a count of the number of leaf nodes in
5.11 assume that a given bst stores integer values in its nodes. write a recursive function that traverses a binary tree, and prints the value of every node who’s grandparent has a value that is a multiple of ﬁve.
(c) all nodes store data and a parent pointer, and internal nodes store two child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
element also stores pointers to its non-zero neighbors following and preceding it in the column. thus, each non-zero element stores its own value, its position within the matrix, and four pointers. non-zero elements are found by traversing a row or column list. note that the ﬁrst non-zero element in a given row could be in any column; likewise, the neighboring non-zero element in any row or column list could be at any (higher) row or column in the array. thus, each non-zero element must also store its row and column position explicitly.
to ﬁnd if a particular position in the matrix contains a non-zero element, we traverse the appropriate row or column list. for example, when looking for the element at row 7 and column 1, we can traverse the list either for row 7 or for column 1. when traversing a row or column list, if we come to an element with the correct position, then its value is non-zero. if we encounter an element with a higher position, then the element we are looking for is not in the sparse matrix. in this case, the element’s value is zero. for example, when traversing the list for row 7 in the matrix of figure 12.7, we ﬁrst reach the element at row 7 and column 1. if this is what we are looking for, then the search can stop. if we are looking for the element at row 7 and column 2, then the search proceeds along the row 7 list to next reach the element at column 6. at this point we know that no element at row 7 and column 2 is stored in the sparse matrix.
each non-zero element stored in the sparse matrix representation takes much more space than an element stored in a simple n × n matrix. when is the sparse matrix more space efﬁcient than the standard representation? to calculate this, we need to determine how much space the standard matrix requires, and how much the sparse matrix requires. the size of the sparse matrix depends on the number of nonzero elements, while the size of the standard matrix representation does not vary. we need to know the (relative) sizes of a pointer and a data value. for simplicity, our calculation will ignore the space taken up by the row and column header (which is not much affected by the number of elements in the sparse array).
as an example, assume that a data value, a row or column index, and a pointer each require four bytes. an n × m matrix requires 4nm bytes. the sparse matrix requires 28 bytes per non-zero element (four pointers, two array indices, and one data value). if we set x to be the percentage of non-zero elements, we can solve for the value of x below which the sparse matrix representation is more space efﬁcient. using the equation
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
figure 17.1 an illustration of pairing. the two lists of numbers are paired up so that the least values from each list make a pair, the next smallest values from each list make a pair, and so on.
output: a pairing of the elements in the two sequences such that the least value in x is paired with the least value in y, the next least value in x is paired with the next least value in y, and so on.
figure 17.1 illustrates pairing. one way to solve pairing is to use an existing sorting program by sorting each of the two sequences, and then pairing-off items based on their position in sorted order. technically, we say that pairing is reduced to sorting, because sorting is used to solve pairing.
notice that reduction is a three-step process. the ﬁrst step is to convert an instance of pairing into two instances of sorting. the conversion step is not very interesting; it simply takes each sequence and assigns it to an array to be passed to sorting. the second step is to sort the two arrays (i.e., apply sorting to each array). the third step is to convert the output of sorting to the output for pairing. this is done by pairing the ﬁrst elements in the sorted arrays, the second elements, and so on.
the records in the a array using the corresponding value in the b array as the sort key and running a simple Θ(n) binsort. the conversion of sorting to pairing can be done in o(n) time, and likewise the conversion of the output of pairing can be converted to the correct output for sorting in o(n) time. thus, the cost of this “sorting algorithm” is dominated by the cost for pairing.
consider any two problems for which a suitable reduction from one to the other can be found. the ﬁrst problem takes an arbitrary instance of its input, which we will call i, and transforms i to a solution, which we will call sln. the second problem takes an arbitrary instance of its input, which we will call i0, and transforms i0 to a solution, which we will call sln0. we can deﬁne reduction more formally as a three-step process:
1. transform an arbitrary instance of the ﬁrst problem to an instance of the second problem. in other words, there must be a transformation from any instance i of the ﬁrst problem to an instance i0 of the second problem.
it is important to note that the reduction process does not give us an algorithm for solving either problem by itself. it merely gives us a method for solving the ﬁrst problem given that we already have a solution to the second. more importantly for the topics to be discussed in the remainder of this chapter, reduction gives us a way to understand the bounds of one problem in terms of another. speciﬁcally, given efﬁcient transformations, the upper bound of the ﬁrst problem is at most the upper bound of the second. conversely, the lower bound of the second problem is at least the lower bound of the ﬁrst.
as a second example of reduction, consider the simple problem of multiplying two n-digit numbers. the standard long-hand method for multiplication is to multiply the last digit of the ﬁrst number by the second number (taking Θ(n) time), multiply the second digit of the ﬁrst number by the second number (again taking Θ(n) time), and so on for each of the n digits of the ﬁrst number. finally, the intermediate results are added together. note that adding two numbers of length m and n can easily be done in Θ(m +n) time. because each digit of the ﬁrst number is multiplied against each digit of the second, this algorithm requires Θ(n2) time. asymptotically faster (but more complicated) algorithms are known, but none is so fast as to be in o(n).
next we ask the question: is squaring an n-digit number as difﬁcult as multiplying two n-digit numbers? we might hope that something about this special case
your computer. (a) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type int.
(b) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type double.
4.14 modify the array-based queue deﬁnition of figure 4.25 to use a separate boolean member to keep track of whether the queue is empty, rather than require that one array position remain empty.
4.15 a palindrome is a string that reads the same forwards as backwards. using only a ﬁxed number of stacks and queues, the stack and queue adt functions, and a ﬁxed number of int and char variables, write an algorithm to determine if a string is a palindrome. assume that the string is read from standard input one character at a time. the algorithm should output true or false as appropriate.
4.18 let q be a non-empty queue, and let s be an empty stack. using only the stack and queue adt functions and a single element variable x, write an algorithm to reverse the order of the elements in q.
example 2.1 for the integers, = is an equivalence relation that partitions each element into a distinct subset. in other words, for any integer a, three things are true.
1. a = a, 2. if a = b then b = a, and 3. if a = b and b = c, then a = c. of course, for distinct integers a, b, and c there are never cases where a = b, b = a, or b = c. so the claims that = is symmetric and transitive are vacuously true (there are never examples in the relation where these events occur). but becausethe requirements for symmetry and transitivity are not violated, the relation is symmetric and transitive.
example 2.2 if we clarify the deﬁnition of sibling to mean that a person is a sibling of him- or herself, then the sibling relation is an equivalence relation that partitions the set of people.
example 2.3 we can use the modulus function (deﬁned in the next section) to deﬁne an equivalence relation. for the set of integers, use the modulus function to deﬁne a binary relation such that two numbers x and y are in the relation if and only if x mod m = y mod m. thus, for m = 4, h1, 5i is in the relation because 1 mod 4 = 5 mod 4. we see that modulus used in this way deﬁnes an equivalence relation on the integers, and this relation can be used to partition the integers into m equivalence classes. this relation is an equivalence relation because
a binary relation is called a partial order if it is antisymmetric and transitive.2 the set on which the partial order is deﬁned is called a partially ordered set or a poset. elements x and y of a set are comparable under a given relation if either
2not all authors use this deﬁnition for partial order. i have seen at least three signiﬁcantly different deﬁnitions in the literature. i have selected the one that lets < and ≤ both deﬁne partial orders on the integers, becausethis seems the most natural to me.
for more about estimating techniques, see two programming pearls by john louis bentley entitled the back of the envelope and the envelope is back [ben84, ben00, ben86, ben88]. genius: the life and science of richard feynman by james gleick [gle92] gives insight into how important back of the envelope calculation was to the developers of the atomic bomb, and to modern theoretical physics in general.
each of the properties reﬂexive, symmetric, antisymmetric, and transitive. (a) “isbrotherof” on the set of people. (b) “isfatherof” on the set of people. (c) the relation r = {hx, yi| x2 + y2 = 1} for real numbers x and y. (d) the relation r = {hx, yi| x2 = y2} for real numbers x and y. (e) the relation r = {hx, yi| x mod y = 0} for x, y ∈ {1, 2, 3, 4}. (f) the empty relation ∅ (i.e., the relation with no ordered pairs for which (g) the empty relation ∅ (i.e., the relation with no ordered pairs for which
relation or prove that it is not an equivalence relation. (a) for integers a and b, a ≡ b if and only if a + b is even. (b) for integers a and b, a ≡ b if and only if a + b is odd. (c) for nonzero rational numbers a and b, a ≡ b if and only if a × b > 0. (d) for nonzero rational numbers a and b, a ≡ b if and only if a/b is an (e) for rational numbers a and b, a ≡ b if and only if a − b is an integer. (f) for rational numbers a and b, a ≡ b if and only if |a − b| ≤ 2.
why or why not. (a) “isfatherof” on the set of people. (b) “isancestorof” on the set of people. (c) “isolderthan” on the set of people. (d) “issisterof” on the set of people. (e) {ha, bi,ha, ai,hb, ai} on the set {a, b}.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
example 2.1 for the integers, = is an equivalence relation that partitions each element into a distinct subset. in other words, for any integer a, three things are true.
1. a = a, 2. if a = b then b = a, and 3. if a = b and b = c, then a = c. of course, for distinct integers a, b, and c there are never cases where a = b, b = a, or b = c. so the claims that = is symmetric and transitive are vacuously true (there are never examples in the relation where these events occur). but becausethe requirements for symmetry and transitivity are not violated, the relation is symmetric and transitive.
example 2.2 if we clarify the deﬁnition of sibling to mean that a person is a sibling of him- or herself, then the sibling relation is an equivalence relation that partitions the set of people.
example 2.3 we can use the modulus function (deﬁned in the next section) to deﬁne an equivalence relation. for the set of integers, use the modulus function to deﬁne a binary relation such that two numbers x and y are in the relation if and only if x mod m = y mod m. thus, for m = 4, h1, 5i is in the relation because 1 mod 4 = 5 mod 4. we see that modulus used in this way deﬁnes an equivalence relation on the integers, and this relation can be used to partition the integers into m equivalence classes. this relation is an equivalence relation because
a binary relation is called a partial order if it is antisymmetric and transitive.2 the set on which the partial order is deﬁned is called a partially ordered set or a poset. elements x and y of a set are comparable under a given relation if either
2not all authors use this deﬁnition for partial order. i have seen at least three signiﬁcantly different deﬁnitions in the literature. i have selected the one that lets < and ≤ both deﬁne partial orders on the integers, becausethis seems the most natural to me.
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
as to which node is set to be the root for the combined tree. in the case of equivalence pair (e, g), the root of e is d while the root of g is f. because f is the root of the larger tree, node d is set to point to f.
not all equivalences will combine two trees. if edge (f, g) is processed when the representation is in the state shown in figure 6.7(c), no change will be made because f is already the root for g.
the weighted union rule helps to minimize the depth of the tree, but we can do better than this. path compression is a method that tends to create extremely shallow trees. path compression takes place while ﬁnding the root for a given node x. call this root r. path compression resets the parent of every node on the path from x to r to point directly to r. this can be implemented by ﬁrst ﬁnding r. a second pass is then made along the path from x to r, assigning the parent ﬁeld of each node encountered to r. alternatively, a recursive algorithm can be implemented as follows. this version of find not only returns the root of the current node, but also makes all ancestors of the current node point to the root.
example 6.4 figure 6.7(d) shows the result of processing equivalence pair (h, e) on the the representation shown in figure 6.7(c) using the standard weighted union rule without path compression. figure 6.8 illustrates the path compression process for the same equivalence pair. after locating the root for node h, we can perform path compression to make h point directly to root object a. likewise, e is set to point directly to its root, f. finally, object a is set to point to root object f.
note that path compression takes place during the find operation, not during the merge operation. in figure 6.8, this means that nodes b, c, and h have node a remain as their parent, rather than changing their parent to be f. while we might prefer to have these nodes point to f, to accomplish this would require that additional information from the find operation be passed back to the union operation. this would not be practical.
path compression keeps the cost of each find operation very close to constant. to be more precise about what is meant by “very close to constant,” the cost of path compression for n find operations on n nodes (when combined with the weighted
union rule for joining sets) is Θ(n log∗ n). the notation “log∗ n” means the number of times that the log of n must be taken before n ≤ 1. for example, log∗ 65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and ﬁnally log 2 = 1. thus, log∗ n grows very slowly, so the cost for a series of n find operations is very close to n.
note that this does not mean that the tree resulting from processing n equivalence pairs necessarily has depth Θ(log∗ n). one can devise a series of equivalence operations that yields Θ(log n) depth for the resulting tree. however, many of the equivalences in such a series will look only at the roots of the trees being merged, requiring little processing time. the total amount of processing time required for n operations will be Θ(n log∗ n), yielding nearly constant time for each equivalence operation. this is an example of the technique of amortized analysis, discussed further in section 14.3.
we now tackle the problem of devising an implementation for general trees that allows efﬁcient processing of all member functions of the adt shown in figure 6.2. this section presents several approaches to implementing general trees. each implementation yields advantages and disadvantages in the amount of space required to store a node and the relative ease with which key operations can be performed. general tree implementations should place no restriction on how many children a node may have. in some applications, once a node is created the number of children never changes. in such cases, a ﬁxed amount of space can be allocated for the node when it is created, based on the number of children for the node. matters become more complicated if children can be added to or deleted from a node, requiring that the node’s space allocation be adjusted accordingly.
6.2 write an algorithm to determine if two binary trees are identical when the ordering of the subtrees for a node is ignored. for example, if a tree has root node with value r, left child with value a and right child with value b, this would be considered identical to another tree with root node value r, left child value b, and right child value a. make the algorithm as efﬁcient as you can. analyze your algorithm’s running time. how much harder would it be to make this algorthm work on a general tree?
6.4 write a function that takes as input a general tree and returns the number of nodes in that tree. write your function to use the gentree and gtnode adts of figure 6.2.
6.5 describe how to implement the weighted union rule efﬁciently. in particular, describe what information must be stored with each node and how this information is updated when two trees are merged. modify the implementation of figure 6.4 to support the weighted union rule.
6.6 a potential alternatative to the weighted union rule for combining two trees is the height union rule. the height union rule requires that the root of the tree with greater height become the root of the union. explain why the height union rule can lead to worse average time behavior than the weighted union rule.
6.7 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (0, 2) (1, 2) (3, 4) (3, 1) (3, 5) (9, 11) (12, 14) (3, 9) (4, 14) (6, 7) (8, 10) (8, 7) (7, 0) (10, 15) (10, 13)
6.8 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (2, 3) (4, 5) (6, 5) (3, 5) (1, 0) (7, 8) (1, 8) (3, 8) (9, 10) (11, 14) (11, 10) (12, 13) (11, 13) (14, 1)
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; long curr, prev, past; if ((n == 1) || (n == 2)) return 1; curr = prev = 1; for (int i=3; i<=n; i++) { // compute next value
2.12 write a recursive function to solve a generalization of the towers of hanoi problem where each ring may begin on any pole so long as no ring sits on top of a smaller ring.
this function makes progress towards the base case on every recursive call. in theory (that is, if double variables acted like true real numbers), would this function ever terminate for input val a nonzero number? in practice (an actual computer implementation), will it terminate?
2.16 the largest common factor (lcf) for two positive integers n and m is the largest integer that divides both n and m evenly. lcf(n, m) is at least one, and at most m, assuming that n ≥ m. over two thousand years ago, euclid provided an efﬁcient algorithm based on the observation that, when n mod m 6= 0, lcf(n, m) = gcd(m, n mod m). use this fact to write two algorithms to ﬁnd lcf for two positive integers. the ﬁrst version should compute the value iteratively. the second version should compute the value using recursion.
one important aspect of algorithm design is referred to as the space/time tradeoff principle. the space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacriﬁce space or vice versa. many programs can be modiﬁed to reduce storage requirements by “packing” or encoding information. “unpacking” or decoding the information requires additional time. thus, the resulting program uses less space but runs slower. conversely, many programs can be modiﬁed to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. typically, such changes in time and space are both by a constant factor.
a classic example of a space/time tradeoff is the lookup table. a lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. for example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. if you are writing a program that often computes factorials, it is likely to be much more time efﬁcient to simply pre-compute the 12 storable values in a table. whenever the program needs the value of n! for n ≤ 12, it can simply check the lookup table. (if n > 12, the value is too large to store as an int variable anyway.) compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table.
lookup tables can also store approximations for an expensive function such as sine or cosine. if you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. note that initially building the lookup table requires a certain amount of time. your application must use the lookup table often enough to make this initialization worthwhile.
another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. here is a simple code fragment for sorting an array of integers. we assume that this is a special case where there are n integers whose values are a permutation of the integers from 0 to n − 1. this is an example of a binsort, which is discussed in section 7.7. binsort assigns each value to an array position corresponding to its value.
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
over the time required to ﬁnd the k largest elements using one of the other sorting methods described earlier. one situation where we are able to take advantage of this concept is in the implementation of kruskal’s minimum-cost spanning tree (mst) algorithm of section 11.5.2. that algorithm requires that edges be visited in ascending order (so, use a min-heap), but this process stops as soon as the mst is complete. thus, only a relatively small fraction of the edges need be sorted.
imagine that for the past year, as you paid your various bills, you then simply piled all the paperwork onto the top of a table somewhere. now the year has ended and its time to sort all of these papers by what the bill was for (phone, electricity, rent, etc.) and date. a pretty natural approach is to make some space on the ﬂoor, and as you go through the pile of papers, put the phone bills into one pile, the electric bills into another pile, and so on. once this initial assignment of bills to piles is done (in one pass), you can sort each pile by date relatively quickly because they are each fairly small. this is the basic idea behind a binsort. numbers 0 through n − 1:
here the key value is used to determine the position for a record in the ﬁnal sorted array. this is the most basic example of a binsort, where key values are used to assign records to bins. this algorithm is extremely efﬁcient, taking Θ(n) time regardless of the initial ordering of the keys. this is far better than the performance of any sorting algorithm that we have seen so far. the only problem is that this algorithm has limited use because it works only for a permutation of the numbers from 0 to n − 1.
we can extend this simple binsort algorithm to be more useful. because binsort must perform direct computation on the key value (as opposed to just asking which of two records comes ﬁrst as our previous sorting algorithms did), we will assume that the records use an integer key type.
the simplest extension is to allow for duplicate values among the keys. this can be done by turning array slots into arbitrary-length bins by turning b into an array of linked lists. in this way, all records with key value i can be placed in bin b[i]. a second extension allows for a key range greater than n. for example, a set of n records might have keys in the range 1 to 2n. the only requirement is
how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equivalent to the number required to successfully search for it. to insert a record, an empty slot along the record’s probe sequence must be found. this is equivalent to an unsuccessful search for the record (recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table).
when the hash table is empty, the ﬁrst record inserted will always ﬁnd its home position free. thus, it will require only one record access to ﬁnd a free slot. if all records are stored in their home positions, then successful searches will also require only one record access. as the table begins to ﬁll up, the probability that a record can be inserted in its home position decreases. if a record hashes to an occupied slot, then the collision resolution policy must locate another slot in which to store it. finding records not stored in their home position also requires additional record accesses as the record is searched for along its probe sequence. as the table ﬁlls up, more and more records are likely to be located ever further from their home positions.
from this discussion, we see that the expected cost of hashing is a function of how full the table is. deﬁne the load factor for the table as α = n/m, where n is the number of records currently in the table.
an estimate of the expected cost for an insertion (or an unsuccessful search) can be derived analytically as a function of α in the case where we assume that the probe sequence follows a random permutation of the slots in the hash table. assuming that every slot in the table has equal probability of being the home slot for the next record, the probability of ﬁnding the home position occupied is α. the probability of ﬁnding both the home position occupied and the next slot on the probe sequence occupied is n (n−1)
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
layout for a document. the letter “c” might reasonably be represented by an object that describes that character’s strokes and bounding box. however, we don’t want to create a separate “c” object everywhere in the document that a “c” appears. the solution is to allocate a single copy of the shared representation for “c” object. then, every place in the document that needs a “c” in a given font, size, and typeface will reference this single copy. the various instances of references to “c” are called ﬂyweights. a ﬂyweight includes the reference to the shared information, and might include additional information speciﬁc to that instance.
we could imagine describing the layout of text on a page by using a tree structure. the root of the tree is a node representing the page. the page has multiple child nodes, one for each column. the column nodes have child nodes for each row. and the rows have child nodes for each character. these representations for characters are the ﬂyweights. the ﬂyweight includes the reference to the shared shape information, and might contain additional information speciﬁc to that instance. for example, each instance for “c” will contain a reference to the shared information about strokes and shapes, and it might also contain the exact location for that instance of the character on the page.
flyweights are used in the implementation for the pr quadtree data structure for storing collections of point objects, described in section 13.3. in a pr quadtree, we again have a tree with leaf nodes. many of these leaf nodes (the ones that represent empty areas) contain the same information. these identical nodes can be implemented using the flyweight design pattern for better memory efﬁciency.
given a tree of objects to describe a page layout, we might wish to perform some activity on every node in the tree. section 5.2 discusses tree traversal, which is the process of visiting every node in the tree in a deﬁned order. a simple example for our text composition application might be to count the number of nodes in the tree that represents the page. at another time, we might wish to print a listing of all the nodes for debugging purposes.
we could write a separate traversal function for each such activity that we intend to perform on the tree. a better approach would be to write a generic traversal function, and pass in the activity to be performed at each node. this organization constitutes the visitor design pattern. the visitor design pattern is used in sections 5.2 (tree traversal) and 11.3 (graph traversal).
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
figure 6.15 a general tree converted to the dynamic “left-child/right-sibling” representation. compared to the representation of figure 6.13, this representation requires less space.
example of a 4-ary tree. because k-ary tree nodes have a ﬁxed number of children, unlike general trees, they are relatively easy to implement. in general, k-ary trees bear many similarities to binary trees, and similar implementations can be used for k-ary tree nodes. note that as k becomes large, the potential number of null pointers grows, and the difference between the required sizes for internal nodes and leaf nodes increases. thus, as k becomes larger, the need to choose different implementations for the internal and leaf nodes becomes more pressing.
full and complete k-ary trees are analogous to full and complete binary trees, respectively. figure 6.16 shows full and complete k-ary trees for k = 3. in practice, most applications of k-ary trees limit them to be either full or complete. many of the properties of binary trees extend to k-ary trees. equivalent theorems to those in section 5.1.1 regarding the number of null pointers in a k-ary tree and the relationship between the number of leaves and the number of internal
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
new leaf node. if the node is a full node, it replaces itself with a subtree. this is an example of the composite design pattern, discussed in section 5.3.1.
the differences between the k-d tree and the pr quadtree illustrate many of the design choices encountered when creating spatial data structures. the k-d tree provides an object space decomposition of the region, while the pr quadtree provides a key space decomposition (thus, it is a trie). the k-d tree stores records at all nodes, while the pr quadtree stores records only at the leaf nodes. finally, the two trees have different structures. the k-d tree is a binary tree, while the pr quadtree is a full tree with 2d branches (in the two-dimensional case, 22 = 4). consider the extension of this concept to three dimensions. a k-d tree for three dimensions would alternate the discriminator through the x, y, and z dimensions. the three-dimensional equivalent of the pr quadtree would be a tree with 23 or eight branches. such a tree is called an octree.
we can also devise a binary trie based on a key space decomposition in each dimension, or a quadtree that uses the two-dimensional equivalent to an object space decomposition. the bintree is a binary trie that uses keyspace decomposition and alternates discriminators at each level in a manner similar to the k-d tree. the bintree for the points of figure 13.11 is shown in figure 13.18. alternatively, we can use a four-way decomposition of space centered on the data points. the tree resulting from such a decomposition is called a point quadtree. the point quadtree for the data points of figure 13.11 is shown in figure 13.19.
this section has barely scratched the surface of the ﬁeld of spatial data structures. by now dozens of distinct spatial data structures have been invented, many with variations and alternate implementations. spatial data structures exist for storing many forms of spatial data other than points. the most important distinctions between are the tree structure (binary or not, regular decompositions or not) and the decomposition rule used to decide when the data contained within a region is so complex that the region must be subdivided.
perhaps the best known spatial data structure is the “region quadtree” for storing images where the pixel values tend to be blocky, such as a map of the countries of the world. the region quadtree uses a four-way regular decomposition scheme similar to the pr quadtree. the decompostion rule is simply to divide any node containing pixels of more than one color or value.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
13.8 some applications do not permit storing two records with duplicate key values. in such a case, an attempt to insert a duplicate-keyed record into a tree structure such as a splay tree should result in a failure on insert. what is the appropriate action to take in a splay tree implementation when the insert routine is called with a duplicate-keyed record?
(a) show the result of building a k-d tree from the following points (inserted in the order given). a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (55, 40), h (45, 35), i (50, 30).
(b) show the result of deleting point a from the tree you built in part (a). (a) show the result of deleting f from the pr quadtree of figure 13.16. (b) show the result of deleting records e and f from the pr quadtree of
(a) show the result of building a pr quadtree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (45, 25), h (45, 30), i (50, 30).
(b) show the result of deleting point c from the tree you built in part (a). (c) show the result of deleting point f from the resulting tree in part (b).
13.14 when performing a region search on a pr quadtree, we need only search those subtrees of an internal node whose corresponding square falls within the query circle. this is most easily computed by comparing the x and y ranges of the query circle against the x and y ranges of the square corresponding to the subtree. however, as illustrated by figure 13.13, the x and y ranges might overlap without the circle actually intersecting the square. write a function that accurately determines if a circle and a square intersect.
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
figure 11.19 a graph and its mst. all edges appear in the original graph. those edges drawn with heavy lines indicate the subset making up the mst. note that edge (c, f) could be replaced with edge (d, f) to form a different mst with equal cost.
the ﬁrst of our two algorithms for ﬁnding msts is commonly referred to as prim’s algorithm. prim’s algorithm is very simple. start with any vertex n in the graph, setting the mst to be n initially. pick the least-cost edge connected to n. this edge connects n to another vertex; call this m. add vertex m and edge (n, m) to the mst. next, pick the least-cost edge coming from either n or m to any other vertex in the graph. add this edge and the new vertex it reaches to the mst. this process continues, at each step expanding the mst by selecting the least-cost edge from a vertex currently in the mst to a vertex not currently in the mst.
prim’s algorithm is quite similar to dijkstra’s algorithm for ﬁnding the singlesource shortest paths. the primary difference is that we are seeking not the next closest vertex to the start vertex, but rather the next closest vertex to any vertex currently in the mst. thus we replae the lines if (d[w] > (d[v] + g->weight(v, w)))
figure 11.20 shows an implementation for prim’s algorithm that searches the distance matrix for the next closest vertex. for each vertex i, when i is processed by prim’s algorithm, an edge going to i is added to the mst that we are building.
figure 11.22 prim’s mst algorithm proof. the left oval contains that portion of the graph where prim’s mst and the “true” mst t agree. the right oval contains the rest of the graph. the two portions of the graph are connected by (at least) edges ej (selected by prim’s algorithm to be in the mst) and e0 (the “correct” edge to be placed in the mst). note that the path from vw to vj cannot include any marked vertex vi, i ≤ j, because to do so would form a cycle.
and (d, f) happen to have equal cost, it is an arbitrary decision as to which gets selected. let’s pick (c, f). the next step marks vertex e and adds edge (f, e) to the mst. following in this manner, vertex b (through edge (c, b)) is marked. at this point, the algorithm terminates.
our next mst algorithm is commonly referred to as kruskal’s algorithm. kruskal’s algorithm is also a simple, greedy algorithm. we ﬁrst partition the set of vertices into |v| equivalence classes (see section 6.2), each consisting of one vertex. we then process the edges in order of weight. an edge is added to the mst, and the two equivalence classes combined, if the edge connects two vertices in different equivalence classes. this process is repeated until only one equivalence class remains.
example 11.4 figure 11.23 shows the ﬁrst three steps of kruskal’s algorithm for the graph of figure 11.19. edge (c, d) has the least cost, and because c and d are currently in separate msts, they are combined. we next select edge (e, f) to process, and combine these vertices into a single
11.10 show the shortest paths generated by running dijkstra’s shortest-paths algorithm on the graph of figure 11.25, beginning at vertex 4. show the d values as each vertex is processed, as in figure 11.18.
11.12 the root of a dag is a vertex r such that every vertex of the dag can be reached by a directed path from r. write an algorithm that takes a directed graph as input and determines the root (if there is one) for the graph. the running time of your algorithm should be Θ(|v| + |e|).
11.13 write an algorithm to ﬁnd the longest path in a dag, where the length of the path is measured by the number of edges that it contains. what is the asymptotic complexity of your algorithm? 11.14 write an algorithm to determine whether a directed graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v| + |e|) time. 11.15 write an algorithm to determine whether an undirected graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v|) time.
11.16 the single-destination shortest-paths problem for a directed graph is to ﬁnd the shortest path from every vertex to a speciﬁed vertex v. write an algorithm to solve the single-destination shortest-paths problem.
11.17 list the order in which the edges of the graph in figure 11.25 are visited when running prim’s mst algorithm starting at vertex 3. show the ﬁnal mst.
11.18 list the order in which the edges of the graph in figure 11.25 are visited when running kruskal’s mst algorithm. each time an edge is added to the mst, show the result on the equivalence array, (e.g., show the array as in figure 6.7).
11.20 when can prim’s and kruskal’s algorithms yield different msts? 11.21 prove that, if the costs for the edges of graph g are distinct, then only one
11.23 consider the collection of edges selected by dijkstra’s algorithm as the shortest paths to the graph’s vertices from the start vertex. do these edges form a spanning tree (not necessarily of minimum cost)? do these edges form an mst? explain why or why not.
11.24 prove that a tree is a bipartite graph. 11.25 prove that any tree can be two-colored. 11.26 write an algorithm that deterimines if an arbitrary undirected graph is a bipartite graph. if the graph is bipartite, then your algorithm should also identify the vertices as to which of the two partitions each belongs to.
11.1 design a format for storing graphs in ﬁles. then implement two functions: one to read a graph from a ﬁle and the other to write a graph to a ﬁle. test your functions by implementing a complete mst program that reads an undirected graph in from a ﬁle, constructs the mst, and then writes to a second ﬁle the graph representing the mst.
11.2 an undirected graph need not explicitly store two separate directed edges to represent a single undirected edge. an alternative would be to store only a single undirected edge (i, j) to connect vertices i and j. however, what if the user asks for edge (j, i)? we can solve this problem by consistently storing the edge such that the lesser of i and j always comes ﬁrst. thus, if we have an edge connecting vertices 5 and 3, requests for edge (5, 3) and (3, 5) both map to (3, 5) because 3 < 5. looking at the adacency matrix, we notice that only the lower triangle of the array is used. thus we could cut the space required by the adjacency matrix from |v|2 positions to |v|(|v|− 1)/2 positions. read section 12.2 on triangular matrices. the reimplement the adjacency matrix representation of figure 11.6 to implement undirected graphs using a triangular array.
11.3 while the underlying implementation (whether adjacency matrix or adjacency list) is hidden behind the graph adt, these two implementations can have an impact on the efﬁciency of the resulting program. for dijkstra’s shortest paths algorithm, two different implementations were given in section 11.4.1 that provide diffent ways for determining the next closest vertex
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
this section presents a simple, compact implementation for complete binary trees. recall that complete binary trees have all levels except the bottom ﬁlled out completely, and the bottom level has all of its nodes ﬁlled in from left to right. thus, a complete binary tree of n nodes has only one possible shape. you might think that a complete binary tree is such an unusual occurrence that there is no reason to develop a special implementation for it. however, the complete binary tree has practical uses, the most important being the heap data structure discussed in section 5.5. heaps are often used to implement priority queues (section 5.5) and for external sorting algorithms (section 8.5.2).
we begin by assigning numbers to the node positions in the complete binary tree, level by level, from left to right as shown in figure 5.12(a). an array can store the tree’s data values efﬁciently, placing each data value in the array position corresponding to that node’s position within the tree. figure 5.12(b) lists the array indices for the children, parent, and siblings of each node in figure 5.12(a). from figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives within the array. simple formulae can be derived for calculating the array index for each relative of a node r from r’s index. no explicit pointers are necessary to reach a node’s left or right child. this means there is no overhead to the array implementation if the array is selected to be of size n for a tree of n nodes.
the formulae for calculating the array indices of the various relatives of a node are as follows. the total number of nodes in the tree is n. the index of the node in question is r, which must fall in the range 0 to n − 1.
• parent(r) = b(r − 1)/2c if r 6= 0. • left child(r) = 2r + 1 if 2r + 1 < n. • right child(r) = 2r + 2 if 2r + 2 < n. • left sibling(r) = r − 1 if r is even. • right sibling(r) = r + 1 if r is odd and r + 1 < n.
section 4.4 presented the dictionary adt, along with dictionary implementations based on sorted and unsorted lists. when implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. however, searching an unsorted list for a particular record requires Θ(n) time in the average case. for a large database, this is probably much too slow. alternatively, the records can be stored in a sorted list. if the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. on the other hand, if we use a sorted
techniques in common use today. the next section presents one such approach to assigning variable-length codes, called huffman coding. while it is not commonly used in its simplest form for ﬁle compression (there are better methods), huffman coding gives the ﬂavor of such coding schemes.
huffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character. thus, it is a variable-length code. if the estimated frequencies for letters match the actual frequency found in an encoded message, then the length of that message will typically be less than if a ﬁxed-length code had been used. the huffman code for each letter is derived from a full binary tree called the huffman coding tree, or simply the huffman tree. each leaf of the huffman tree corresponds to a letter, and we deﬁne the weight of the leaf node to be the weight (frequency) of its associated letter. the goal is to build a tree with the minimum external path weight. deﬁne the weighted path length of a leaf to be its weight times its depth. the binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. a letter with high weight should have low depth, so that it will count the least against the total path length. as a result, another letter might be pushed deeper in the tree if it has less weight.
the process of building the huffman tree for n letters is quite simple. first, create a collection of n initial huffman trees, each of which is a single leaf node containing one of the letters. put the n partial trees onto a min-heap (a priority queue) organized by weight (frequency). next, remove the ﬁrst two trees (the ones with lowest weight) from the heap. join these two trees together to create a new tree whose root has the two trees as children, and whose weight is the sum of the weights of the two trees. put this new tree back on the heap. this process is repeated until all of the partial huffman trees have been combined into one.
example 5.8 figure 5.25 illustrates part of the huffman tree construction process for the eight letters of figure 5.24. ranking d and l arbitrarily by alphabetical order, the letters are ordered by frequency as
the bst should be organized by city name. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. collect running-time statistics for each operation. which operations can be implemented reasonably efﬁciently (i.e., in Θ(log n) time in the average case) using a bst? can the database system be made more efﬁcient by using one or more additional bsts to organize the records by location?
5.4 create a binary tree adt that includes generic traversal methods that take a visitor, as described in section 5.2. write functions count and bstcheck of section 5.2 as visitors to be used with the generic traversal method.
5.5 implement a priority queue class based on the max-heap class implementation of figure 5.19. the following methods should be supported for manipulating the priority queue: void enqueue(int objectid, int priority); int dequeue(); void changeweight(int objectid, int newpriority); method enqueue inserts a new object into the priority queue with id number objectid and priority priority. method dequeue removes the object with highest priority from the priority queue and returns its object id. method changeweight changes the priority of the object with id number objectid to be newpriority. the type for elem should be a class that stores the object id and the priority for that object. you will need a mechanism for ﬁnding the position of the desired object within the heap. use an array, storing the object with objectid i in position i. (be sure in your testing to keep the objectids within the array bounds.) you must also modify the heap implementation to store the object’s position in the auxiliary array so that updates to objects in the heap can be updated as well in the array.
5.6 the huffman coding tree function buildhuff of figure 5.29 manipulates a sorted list. this could result in a Θ(n2) algorithm, because placing an intermediate huffman tree on the list could take Θ(n) time. revise this algorithm to use a priority queue based on a min-heap instead of a list.
5.7 complete the implementation of the huffman coding tree, building on the code presented in section 5.6. include a function to compute and store in a table the codes for each letter, and functions to encode and decode messages. this project can be further extended to support ﬁle compression. to do so requires adding two steps: (1) read through the input ﬁle to generate actual
int v = minvertex(g, d); g.setmark(v, visited); if (d[v] == integer.max value) return; // unreachable for (int w = g.first(v); w < g.n(); w = g.next(v, w))
because this scan is done |v| times, and because each edge requires a constanttime update to d, the total cost for this approach is Θ(|v|2 + |e|) = Θ(|v|2), because |e| is in o(|v|2). the second method is to store unprocessed vertices in a min-heap ordered by distance values. the next-closest vertex can be found in the heap in Θ(log |v|) time. every time we modify d(x), we could reorder x in the heap by deleting and reinserting it. this is an example of a priority queue with priority update, as described in section 5.5. to implement true priority updating, we would need to store with each vertex its array index within the heap. a simpler approach is to add the new (smaller) distance value for a given vertex as a new record in the heap. the smallest value for a given vertex currently in the heap will be found ﬁrst, and greater distance values found later will be ignored because the vertex will already be marked as visited. the only disadvantage to repeatedly inserting distance values is that it will raise the number of elements in the heap from Θ(|v|) to Θ(|e|) in the worst case. the time complexity is Θ((|v| + |e|) log |e|), because for each edge we must reorder the heap. because the objects stored on the heap need to know both their vertex number and their distance, we create a simple class for the purpose called dijkelem, as follows. dijkelem is quite similar to the edge class used by the adjacency list representation.
this chapter talks about some fundamental patterns of algorithms. we discuss greed algorithms, dynamic programming, randomized algorithms, numerical algorithms, and the concept of a transform.
section 16.3.1 presents the skip list, a probabilistic data structure that can be used to implement the dictionary adt. the skip list is comparable in complexity to the bst, yet often outperforms the bst, because the skip list’s efﬁciency is not tied to the values of the dataset being stored.
observe that the following are all greedy algorithms (that work!): kruskal’s mst, prim’s mst, dijkstra’s shortest paths, huffman’s coding algorithm. various greedy knapsack algorithms, such as continuous-knapsack problem (see johnsonbaugh and schaefer, sec 7.6).
see the treatment by kleinberg & tardos. consider that a heap has a “greedy” deﬁnition: the value of any node a is bigger than its children. the bst’s deﬁnition is that the value of any node a is greater than all nodes in the left subtree, and less than all nodes in the right subtree. if we try a greedy deﬁnition (a is greater than its left child and less than its right child), we can get a tree that meets this deﬁnition but is not a bst. see the example in section 5.2.
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
problems: as your intuition would suggest, a problem is a task to be performed. it is best thought of in terms of inputs and matching outputs. a problem deﬁnition should not include any constraints on how the problem is to be solved. the solution method should be developed only after the problem is precisely deﬁned and thoroughly understood. however, a problem deﬁnition should include constraints on the resources that may be consumed by any acceptable solution. for any problem to be solved by a computer, there are always such constraints, whether stated or implied. for example, any computer program may use only the main memory and disk space available, and it must run in a “reasonable” amount of time.
problems can be viewed as functions in the mathematical sense. a function is a matching between inputs (the domain) and outputs (the range). an input to a function might be a single value or a collection of information. the values making up an input are called the parameters of the function. a speciﬁc selection of values for the parameters is called an instance of the problem. for example, the input parameter to a sorting function might be an array of integers. a particular array of integers, with a given size and speciﬁc values for each position in the array, would be an instance of the sorting problem. different instances might generate the same output. however, any problem instance must always result in the same output every time the function is computed using that particular input.
this concept of all problems behaving like mathematical functions might not match your intuition for the behavior of computer programs. you might know of programs to which you can give the same input value on two separate occasions, and two different outputs will result. for example, if you type “date” to a typical unix command line prompt, you will get the current date. naturally the date will be different on different days, even though the same command is given. however, there is obviously more to the input for the date program than the command that you type to run the program. the date program computes a function. in other words, on any particular day there can only be a single answer returned by a properly running date program on a completely speciﬁed input. for all computer programs, the output is completely determined by the program’s full set of inputs. even a “random number generator” is completely determined by its inputs (although some random number generating systems appear to get around this by accepting a random input from a physical process beyond the user’s control). the relationship between programs and functions is explored further in section 17.3.
4. it must be composed of a ﬁnite number of steps. if the description for the algorithm were made up of an inﬁnite number of steps, we could never hope to write it down, nor implement it as a computer program. most languages for describing algorithms (including english and “pseudocode”) provide some way to perform repeated actions, known as iteration. examples of iteration in programming languages include the while and for loop constructs of java. iteration allows for short descriptions, with the number of steps actually performed controlled by the input.
programs: we often think of a computer program as an instance, or concrete representation, of an algorithm in some programming language. in this book, nearly all of the algorithms are presented in terms of programs, or parts of programs. naturally, there are many programs that are instances of the same algorithm, because any modern computer programming language can be used to implement the same collection of algorithms (although some programming languages can make life easier for the programmer). to simplify presentation throughout the remainder of the text, i often use the terms “algorithm” and “program” interchangeably, despite the fact that they are really separate concepts. by deﬁnition, an algorithm must provide sufﬁcient detail that it can be converted into a program when needed.
the requirement that an algorithm must terminate means that not all computer programs meet the technical deﬁnition of an algorithm. your operating system is one such program. however, you can think of the various tasks for an operating system (each with associated inputs and outputs) as individual problems, each solved by speciﬁc algorithms implemented by a part of the operating system program, and each one of which terminates once its output is produced.
to summarize: a problem is a function or a mapping of inputs to outputs. an algorithm is a recipe for solving a problem whose steps are concrete and unambiguous. the algorithm must be correct, of ﬁnite length, and must terminate for all inputs. a program is an instantiation of an algorithm in a computer programming language.
the ﬁrst authoritative work on data structures and algorithms was the series of books the art of computer programming by donald e. knuth, with volumes 1 and 3 being most relevant to the study of data structures [knu97, knu98]. a modern encyclopedic approach to data structures and algorithms that should be easy
slow, but perhaps there are better ones waiting to be discovered. of course, while having a problem with high running time is bad, it is even worse to have a problem that cannot be solved at all! problems of the later type do exist, and some are presented in section 17.3.
this chapter presents a brief introduction to the theory of expensive and impossible problems. section 17.1 presents the concept of a reduction, which is the central tool used for analyzing the difﬁculty of a problem (as opposed to analyzing the cost of an algorithm). reductions allow us to relate the difﬁculty of various problems, which is often much easier than doing the analysis for a problem from ﬁrst principles. section 17.2 discusses “hard” problems, by which we mean problems that require, or at least appear to require, time exponential on the input size. finally, section 17.3 considers various problems that, while often simple to deﬁne and comprehend, are in fact impossible to solve using a computer program. the classic example of such a problem is deciding whether an arbitrary computer program will go into an inﬁnite loop when processing a speciﬁed input. this is known as the halting problem.
we begin with an important concept for understanding the relationships between problems, called reduction. reduction allows us to solve one problem in terms of another. equally importantly, when we wish to understand the difﬁculty of a problem, reduction allows us to make relative statements about upper and lower bounds on the cost of a problem (as opposed to an algorithm or program).
because the concept of a problem is discussed extensively in this chapter, we want notation to simplify problem descriptions. throughout this chapter, a problem will be deﬁned in terms of a mapping between inputs and outputs, and the name of the problem will be given in all capital letters. thus, a complete deﬁnition of the sorting problem could appear as follows:
once you have bought or written a program to solve one problem, such as sorting, you might be able to use it as a tool to solve a different problem. this is
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
comparing sequential search to binary search, we see that as n grows, the Θ(n) running time for sequential search in the average and worst cases quickly becomes much greater than the Θ(log n) running time for binary search. taken in isolation, binary search appears to be much more efﬁcient than sequential search. this is despite the fact that the constant factor for binary search is greater than that for sequential search, because the calculation for the next search position in binary search is more expensive than just incrementing the current position, as sequential search does.
note however that the running time for sequential search will be roughly the same regardless of whether or not the array values are stored in order. in contrast, binary search requires that the array values be ordered from lowest to highest. depending on the context in which binary search is to be used, this requirement for a sorted array could be detrimental to the running time of a complete program, because maintaining the values in sorted order requires to greater cost when inserting new elements into the array. this is an example of a tradeoff between the advantage of binary search during search and the disadvantage related to maintaining a sorted array. only in the context of the complete problem to be solved can we know whether the advantage outweighs the disadvantage.
you most often use the techniques of “algorithm” analysis to analyze an algorithm, or the instantiation of an algorithm as a program. you can also use these same techniques to analyze the cost of a problem. it should make sense to you to say that the upper bound for a problem cannot be worse than the upper bound for the best algorithm that we know for that problem. but what does it mean to give a lower bound for a problem?
consider a graph of cost over all inputs of a given size n for some algorithm for a given problem. deﬁne a to be the collection of all algorithms that solve the problem (theoretically, there an inﬁnite number of such algorithms). now, consider the collection of all the graphs for all of the (inﬁnitely many) algorithms in a. the worst case lower bound is the least of all the highest points on all the graphs.
it is much easier to show that an algorithm (or program) is in Ω(f(n)) than it is to show that a problem is in Ω(f(n)). for a problem to be in Ω(f(n)) means that every algorithm that solves the problem is in Ω(f(n)), even algorithms that we have not thought of!
so far all of our examples of algorithm analysis give “obvious” results, with big-oh always matching Ω. to understand how big-oh, Ω, and Θ notations are
properly used to describe our understanding of a problem or an algorithm, it is best to consider an example where you do not already know a lot about the problem.
let us look ahead to analyzing the problem of sorting to see how this process works. what is the least possible cost for any sorting algorithm in the worst case? the algorithm must at least look at every element in the input, just to determine that the input is truly sorted. it is also possible that each of the n values must be moved to another location in the sorted output. thus, any sorting algorithm must take at least cn time. for many problems, this observation that each of the n inputs must be looked at leads to an easy Ω(n) lower bound.
in your previous study of computer science, you have probably seen an example of a sorting algorithm whose running time is in o(n2) in the worst case. the simple bubble sort and insertion sort algorithms typically given as examples in a ﬁrst year programming course have worst case running times in o(n2). thus, the problem of sorting can be said to have an upper bound in o(n2). how do we close the gap between Ω(n) and o(n2)? can there be a better sorting algorithm? if you can think of no algorithm whose worst-case growth rate is better than o(n2), and if you have discovered no analysis technique to show that the least cost for the problem of sorting in the worst case is greater than Ω(n), then you cannot know for sure whether or not there is a better algorithm.
chapter 7 presents sorting algorithms whose running time is in o(n log n) for the worst case. this greatly narrows the gap. witht his new knowledge, we now have a lower bound in Ω(n) and an upper bound in o(n log n). should we search for a faster algorithm? many have tried, without success. fortunately (or perhaps unfortunately?), chapter 7 also includes a proof that any sorting algorithm must have running time in Ω(n log n) in the worst case.2 this proof is one of the most important results in the ﬁeld of algorithm analysis, and it means that no sorting algorithm can possibly run faster than cn log n for the worst-case input of size n. thus, we can conclude that the problem of sorting is Θ(n log n) in the worst case, because the upper and lower bounds have met.
knowing the lower bound for a problem does not give you a good algorithm. but it does help you to know when to stop looking. if the lower bound for the problem matches the upper bound for the algorithm (within a constant factor), then we know that we can ﬁnd an algorithm that is better only by a constant factor.
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
symmetric. we can convert matrix b to a symmetric matrix in a similar manner. if symmetric matrices could be multiplied “quickly” (in particular, if they could be multiplied together in Θ(n2) time), then we could ﬁnd the result of multiplying two arbitrary n × n matrices in Θ(n2) time by taking advantage of the following observation:
there are several ways that a problem could be considered hard. for example, we might have trouble understanding the deﬁnition of the problem itself. at the beginning of a large data collection and analysis project, developers and their clients might have only a hazy notion of what their goals actually are, and need to work that out over time. for other types of problems, we might have trouble ﬁnding or understanding an algorithm to solve the problem. understanding spoken engish and translating it to written text is an example of a problem whose goals are easy to deﬁne, but whose solution is not easy to discover. but even though a natural language processing algorithm might be difﬁcult to write, the program’s running time might be fairly fast. there are many practical systems today that solve aspects of this problem in reasonable time.
none of these is what is commonly meant when a computer theoretician uses the word “hard.” throughout this section, “hard” means that the best-known algorithm for the problem is expensive in its running time. one example of a hard problem is towers of hanoi. it is easy to understand this problem and its solution. it is also easy to write a program to solve this problem. but, it takes an extremely long time to run for any “reasonably” large value of n. try running a program to solve towers of hanoi for only 30 disks!
the towers of hanoi problem takes exponential time, that is, its running time is Θ(2n). this is radically different from an algorithm that takes Θ(n log n) time or Θ(n2) time. it is even radically different from a problem that takes Θ(n4) time. these are all examples of polynomial running time, because the exponents for all terms of these equations are constants. recall from chapter 3 that if we buy a new computer that runs twice as fast, the size of problem with complexity Θ(n4) that we can solve in a certain amount of time is increased by the fourth root of two. in other words, there is a multiplicative factor increase, even if it is a rather small one. this is true for any algorithm whose running time can be represented by a polynomial.
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
some input for which p halts. we could modify our compiler (or write a function as part of a program) to take p and some input string w, and modify it so that w is hardcoded inside p, with p reading no input. call this modiﬁed program p 0. now, p 0 always behaves the same way regardless of its input, because it ignores all input. however, because w is now hardwired inside of p 0, the behavior we get is that of p when given w as input. so, p 0 will halt on any arbitrary input if and only if p would halt on input w. we now feed p 0 to function ahalt. if ahalt could determine that p 0 halts on some input, then that is the same as determining that p halts on input w. but we know that that is impossible. therefore, ahalt cannot exist. 2
there are many things that we would like to have a computer do that are unsolvable. many of these have to do with program behavior. for example, proving that an arbitrary program is “correct,” that is, proving that a program computes a particular function, is a proof regarding program behavior. as such, what can be accomplished is severely limited. some other unsolvable problems include:
• does a program halt on every input? • does a program compute a particular function? • do two programs compute the same function? • does a particular line in a program get executed? this does not mean that a computer program cannot be written that works on special cases, possibly even on most programs that we would be interested in checking. for example, some c compilers will check if the control expression for a while loop is a constant expression that evaluates to false. if it is, the compiler will issue a warning that the while loop code will never be executed. however, it is not possible to write a computer program that can check for all input programs whether a speciﬁed line of code will be executed when the program is given some speciﬁed input.
another unsolvable problem is whether a program contains a computer virus. the property “contains a computer virus” is a matter of behavior. thus, it is not possible to determine positively whether an arbitrary program contains a computer virus. fortunately, there are many good heuristics for determining if a program is likely to contain a virus, and it is usually possible to determine if a program contains a particular virus, at least for the ones that are now known. real virus checkers do a pretty good job, but, it will always be possible for malicious people to invent new viruses that no existing virus checker can recognize.
consider what happens if you buy a computer that is twice as fast and try to solve a bigger towers of hanoi problem in a given amount of time. because its complexity is Θ(2n), we can solve a problem only one disk bigger! there is no multiplicative factor, and this is true for any exponential algorithm: a constant factor increase in processing power results in only a ﬁxed addition in problemsolving power.
there are a number of other fundamental differences between polynomial running times and exponential running times that argues for treating them as qualitatively different. polynomials are closed under composition and addition. thus, running polynomial-time programs in sequence, or having one program with polynomial running time call another a polynomial number of times yields polynomial time. also, all computers known are polynomially related. that is, any program that runs in polynomial time on any computer today, when tranferred to any other computer, will still run in polynomial time.
there is a practical reason for recognizing a distinction. in practice, most polynomial time algorithms are “feasible” in that they can run reasonably large inputs in reasonable time. in contrast, most algorithms requiring exponential time are not practical to run even for fairly modest sizes of input. one could argue that a program with high polynomial degree (such as n100) is not practical, while an exponential-time program with cost 1.001n is practical. but the reality is that we know of almost no problems where the best polynomial-time algorithm has high degree (they nearly all have degree four or less), while almost no exponential-time algorithms (whose cost is (o(cn)) have their constant c close to one. so there is not much gray area between polynomial and exponential time algorithms in practice. for the rest of this chapter, we deﬁne a hard algorithm to be one that runs in exponential time, that is, in Ω(cn) for some constant c > 1. a deﬁnition for a hard problem will be presented in the next section. 17.2.1 the theory of np-completeness imagine a magical computer that works by guessing the correct solution from among all of the possible solutions to a problem. another way to look at this is to imagine a super parallel computer that could test all possible solutions simultaneously. certainly this magical (or highly parallel) computer can do anything a normal computer can do. it might also solve some problems more quickly than a normal computer can. consider some problem where, given a guess for a solution, checking the solution to see if it is correct can be done in polynomial time. even if the number of possible solutions is exponential, any given guess can be checked in polynomial time (equivalently, all possible solutions are checked simultaneously
because every bin (except perhaps one) must be at least half full. however, this “ﬁrst ﬁt” heuristic can give us a result that is much worse than optimal. consider the following collection of numbers: 6 of 1/7 + , 6 of 1/3 + , and 6 of 1/2 + , where  is a small, positive number. properly organized, this requires 6 bins. but if done wrongly, we might end up putting the numbers into 10 bins.
a better heuristic is to use decreasing ﬁrst ﬁt. this is the same as ﬁrst ﬁt, except that we keep the bins sorted from most full to least full. then when deciding where to put the next item, we place it in the fullest bin that can hold it. this is similar to the “best ﬁt” heuristic for memory management discussed in section 12.3. the signiﬁcant thing about this heuristic is not just that it tends to give better performance than simple ﬁrst ﬁt. this decreasing ﬁrst ﬁt heurstic can be proven to require no more than 11/9 the optimal number of bins. thus, we have a guarentee on how much inefﬁciency can result when using the heuristic. the theory of np-completeness gives a technique for separating tractable from (probably) untractable problems. recalling the algorithm for generating algorithms in section 15.1, we can reﬁne it for problems that we suspect are np-complete. when faced with a new problem, we might alternate between checking if it is tractable (that is, we try to ﬁnd a polynomial-time solution) and checking if it is intractable (we try to prove the problem is np-complete). while proving that some problem is np-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. once we realize that a problem is np-complete, then we know that our next step must either be to redeﬁne the problem to make it easier, or else use one of the “coping” strategies discussed in this section.
even the best programmer sometimes writes a program that goes into an inﬁnite loop. of course, when you run a program that has not stopped, you do not know for sure if it is just a slow program or a program in an inﬁnite loop. after “enough time,” you shut it down. wouldn’t it be great if your compiler could look at your program and tell you before you run it that it might get into an inﬁnite loop? alternatively, given a program and a particular input, it would be useful to know if executing the program on that input will result in an inﬁnite loop without actually running the program.
unfortunately, the halting problem, as this is called, cannot be solved. there will never be a computer program that can positively determine, for an arbitrary program p, if p will halt for all input. nor will there even be a computer program that can positively determine if arbitrary program p will halt for a speciﬁed input i.
17.4 further reading the classic text on the theory of np-completeness is computers and intractability: a guide to the theory of np-completeness by garey and johnston [gj79]. the traveling salesman problem, edited by lawler et al. [llks85], discusses many approaches to ﬁnding an acceptable solution to this particular np-complete problem in a reasonable amount of time.
for more information about the collatz function see “on the ups and downs of hailstone numbers” by b. hayes [hay84], and “the 3x + 1 problem and its generalizations” by j.c. lagarias [lag85].
17.1 consider this algorithm for ﬁnding the maximum element in an array: first sort the array and then select the last (maximum) element. what (if anything) does this reduction tell us about the upper and lower bounds to the problem of ﬁnding the maximum element in a sequence? why can we not reduce sorting to ﬁnding the maximum element? 17.2 use a reduction to prove that squaring an n × n matrix is just as expensive (asymptotically) as multiplying two n × n matrices. 17.3 use a reduction to prove that multiplying two upper triangular n × n matrices is just as expensive (asymptotically) as multiplying two arbitrary n × n matrices. (a) explain why computing the factorial of n by multiplying all values
(b) explain why computing an approximation to the factorial of n by making use of stirling’s formula (see section 2.2) is a polynomial time algorithm.
17.5 consider this algorithm for solving the clique problem. first, generate all subsets of the vertices containing exactly k vertices. there are o(nk) such subsets altogether. then, check whether any subgraphcs induced by these subsets is complete. if this algorithm ran in polynomial time, what would be its signiﬁcance? why is this not a polynomial-time algorithm for the clique problem?
to understand once you have mastered this book is algorithms by robert sedgewick [sed03]. for an excellent and highly readable (but more advanced) teaching introduction to algorithms, their design, and their analysis, see introduction to algorithms: a creative approach by udi manber [man89]. for an advanced, encyclopedic approach, see introduction to algorithms by cormen, leiserson, and rivest [clrs01]. steven s. skiena’s the algorithm design manual [ski98] provides pointers to many implementations for data structures and algorithms that are available on the web.
for a gentle introduction to adts and program speciﬁcation, see abstract data types: their speciﬁcation, representation, and use by thomas, robinson, and emms [tre88].
the claim that all modern programming languages can implement the same algorithms (stated more precisely, any function that is computable by one programming language is computable by any programming language with certain standard capabilities) is a key result from computability theory. for an easy introduction to this ﬁeld see james l. hein, discrete structures, logic, and computability [hei03]. much of computer science is devoted to problem solving. indeed, this is what attracts many people to the ﬁeld. how to solve it by george p´olya [p´ol57] is considered to be the classic work on how to improve your problem-solving abilities. if you want to be a better student (as well as a better problem solver in general), see strategies for creative problem solving by folger and leblanc [fl95], effective problem solving by marvin levine [lev94], and problem solving & comprehension by arthur whimbey and jack lochhead [wl99].
see the origin of consciousness in the breakdown of the bicameral mind by julian jaynes [jay90] for a good discussion on how humans use the concept of metaphor to handle complexity. more directly related to computer science education and programming, see “cogito, ergo sum! cognitive processes of students dealing with data structures” by dan aharoni [aha00] for a discussion on moving from programming-context thinking to higher-level (and more design-oriented) programming-free thinking.
on a more pragmatic level, most people study data structures to write better programs. if you expect your program to work correctly and efﬁciently, it must ﬁrst be understandable to yourself and your co-workers. kernighan and pike’s the practice of programming [kp99] discusses a number of practical issues related to programming, including good coding and documentation style. for an excellent (and entertaining!) introduction to the difﬁculties involved with writing large programs, read the classic the mythical man-month: essays on software engineering by frederick p. brooks [bro95].
how many cities with more than 250,000 people lie within 500 miles of dallas, texas? how many people in my company make over $100,000 per year? can we connect all of our telephone customers with less than 1,000 miles of cable? to answer questions like these, it is not enough to have the necessary information. we must organize that information in a way that allows us to ﬁnd the answers in time to satisfy our needs.
representing information is fundamental to computer science. the primary purpose of most computer programs is not to perform calculations, but to store and retrieve information — usually as fast as possible. for this reason, the study of data structures and the algorithms that manipulate them is at the heart of computer science. and that is what this book is about — helping you to understand how to structure information to support efﬁcient processing.
this book has three primary goals. the ﬁrst is to present the commonly used data structures. these form a programmer’s basic data structure “toolkit.” for many problems, some data structure in the toolkit provides a good solution.
the second goal is to introduce the idea of tradeoffs and reinforce the concept that there are costs and beneﬁts associated with every data structure. this is done by describing, for each data structure, the amount of space and time required for typical operations.
the third goal is to teach how to measure the effectiveness of a data structure or algorithm. only through such measurement can you determine which data structure in your toolkit is most appropriate for a new problem. the techniques presented also allow you to judge the merits of new data structures that you or others might invent.
there are often many approaches to solving a problem. how do we choose between them? at the heart of computer program design are two (sometimes conﬂicting) goals:
4. it must be composed of a ﬁnite number of steps. if the description for the algorithm were made up of an inﬁnite number of steps, we could never hope to write it down, nor implement it as a computer program. most languages for describing algorithms (including english and “pseudocode”) provide some way to perform repeated actions, known as iteration. examples of iteration in programming languages include the while and for loop constructs of java. iteration allows for short descriptions, with the number of steps actually performed controlled by the input.
programs: we often think of a computer program as an instance, or concrete representation, of an algorithm in some programming language. in this book, nearly all of the algorithms are presented in terms of programs, or parts of programs. naturally, there are many programs that are instances of the same algorithm, because any modern computer programming language can be used to implement the same collection of algorithms (although some programming languages can make life easier for the programmer). to simplify presentation throughout the remainder of the text, i often use the terms “algorithm” and “program” interchangeably, despite the fact that they are really separate concepts. by deﬁnition, an algorithm must provide sufﬁcient detail that it can be converted into a program when needed.
the requirement that an algorithm must terminate means that not all computer programs meet the technical deﬁnition of an algorithm. your operating system is one such program. however, you can think of the various tasks for an operating system (each with associated inputs and outputs) as individual problems, each solved by speciﬁc algorithms implemented by a part of the operating system program, and each one of which terminates once its output is produced.
to summarize: a problem is a function or a mapping of inputs to outputs. an algorithm is a recipe for solving a problem whose steps are concrete and unambiguous. the algorithm must be correct, of ﬁnite length, and must terminate for all inputs. a program is an instantiation of an algorithm in a computer programming language.
the ﬁrst authoritative work on data structures and algorithms was the series of books the art of computer programming by donald e. knuth, with volumes 1 and 3 being most relevant to the study of data structures [knu97, knu98]. a modern encyclopedic approach to data structures and algorithms that should be easy
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
to understand once you have mastered this book is algorithms by robert sedgewick [sed03]. for an excellent and highly readable (but more advanced) teaching introduction to algorithms, their design, and their analysis, see introduction to algorithms: a creative approach by udi manber [man89]. for an advanced, encyclopedic approach, see introduction to algorithms by cormen, leiserson, and rivest [clrs01]. steven s. skiena’s the algorithm design manual [ski98] provides pointers to many implementations for data structures and algorithms that are available on the web.
for a gentle introduction to adts and program speciﬁcation, see abstract data types: their speciﬁcation, representation, and use by thomas, robinson, and emms [tre88].
the claim that all modern programming languages can implement the same algorithms (stated more precisely, any function that is computable by one programming language is computable by any programming language with certain standard capabilities) is a key result from computability theory. for an easy introduction to this ﬁeld see james l. hein, discrete structures, logic, and computability [hei03]. much of computer science is devoted to problem solving. indeed, this is what attracts many people to the ﬁeld. how to solve it by george p´olya [p´ol57] is considered to be the classic work on how to improve your problem-solving abilities. if you want to be a better student (as well as a better problem solver in general), see strategies for creative problem solving by folger and leblanc [fl95], effective problem solving by marvin levine [lev94], and problem solving & comprehension by arthur whimbey and jack lochhead [wl99].
see the origin of consciousness in the breakdown of the bicameral mind by julian jaynes [jay90] for a good discussion on how humans use the concept of metaphor to handle complexity. more directly related to computer science education and programming, see “cogito, ergo sum! cognitive processes of students dealing with data structures” by dan aharoni [aha00] for a discussion on moving from programming-context thinking to higher-level (and more design-oriented) programming-free thinking.
on a more pragmatic level, most people study data structures to write better programs. if you expect your program to work correctly and efﬁciently, it must ﬁrst be understandable to yourself and your co-workers. kernighan and pike’s the practice of programming [kp99] discusses a number of practical issues related to programming, including good coding and documentation style. for an excellent (and entertaining!) introduction to the difﬁculties involved with writing large programs, read the classic the mythical man-month: essays on software engineering by frederick p. brooks [bro95].
thought process, which in turn clariﬁes your explanations. second, if you use one of the standard proof structures such as proof by contradiction or an induction proof, then both you and your reader are working from a shared understanding of that structure. that makes for less complexity to your reader to understand your proof, because the reader need not decode the structure of your argument from scratch.
this section brieﬂy introduces three commonly used proof techniques: (i) deduction, or direct proof; (ii) proof by contradiction, and (iii) proof by mathematical induction.
2.6.1 direct proof in general, a direct proof is just a “logical explanation.” a direct proof is sometimes referred to as an argument by deduction. this is simply an argument in terms of logic. often written in english with words such as “if ... then,” it could also be written with logic notation such as “p ⇒ q.” even if we don’t wish to use symbolic logic notation, we can still take advantage of fundamental theorems of logic to structure our arguments. for example, if we want to prove that p and q are equivalent, we can ﬁrst prove p ⇒ q and then prove q ⇒ p .
in some domains, proofs are essentially a series of state changes from a start state to an end state. formal predicate logic can be viewed in this way, with the various “rules of logic” being used to make the changes from one formula or combining a couple of formulas to make a new formula on the route to the destination. symbolic manipulations to solve integration problems in introductory calculus classes are similar in spirit, as are high school geometry proofs.
the simplest way to disprove a theorem or statement is to ﬁnd a counterexample to the theorem. unfortunately, no number of examples supporting a theorem is sufﬁcient to prove that the theorem is correct. however, there is an approach that is vaguely similar to disproving by counterexample, called proof by contradiction. to prove a theorem by contradiction, we ﬁrst assume that the theorem is false. we then ﬁnd a logical contradiction stemming from this assumption. if the logic used to ﬁnd the contradiction is correct, then the only way to resolve the contradiction is to recognize that the assumption that the theorem is false must be incorrect. that is, we conclude that the theorem must be true.
step 2. show this assumption leads to a contradiction: consider c = b + 1. c is an integer because it is the sum of two integers. also, c > b, which means that b is not the largest integer after all. thus, we have reached a contradiction. the only ﬂaw in our reasoning is the initial assumption that the theorem is false. thus, we conclude that the theorem is correct. 2
mathematical induction is much like recursion. it is applicable to a wide variety of theorems. induction also provides a useful way to think about algorithm design, because it encourages you to think about solving a problem by building up from simple subproblems. induction can help to prove that a recursive function produces the correct result.
within the context of algorithm analysis, one of the most important uses for mathematical induction is as a method to test a hypothesis. as explained in section 2.4, when seeking a closed-form solution for a summation or recurrence we might ﬁrst guess or otherwise acquire evidence that a particular formula is the correct solution. if the formula is indeed correct, it is often an easy matter to prove that fact with an induction proof.
let thrm be a theorem to prove, and express thrm in terms of a positive integer parameter n. mathematical induction states that thrm is true for any value of parameter n (for n ≥ c, where c is some constant) if the following two conditions are true:
1. base case: thrm holds for n = c, and 2. induction step: if thrm holds for n − 1, then thrm holds for n. proving the base case is usually easy, typically requiring that some small value such as 1 be substituted for n in the theorem and applying simple algebra or logic as necessary to verify the theorem. proving the induction step is sometimes easy, and sometimes difﬁcult. an alternative formulation of the induction step is known as strong induction. the induction step for strong induction is: 2a. induction step: if thrm holds for all k, c ≤ k < n, then thrm holds for n.
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
thought process, which in turn clariﬁes your explanations. second, if you use one of the standard proof structures such as proof by contradiction or an induction proof, then both you and your reader are working from a shared understanding of that structure. that makes for less complexity to your reader to understand your proof, because the reader need not decode the structure of your argument from scratch.
this section brieﬂy introduces three commonly used proof techniques: (i) deduction, or direct proof; (ii) proof by contradiction, and (iii) proof by mathematical induction.
2.6.1 direct proof in general, a direct proof is just a “logical explanation.” a direct proof is sometimes referred to as an argument by deduction. this is simply an argument in terms of logic. often written in english with words such as “if ... then,” it could also be written with logic notation such as “p ⇒ q.” even if we don’t wish to use symbolic logic notation, we can still take advantage of fundamental theorems of logic to structure our arguments. for example, if we want to prove that p and q are equivalent, we can ﬁrst prove p ⇒ q and then prove q ⇒ p .
in some domains, proofs are essentially a series of state changes from a start state to an end state. formal predicate logic can be viewed in this way, with the various “rules of logic” being used to make the changes from one formula or combining a couple of formulas to make a new formula on the route to the destination. symbolic manipulations to solve integration problems in introductory calculus classes are similar in spirit, as are high school geometry proofs.
the simplest way to disprove a theorem or statement is to ﬁnd a counterexample to the theorem. unfortunately, no number of examples supporting a theorem is sufﬁcient to prove that the theorem is correct. however, there is an approach that is vaguely similar to disproving by counterexample, called proof by contradiction. to prove a theorem by contradiction, we ﬁrst assume that the theorem is false. we then ﬁnd a logical contradiction stemming from this assumption. if the logic used to ﬁnd the contradiction is correct, then the only way to resolve the contradiction is to recognize that the assumption that the theorem is false must be incorrect. that is, we conclude that the theorem must be true.
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
thought process, which in turn clariﬁes your explanations. second, if you use one of the standard proof structures such as proof by contradiction or an induction proof, then both you and your reader are working from a shared understanding of that structure. that makes for less complexity to your reader to understand your proof, because the reader need not decode the structure of your argument from scratch.
this section brieﬂy introduces three commonly used proof techniques: (i) deduction, or direct proof; (ii) proof by contradiction, and (iii) proof by mathematical induction.
2.6.1 direct proof in general, a direct proof is just a “logical explanation.” a direct proof is sometimes referred to as an argument by deduction. this is simply an argument in terms of logic. often written in english with words such as “if ... then,” it could also be written with logic notation such as “p ⇒ q.” even if we don’t wish to use symbolic logic notation, we can still take advantage of fundamental theorems of logic to structure our arguments. for example, if we want to prove that p and q are equivalent, we can ﬁrst prove p ⇒ q and then prove q ⇒ p .
in some domains, proofs are essentially a series of state changes from a start state to an end state. formal predicate logic can be viewed in this way, with the various “rules of logic” being used to make the changes from one formula or combining a couple of formulas to make a new formula on the route to the destination. symbolic manipulations to solve integration problems in introductory calculus classes are similar in spirit, as are high school geometry proofs.
the simplest way to disprove a theorem or statement is to ﬁnd a counterexample to the theorem. unfortunately, no number of examples supporting a theorem is sufﬁcient to prove that the theorem is correct. however, there is an approach that is vaguely similar to disproving by counterexample, called proof by contradiction. to prove a theorem by contradiction, we ﬁrst assume that the theorem is false. we then ﬁnd a logical contradiction stemming from this assumption. if the logic used to ﬁnd the contradiction is correct, then the only way to resolve the contradiction is to recognize that the assumption that the theorem is false must be incorrect. that is, we conclude that the theorem must be true.
figure 2.3 a two-coloring for the regions formed by three lines in the plane. n − 1 lines. unfortunately, the regions newly split by the nth line violate the rule for a two-coloring. take all regions on one side of the nth line and reverse their coloring (after doing so, this half-plane is still two-colored). those regions split by the nth line are now properly two-colored, because the part of the region to one side of the line is now black and the region to the other side is now white. thus, by mathematical induction, the entire plane is two-colored. 2
compare the proof of theorem 2.7 with that of theorem 2.5. for theorem 2.5, we took a collection of stamps of size n − 1 (which, by the induction hypothesis, must have the desired property) and from that “built” a collection of size n that has the desired property. we therefore proved the existence of some collection of stamps of size n with the desired property.
for theorem 2.7 we must prove that any collection of n lines has the desired property. thus, our strategy is to take an arbitrary collection of n lines, and “reduce” it so that we have a set of lines that must have the desired property because it matches the induction hypothesis. from there, we merely need to show that reversing the original reduction process preserves the desired property. in contrast, consider what would be required if we attempted to “build” from a set of lines of size n − 1 to one of size n. we would have great difﬁculty justifying that all possible collections of n lines are covered by our building process. by reducing from an arbitrary collection of n lines to something less, we avoid this problem.
typical car is driven about 12,000 miles per year. if gasoline costs $2/gallon, then the yearly gas bill is $1200 for the less efﬁcient car and $800 for the more efﬁcient car. if we ignore issues such as the payback that would be received if we invested $2000 in a bank, it would take 5 years to make up the difference in price. at this point, the buyer must decide if price is the only criterion and if a 5-year payback time is acceptable. naturally, a person who drives more will make up the difference more quickly, and changes in gasoline prices will also greatly affect the outcome.
example 2.20 when at the supermarket doing the week’s shopping, can you estimate about how much you will have to pay at the checkout? one simple way is to round the price of each item to the nearest dollar, and add this value to a mental running total as you put the item in your shopping cart. this will likely give an answer within a couple of dollars of the true total.
most of the topics covered in this chapter are considered part of discrete mathematics. an introduction to this ﬁeld is discrete mathematics with applications by susanna s. epp [epp04]. an advanced treatment of many mathematical topics useful to computer scientists is concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
see “technically speaking” from the february 1995 issue of ieee spectrum [sel95] for a discussion on the standard for indicating units of computer storage used in this book.
for more information on recursion, see thinking recursively by eric s. roberts [rob86]. to learn recursion properly, it is worth your while to learn the programming language lisp, even if you never intend to write a lisp program. in particular, friedman and felleisen’s the little lisper [ff89] is designed to teach you how to think recursively as well as teach you lisp. this book is entertaining reading as well.
a good book on writing mathematical proofs is daniel solow’s how to read and do proofs [sol90]. to improve your general mathematical problem-solving abilities, see the art and craft of problem solving by paul zeitz [zei07]. zeitz
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
int v = minvertex(g, d); g.setmark(v, visited); if (v != s) addedgetomst(v[v], v); if (d[v] == integer.max value) return; // unreachable for (int w = g.first(v); w < g.n(); w = g.next(v, w))
array v[i] stores the previously visited vertex that is closest to vertex i. this information lets us know which edge goes into the mst when vertex i is processed. the implementation of figure 11.20 also contains calls to addedgetomst to indicate which edges are actually added to the mst.
alternatively, we can implement prim’s algorithm using a priority queue to ﬁnd the next closest vertex, as shown in figure 11.21. as with the priority queue version of dijkstra’s algorithm, the heap’s elem type stores a dijkelem object.
prim’s algorithm is an example of a greedy algorithm. at each step in the for loop, we select the least-cost edge that connects some marked vertex to some unmarked vertex. the algorithm does not otherwise check that the mst really should include this least-cost edge. this leads to an important question: does prim’s algorithm work correctly? clearly it generates a spanning tree (because each pass through the for loop adds one edge and one unmarked vertex to the spanning tree until all vertices have been added), but does this tree have minimum cost?
theorem 11.1 prim’s algorithm produces a minimum-cost spanning tree. proof: we will use a proof by contradiction. let g = (v, e) be a graph for which prim’s algorithm does not generate an mst. deﬁne an ordering on the vertices according to the order in which they were added by prim’s algorithm to the mst: v0, v1, ..., vn−1. let edge ei connect (vx, vi) for some x < i and i ≥ 1. let ej be the lowest numbered (ﬁrst) edge added by prim’s algorithm such that the set of edges selected so far cannot be extended to form an mst for g. in other words, ej is the
ﬁnd one method to convert the inputs to pairing into inputs to sorting “fast enough,” and a second method to convert the result of sorting back to the correct result for pairing “fast enough,” then the asymptotic cost of pairing cannot be more than the cost of sorting. in this case, there is little work to be done to convert from pairing to sorting, or to convert the answer from sorting back to the answer for pairing, so the dominant cost of this solution is performing the sort operation. thus, an upper bound for pairing is in o(n log n).
it is important to note that the pairing problem does not require that elements of the two sequences be sorted. this is merely one possible way to solve the problem. pairing only requires that the elements of the sequences be paired correctly. perhaps there is another way to do it? certainly if we use sorting to solve pairing, the algorithms will require Ω(n log n) time. but, another approach might conceivably be faster.
there is another use of reductions aside from applying an old algorithm to solve a new problem (and thereby establishing an upper bound for the new problem). that is to prove a lower bound on the cost of a new problem by showing that it could be used as a solution for an old problem with a known lower bound.
assume we can go the other way and convert sorting to pairing “fast enough.” what does this say about the minimum cost of pairing? we know from section 7.9 that the cost of sorting in the worst and average cases is in Ω(n log n). in other words, the best possible algorithm for sorting requires at least n log n time.
assume that pairing could be done in o(n) time. then, one way to create a sorting algorithm would be to convert sorting into pairing, run the algorithm for pairing, and ﬁnally convert the answer back to the answer for sorting. provided that we can convert sorting to/from pairing “fast enough,” this process would yield an o(n) algorithm for sorting! because this contradicts what we know about the lower bound for sorting, and the only ﬂaw in the reasoning is the initial assumption that pairing can be done in o(n) time, we can conclude that there is no o(n) time algorithm for pairing. this reduction process tells us that pairing must be at least as expensive as sorting and so must itself have a lower bound in Ω(n log n).
to complete this proof regarding the lower bound for pairing, we need now to ﬁnd a way to reduce sorting to pairing. this is easily done. take an instance of sorting (i.e., an array a of n elements). a second array b is generated that simply stores i in position i for 0 ≤ i < n. pass the two arrays to pairing. take the resulting set of pairs, and use the value from the b half of the pair to tell which position in the sorted array the a half should take; that is, we can now reorder
we begin by assuming that there is a function named halt that can solve the halting problem. obviously, it is not possible to write out something that does not exist, but here is a plausible sketch of what a function to solve the halting problem might look like if it did exist. function halt takes two inputs: a string representing the source code for a program or function, and another string representing the input that we wish to determine if the input program or function halts on. function halt does some work to make a decision (which is encasulated into some ﬁctitious function named program halts). function halt then returns true if the input program or function does halt on the given input, and false otherwise.
what happens if we make a program whose sole purpose is to execute the function contrary and run that program with itself as input? one possibility is that the call to selfhalt returns true; that is, selfhalt claims that contrary will halt when run on itself. in that case, contrary goes into an inﬁnite loop (and thus does not halt). on the other hand, if selfhalt returns false, then halt is proclaiming that contrary does not halt on itself, and contrary then returns, that is, it halts. thus, contrary does the contrary of what halt says that it will do.
figure 17.4 the graph generated from boolean expression b = (x1+x2)·(x1+ x2 + x3)· (x1 + x3). literals from the ﬁrst clause are labeled c1, and literals from the second clause are labeled c2. there is an edge between every two pairs of vertices except when both vertices represent instances of literals from the same clause, or a negation of the same variable. thus, the vertex labeled c1 : y1 does not connect to the vertex labeled c1 : y2 (because they are literals in the same clause) or the vertex labeled c2 : y1 (because they are opposite values for the same variable).
there are several techniques to try. one approach is to run only small instances of the problem. for some problems, this is not acceptable. for example, traveling salesman grows so quickly that it cannot be run on modern computers for problem sizes much over 20 cities, which is not an unreasonable problem size for real-life situations. however, some other problems in np, while requiring exponential time, still grow slowly enough that they allow solutions for problems of a useful size.
consider the knapsack problem from section 16.2.1. we have a dynamic programming algorithm whose cost is Θ(nk) for n objects being ﬁt into a knapsack of size k. but it turns out that knapsack is np-complete. isn’t this a contradiction? not when we consider the relationship between n and k. how big is k? input size is typically o(n lg k) because the item sizes are smaller than k. thus, Θ(nk) is exponential on input size.
this dynamic programming algorithm is tractable if the numbers are “reasonable.” that is, we can successfully ﬁnd solutions to the problem when nk is in the thousands. such an algorithm is called a pseudo-polynomial time algorithm. this is different from traveling salesman which cannot possibly be solved when n = 100 given current algorithms. a second approach to handling np-complete problems is to solve a special instance of the problem that is not so hard. for example, many problems on graphs
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
we ﬁrst deﬁne an enumerated type called tohop, with two values move and toh, to indicate calls to the move function and recursive calls to toh, respectively. note that an array-based stack is used, because we know that the stack will need to store exactly 2n + 1 elements. the new version of toh begins by placing on the stack a description of the initial problem of n rings. the rest of the function is simply a while loop that pops the stack and executes the appropriate operation. in the case of a toh operation (for n > 0), we store on the stack representations for the three operations executed by the recursive version. however, these operations must be placed on the stack in reverse order, so that they will be popped off in the correct order.
some “naturally recursive” applications lend themselves to efﬁcient implementation with a stack, because the amount of information needed to describe a subproblem is small. for example, section 7.5 discusses a stack-based implementation for quicksort.
like the stack, the queue is a list-like structure that provides restricted access to its elements. queue elements may only be inserted at the back (called an enqueue
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
your computer. (a) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type int.
(b) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type double.
4.14 modify the array-based queue deﬁnition of figure 4.25 to use a separate boolean member to keep track of whether the queue is empty, rather than require that one array position remain empty.
4.15 a palindrome is a string that reads the same forwards as backwards. using only a ﬁxed number of stacks and queues, the stack and queue adt functions, and a ﬁxed number of int and char variables, write an algorithm to determine if a string is a palindrome. assume that the string is read from standard input one character at a time. the algorithm should output true or false as appropriate.
4.18 let q be a non-empty queue, and let s be an empty stack. using only the stack and queue adt functions and a single element variable x, write an algorithm to reverse the order of the elements in q.
this implementation contains calls to functions previsit and postvisit. these functions specify what activity should take place during the search. just as a preorder tree traversal requires action before the subtrees are visited, some graph traversals require that a vertex be processed before ones further along in the dfs. alternatively, some applications require activity after the remaining vertices are processed; hence the call to function postvisit. this would be a natural opportunity to make use of the visitor design pattern described in section 1.3.2.
dfs processes each edge once in a directed graph. in an undirected graph, dfs processes each edge from both directions. each vertex must be visited, but only once, so the total cost is Θ(|v| + |e|).
our second graph traversal algorithm is known as a breadth-ﬁrst search (bfs). bfs examines all vertices connected to the start vertex before visiting vertices further away. bfs is implemented similarly to dfs, except that a queue replaces the recursion stack. note that if the graph is a tree and the start vertex is at the root, bfs is equivalent to visiting vertices level by level from top to bottom. figure 11.10 provides an implementation for the bfs algorithm. figure 11.11 shows a graph and the corresponding breadth-ﬁrst search tree. figure 11.12 illustrates the bfs process for the graph of figure 11.11(a).
operation) and removed from the front (called a dequeue operation). queues operate like standing in line at a movie theater ticket counter.1 if nobody cheats, then newcomers go to the back of the line. the person at the front of the line is the next to be served. thus, queues release their elements in order of arrival. accountants have used queues since long before the existence of computers. they call a queue a “fifo” list, which stands for “first-in, first-out.” figure 4.22 shows a sample queue adt. this section presents two implementations for queues: the array-based queue and the linked queue.
assume that there are n elements in the queue. by analogy to the array-based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only Θ(1) time because the front element of the queue (the one being removed) is the last element in the array. however,
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
figure 4.23 after repeated use, elements in the array-based queue will drift to the back of the array. (a) the queue after the initial four numbers 20, 5, 12, and 17 have been inserted. (b) the queue after elements 20 and 5 are deleted, following which 3, 30, and 4 are inserted.
enqueue operations will require Θ(n) time, because the n elements currently in the queue must each be shifted one position in the array. if instead we chose the rear element of the queue to be in position n − 1, then an enqueue operation is equivalent to an append operation on a list. this requires only Θ(1) time. but now, a dequeue operation requires Θ(n) time, because all of the elements must be shifted down by one position to retain the property that the remaining n − 1 queue elements reside in the ﬁrst n − 1 positions of the array.
a far more efﬁcient implementation can be obtained by relaxing the requirement that all elements of the queue must be in the ﬁrst n positions of the array. we will still require that the queue be stored be in contiguous array positions, but the contents of the queue will be permitted to drift within the array, as illustrated by figure 4.23. now, both the enqueue and the dequeue operations can be performed in Θ(1) time because no other elements in the queue need be moved.
this implementation raises a new problem. assume that the front element of the queue is initially at position 0, and that elements are added to successively higher-numbered positions in the array. when elements are removed from the queue, the front index increases. over time, the entire queue will drift toward the higher-numbered positions in the array. once an element is inserted into the highest-numbered position in the array, the queue has run out of space. this happens despite the fact that there might be free positions at the low end of the array where elements have previously been removed from the queue.
the “drifting queue” problem can be solved by pretending that the array is circular and so allow the queue to continue directly from the highest-numbered position in the array to the lowest-numbered position. this is easily implemented
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
your computer. (a) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type int.
(b) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type double.
4.14 modify the array-based queue deﬁnition of figure 4.25 to use a separate boolean member to keep track of whether the queue is empty, rather than require that one array position remain empty.
4.15 a palindrome is a string that reads the same forwards as backwards. using only a ﬁxed number of stacks and queues, the stack and queue adt functions, and a ﬁxed number of int and char variables, write an algorithm to determine if a string is a palindrome. assume that the string is read from standard input one character at a time. the algorithm should output true or false as appropriate.
4.18 let q be a non-empty queue, and let s be an empty stack. using only the stack and queue adt functions and a single element variable x, write an algorithm to reverse the order of the elements in q.
operation) and removed from the front (called a dequeue operation). queues operate like standing in line at a movie theater ticket counter.1 if nobody cheats, then newcomers go to the back of the line. the person at the front of the line is the next to be served. thus, queues release their elements in order of arrival. accountants have used queues since long before the existence of computers. they call a queue a “fifo” list, which stands for “first-in, first-out.” figure 4.22 shows a sample queue adt. this section presents two implementations for queues: the array-based queue and the linked queue.
assume that there are n elements in the queue. by analogy to the array-based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only Θ(1) time because the front element of the queue (the one being removed) is the last element in the array. however,
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
figure 4.24 the circular queue with array positions increasing in the clockwise direction. (a) the queue after the initial four numbers 20, 5, 12, and 17 have been inserted. (b) the queue after elements 20 and 5 are deleted, following which 3, 30, and 4 are inserted.
through use of the modulus operator (denoted by % in java). in this way, positions in the array are numbered from 0 through size−1, and position size−1 is deﬁned to immediately precede position 0 (which is equivalent to position size % size). figure 4.24 illustrates this solution.
there remains one more serious, though subtle, problem to the array-based queue implementation. how can we recognize when the queue is empty or full? assume that front stores the array index for the front element in the queue, and rear stores the array index for the rear element. if both front and rear have the same position, then with this scheme there must be one element in the queue. thus, an empty queue would be recognized by having rear be one less than front (taking into account the fact that the queue is circular, so position size−1 is actually considered to be one less than position 0). but what if the queue is completely full? in other words, what is the situation when a queue with n array positions available contains n elements? in this case, if the front element is in position 0, then the rear element is in position size−1. but this means that the value for rear is one less than the value for front when the circular nature of the queue is taken into account. in other words, the full queue is indistinguishable from the empty queue! you might think that the problem is in the assumption about front and rear being deﬁned to store the array indices of the front and rear elements, respectively, and that some modiﬁcation in this deﬁnition will allow a solution. unfortunately, the problem cannot be remedied by a simple change to the deﬁnition for front and rear, because of the number of conditions or states that the queue can be in. ignoring the actual position of the ﬁrst element, and ignoring the actual values of the elements stored in the queue, how many different states are there? there can
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
we ﬁrst deﬁne an enumerated type called tohop, with two values move and toh, to indicate calls to the move function and recursive calls to toh, respectively. note that an array-based stack is used, because we know that the stack will need to store exactly 2n + 1 elements. the new version of toh begins by placing on the stack a description of the initial problem of n rings. the rest of the function is simply a while loop that pops the stack and executes the appropriate operation. in the case of a toh operation (for n > 0), we store on the stack representations for the three operations executed by the recursive version. however, these operations must be placed on the stack in reverse order, so that they will be popped off in the correct order.
some “naturally recursive” applications lend themselves to efﬁcient implementation with a stack, because the amount of information needed to describe a subproblem is small. for example, section 7.5 discusses a stack-based implementation for quicksort.
like the stack, the queue is a list-like structure that provides restricted access to its elements. queue elements may only be inserted at the back (called an enqueue
figure 4.23 after repeated use, elements in the array-based queue will drift to the back of the array. (a) the queue after the initial four numbers 20, 5, 12, and 17 have been inserted. (b) the queue after elements 20 and 5 are deleted, following which 3, 30, and 4 are inserted.
enqueue operations will require Θ(n) time, because the n elements currently in the queue must each be shifted one position in the array. if instead we chose the rear element of the queue to be in position n − 1, then an enqueue operation is equivalent to an append operation on a list. this requires only Θ(1) time. but now, a dequeue operation requires Θ(n) time, because all of the elements must be shifted down by one position to retain the property that the remaining n − 1 queue elements reside in the ﬁrst n − 1 positions of the array.
a far more efﬁcient implementation can be obtained by relaxing the requirement that all elements of the queue must be in the ﬁrst n positions of the array. we will still require that the queue be stored be in contiguous array positions, but the contents of the queue will be permitted to drift within the array, as illustrated by figure 4.23. now, both the enqueue and the dequeue operations can be performed in Θ(1) time because no other elements in the queue need be moved.
this implementation raises a new problem. assume that the front element of the queue is initially at position 0, and that elements are added to successively higher-numbered positions in the array. when elements are removed from the queue, the front index increases. over time, the entire queue will drift toward the higher-numbered positions in the array. once an element is inserted into the highest-numbered position in the array, the queue has run out of space. this happens despite the fact that there might be free positions at the low end of the array where elements have previously been removed from the queue.
the “drifting queue” problem can be solved by pretending that the array is circular and so allow the queue to continue directly from the highest-numbered position in the array to the lowest-numbered position. this is easily implemented
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
be no elements in the queue, one element, two, and so on. at most there can be n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue (0 through n elements are possible).
if the value of front is ﬁxed, then n + 1 different values for rear are needed to distinguish among the n+1 states. however, there are only n possible values for rear unless we invent a special case for, say, empty queues. this is an example of the pigeonhole principle deﬁned in exercise 2.29. the pigeonhole principle states that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. in similar manner, we can be sure that two of the n + 1 states are indistinguishable by their relative values of front and rear. we must seek some other way to distinguish full from empty queues.
one obvious solution is to keep an explicit count of the number of elements in the queue, or at least a boolean variable that indicates whether the queue is empty or not. another solution is to make the array be of size n + 1, and only allow n elements to be stored. which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. my choice is to use an array of size n + 1.
figure 4.25 shows an array-based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional parameter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. method size is used to control the circular motion of the queue (it is the base for the modulus operator). method rear is set to the position of the rear element.
in this implementation, the front of the queue is deﬁned to be toward the lower numbered positions in the array (in the counter-clockwise direction in figure 4.24), and the rear is deﬁned to be toward the higher-numbered positions. thus, enqueue increments the rear pointer (modulus size), and dequeue increments the front pointer. implementation of all member functions is straightforward.
the linked queue implementation is a straightforward adaptation of the linked list. figure 4.26 shows the linked queue class declaration. methods front and rear are pointers to the front and rear queue elements, respectively. we will use a header link node, which allows for a simpler implementation of the enqueue operation by avoiding any special cases when the queue is empty. on initialization, the front and rear pointers will point to the header node, and front will always point to the header node while rear points to the true last link node in the queue. method
assert size != 0 : "queue is empty"; e it = front.next().element(); front.setnext(front.next().next()); if (front.next() == null) rear = front; // last object size--; return it;
enqueue places the new element in a link node at the end of the linked list (i.e., the node that rear points to) and then advances rear to point to the new link node. method dequeue grabs the ﬁrst element of the list removes it.
all member functions for both the array-based and linked queue implementations require constant time. the space comparison issues are the same as for the equivalent stack implementations. unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.
the most common objective of computer programs is to store and retrieve data. much of this book is about efﬁcient ways to organize collections of data records so that they can be stored and retrieved quickly. in this section we describe a simple interface for such a collection, called a dictionary. the dictionary adt provides operations for storing records, ﬁnding records, and removing records from the collection. this adt gives us a standard basis for comparing various data structures.
before we can discuss the interface for a dictionary, we must ﬁrst deﬁne the concepts of a key and comparable objects. if we want to search for a given record in a database, how should we describe what we are looking for? a database record could simply be a number, or it could be quite complicated, such as a payroll record with many ﬁelds of varying types. we do not want to describe what we are looking for by detailing and matching the entire contents of the record. if we knew everything about the record already, we probably would not need to look for it. instead, we typically deﬁne what record we want in terms of a key value. for example, if searching for payroll records, we might wish to search for the record that matches a particular id number. in this example the id number is the search key.
to implement the search function, we require that keys be comparable. at a minimum, we must be able to take two keys and reliably determine whether they are equal or not. that is enough to enable a sequential search through a database of records and ﬁnd one that matches a given key. however, we typically would like for the keys to deﬁne a total order (see section 2.1), which means that we can tell which of two keys is greater than the other. using key types with total orderings gives the database implementor the opportunity to organize a collection of records in a way that makes searching more efﬁcient. an example is storing the
we ﬁrst deﬁne an enumerated type called tohop, with two values move and toh, to indicate calls to the move function and recursive calls to toh, respectively. note that an array-based stack is used, because we know that the stack will need to store exactly 2n + 1 elements. the new version of toh begins by placing on the stack a description of the initial problem of n rings. the rest of the function is simply a while loop that pops the stack and executes the appropriate operation. in the case of a toh operation (for n > 0), we store on the stack representations for the three operations executed by the recursive version. however, these operations must be placed on the stack in reverse order, so that they will be popped off in the correct order.
some “naturally recursive” applications lend themselves to efﬁcient implementation with a stack, because the amount of information needed to describe a subproblem is small. for example, section 7.5 discusses a stack-based implementation for quicksort.
like the stack, the queue is a list-like structure that provides restricted access to its elements. queue elements may only be inserted at the back (called an enqueue
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
must make its way to the top of the array. this would occur if the keys are initially arranged from highest to lowest, in the reverse of sorted order. in this case, the number of comparisons will be one the ﬁrst time through the for loop, two the second time, and so on. thus, the total number of comparisons will be
in contrast, consider the best-case cost. this occurs when the keys begin in sorted order from lowest to highest. in this case, every pass through the inner for loop will fail immediately, and no values will be moved. the total number of comparisons will be n − 1, which is the number of times the outer for loop executes. thus, the cost for insertion sort in the best case is Θ(n).
while the best case is signiﬁcantly faster than the worst case, the worst case is usually a more reliable indication of the “typical” running time. however, there are situations where we can expect the input to be in sorted or nearly sorted order. one example is when an already sorted list is slightly disordered; restoring sorted order using insertion sort might be a good idea if we know that the disordering is slight. examples of algorithms that take advantage of insertion sort’s best-case running time are the shellsort algorithm of section 7.3 and the quicksort algorithm of section 7.5.
what is the average-case cost of insertion sort? when record i is processed, the number of times through the inner for loop depends on how far “out of order” the record is. in particular, the inner for loop is executed once for each key greater than the key of record i that appears in array positions 0 through i−1. for example, in the leftmost column of figure 7.1 the value 15 is preceded by ﬁve values greater than 15. each such occurrence is called an inversion. the number of inversions (i.e., the number of values greater than a given value that occur prior to it in the array) will determine the number of comparisons and swaps that must take place. we need to determine what the average number of inversions will be for the record in position i. we expect on average that half of the keys in the ﬁrst i − 1 array positions will have a value greater than that of the key at position i. thus, the average case should be about half the cost of the worst case, which is still Θ(n2). so, the average case is no better than the worst case in asymptotic complexity.
counting comparisons or swaps yields similar results because each time through the inner for loop yields both a comparison and a swap, except the last (i.e., the comparison that fails the inner for loop’s test), which has no swap. thus, the number of swaps for the entire sort operation is n − 1 less than the number of comparisons. this is 0 in the best case, and Θ(n2) in the average and worst cases.
int i, j, k, mid = (l+r)/2; if (l == r) return; if ((mid-l) >= threshold) mergesort(a, temp, l, mid); else inssort(a, l, mid-l+1); if ((r-mid) > threshold) mergesort(a, temp, mid+1, r); else inssort(a, mid+1, r-mid); // do the merge operation. for (i=l; i<=mid; i++) temp[i] = a[i]; for (j=1; j<=r-mid; j++) temp[r-j+1] = a[j+mid]; // merge sublists back to array for (i=l,j=r,k=l; k<=r; k++)
analysis of mergesort is straightforward, despite the fact that it is a recursive algorithm. the merging part takes time Θ(i) where i is the total length of the two subarrays being merged. the array to be sorted is repeatedly split in half until subarrays of size 1 are reached, at which time they are merged to be of size 2, these merged to subarrays of size 4, and so on as shown in figure 7.7. thus, the depth of the recursion is log n for n elements (assume for simplicity that n is a power of two). the ﬁrst level of recursion can be thought of as working on one array of size n, the next level working on two arrays of size n/2, the next on four arrays of size n/4, and so on. the bottom of the recursion has n arrays of size 1. thus, n arrays of size 1 are merged (requiring n total steps), n/2 arrays of size 2 (again requiring n total steps), n/4 arrays of size 4, and so on. at each of the log n levels of recursion, Θ(n) work is done, for a total cost of Θ(n log n). this cost is unaffected by the relative order of the values being sorted, thus this analysis holds for the best, average, and worst cases.
while mergesort uses the most obvious form of divide and conquer (split the list in half then sort the halves), it is not the only way that we can break down the sorting problem. and we saw that doing the merge step for mergesort when using an array implementation is not so easy. so prehaps a different divide and conquer strategy might turn out to be more efﬁcient?
quicksort is aptly named because, when properly implemented, it is the fastest known general-purpose in-memory sorting algorithm in the average case. it does not require the extra array needed by mergesort, so it is space efﬁcent as well. quicksort is widely used, and is typically the algorithm implemented in a library
and fast. the algorithm should take advantage of the fact that sorting is a specialpurpose application in that all of the values to be stored are available at the start. this means that we do not necessarily need to insert one value at a time into the tree structure.
heapsort is based on the heap data structure presented in section 5.5. heapsort has all of the advantages just listed. the complete binary tree is balanced, its array representation is space efﬁcient, and we can load all values into the tree at once, taking advantage of the efﬁcient buildheap function. the asymptotic performance of heapsort is Θ(n log n) in the best, average, and worst cases. it is not as fast as quicksort in the average case (by a constant factor), but heapsort has special properties that will make it particularly useful when sorting data sets too large to ﬁt in main memory, as discussed in chapter 8.
a sorting algorithm based on max-heaps is quite straightforward. first we use the heap building algorithm of section 5.5 to convert the array into max-heap order. then we repeatedly remove the maximum value from the heap, restoring the heap property each time that we do so, until the heap is empty. note that each time we remove the maximum element from the heap, it is placed at the end of the array. assume the n elements are stored in array positions 0 through n−1. after removing the maximum value from the heap and readjusting, the maximum value will now be placed in position n − 1 of the array. the heap is now considered to be of size n − 1. removing the new maximum (root) value places the second largest value in position n − 2 of the array. at the end of the process, the array will be properly sorted from least to greatest. this is why heapsort uses a max-heap rather than a min-heap as might have been expected. figure 7.10 illustrates heapsort. the complete java implementation is as follows:
because building the heap takes Θ(n) time (see section 5.5), and because n deletions of the maximum element each take Θ(log n) time, we see that the entire heapsort operation takes Θ(n log n) time in the worst, average, and best cases. while typically slower than quicksort by a constant factor, heapsort has one special advantage over the other sorts studied so far. building the heap is relatively cheap, requiring Θ(n) time. removing the maximum element from the heap requires Θ(log n) time. thus, if we wish to ﬁnd the k largest elements in an array, we can do so in time Θ(n+ k log n). if k is small, this is a substantial improvement
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
7.13 graph f1(n) = n log n, f2(n) = n1.5, and f3(n) = n2 in the range 1 ≤ n ≤ 1000 to visually compare their growth rates. typically, the constant factor in the running-time expression for an implementation of insertion sort will be less than the constant factors for shellsort or quicksort. how many times greater can the constant factor be for shellsort to be faster than insertion sort when n = 1000? how many times greater can the constant factor be for quicksort to be faster than insertion sort when n = 1000?
7.14 imagine that there exists an algorithm splitk that can split a list l of n elements into k sublists, each containing one or more elements, such that sublist i contains only elements whose values are less than all elements in sublist j for i < j <= k. if n < k, then k− n sublists are empty, and the rest are of length 1. assume that splitk has time complexity o(length of l). furthermore, assume that the k lists can be concatenated again in constant time. consider the following algorithm: list sortk(list l) {
7.15 here is a variation on sorting. the problem is to sort a collection of n nuts and n bolts by size. it is assumed that for each bolt in the collection, there is a corresponding nut of the same size, but initially we do not know which nut goes with which bolt. the differences in size between two nuts or two bolts can be too small to see by eye, so you cannot rely on comparing the sizes of two nuts or two bolts directly. instead, you can only compare the sizes of a nut and a bolt by attempting to screw one into the other (assume this comparison to be a constant time operation). this operation tells you that either the nut is bigger than the bolt, the bolt is bigger than the nut, or they are the same size. what is the minimum number of comparisons needed to sort the nuts and bolts in the worst case?
can stop early. this makes the best case performance become o(n) (because if the list is already sorted, then no iterations will take place on the ﬁrst pass, and the sort will stop right there). modify the bubble sort implementation to add this ﬂag and test. compare the modiﬁed implementation on a range of inputs to determine if it does or does not improve performance in practice.
7.2 starting with the java code for quicksort given in this chapter, write a series of quicksort implementations to test the following optimizations on a wide range of input data sizes. try these optimizations in various combinations to try and develop the fastest possible quicksort implementation that you can. (a) look at more values when selecting a pivot. (b) do not make a recursive call to qsort when the list size falls below a given threshold, and use insertion sort to complete the sorting process. test various values for the threshold size.
7.3 write your own collection of sorting programs to implement the algorithms described in this chapter, and compare their running times. be sure to implement optimized versions, trying to make each program as fast as possible. do you get the same relative timings as shown in figure 7.13? if not, why do you think this happened? how do your results compare with those of your classmates? what does this say about the difﬁculty of doing empirical timing studies?
7.4 perform a study of shellsort, using different increments. compare the version shown in section 7.3, where each increment is half the previous one, with others. in particular, try implementing “division by 3” where the increments on a list of length n will be n/3, n/9, etc. do other increment schemes work as well?
7.5 the implementation for mergesort given in section 7.4 takes an array as input and sorts that array. at the beginning of section 7.4 there is a simple pseudocode implementation for sorting a linked list using mergesort. implement both a linked list-based version of mergesort and the array-based version of mergesort, and compare their running times.
7.6 radix sort is typically implemented to support only a radix that is a power of two. this allows for a direct conversion from the radix to some number of bits in an integer key value. for example, if the radix is 16, then a 32-bit key will be processed in 8 steps of 4 bits each. this can lead to a more efﬁcient implementation because bit shifting can replace the division operations shown in the implementation of section 7.7. reimplement the radix sort
block contains the same number of ﬁxed-size data records. depending on the application, a record might be only a few bytes — composed of little or nothing more than the key — or might be hundreds of bytes with a relatively small key ﬁeld. records are assumed not to cross block boundaries. these assumptions can be relaxed for special-purpose sorting applications, but ignoring such complications makes the principles clearer.
as explained in section 8.2, a sector is the basic unit of i/o. in other words, all disk reads and writes are for one or more complete sectors. sector sizes are typically a power of two, in the range 512 to 16k bytes, depending on the operating system and the size and speed of the disk drive. the block size used for external sorting algorithms should be equal to or a multiple of the sector size.
under this model, a sorting algorithm reads a block of data into a buffer in main memory, performs some processing on it, and at some future time writes it back to disk. from section 8.1 we see that reading or writing a block from disk takes on the order of one million times longer than a memory access. based on this fact, we can reasonably expect that the records contained in a single block can be sorted by an internal sorting algorithm such as quicksort in less time than is required to read or write the block.
under good conditions, reading from a ﬁle in sequential order is more efﬁcient than reading blocks in random order. given the signiﬁcant impact of seek time on disk access, it might seem obvious that sequential processing is faster. however, it is important to understand precisely under what circumstances sequential ﬁle processing is actually faster than random access, because it affects our approach to designing an external sorting algorithm.
efﬁcient sequential access relies on seek time being kept to a minimum. the ﬁrst requirement is that the blocks making up a ﬁle are in fact stored on disk in sequential order and close together, preferably ﬁlling a small number of contiguous tracks. at the very least, the number of extents making up the ﬁle should be small. users typically do not have much control over the layout of their ﬁle on disk, but writing a ﬁle all at once in sequential order to a disk drive with a high percentage of free space increases the likelihood of such an arrangement.
the second requirement is that the disk drive’s i/o head remain positioned over the ﬁle throughout sequential processing. this will not happen if there is competition of any kind for the i/o head. for example, on a multi-user timeshared computer the sorting process might compete for the i/o head with the processes of other users. even when the sorting process has sole control of the i/o head, it is still likely that sequential processing will not be efﬁcient. imagine the situation where all processing is done on a single disk drive, with the typical arrangement
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
this book contains many examples of asymptotic analysis of the time requirements for algorithms and the space requirements for data structures. often it is easy to invent an equation to model the behavior of the algorithm or data structure in question, and also easy to derive a closed-form solution for the equation should it contain a recurrence or summation.
sometimes an analysis proves more difﬁcult. it may take a clever insight to derive the right model, such as the snowplow argument for analyzing the average run length resulting from replacement selection (section 8.5.2). in this case, once the snowplow argument is understood, the resulting equations are simple. sometimes, developing the model is straightforward but analyzing the resulting equations is not. an example is the average-case analysis for quicksort. the equation given in section 7.5 simply enumerates all possible cases for the pivot position, summing corresponding costs for the recursive calls to quicksort. however, deriving a closed-form solution for the resulting recurrence relation is not as easy.
many iterative algorithms require that we compute a summation to determine the cost of a loop. techniques for ﬁnding closed-form solutions to summations are presented in section 14.1. time requirements for many algorithms based on recursion are best modeled by recurrence relations. a discussion of techniques for solving recurrences is provided in section 14.2. these sections extend the introduction to summations and recurrences provided in section 2.4, so the reader should already be familiar with that material.
section 14.3 provides an introduction to the topic of amortized analysis. amortized analysis deals with the cost of a series of operations. perhaps a single operation in the series has high cost, but as a result the cost of the remaining operations is limited in such a way that the entire series can be done efﬁciently. amortized analysis has been used successfully to analyze several of the algorithms presented in
the cn term is an upper bound on the findpivot and partition steps. this equation comes from assuming that the partitioning element is equally likely to occur in any position k. it can be simpliﬁed by observing that the two recurrence terms t(k) and t(n − 1 − k) are equivalent, because one simply counts up from t (0) to t (n − 1) while the other counts down from t (n − 1) to t (0). this yields
this form is known as a recurrence with full history. the key to solving such a recurrence is to cancel out the summation terms. the shifting method for summations provides a way to do this. multiply both sides by n and subtract the result from the formula for nt(n + 1):
at this point, we have eliminated the summation and can now use our normal methods for solving recurrences to get a closed-form solution. note that c(2n+1) n+1 < 2c, so we can simplify the result. expanding the recurrence, we get
this section presents the concept of amortized analysis, which is the analysis for a series of operations taken as a whole. in particular, amortized analysis allows us to deal with the situation where the worst-case cost for n operations is less than n times the worst-case cost of any one operation. rather than focusing on the individual cost of each operation independently and summing them, amortized analysis looks at the cost of the entire series and “charges” each individual operation with a share of the total cost.
we can apply the technique of amortized analysis in the case of a series of sequential searches in an unsorted array. for n random searches, the average-case cost for each search is n/2, and so the expected total cost for the series is n2/2. unfortunately, in the worst case all of the searches would be to the last item in the array. in this case, each search costs n for a total worst-case cost of n2. compare this to the cost for a series of n searches such that each item in the array is searched for precisely once. in this situation, some of the searches must be expensive, but also some searches must be cheap. the total number of searches, in the best, avi=i i ≈ n2/2. this is a factor
this version of binsort can sort any collection of records whose key values fall in the range from 0 to maxkeyvalue−1. the total work required is simply that needed to place each record into the appropriate bin and then take all of the records out of the bins. thus, we need to process each record twice, for Θ(n) work.
unfortunately, there is a crucial oversight in this analysis. binsort must also look at each of the bins to see if it contains a record. the algorithm must process maxkeyvalue bins, regardless of how many actually hold records. if maxkeyvalue is small compared to n, then this is not a great expense. suppose that maxkeyvalue = n2. in this case, the total amount of work done will be Θ(n + n2) = Θ(n2). this results in a poor sorting algorithm, and the algorithm becomes even worse as the disparity between n and maxkeyvalue increases. in addition, a large key range requires an unacceptably large array b. thus, even the extended binsort is useful only for a limited key range.
a further generalization to binsort yields a bucket sort. each bin is associated with not just one key, but rather a range of key values. a bucket sort assigns records to bins and then relies on some other sorting technique to sort the records within each bin. the hope is that the relatively inexpensive bucketing process will put only a small number of records in each bin, and that a “cleanup sort” within the bins will then be relatively cheap.
there is a way to keep the number of bins and the related processing small while allowing the cleanup sort to be based on binsort. consider a sequence of records with keys in the range 0 to 99. if we have ten bins available, we can ﬁrst assign records to bins by taking their key value modulo 10. thus, every key will be assigned to the bin matching its rightmost decimal digit. we can then take these records from the bins in order and reassign them to the bins on the basis of their leftmost (10’s place) digit (deﬁne values in the range 0 to 9 to have a leftmost digit
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
can stop early. this makes the best case performance become o(n) (because if the list is already sorted, then no iterations will take place on the ﬁrst pass, and the sort will stop right there). modify the bubble sort implementation to add this ﬂag and test. compare the modiﬁed implementation on a range of inputs to determine if it does or does not improve performance in practice.
7.2 starting with the java code for quicksort given in this chapter, write a series of quicksort implementations to test the following optimizations on a wide range of input data sizes. try these optimizations in various combinations to try and develop the fastest possible quicksort implementation that you can. (a) look at more values when selecting a pivot. (b) do not make a recursive call to qsort when the list size falls below a given threshold, and use insertion sort to complete the sorting process. test various values for the threshold size.
7.3 write your own collection of sorting programs to implement the algorithms described in this chapter, and compare their running times. be sure to implement optimized versions, trying to make each program as fast as possible. do you get the same relative timings as shown in figure 7.13? if not, why do you think this happened? how do your results compare with those of your classmates? what does this say about the difﬁculty of doing empirical timing studies?
7.4 perform a study of shellsort, using different increments. compare the version shown in section 7.3, where each increment is half the previous one, with others. in particular, try implementing “division by 3” where the increments on a list of length n will be n/3, n/9, etc. do other increment schemes work as well?
7.5 the implementation for mergesort given in section 7.4 takes an array as input and sorts that array. at the beginning of section 7.4 there is a simple pseudocode implementation for sorting a linked list using mergesort. implement both a linked list-based version of mergesort and the array-based version of mergesort, and compare their running times.
7.6 radix sort is typically implemented to support only a radix that is a power of two. this allows for a direct conversion from the radix to some number of bits in an integer key value. for example, if the radix is 16, then a 32-bit key will be processed in 8 steps of 4 bits each. this can lead to a more efﬁcient implementation because bit shifting can replace the division operations shown in the implementation of section 7.7. reimplement the radix sort
computer storage devices are typically classiﬁed into primary or main memory and secondary or peripheral storage. primary memory usually refers to random access memory (ram), while secondary storage refers to devices such as hard disk drives, removeable “ﬂash” drives, ﬂoppy disks, cds, dvds, and tape drives. primary memory also includes registers, cache, and video memories, but we will ignore them for this discussion because their existence does not affect the principal differences between primary and secondary memory.
along with a faster cpu, every new model of computer seems to come with more main memory. as memory size continues to increase, is it possible that relatively slow disk storage will be unnecessary? probably not, because the desire to store and process larger ﬁles grows at least as fast as main memory size. prices for both main memory and peripheral storage devices have dropped dramatically in recent years, as demonstrated by figure 8.1. however, the cost for disk drive storage per megabyte is about two orders of magnitude less than ram and has been for many years.
there is now a wide range of removable media available for transfering data or storing data ofﬂine in relative safety. these include ﬂoppy disks (now largely obsolete), writable cds and dvds, “ﬂash” drives, and magnetic tape. optical storage such as cds and dvds costs roughly half the price of hard disk drive space per megabyte, and have become practical for use as backup storage within the past few years. tape used to be much cheaper than other media, and was the preferred means of backup. “flash” drives cost the most per megabyte, but due to their storage capacity and ﬂexibility, have now replaced ﬂoppy disks as the primary storage device for transferring data between computer when direct network transfer is not available.
secondary storage devices have at least two other advantages over ram memory. perhaps most importantly, disk and tape ﬁles are persistent, meaning that
they are not erased from disk and tape when the power is turned off. in contrast, ram used for main memory is usually volatile — all information is lost with the power. a second advantage is that ﬂoppy disks, cd-roms, and “ﬂash” drives can easily be transferred between computers. this provides a convenient way to take information from one computer to another.
in exchange for reduced storage costs, persistence, and portability, secondary storage devices pay a penalty in terms of increased access time. while not all accesses to disk take the same amount of time (more on this later), the typical time required to access a byte of storage from a disk drive in 2007 is around 9 ms (i.e., 9 thousandths of a second). this might not seem slow, but compared to the time required to access a byte from main memory, this is fantastically slow. typical access time from standard personal computer ram in 2007 is about 5-10 nanoseconds (i.e., 5-10 billionths of a second). thus, the time to access a byte of data from a disk drive is about six orders of magnitude greater than that required to access a byte from main memory. while disk drive and ram access times are both decreasing, they have done so at roughly the same rate. the relative speeds have remained the same for over twenty-ﬁve years, in that the difference in access time between ram and a disk drive has remained in the range between a factor of 100,000 and 1,000,000.
to gain some intuition for the signiﬁcance of this speed difference, consider the time that it might take for you to look up the entry for disk drives in the index of this book, and then turn to the appropriate page. call this your “primary memory” access time. if it takes you about 20 seconds to perform this access, then an access taking 500,000 times longer would require months.
it is interesting to note that while processing speeds have increased dramatically, and hardware prices have dropped dramatically, disk and memory access times have improved by less than an order of magnitude over the past ten years. however, the situation is really much better than that modest speedup would suggest. during the same time period, the size of both disk and main memory has increased by about three orders of magnitude. thus, the access times have actually decreased in the face of a massive increase in the density of these storage devices. due to the relatively slow access time for data on disk as compared to main memory, great care is required to create efﬁcient applications that process diskbased information. the million-to-one ratio of disk access time versus main memory access time makes the following rule of paramount importance when designing disk-based applications:
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
n values. if we implement sequential search as a program and run it many times on many different arrays of size n, or search for many different values of k within the same array, we expect the algorithm on average to go halfway through the array before ﬁnding the value we seek. on average, the algorithm examines about n/2 values. we call this the average case for this algorithm.
when analyzing an algorithm, should we study the best, worst, or average case? normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. in other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. however, there are rare instances where a best-case analysis is useful — in particular, when the best case has high probability of occurring. in chapter 7 you will see some examples where taking advantage of the best-case running time for one sorting algorithm makes a second more efﬁcient.
how about the worst case? the advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. this is especially important for real-time applications, such as for the computers that monitor an air trafﬁc control system. here, it would not be acceptable to use an algorithm that can handle n airplanes quickly enough most of the time, but which fails to perform quickly enough when all n airplanes are coming from the same direction.
for other applications — particularly when we wish to aggregate the cost of running the program many times on many different inputs — worst-case analysis might not be a representative measure of the algorithm’s performance. often we prefer to know the average-case running time. this means that we would like to know the typical behavior of the algorithm on inputs of size n. unfortunately, average-case analysis is not always possible. average-case analysis ﬁrst requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. for example, it was stated previously that the sequential search algorithm on average examines half of the array values. this is only true if the element with value k is equally likely to appear in any position in the array. if this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. see section 9.2 for further discussion regarding the effects of data distribution on the sequential search algorithm.
the characteristics of a data distribution have a signiﬁcant effect on many search algorithms, such as those based on hashing (section 9.4) and search trees (e.g., see section 5.4). incorrect assumptions about data distribution can have dis-
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
because we have merely guessed at a pattern and not actually proved that this is the correct closed form solution, we can use an induction proof to complete the process (see example 2.13).
techniques to ﬁnd closed-form solutions for recurrence relations are discussed in section 14.2. prior to chapter 14, recurrence relations are used infrequently in this book, and the corresponding closed-form solution and an explanation for how it was derived will be supplied at the time of use.
an algorithm is recursive if it calls itself to do part of its work. for this approach to be successful, the “call to itself” must be on a smaller problem then the one originally attempted. in general, a recursive algorithm must have two parts: the base case, which handles a simple input that can be solved without resorting to a recursive call, and the recursive part which contains one or more recursive calls to the algorithm where the parameters are in some sense “closer” to the base case than those of the original call. here is a recursive java function to compute the
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
in the worst case, quicksort is Θ(n2). this is terrible, no better than bubble sort.2 when will this worst case occur? only when each pivot yields a bad partitioning of the array. if the pivot values are selected at random, then this is extremely unlikely to happen. when selecting the middle position of the current subarray, it is still unlikely to happen. it does not take many good partitionings for quicksort to work fairly well.
quicksort’s best case occurs when findpivot always breaks the array into two equal halves. quicksort repeatedly splits the array into smaller partitions, as shown in figure 7.9. in the best case, the result will be log n levels of partitions, with the top level having one array of size n, the second level two arrays of size n/2, the next with four arrays of size n/4, and so on. thus, at each level, all partition steps for that level do a total of n work, for an overall cost of n log n work when quicksort ﬁnds perfect pivots.
quicksort’s average-case behavior falls somewhere between the extremes of worst and best case. average-case analysis considers the cost for all possible arrangements of input, summing the costs and dividing by the number of cases. we make one reasonable simplifying assumption: at each partition step, the pivot is equally likely to end in any position in the (sorted) array. in other words, the pivot is equally likely to break an array into partitions of sizes 0 and n−1, or 1 and n−2, and so on.
this equation is in the form of a recurrence relation. recurrence relations are discussed in chapters 2 and 14, and this one is solved in section 14.2.4. this equation says that there is one chance in n that the pivot breaks the array into subarrays of size 0 and n − 1, one chance in n that the pivot breaks the array into subarrays of size 1 and n− 2, and so on. the expression “t(k) + t(n− 1− k)” is the cost for the two recursive calls to quicksort on two arrays of size k and n−1−k.
recurrence relations are often used to model the cost of recursive functions. for example, the standard mergesort (section 7.4) takes a list of size n, splits it in half, performs mergesort on each half, and ﬁnally merges the two sublists in n steps. the cost for this can be modeled as
in other words, the cost of the algorithm on input of size n is two times the cost for input of size n/2 (due to the two recursive calls to mergesort) plus n (the time to merge the sublists together again).
there are many approaches to solving recurrence relations, and we brieﬂy consider three here. the ﬁrst is an estimation technique: guess the upper and lower bounds for the recurrence, use induction to prove the bounds, and tighten as required. the second approach is to expand the recurrence to convert it to a summation and then use summation techniques. the third approach is to take advantage of already proven theorems when the recurrence is of a suitable form. in particular, typical divide and conquer algorithms such as mergesort yield recurrences of a form that ﬁts a pattern for which we have a ready solution.
14.2.1 estimating upper and lower bounds the ﬁrst approach to solving recurrences is to guess the answer and then attempt to prove it correct. if a correct upper or lower bound estimate is given, an easy induction proof will verify this fact. if the proof is successful, then try to tighten the bound. if the induction proof fails, then loosen the bound and try again. once the upper and lower bounds match, you are ﬁnished. this is a useful technique when you are only looking for asymptotic complexities. when seeking a precise closed-form solution (i.e., you seek the constants for the expression), this method will not be appropriate.
this section presents the concept of amortized analysis, which is the analysis for a series of operations taken as a whole. in particular, amortized analysis allows us to deal with the situation where the worst-case cost for n operations is less than n times the worst-case cost of any one operation. rather than focusing on the individual cost of each operation independently and summing them, amortized analysis looks at the cost of the entire series and “charges” each individual operation with a share of the total cost.
we can apply the technique of amortized analysis in the case of a series of sequential searches in an unsorted array. for n random searches, the average-case cost for each search is n/2, and so the expected total cost for the series is n2/2. unfortunately, in the worst case all of the searches would be to the last item in the array. in this case, each search costs n for a total worst-case cost of n2. compare this to the cost for a series of n searches such that each item in the array is searched for precisely once. in this situation, some of the searches must be expensive, but also some searches must be cheap. the total number of searches, in the best, avi=i i ≈ n2/2. this is a factor
proof: each comparison of the search key with a record in the list is either successful or unsuccessful. for m searches, there must be exactly m successful comparisons for both the self-organizing list and the static list. the total number of unsuccessful comparisons in the self-organizing list is the sum, over all pairs of distinct keys, of the number of unsuccessful comparisons made between that pair. consider a particular pair of keys a and b. for any sequence of searches s, the total number of (unsuccessful) comparisons between a and b is identical to the number of comparisons between a and b required for the subsequence of s made up only of searches for a or b. call this subsequence sab. in other words, including searches for other keys does not affect the relative position of a and b and so does not affect the relative contribution to the total cost of the unsuccessful comparisons between a and b.
the number of unsuccessful comparisons between a and b made by the moveto-front heuristic on subsequence sab is at most twice the number of unsuccessful comparisons between a and b required when sab is applied to the optimal static ordering for the list. to see this, assume that sab contains i as and j bs, with i ≤ j. under the optimal static ordering, i unsuccessful comparisons are required because b must appear before a in the list (because its access frequency is higher). move-tofront will yield an unsuccessful comparison whenever the request sequence changes from a to b or from b to a. the total number of such changes possible is 2i because each change involves an a and each a can be part of at most two changes.
because the total number of unsuccessful comparisons required by move-tofront for any given pair of keys is at most twice that required by the optimal static ordering, the total number of unsuccessful comparisons required by move-to-front for all pairs of keys is also at most twice as high. because the number of successful comparisons is the same for both methods, the total number of comparisons required by move-to-front is less than twice the number of comparisons required by the optimal static ordering. 2
a good introduction to solving recurrence relations appears in applied combinatorics by fred s. roberts [rob84]. for a more advanced treatment, see concrete mathematics by graham, knuth, and patashnik [gkp94].
cormen, leiserson, and rivest provide a good discussion on various methods for performing amortized analysis in introduction to algorithms [clrs01]. for an amortized analysis that the splay tree requires m log n time to perform a series of m operations on n nodes when m > n, see “self-adjusting binary search trees” by sleator and tarjan [st85]. the proof for theorem 14.2 comes from
14.8 a chocolate company decides to promote its chocolate bars by including a coupon with each bar. a bar costs a dollar, and with c coupons you get a free bar. so depending on the value of c, you get more than one bar of chocolate for a dollar when considering the value of the coupons. how much chocolate is a dollar worth (as a function of c)?
14.15 for the following recurrence, give a closed-form solution. you should not give an exact solution, but only an asymptotic solution (i.e., using Θ notation). you may assume that n is a power of 2. prove that your answer is correct.
14.17 for each of the following recurrences, ﬁnd and then prove (using induction) an exact closed-form solution. when convenient, you may assume that n is a power of 2. (a) t(n) = t(n − 1) + n/2 for n > 1; t(1) = 1.
example 14.8 our next example comes from the algorithm to build a heap. recall from section 5.5 that to build a heap, we ﬁrst heapify the two subheaps, then push down the root to its proper position. the cost is:
14.2.3 divide and conquer recurrences the third approach to solving recurrences is to take advantage of known theorems that describe the solution for classes of recurrences. one useful example is a theorem that gives the answer for a class known as divide and conquer recurrences. these have the form
recurrence relations are often used to model the cost of recursive functions. for example, the standard mergesort (section 7.4) takes a list of size n, splits it in half, performs mergesort on each half, and ﬁnally merges the two sublists in n steps. the cost for this can be modeled as
in other words, the cost of the algorithm on input of size n is two times the cost for input of size n/2 (due to the two recursive calls to mergesort) plus n (the time to merge the sublists together again).
there are many approaches to solving recurrence relations, and we brieﬂy consider three here. the ﬁrst is an estimation technique: guess the upper and lower bounds for the recurrence, use induction to prove the bounds, and tighten as required. the second approach is to expand the recurrence to convert it to a summation and then use summation techniques. the third approach is to take advantage of already proven theorems when the recurrence is of a suitable form. in particular, typical divide and conquer algorithms such as mergesort yield recurrences of a form that ﬁts a pattern for which we have a ready solution.
14.2.1 estimating upper and lower bounds the ﬁrst approach to solving recurrences is to guess the answer and then attempt to prove it correct. if a correct upper or lower bound estimate is given, an easy induction proof will verify this fact. if the proof is successful, then try to tighten the bound. if the induction proof fails, then loosen the bound and try again. once the upper and lower bounds match, you are ﬁnished. this is a useful technique when you are only looking for asymptotic complexities. when seeking a precise closed-form solution (i.e., you seek the constants for the expression), this method will not be appropriate.
example 14.6 what is the growth rate of the ﬁbonacci sequence f(n) = f(n − 1) + f(n − 2) for n ≥ 2; f(0) = f(1) = 1? in this case it is useful to compare the ratio of f(n) to f(n − 1). the
following this out a few more terms, it appears to settle to a ratio of approximately 1.618. assuming f(n)/f(n − 1) really does tend to a ﬁxed value, we can determine what that value must be.
this comes from knowing that f(n) = f(n− 1) + f(n− 2). we divide by f(n − 2) to make the second term go away, and we also get something useful in the ﬁrst term. remember that the goal of such manipulations is to give us an equation that relates f(n) to something without recursive calls.
as n gets big. this comes from multiplying f(n)/f(n − 2) by f(n − 1)/f(n − 1) and rearranging. if x exists, then x2 − x− 1 → 0. using the quadratic equation, the only
this expression also has the name φ. what does this say about the growth rate of the ﬁbonacci sequence? it is exponential, with f(n) = Θ(φn). more precisely, f(n) converges to
14.2.2 expanding recurrences estimating bounds is effective if you only need an approximation to the answer. more precise techniques are required to ﬁnd an exact solution. one such technique is called expanding the recurrence. in this method, the smaller terms on the right side of the equation are in turn replaced by their deﬁnition. this is the expanding step. these terms are again expanded, and so on, until a full series with no recurrence results. this yields a summation, and techniques for solving summations can then be used. a couple of simple expansions were shown in section 2.4. a more complex example is given below.
this is the exact solution to the recurrence for n a power of two. at this point, we should use a simple induction proof to verify that our solution is indeed correct.
example 14.8 our next example comes from the algorithm to build a heap. recall from section 5.5 that to build a heap, we ﬁrst heapify the two subheaps, then push down the root to its proper position. the cost is:
14.2.3 divide and conquer recurrences the third approach to solving recurrences is to take advantage of known theorems that describe the solution for classes of recurrences. one useful example is a theorem that gives the answer for a class known as divide and conquer recurrences. these have the form
14.8 a chocolate company decides to promote its chocolate bars by including a coupon with each bar. a bar costs a dollar, and with c coupons you get a free bar. so depending on the value of c, you get more than one bar of chocolate for a dollar when considering the value of the coupons. how much chocolate is a dollar worth (as a function of c)?
14.15 for the following recurrence, give a closed-form solution. you should not give an exact solution, but only an asymptotic solution (i.e., using Θ notation). you may assume that n is a power of 2. prove that your answer is correct.
14.17 for each of the following recurrences, ﬁnd and then prove (using induction) an exact closed-form solution. when convenient, you may assume that n is a power of 2. (a) t(n) = t(n − 1) + n/2 for n > 1; t(1) = 1.
because we have merely guessed at a pattern and not actually proved that this is the correct closed form solution, we can use an induction proof to complete the process (see example 2.13).
techniques to ﬁnd closed-form solutions for recurrence relations are discussed in section 14.2. prior to chapter 14, recurrence relations are used infrequently in this book, and the corresponding closed-form solution and an explanation for how it was derived will be supplied at the time of use.
an algorithm is recursive if it calls itself to do part of its work. for this approach to be successful, the “call to itself” must be on a smaller problem then the one originally attempted. in general, a recursive algorithm must have two parts: the base case, which handles a simple input that can be solved without resorting to a recursive call, and the recursive part which contains one or more recursive calls to the algorithm where the parameters are in some sense “closer” to the base case than those of the original call. here is a recursive java function to compute the
figure 4.21 implementing recursion with a stack. β values indicate the address of the program instruction to return to after completing the current function call. on each recursive function call to fact (from section 2.5), both the return address and the current value of n must be saved. each return from fact pops the top activation record off the stack.
consider what happens when we call fact with the value 4. we use β to indicate the address of the program instruction where the call to fact is made. thus, the stack must ﬁrst store the address β, and the value 4 is passed to fact. next, a recursive call to fact is made, this time with value 3. we will name the program address from which the call is made β1. the address β1, along with the current value for n (which is 4), is saved on the stack. function fact is invoked with input parameter 3.
in similar manner, another recursive call is made with input parameter 2, requiring that the address from which the call is made (say β2) and the current value for n (which is 3) are stored on the stack. a ﬁnal recursive call with input parame-
must be visited in an order that preserves some relationship. for example, we might wish to make sure that we visit any given node before we visit its children. this is called a preorder traversal.
alternatively, we might wish to visit each node only after we visit its children (and their subtrees). for example, this would be necessary if we wish to return all nodes in the tree to free store. we would like to delete the children of a node before deleting the node itself. but to do that requires that the children’s children be deleted ﬁrst, and so on. this is called a postorder traversal.
an inorder traversal ﬁrst visits the left child (including its entire subtree), then visits the node, and ﬁnally visits the right child (including its entire subtree). the binary search tree of section 5.4 makes use of this traversal.
a traversal routine is naturally written as a recursive function. its input parameter is a pointer to a node which we will call root because each node can be viewed as the root of a some subtree. the initial call to the traversal function passes in a pointer to the root node of the tree. the traversal function visits root and its children (if any) in the desired order. for example, a preorder traversal speciﬁes that root be visited before its children. this can easily be implemented as follows. void preorder(binnode rt) // rt is the root of the subtree {
another issue to consider when designing a traversal is how to deﬁne the visitor function that is to be executed on every node. one approach is simply to write a new version of the traversal for each such visitor function as needed. the disadvantage to this is that whatever function does the traversal must have access to the binnode class. it is probably better design to permit only the tree class to have access to the binnode class.
another approach is for the tree class to supply a generic traversal function which takes the visitor as a function parameter. this is known as the visitor design pattern. a major contraint on this approach is that the signature for all visitor functions, that is, their return type and parameters, must be ﬁxed in advance. thus, the designer of the generic traversal function must be able to adequately judge what parameters and return type will likely be needed by potential visitor functions.
properly handling information ﬂow between parts of a program can often be a signiﬁcant design challenge. this issue tends to be particularly confusing when dealing with recursive functions such as tree traversals. in general, we can run into trouble either with passing in the correct information needed by the function to do its work, or with returning information to the recursive function’s caller. we will see many examples throughout the book that illustrate methods for passing information in and out of recursive functions as they traverse a tree structure. before leaving this section, we will study a few simple examples.
example 5.4 we wish to write a function that counts the number of nodes in a binary tree. the key insight is that the total count for any (non-empty) subtree is one for the root plus the counts for the left and right subtrees. we can implement the function as follows.
another problem that occurs when recursively processing data collections is controlling which members of the collection will be visited. for example, some tree “traversals” might in fact visit only some tree nodes, while avoiding processing of others. exercise 5.20 must solve exactly this problem in the context of a binary search tree. it must visit only those children of a given node that might possibly
the record if it is found. however, the ﬁnd operation is most easily implemented as a recursive function whose parameters are the root of a bst subtree, the search key, and space for the element once it is found. member findhelp is of the desired form for this recursive subroutine and is implemented as follows:
once the desired record is found, it is passed up the chain of recursive calls to findhelp in the third parameter, “it.” the return value for the function (true or false, depending on whether a suitable element has been found) is also simply passed back up the chain of recursive calls.
inserting a record with key value k requires that we ﬁrst ﬁnd where that record would have been if it were in the tree. this takes us to either a leaf node, or to an internal node with no child in the appropriate direction.3 call this node r 0. we then add a new node containing the new record as a child of r 0. figure 5.15 illustrates this operation. the value 35 is added as the right child of the node with value 32. here is the implementation for inserthelp: private bstnode<k,e> inserthelp(bstnode<k,e> rt, k k, e e) {
3this assumes that no node has a key value equal to the one being inserted. if we ﬁnd a node that duplicates the key value to be inserted, we have two options. if the application does not allow nodes with equal keys, then this insertion should be treated as an error (or ignored). if duplicate keys are allowed, our convention will be to insert the duplicate in the right subtree.
for each of the following scenarios, which of these choices would be best? explain your answer. (a) the records are guaranteed to arrive already sorted from lowest to highest (i.e., whenever a record is inserted, its key value will always be greater than that of the last record inserted). a total of 1000 inserts will be interspersed with 1000 searches.
(b) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1,000,000 insertions are performed, followed by 10 searches.
(c) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are interspersed with 1000 searches.
(d) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are performed, followed by 1,000,000 searches.
5.2 one way to deal with the “problem” of null pointers in binary trees is to use that space for some other purpose. one example is the threaded binary tree. extending the node implementation of figure 5.7, the threaded binary tree stores with each node two additional bit ﬁelds that indicate if the child pointers lc and rc are regular pointers to child nodes or threads. if lc is not a pointer to a non-empty child (i.e., if it would be null in a regular binary tree), then it instead stores a pointer to the inorder predecessor of that node. the inorder predecessor is the node that would be printed immediately before the current node in an inorder traversal. if rc is not a pointer to a child, then it instead stores a pointer to the node’s inorder successor. the inorder successor is the node that would be printed immediately after the current node in an inorder traversal. the main advantage of threaded binary trees is that operations such as inorder traversal can be implemented without using recursion or a stack. reimplement the bst as a threaded binary tree, and include a non-recursive version of the preorder traversal
5.3 implement a city database using a bst to store the database records. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates.
figure 7.7 an illustration of mergesort. the ﬁrst row shows eight numbers that are to be sorted. mergesort will recursively subdivide the list into sublists of one element each, then recombine the sublists. the second row shows the four sublists of size 2 created by the ﬁrst merging pass. the third row shows the two sublists of size 4 created by the next merging pass on the sublists of row 2. the last row shows the ﬁnal sorted list created by merging the two sublists of row 3.
implementing mergesort presents a number of technical difﬁculties. the ﬁrst decision is how to represent the lists. mergesort lends itself well to sorting a singly linked list because merging does not require random access to the list elements. thus, mergesort is the method of choice when the input is in the form of a linked list. implementing merge for linked lists is straightforward, because we need only remove items from the front of the input lists and append items to the output list. breaking the input list into two equal halves presents some difﬁculty. ideally we would just break the lists into front and back halves. however, even if we know the length of the list in advance, it would still be necessary to traverse halfway down the linked list to reach the beginning of the second half. a simpler method, which does not rely on knowing the length of the list in advance, assigns elements of the input list alternating between the two sublists. the ﬁrst element is assigned to the ﬁrst sublist, the second element to the second sublist, the third to ﬁrst sublist, the fourth to the second sublist, and so on. this requires one complete pass through the input list to build the sublists.
when the input to mergesort is an array, splitting input into two subarrays is easy if we know the array bounds. merging is also easy if we merge the subarrays into a second array. note that this approach requires twice the amount of space as any of the sorting methods presented so far, which is a serious disadvantage for mergesort. it is possible to merge the subarrays without using a second array, but this is extremely difﬁcult to do efﬁciently and is not really practical. merging the two subarrays into a second array, while simple to implement, presents another difﬁculty. the merge process ends with the sorted list in the auxiliary array. consider how the recursive nature of mergesort breaks the original array into subarrays, as shown in figure 7.7. mergesort is recursively called until subarrays of size 1 have been created, requiring log n levels of recursion. these subarrays are merged into
int i, j, k, mid = (l+r)/2; if (l == r) return; if ((mid-l) >= threshold) mergesort(a, temp, l, mid); else inssort(a, l, mid-l+1); if ((r-mid) > threshold) mergesort(a, temp, mid+1, r); else inssort(a, mid+1, r-mid); // do the merge operation. for (i=l; i<=mid; i++) temp[i] = a[i]; for (j=1; j<=r-mid; j++) temp[r-j+1] = a[j+mid]; // merge sublists back to array for (i=l,j=r,k=l; k<=r; k++)
analysis of mergesort is straightforward, despite the fact that it is a recursive algorithm. the merging part takes time Θ(i) where i is the total length of the two subarrays being merged. the array to be sorted is repeatedly split in half until subarrays of size 1 are reached, at which time they are merged to be of size 2, these merged to subarrays of size 4, and so on as shown in figure 7.7. thus, the depth of the recursion is log n for n elements (assume for simplicity that n is a power of two). the ﬁrst level of recursion can be thought of as working on one array of size n, the next level working on two arrays of size n/2, the next on four arrays of size n/4, and so on. the bottom of the recursion has n arrays of size 1. thus, n arrays of size 1 are merged (requiring n total steps), n/2 arrays of size 2 (again requiring n total steps), n/4 arrays of size 4, and so on. at each of the log n levels of recursion, Θ(n) work is done, for a total cost of Θ(n log n). this cost is unaffected by the relative order of the values being sorted, thus this analysis holds for the best, average, and worst cases.
while mergesort uses the most obvious form of divide and conquer (split the list in half then sort the halves), it is not the only way that we can break down the sorting problem. and we saw that doing the merge step for mergesort when using an array implementation is not so easy. so prehaps a different divide and conquer strategy might turn out to be more efﬁcient?
quicksort is aptly named because, when properly implemented, it is the fastest known general-purpose in-memory sorting algorithm in the average case. it does not require the extra array needed by mergesort, so it is space efﬁcent as well. quicksort is widely used, and is typically the algorithm implemented in a library
7.7 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. we can make any algorithm stable if we alter the input keys so that (potentially) duplicate key values are made unique in a way that the ﬁrst occurance of the original duplicate value is less than the second occurance, which in turn is less than the third, and so on. in the worst case, it is possible that all n input records have the same key value. give an algorithm to modify the key values such that every modiﬁed key value is unique, the resulting key values give the same sort order as the original keys, the result is stable (in that the duplicate original key values remain in their original order), and the process of altering the keys is done in linear time using only a constant amount of additional space.
recursion to reduce the number of function calls made. (a) how deep can the stack get in the worst case? (b) quicksort makes two recursive calls. the algorithm could be changed to make these two calls in a speciﬁc order. in what order should the two calls be made, and how does this affect how deep the stack can become?
7.10 assume l is an array, length(l) returns the number of records in the array, and qsort(l, i, j) sorts the records of l from i to j (leaving the records sorted in l) using the quicksort algorithm. what is the averagecase time complexity for each of the following code fragments? (a) for (i=0; i<l.length; i++)
7.11 modify quicksort to ﬁnd the smallest k values in an array of records. your output should be the array modiﬁed so that the k smallest values are sorted in the ﬁrst k positions of the array. your algorithm should do the minimum amount of work necessary.
7.12 modify quicksort to sort a sequence of variable-length strings stored one after the other in a character array, with a second array (storing pointers to strings) used to index the strings. your function should modify the index array so that the ﬁrst pointer points to the beginning of the lowest valued string, and so on.
(a) devise an algorithm to sort three numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
(b) devise an algorithm to sort ﬁve numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
(c) devise an algorithm to sort eight numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
7.17 devise an efﬁcient algorithm to sort a set of numbers with values in the range 0 to 30,000. there are no duplicates. keep memory requirements to a minimum.
7.18 which of the following operations are best implemented by ﬁrst sorting the list of numbers? for each operation, brieﬂy describe an algorithm to implement it, and state the algorithm’s asymptotic complexity. (a) find the minimum value. (b) find the maximum value. (c) compute the arithmetic mean. (d) find the median (i.e., the middle value). (e) find the mode (i.e., the value that appears the most times).
7.19 consider a recursive mergesort implementation that calls insertion sort on sublists smaller than some threshold. if there are n calls to mergesort, how many calls will there be to insertion sort? why?
7.20 implement mergesort for the case where the input is a linked list. 7.21 counting sort (assuming the input key values are integers in the range 0 to m − 1) works by counting the number of records with each key value in the ﬁrst pass, and then uses this information to place the records in order in a second pass. write an implementation of counting sort (see the implementation of radix sort for some ideas). what can we say about the relative values of m and n for this to be effective? if m < n, what is the running time of this algorithm?
7.22 use an argument similar to that given in section 7.9 to prove that log n is a worst-case lower bound for the problem of searching for a given value in a sorted array containing n elements.
7.1 one possible improvement for bubble sort would be to add a ﬂag variable and a test that determines if an exchange was made during the current iteration. if no exchange was made, then the list is sorted and so the algorithm
figure 12.17 garbage cycle example. all memory elements in the cycle have non-zero reference counts because each element has one pointer to it, even though the entire cycle is garbage.
another approach to garbage collection is the mark/sweep strategy. here, each memory object needs only a single mark bit rather than a reference counter ﬁeld. when free store is exhausted, a separate garbage collection phase takes place as follows.
1. clear all mark bits. 2. perform depth-ﬁrst search (dfs) following pointers from each variable on the system’s list of variables. each memory element encountered during the dfs has its mark bit turned on.
the advantages of the mark/sweep approach are that it needs less space than is necessary for reference counts, and it works for cycles. however, there is a major disadvantage. this is a “hidden” space requirement needed to do the processing. dfs is a recursive algorithm: either it must be implemented recursively, in which case the compiler’s runtime system maintains a stack, or else the memory manager can maintain its own stack. what happens if all memory is contained in a single linked list? then the depth of the recursion (or the size of the stack) is the number of memory cells! unfortunately, the space for the dfs stack must be available at the worst conceivable time, that is, when free memory has been exhausted.
fortunately, a clever technique allows dfs to be performed without requiring additional space for a stack. instead, the structure being traversed is used to hold the stack. at each step deeper into the traversal, instead of storing a pointer on the stack, we “borrow” the pointer being followed. this pointer is set to point back to the node we just came from in the previous step, as illustrated by figure 12.18. each borrowed pointer stores an additional bit to tell us whether we came down the left branch or the right branch of the link node being pointed to. at any given instant we have passed down only one path from the root, and we can follow the trail of pointers back up. as we return (equivalent to popping the recursion stack), we set the pointer back to its original position so as to return the structure to its
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
we ﬁrst deﬁne an enumerated type called tohop, with two values move and toh, to indicate calls to the move function and recursive calls to toh, respectively. note that an array-based stack is used, because we know that the stack will need to store exactly 2n + 1 elements. the new version of toh begins by placing on the stack a description of the initial problem of n rings. the rest of the function is simply a while loop that pops the stack and executes the appropriate operation. in the case of a toh operation (for n > 0), we store on the stack representations for the three operations executed by the recursive version. however, these operations must be placed on the stack in reverse order, so that they will be popped off in the correct order.
some “naturally recursive” applications lend themselves to efﬁcient implementation with a stack, because the amount of information needed to describe a subproblem is small. for example, section 7.5 discusses a stack-based implementation for quicksort.
like the stack, the queue is a list-like structure that provides restricted access to its elements. queue elements may only be inserted at the back (called an enqueue
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
comparisons). second, this proof is one of the few non-trivial lower-bounds proofs that we have for any problem; that is, this proof provides one of the relatively few instances where our lower bound is tighter than simply measuring the size of the input and output. as such, it provides a useful model for proving lower bounds on other problems. finally, knowing a lower bound for sorting gives us a lower bound in turn for other problems whose solution could be used as the basis for a sorting algorithm. the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another is called a reduction, a concept further explored in chapter 17.
except for the radix sort and binsort, all of the sorting algorithms presented in this chapter make decisions based on the direct comparison of two key values. for example, insertion sort sequentially compares the value to be inserted into the sorted list until a comparison against the next value in the list fails. in contrast, radix sort has no direct comparison of key values. all decisions are based on the value of speciﬁc digits in the key value, so it is possible to take approaches to sorting that do not involve key comparisons. of course, radix sort in the end does not provide a more efﬁcient sorting algorithm than comparison-based sorting. thus, empirical evidence suggests that comparison-based sorting is a good approach.3
the proof that any comparison sort requires Ω(n log n) comparisons in the worst case is structured as follows. first, you will see how comparison decisions can be modeled as the branches in a binary tree. this means that any sorting algorithm based on comparisons can be viewed as a binary tree whose nodes correspond to the results of making comparisons. next, the minimum number of leaves in the resulting tree is shown to be the factorial of n. finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).
before presenting the proof of an Ω(n log n) lower bound for sorting, we ﬁrst must deﬁne the concept of a decision tree. a decision tree is a binary tree that can model the processing for any algorithm that makes decisions. each (binary) decision is represented by a branch in the tree. for the purpose of modeling sorting algorithms, we count all comparisons of key values as decisions. if two keys are compared and the ﬁrst is less than the second, then this is modeled as a left branch in the decision tree. in the case where the ﬁrst value is greater than the second, the algorithm takes the right branch.
figure 7.14 shows the decision tree that models insertion sort on three input values. the ﬁrst input value is labeled x, the second y, and the third z. they are
3the truth is stronger than this statement implies. in reality, radix sort relies on comparisons as well and so can be modeled by the technique used in this section. the result is an Ω(n log n) bound in the general case even for algorithms that look like radix sort.
slow, but perhaps there are better ones waiting to be discovered. of course, while having a problem with high running time is bad, it is even worse to have a problem that cannot be solved at all! problems of the later type do exist, and some are presented in section 17.3.
this chapter presents a brief introduction to the theory of expensive and impossible problems. section 17.1 presents the concept of a reduction, which is the central tool used for analyzing the difﬁculty of a problem (as opposed to analyzing the cost of an algorithm). reductions allow us to relate the difﬁculty of various problems, which is often much easier than doing the analysis for a problem from ﬁrst principles. section 17.2 discusses “hard” problems, by which we mean problems that require, or at least appear to require, time exponential on the input size. finally, section 17.3 considers various problems that, while often simple to deﬁne and comprehend, are in fact impossible to solve using a computer program. the classic example of such a problem is deciding whether an arbitrary computer program will go into an inﬁnite loop when processing a speciﬁed input. this is known as the halting problem.
we begin with an important concept for understanding the relationships between problems, called reduction. reduction allows us to solve one problem in terms of another. equally importantly, when we wish to understand the difﬁculty of a problem, reduction allows us to make relative statements about upper and lower bounds on the cost of a problem (as opposed to an algorithm or program).
because the concept of a problem is discussed extensively in this chapter, we want notation to simplify problem descriptions. throughout this chapter, a problem will be deﬁned in terms of a mapping between inputs and outputs, and the name of the problem will be given in all capital letters. thus, a complete deﬁnition of the sorting problem could appear as follows:
once you have bought or written a program to solve one problem, such as sorting, you might be able to use it as a tool to solve a different problem. this is
symmetric. we can convert matrix b to a symmetric matrix in a similar manner. if symmetric matrices could be multiplied “quickly” (in particular, if they could be multiplied together in Θ(n2) time), then we could ﬁnd the result of multiplying two arbitrary n × n matrices in Θ(n2) time by taking advantage of the following observation:
there are several ways that a problem could be considered hard. for example, we might have trouble understanding the deﬁnition of the problem itself. at the beginning of a large data collection and analysis project, developers and their clients might have only a hazy notion of what their goals actually are, and need to work that out over time. for other types of problems, we might have trouble ﬁnding or understanding an algorithm to solve the problem. understanding spoken engish and translating it to written text is an example of a problem whose goals are easy to deﬁne, but whose solution is not easy to discover. but even though a natural language processing algorithm might be difﬁcult to write, the program’s running time might be fairly fast. there are many practical systems today that solve aspects of this problem in reasonable time.
none of these is what is commonly meant when a computer theoretician uses the word “hard.” throughout this section, “hard” means that the best-known algorithm for the problem is expensive in its running time. one example of a hard problem is towers of hanoi. it is easy to understand this problem and its solution. it is also easy to write a program to solve this problem. but, it takes an extremely long time to run for any “reasonably” large value of n. try running a program to solve towers of hanoi for only 30 disks!
the towers of hanoi problem takes exponential time, that is, its running time is Θ(2n). this is radically different from an algorithm that takes Θ(n log n) time or Θ(n2) time. it is even radically different from a problem that takes Θ(n4) time. these are all examples of polynomial running time, because the exponents for all terms of these equations are constants. recall from chapter 3 that if we buy a new computer that runs twice as fast, the size of problem with complexity Θ(n4) that we can solve in a certain amount of time is increased by the fourth root of two. in other words, there is a multiplicative factor increase, even if it is a rather small one. this is true for any algorithm whose running time can be represented by a polynomial.
the action of contrary is logically inconsistent with the assumption that halt solves the halting problem correctly. there are no other assumptions we made that might cause this inconsistency. thus, by contradiction, we have proved that halt cannot solve the halting problem correctly, and thus there is no program that can solve the halting problem.
now that we have proved that the halting problem is unsolvable, we can use reduction arguments to prove that other problems are also unsolvable. the strategy is to assume the existence of a computer program that solves the problem in question and use that program to solve another problem that is already known to be unsolvable.
example 17.4 consider the following variation on the halting problem. given a computer program, will it halt when its input is the empty string (i.e., will it halt when it is given no input)? to prove that this problem is unsolvable, we will employ a standard technique for computability proofs: use a computer program to modify another computer program. proof: assume that there is a function ehalt that determines whether a given program halts when given no input. recall that our proof for the halting problem involved functions that took as parameters a string representing a program and another string representing an input. consider another function combine that takes a program p and an input string i as parameters. function combine modiﬁes p to store i as a static variable s and further modiﬁes all calls to input functions within p to instead get their input from s. call the resulting program p 0. it should take no stretch of the imagination to believe that any decent compiler could be modiﬁed to take computer programs and input strings and produce a new computer program that has been modiﬁed in this way. now, take p 0 and feed it to ehalt. if ehalt says that p 0 will halt, then we know that p would halt on input i. in other words, we now have a procedure for solving the original halting problem. the only assumption that we made was the existence of ehalt. thus, the problem of determining if a program will halt on no input must be unsolvable. 2
example 17.5 for arbitrary program p, does there exist any input for which p halts? proof: this problem is also uncomputable. assume that we had a function ahalt that, when given program p as input would determine if there is
17.4 further reading the classic text on the theory of np-completeness is computers and intractability: a guide to the theory of np-completeness by garey and johnston [gj79]. the traveling salesman problem, edited by lawler et al. [llks85], discusses many approaches to ﬁnding an acceptable solution to this particular np-complete problem in a reasonable amount of time.
for more information about the collatz function see “on the ups and downs of hailstone numbers” by b. hayes [hay84], and “the 3x + 1 problem and its generalizations” by j.c. lagarias [lag85].
17.1 consider this algorithm for ﬁnding the maximum element in an array: first sort the array and then select the last (maximum) element. what (if anything) does this reduction tell us about the upper and lower bounds to the problem of ﬁnding the maximum element in a sequence? why can we not reduce sorting to ﬁnding the maximum element? 17.2 use a reduction to prove that squaring an n × n matrix is just as expensive (asymptotically) as multiplying two n × n matrices. 17.3 use a reduction to prove that multiplying two upper triangular n × n matrices is just as expensive (asymptotically) as multiplying two arbitrary n × n matrices. (a) explain why computing the factorial of n by multiplying all values
(b) explain why computing an approximation to the factorial of n by making use of stirling’s formula (see section 2.2) is a polynomial time algorithm.
17.5 consider this algorithm for solving the clique problem. first, generate all subsets of the vertices containing exactly k vertices. there are o(nk) such subsets altogether. then, check whether any subgraphcs induced by these subsets is complete. if this algorithm ran in polynomial time, what would be its signiﬁcance? why is this not a polynomial-time algorithm for the clique problem?
17.12 imagine that you have a problem p that you know is np-complete. for this problem you have two algorithms to solve it. for each algorithm, some problem instances of p run in polynomial time and others run in exponential time (there are lots of heuristic-based algorithms for real np-complete problems with this behavior). you can’t tell beforehand for any given problem instance whether it will run in polynomial or exponential time on either algorithm. however, you do know that for every problem instance, at least one of the two algorithms will solve it in polynomial time. (a) what should you do? (b) what is the running time of your solution? (c) what does it say about the question of p = np if the conditions
17.13 the last paragraph of section 17.2.3 discusses a strategy for developing a solution to a new problem by alternating between ﬁnding a polynomial time solution and proving the problem np-complete. reﬁne the “algorithm for designing algorithms” from section 15.1 to incorporate identifying and dealing with np-complete problems.
17.15 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining if an arbitrary program will print any output is unsolvable.
17.16 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining if an arbitrary program executes a particular statement within that program is unsolvable.
17.17 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining if two arbitrary programs halt on exactly the same inputs is unsolvable.
17.18 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining whether there is some input on which two arbitrary programs will both halt is unsolvable.
17.19 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining whether an arbitrary program halts on all inputs is unsolvable.
17.20 prove, using a reduction argument such as given in section 17.3.2, that the problem of determining whether an arbitrary program computes a speciﬁed function is unsolvable.
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
for more about estimating techniques, see two programming pearls by john louis bentley entitled the back of the envelope and the envelope is back [ben84, ben00, ben86, ben88]. genius: the life and science of richard feynman by james gleick [gle92] gives insight into how important back of the envelope calculation was to the developers of the atomic bomb, and to modern theoretical physics in general.
each of the properties reﬂexive, symmetric, antisymmetric, and transitive. (a) “isbrotherof” on the set of people. (b) “isfatherof” on the set of people. (c) the relation r = {hx, yi| x2 + y2 = 1} for real numbers x and y. (d) the relation r = {hx, yi| x2 = y2} for real numbers x and y. (e) the relation r = {hx, yi| x mod y = 0} for x, y ∈ {1, 2, 3, 4}. (f) the empty relation ∅ (i.e., the relation with no ordered pairs for which (g) the empty relation ∅ (i.e., the relation with no ordered pairs for which
relation or prove that it is not an equivalence relation. (a) for integers a and b, a ≡ b if and only if a + b is even. (b) for integers a and b, a ≡ b if and only if a + b is odd. (c) for nonzero rational numbers a and b, a ≡ b if and only if a × b > 0. (d) for nonzero rational numbers a and b, a ≡ b if and only if a/b is an (e) for rational numbers a and b, a ≡ b if and only if a − b is an integer. (f) for rational numbers a and b, a ≡ b if and only if |a − b| ≤ 2.
why or why not. (a) “isfatherof” on the set of people. (b) “isancestorof” on the set of people. (c) “isolderthan” on the set of people. (d) “issisterof” on the set of people. (e) {ha, bi,ha, ai,hb, ai} on the set {a, b}.
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
this section treats the problem of creating initial runs as large as possible from a disk ﬁle, assuming a ﬁxed amount of ram is available for processing. as mentioned previously, a simple approach is to allocate as much ram as possible to a large array, ﬁll this array from disk, and sort the array using quicksort. thus, if the size of memory available for the array is m records, then the input ﬁle can be broken into initial runs of length m. a better approach is to use an algorithm called replacement selection that, on average, creates runs of 2m records in length. replacement selection is actually a slight variation on the heapsort algorithm. the fact that heapsort is slower than quicksort is irrelevant in this context because i/o time will dominate the total running time of any reasonable external sorting algorithm. building longer initial runs will reduce the total i/o time required.
replacement selection views ram as consisting of an array of size m in addition to an input buffer and an output buffer. (additional i/o buffers might be desirable if the operating system supports double buffering, because replacement selection does sequential processing on both its input and its output.) imagine that the input and output ﬁles are streams of records. replacement selection takes the next record in sequential order from the input stream when needed, and outputs runs one record at a time to the output stream. buffering is used so that disk i/o is performed one block at a time. a block of records is initially read and held in the input buffer. replacement selection removes records from the input buffer one at a time until the buffer is empty. at this point the next block of records is read in. output to a buffer is similar: once the buffer ﬁlls up it is written to disk as a unit. this process is illustrated by figure 8.7.
figure 8.9 the snowplow analogy showing the action during one revolution of the snowplow. a circular track is laid out straight for purposes of illustration, and is shown in cross section. at any time t , the most snow is directly in front of the snowplow. as the plow moves around the track, the same amount of snow is always in front of the plow. as the plow moves forward, less of this is snow that was in the track at time t ; more is snow that has fallen since.
this is the place least recently plowed. at any instant, there is a certain amount of snow s on the track. snow is constantly falling throughout the track at a steady rate, with some snow falling “in front” of the plow and some “behind” the plow. (on a circular track, everything is actually “in front” of the plow, but figure 8.9 illustrates the idea.) during the next revolution of the plow, all snow s on the track is removed, plus half of what falls. because everything is assumed to be in steady state, after one revolution s snow is still on the track, so 2s snow must fall during a revolution, and 2s snow is removed during a revolution (leaving s snow behind). at the beginning of replacement selection, nearly all values coming from the input ﬁle are greater (i.e., “in front of the plow”) than the latest key value output for this run, because the run’s initial key values should be small. as the run progresses, the latest key value output becomes greater and so new key values coming from the input ﬁle are more likely to be too small (i.e., “after the plow”); such records go to the bottom of the array. the total length of the run is expected to be twice the size of the array. of course, this assumes that incoming key values are evenly distributed within the key range (in terms of the snowplow analogy, we assume that snow falls evenly throughout the track). sorted and reverse sorted inputs do not meet this expectation and so change the length of the run.
the second stage of a typical external sorting algorithm merges the runs created by the ﬁrst stage. assume that we have r runs to merge. if a simple two-way merge is used, then r runs (regardless of their sizes) will require log r passes through the ﬁle. while r should be much less than the total number of records (because
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
8.3 implement a disk-based buffer pool class based on the lru buffer pool replacement strategy. disk blocks are numbered consecutively from the beginning of the ﬁle with the ﬁrst block numbered as 0. assume that blocks are 4096 bytes in size, with the ﬁrst 4 bytes used to store the block id corresponding to that buffer. use the ﬁrst bufferpool abstract class given in section 8.3 as the basis for your implementation.
8.4 implement an external sort based on replacement selection and multiway merging as described in this chapter. test your program both on ﬁles with small records and on ﬁles with large records. for what size record do you ﬁnd that key sorting would be worthwhile?
8.5 implement a quicksort for large ﬁles on disk by replacing all array access in the normal quicksort application with access to a virtual array implemented using a buffer pool. that is, whenever a record in the array would be read or written by quicksort, use a call to a buffer pool function instead. compare the running time of this implementation with implementations for external sorting based on mergesort as described in this chapter.
8.6 section 8.5.1 suggests that an easy modiﬁcation to the basic 2-way mergesort is to read in a large chunk of data into main memory, sort it with quicksort, and write it out for initial runs. then, a standard 2-way merge is used in a series of passes to merge the runs together. however, this makes use of only two blocks of working memory at a time. each block read is essentially random access, because the various ﬁles are read in an unknown order, even though each of the input and output ﬁles is processed sequentially on each pass. a possible improvement would be, on the merge passes, to divide working memory into four equal sections. one section is allocated to each of the two input ﬁles and two output ﬁles. all reads during merge passes would be in full sections, rather than single blocks. while the total number of blocks read and written would be the same as a regular 2-way mergsort, it is possible that this would speed processing because a series of blocks that are logically adjacent in the various input and output ﬁles would be read/written each time. implement this variation, and compare its running time against a standard series of 2-way merge passes that read/write only a single block at a time. before beginning implementation, write down your hypothesis on how the running time will be affected by this change. after implementing, did you ﬁnd that this change has any meaningful effect on performance?
this book contains many examples of asymptotic analysis of the time requirements for algorithms and the space requirements for data structures. often it is easy to invent an equation to model the behavior of the algorithm or data structure in question, and also easy to derive a closed-form solution for the equation should it contain a recurrence or summation.
sometimes an analysis proves more difﬁcult. it may take a clever insight to derive the right model, such as the snowplow argument for analyzing the average run length resulting from replacement selection (section 8.5.2). in this case, once the snowplow argument is understood, the resulting equations are simple. sometimes, developing the model is straightforward but analyzing the resulting equations is not. an example is the average-case analysis for quicksort. the equation given in section 7.5 simply enumerates all possible cases for the pivot position, summing corresponding costs for the recursive calls to quicksort. however, deriving a closed-form solution for the resulting recurrence relation is not as easy.
many iterative algorithms require that we compute a summation to determine the cost of a loop. techniques for ﬁnding closed-form solutions to summations are presented in section 14.1. time requirements for many algorithms based on recursion are best modeled by recurrence relations. a discussion of techniques for solving recurrences is provided in section 14.2. these sections extend the introduction to summations and recurrences provided in section 2.4, so the reader should already be familiar with that material.
section 14.3 provides an introduction to the topic of amortized analysis. amortized analysis deals with the cost of a series of operations. perhaps a single operation in the series has high cost, but as a result the cost of the remaining operations is limited in such a way that the entire series can be done efﬁciently. amortized analysis has been used successfully to analyze several of the algorithms presented in
in the most general sense, a data structure is any data representation and its associated operations. even an integer or ﬂoating point number stored on the computer can be viewed as a simple data structure. more typically, a data structure is meant to be an organization or structuring for a collection of data items. a sorted list of integers stored in an array is an example of such a structuring.
given sufﬁcient space to store a collection of data items, it is always possible to search for speciﬁed items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. thus, it is possible to perform all necessary operations on any data structure. however, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days.
a solution is said to be efﬁcient if it solves the problem within the required resource constraints. examples of resource constraints include the total space available to store the data — possibly divided into separate main memory and disk space constraints — and the time allowed to perform each subtask. a solution is sometimes said to be efﬁcient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. the cost of a solution is the amount of resources that the solution consumes. most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints.
it should go without saying that people write programs to solve problems. however, it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. only by ﬁrst analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. the result is typically a slow program. conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design.
1. analyze your problem to determine the basic operations that must be supported. examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and ﬁnding a speciﬁed data item.
this three-step approach to selecting a data structure operationalizes a datacentered view of the design process. the ﬁrst concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the ﬁnal concern is the implementation of that representation.
resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure:
• are all data items inserted into the data structure at the beginning, or are • can data items be deleted? • are all data items processed in some well-deﬁned order, or is search for
typically, interspersing insertions with other operations, allowing deletion, and supporting search for data items all require more complex representations.
each data structure has associated costs and beneﬁts. in practice, it is hardly ever true that one data structure is better than another for use in all situations. if one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. for nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. some of the examples might surprise you.
a data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. each problem has constraints on available space and time. each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task.
example 1.1 a bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. we can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workﬂow process that the
problems: as your intuition would suggest, a problem is a task to be performed. it is best thought of in terms of inputs and matching outputs. a problem deﬁnition should not include any constraints on how the problem is to be solved. the solution method should be developed only after the problem is precisely deﬁned and thoroughly understood. however, a problem deﬁnition should include constraints on the resources that may be consumed by any acceptable solution. for any problem to be solved by a computer, there are always such constraints, whether stated or implied. for example, any computer program may use only the main memory and disk space available, and it must run in a “reasonable” amount of time.
problems can be viewed as functions in the mathematical sense. a function is a matching between inputs (the domain) and outputs (the range). an input to a function might be a single value or a collection of information. the values making up an input are called the parameters of the function. a speciﬁc selection of values for the parameters is called an instance of the problem. for example, the input parameter to a sorting function might be an array of integers. a particular array of integers, with a given size and speciﬁc values for each position in the array, would be an instance of the sorting problem. different instances might generate the same output. however, any problem instance must always result in the same output every time the function is computed using that particular input.
this concept of all problems behaving like mathematical functions might not match your intuition for the behavior of computer programs. you might know of programs to which you can give the same input value on two separate occasions, and two different outputs will result. for example, if you type “date” to a typical unix command line prompt, you will get the current date. naturally the date will be different on different days, even though the same command is given. however, there is obviously more to the input for the date program than the command that you type to run the program. the date program computes a function. in other words, on any particular day there can only be a single answer returned by a properly running date program on a completely speciﬁed input. for all computer programs, the output is completely determined by the program’s full set of inputs. even a “random number generator” is completely determined by its inputs (although some random number generating systems appear to get around this by accepting a random input from a physical process beyond the user’s control). the relationship between programs and functions is explored further in section 17.3.
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
run them on a suitable range of inputs, measuring how much of the resources in question each program uses. this approach is often unsatisfactory for four reasons. first, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and that the relative qualities of the underlying algorithms are not truly represented by their implementations. this is especially likely to occur when the programmer has a bias regarding the algorithms. third, the choice of empirical test cases might unfairly favor one algorithm. fourth, you could ﬁnd that even the better of the two algorithms does not fall within your resource budget. in that case you must begin the entire process again with yet another program implementing a new algorithm. but, how would you know if any algorithm can meet the resource budget? perhaps the problem is simply too difﬁcult for any implementation to be within budget.
these problems can often be avoided by using asymptotic analysis. asymptotic analysis measures the efﬁciency of an algorithm, or its implementation as a program, as the input size becomes large. it is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. however, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation.
the critical resource for a program is most often its running time. however, you cannot pay attention to running time alone. you must also be concerned with other factors such as the space required to run the program (both main memory and disk space). typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure.
many factors affect the running time of a program. some relate to the environment in which the program is compiled and run. such factors include the speed of the computer’s cpu, bus, and peripheral hardware. competition with other users for the computer’s resources can make a program slow to a crawl. the programming language and the quality of code generated by a particular compiler can have a signiﬁcant effect. the “coding efﬁciency” of the programmer who converts the algorithm to a program can have a tremendous impact as well.
if you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. yet, none of these factors address the differences between two algorithms or data structures. to be fair, programs derived from two algorithms for solving the same problem should both be
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
figure 8.6 a simple external mergesort algorithm. input records are divided equally between two input ﬁles. the ﬁrst runs from each input ﬁle are merged and placed into the ﬁrst output ﬁle. the second runs from each input ﬁle are merged and placed in the second output ﬁle. merging alternates between the two output ﬁles until the input ﬁles are empty. the roles of input and output ﬁles are then reversed, allowing the runlength to be doubled with each pass.
5. repeat until ﬁnished, alternating output between the two output run buffers. whenever the end of an input block is reached, read the next block from the appropriate input ﬁle. when an output buffer is full, write it to the appropriate output ﬁle.
6. repeat steps 2 through 5, using the original output ﬁles as input ﬁles. on the second pass, the ﬁrst two records of each input run ﬁle are already in sorted order. thus, these two runs may be merged and output as a single run of four elements.
example 8.6 using the input of figure 8.6, we ﬁrst create runs of length one split between two input ﬁles. we then process these two input ﬁles sequentially, making runs of length two. the ﬁrst run has the values 20 and 36, which are output to the ﬁrst output ﬁle. the next run has 13 and 17, which is ouput to the second ﬁle. the run 14, 28 is sent to the ﬁrst ﬁle, then run 15, 23 is sent to the second ﬁle, and so on. once this pass has completed, the roles of the input ﬁles and output ﬁles are reversed. the next pass will merge runs of length two into runs of length four. runs 20, 36 and 13, 17 are merged to send 13, 17, 20, 36 to the ﬁrst output ﬁle. then runs 14, 28 and 15, 23 are merged to send run 14, 15, 23, 28 to the second output ﬁle. in the ﬁnal pass, these runs are merged to form the ﬁnal run 13, 14, 15, 17, 20, 23, 28, 36.
this algorithm can easily take advantage of the double buffering techniques described in section 8.3. note that the various passes read the input run ﬁles se-
seen so far. it is reasonable to assume that it takes a ﬁxed amount of time to do one such comparison, regardless of the value of the two integers or their positions in the array.
because the most important factor affecting running time is normally size of the input, for a given input size n we often express the time t to run the algorithm as a function of n, written as t(n). we will always assume t(n) is a non-negative value.
let us call c the amount of time required to compare two integers in function largest. we do not care right now what the precise value of c might be. nor are we concerned with the time required to increment variable i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. we just want a reasonable approximation for the time taken to execute the algorithm. the total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. we say that function largest (and the largest-value sequential search algorithm in general) has a running time expressed by the equation
example 3.2 the running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. we can assume this assignment takes a constant amount of time regardless of the value. let us call c1 the amount of time necessary to copy an integer. no matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. thus, the equation for this algorithm is simply
17.2.2 np-completeness proofs to start the process of being able to prove problems are np-complete, we need to prove just one problem h is np-complete. after that, to show that any problem x is np-hard, we just need to reduce h to x. when doing np-completeness proofs, it is very important not to get this reduction backwards! if we reduce candidate problem x to known hard problem h, this means that we use h as a step to solving x. all that means is that we have found a (known) hard way to solve x. however, when we reduce known hard problem h to candidate problem x, that means we are using x as a step to solve h. and if we know that h is hard, that means x must also be hard. so a crucial ﬁrst step to getting this whole theory off the ground is ﬁnding one problem that is np-hard. the ﬁrst proof that a problem is np-hard (and because it is in np, therefore np-complete) was done by stephen cook. for this feat, cook won the ﬁrst turing award, which is the closest computer science equivalent to the nobel prize. the “grand-daddy” np-complete problem that cook used is call satisfiability (or sat for short). a boolean expression includes boolean variables combined using the operators and (·), or (+), and not (to negate boolean variable x we write x). a literal is a boolean variable or its negation. a clause is one or more literals or’ed together. let e be a boolean expression over variables x1, x2, ..., xn. then we deﬁne conjunctive normal form (cnf) to be a boolean expression written as a series of clauses that are and’ed together. for example,
cook proved that sat is np-hard. explaining this proof is beyond the scope of this book. but we can brieﬂy summarize it as follows. any decision problem f can be recast as some language acceptance problem l:
proof: clique is in np, because we can just guess a collection of k vertices and test in polynomial time if it is a clique. now we show that clique is np-hard by using a reduction from sat. an instance of sat is a boolean expression
that is, there is a vertex in g corresponding to every literal in boolean expression b. we will draw an edge between each pair of vertices v[i1, j1] and v[i2, j2] unless (1) they are two literals within the same clause (i1 = i2) or (2) they are opposite values for the same variable (i.e., one is negated and the other is not). set k = m. figure 17.4 shows an example of this transformation.
b is satisﬁable if and only if g has a clique of size k or greater. b being satisﬁable implies that there is a truth assignment such that at least one literal y[i, ji] is true for each i. if so, then these m literals must correspond to m vertices in a clique of size k = m. conversely, if g has a clique of size k or greater, then the clique must have size exactly k (because no two vertices corresponding to literals in the same clause can be in the clique) and there is one vertex v[i, ji] in the clique for each i. there is a truth assignment making each y[i, ji] true. that truth assignment satisﬁes b. we conclude that clique is np-hard, therefore np-complete. 2
17.2.3 coping with np-complete problems finding that your problem is np-complete might not mean that you can just forget about it. traveling salesmen need to ﬁnd reasonable sales routes regardless of the complexity of the problem. what do you do when faced with an np-complete problem that you must solve?
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
1.4 deﬁne an adt for a list of integers. first, decide what functionality your adt should provide. example 1.4 should give you some ideas. then, specify your adt in java in the form of an abstract class declaration, showing the functions, their parameters, and their return types.
1.5 brieﬂy describe how integer variables are typically represented on a computer. (look up one’s complement and two’s complement arithmetic in an introductory computer science textbook if you are not familiar with these.) why does this representation for integers qualify as a data structure as deﬁned in section 1.2?
1.6 deﬁne an adt for a two-dimensional array of integers. specify precisely the basic operations that can be performed on such arrays. next, imagine an application that stores an array with 1000 rows and 1000 columns, where less than 10,000 of the array values are non-zero. describe two different implementations for such arrays that would be more space efﬁcient than a standard two-dimensional array implementation requiring one million positions.
1.7 you have been assigned to implement a sorting program. the goal is to make this program general purpose, in that you don’t want to deﬁne in advance what record or key types are used. describe ways to generalize a simple sorting algorithm (such as insertion sort, or any other sort you are familiar with) to support this generalization.
1.8 you have been assigned to implement a simple seqential search on an array. the problem is that you want the search to be as general as possible. this means that you need to support arbitrary record and key types. describe ways to generalize the search function to support this goal. consider the possibility that the function will be used multiple times in the same program, on differing record types. consider the possibility that the function will need to be used on different keys (possibly with the same or different types) of the same record. for example, a student data record might be searched by zip code, by name, by salary, or by gpa.
1.9 does every problem have an algorithm? 1.10 does every algorithm have a java program? 1.11 consider the design for a spelling checker program meant to run on a home computer. the spelling checker should be able to handle quickly a document of less than twenty pages. assume that the spelling checker comes with a dictionary of about 20,000 words. what primitive operations must be implemented on the dictionary, and what is a reasonable time constraint for each operation?
1.12 imagine that you have been hired to design a database service containing information about cities and towns in the united states, as described in example 1.2. suggest two possible implementations for the database.
1.13 imagine that you are given an array of records that is sorted with respect to some key ﬁeld contained in each record. give two different algorithms for searching the array to ﬁnd the record with a speciﬁed key value. which one do you consider “better” and why?
1.15 a common problem for compilers and text editors is to determine if the parentheses (or other brackets) in a string are balanced and properly nested. for example, the string “((())())()” contains properly nested pairs of parentheses, but the string “)()(” does not; and the string “())” does not contain properly matching parentheses. (a) give an algorithm that returns true if a string contains properly nested and balanced parentheses, and false if otherwise. hint: at no time while scanning a legal string from left to right will you have encountered more right parentheses than left parentheses.
(b) give an algorithm that returns the position in the string of the ﬁrst offending parenthesis if the string is not properly nested and balanced. that is, if an excess right parenthesis is found, return its position; if there are too many left parentheses, return the position of the ﬁrst excess left parenthesis. return −1 if the string is properly balanced and nested.
1.16 a graph consists of a set of objects (called vertices) and a set of edges, where each edge connects two vertices. any given pair of vertices can be connected by only one edge. describe at least two different ways to represent the connections deﬁned by the vertices and edges of a graph.
1.17 imagine that you are a shipping clerk for a large company. you have just been handed about 1000 invoices, each of which is a single sheet of paper with a large number in the upper right corner. the invoices must be sorted by this number, in order from lowest to highest. write down as many different approaches to sorting the invoices as you can think of.
example 3.18 the following is a true story. a few years ago, one of my graduate students had a big problem. his thesis work involved several intricate operations on a large database. he was now working on the ﬁnal step. “dr. shaffer,” he said, “i am running this program and it seems to be taking a long time.” after examining the algorithm we realized that its running time was Θ(n2), and that it would likely take one to two weeks to complete. even if we could keep the computer running uninterrupted for that long, he was hoping to complete his thesis and graduate before then. fortunately, we realized that there was a fairly easy way to convert the algorithm so that its running time was Θ(n log n). by the next day he had modiﬁed the program. it ran in only a few hours, and he ﬁnished his thesis on time.
while not nearly so important as changing an algorithm to reduce its growth rate, “code tuning” can also lead to dramatic improvements in running time. code tuning is the art of hand-optimizing a program to run faster or require less storage. for many programs, code tuning can reduce running time by a factor of ten, or cut the storage requirements by a factor of two or more. i once tuned a critical function in a program — without changing its basic algorithm — to achieve a factor of 200 speedup. to get this speedup, however, i did make major changes in the representation of the information, converting from a symbolic coding scheme to a numeric coding scheme on which i was able to do direct computation.
here are some suggestions for ways to speed up your programs by code tuning. the most important thing to realize is that most statements in a program do not have much effect on the running time of that program. there are normally just a few key subroutines, possibly even key lines of code within the key subroutines, that account for most of the running time. there is little point to cutting in half the running time of a subroutine that accounts for only 1% of the total running time. focus your attention on those parts of the program that have the most impact.
when tuning code, it is important to gather good timing statistics. many compilers and operating systems include proﬁlers and other special tools to help gather information on both time and space use. these are invaluable when trying to make a program more efﬁcient, because they can tell you where to invest your effort.
a lot of code tuning is based on the principle of avoiding work rather than speeding up work. a common situation occurs when we can test for a condition
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
unfortunately, there is more than one way to assign values to q and r, depending on how integer division is interpreted. the most common mathematical deﬁnition computes the mod function as n mod m = n − mbn/mc. in this case, −3 mod 5 = 2. however, java and c++ compilers typically use the underlying processor’s machine instruction for computing integer arithmetic. on many computers this is done by truncating the resulting fraction, meaning n mod m = n − m(trunc(n/m)). under this deﬁnition, −3 mod 5 = −3.
unfortunately, for many applications this is not what the user wants or expects. for example, many hash systems will perform some computation on a record’s key value and then take the result modulo the hash table size. the expectation here would be that the result is a legal index into the hash table, not a negative number. implementors of hash functions must either insure that the result of the computation is always postive, or else add the hash table size to the result of the modulo function when that result is negative.
a logarithm of base b for value y is the power to which b is raised to get y. normally, this is written as logb y = x. thus, if logb y = x then bx = y, and blogby = y.
example 2.6 many programs require an encoding for a collection of objects. what is the minimum number of bits needed to represent n distinct code values? the answer is dlog2 ne bits. for example, if you have 1000 codes to store, you will require at least dlog2 1000e = 10 bits to have 1000 different codes (10 bits provide 1024 distinct code values).
example 2.7 consider the binary search algorithm for ﬁnding a given value within an array sorted by value from lowest to highest. binary search ﬁrst looks at the middle element and determines if the value being searched for is in the upper half or the lower half of the array. the algorithm then continues splitting the appropriate subarray in half until the desired value is found. (binary search is described in more detail in section 3.5.) how many times can an array of size n be split in half until only one element remains in the ﬁnal subarray? the answer is dlog2 ne times.
then multiplied by the input value, which takes constant time. thus, the cost of the factorial function, if we wish to measure cost in terms of the number of multiplication operations, is one more than the number of multiplications made by the recursive call on the smaller input. because the base case does no multiplications, its cost is zero. thus, the running time for this function can be expressed as
the ﬁnal example of algorithm analysis for this section will compare two algorithms for performing search in an array. earlier, we determined that the running time for sequential search on an array where the search value k is equally likely to appear in any location is Θ(n) in both the average and worst cases. we would like to compare this running time to that required to perform a binary search on an array whose values are stored in order from lowest to highest.
binary search begins by examining the value in the middle position of the array; call this position mid and the corresponding value kmid. if kmid = k, then processing can stop immediately. this is unlikely to be the case, however. fortunately, knowing the middle value provides useful information that can help guide the search process. in particular, if kmid > k, then you know that the value k cannot appear in the array at any position greater than mid. thus, you can eliminate future search in the upper half of the array. conversely, if kmid < k, then you know that you can ignore all positions in the array less than mid. either way, half of the positions are eliminated from further consideration. binary search next looks at the middle position in that part of the array where value k may exist. the value at this position again allows us to eliminate half of the remaining positions from consideration. this process repeats until either the desired value is found, or there are no positions remaining in the array that might contain the value k. figure 3.4 illustrates the binary search method. here is a java implementation for binary search:
comparing sequential search to binary search, we see that as n grows, the Θ(n) running time for sequential search in the average and worst cases quickly becomes much greater than the Θ(log n) running time for binary search. taken in isolation, binary search appears to be much more efﬁcient than sequential search. this is despite the fact that the constant factor for binary search is greater than that for sequential search, because the calculation for the next search position in binary search is more expensive than just incrementing the current position, as sequential search does.
note however that the running time for sequential search will be roughly the same regardless of whether or not the array values are stored in order. in contrast, binary search requires that the array values be ordered from lowest to highest. depending on the context in which binary search is to be used, this requirement for a sorted array could be detrimental to the running time of a complete program, because maintaining the values in sorted order requires to greater cost when inserting new elements into the array. this is an example of a tradeoff between the advantage of binary search during search and the disadvantage related to maintaining a sorted array. only in the context of the complete problem to be solved can we know whether the advantage outweighs the disadvantage.
you most often use the techniques of “algorithm” analysis to analyze an algorithm, or the instantiation of an algorithm as a program. you can also use these same techniques to analyze the cost of a problem. it should make sense to you to say that the upper bound for a problem cannot be worse than the upper bound for the best algorithm that we know for that problem. but what does it mean to give a lower bound for a problem?
consider a graph of cost over all inputs of a given size n for some algorithm for a given problem. deﬁne a to be the collection of all algorithms that solve the problem (theoretically, there an inﬁnite number of such algorithms). now, consider the collection of all the graphs for all of the (inﬁnitely many) algorithms in a. the worst case lower bound is the least of all the highest points on all the graphs.
it is much easier to show that an algorithm (or program) is in Ω(f(n)) than it is to show that a problem is in Ω(f(n)). for a problem to be in Ω(f(n)) means that every algorithm that solves the problem is in Ω(f(n)), even algorithms that we have not thought of!
so far all of our examples of algorithm analysis give “obvious” results, with big-oh always matching Ω. to understand how big-oh, Ω, and Θ notations are
3.15 does every algorithm have a Θ running-time equation? in other words, are the upper and lower bounds for the running time (on any speciﬁed class of inputs) always the same?
3.16 does every problem for which there exists some algorithm have a Θ runningtime equation? in other words, for every problem, and for any speciﬁed class of inputs, is there some algorithm whose upper bound is equal to the problem’s lower bound?
3.17 given an array storing integers ordered by value, modify the binary search routine to return the position of the ﬁrst integer with value k in the situation where k can appear multiple times in the array. be sure that your algorithm is Θ(log n), that is, do not resort to sequential search once an occurrence of k is found.
3.18 given an array storing integers ordered by value, modify the binary search routine to return the position of the integer with the greatest value less than k when k itself does not appear in the array. return error if the least value in the array is greater than k.
3.19 modify the binary search routine to support search in an array of inﬁnite size. in particular, you are given as input a sorted array and a key value k to search for. call n the position of the smallest value in the array that is equal to or larger than x. provide an algorithm that can determine n in o(log n) comparisons in the worst case. explain why your algorithm meets the required time bound.
3.20 it is possible to change the way that we pick the dividing point in a binary search, and still get a working search routine. however, where we pick the dividing point could affect the performance of the algorithm. (a) if we change the dividing point computation in function binary from i = (l + r)/2 to i = (l + ((r − l)/3)), what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
use a variety of random search values in the range 0 to n − 1 on each size n. graph the resulting times. when is sequential search faster than binary search for a sorted array?
3.3 implement a program that runs and gives timings for the two fibonacci sequence functions provided in exercise 2.11. graph the resulting running times for as many values of n as your computer can handle.
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
element in l, that is, we check elements l[j], l[2j], and so on. so long as k is greater than the values we are checking, we continue on. but when we reach a value in l greater than k, we do a linear search on the piece of length j − 1 that we know brackets k if it is in the list. if mj ≤ n < (m+1)j, then the total cost of this algorithm is at most m+ j−1 3-way comparisons. therefore, the cost to run the algorithm on n items with a jump of size j is
take the derivative and solve for f0(j) = 0 to ﬁnd the minimum, which is √ this example teaches us some lessons about algorithm design. we want to balance the work done while selecting a sublist with the work done while searching a sublist. in general, it is a good strategy to make subproblems of equal effort. this is an example of a divide and conquer algorithm. what if we extend this idea to three levels? we would ﬁrst make jumps of some size j to ﬁnd a sublist of size j − 1 whose end vlaues bracket value k. ww would then work through this sublist by making jumps of some smaller size, say j1. finally, once we ﬁnd a bracketed sublist of size j1 − 1, we would do sequential search to complete the process.
this probably sounds convoluted to do two levels of jumping to be followed by a sequential search. while it might make sense to do a two-level algorithm (that is, jump search jumps to ﬁnd a sublist and then does sequential search on the sublist), it almost never seems to make sense to do a three-level algorithm. instead, when we go beyond two levels, we nearly always generalize by using recursion. this leads us to the most commonly used search algorithm for sorted arrays, the binary search described in section 3.5.
if we know nothing about the distribution of key values, then binary search is the best algorithm available for searching a sorted array (see exercise 9.22). however, sometimes we do know something about the expected key distribution. consider the typical behavior of a person looking up a word in a large dictionary. most people certainly do not use sequential search! typically, people use a modiﬁed form of binary search, at least until they get close to the word that they are
figure 10.1 linear indexing for variable-length records. each record in the index ﬁle is of ﬁxed length and contains a pointer to the beginning of the corresponding record in the database ﬁle.
begins with a discussion of the variant normally referred to simply as a “b-tree.” section 10.5.1 presents the most widely implemented variant, the b+-tree.
a linear index is an index ﬁle organized as a sequence of key/pointer pairs where the keys are in sorted order and the pointers either (1) point to the position of the complete record on disk, (2) point to the position of the primary key in the primary index, or (3) are actually the value of the primary key. depending on its size, a linear index might be stored in main memory or on disk. a linear index provides a number of advantages. it provides convenient access to variable-length database records, because each entry in the index ﬁle contains a ﬁxed-length key ﬁeld and a ﬁxed-length pointer to the beginning of a (variable-length) record as shown in figure 10.1. a linear index also allows for efﬁcient search and random access to database records, becase it is amenable to binary search.
if the database contains enough records, the linear index might be too large to store in main memory. this makes binary search of the index more expensive because many disk accesses would typically be required by the search process. one solution to this problem is to store a second-level linear index in main memory that indicates which disk block in the index ﬁle stores a desired key. for example, the linear index on disk might reside in a series of 1024-byte blocks. if each key/pointer pair in the linear index requires 8 bytes, then 128 keys are stored per block. the second-level index, stored in main memory, consists of a simple table storing the value of the key in the ﬁrst position of each block in the linear index ﬁle. this arrangement is shown in figure 10.2. if the linear index requires 1024 disk blocks (1mb), the second-level index contains only 1024 entries, one per disk block. to ﬁnd which disk block contains a desired search key value, ﬁrst search through the
figure 10.2 a simple two-level linear index. the linear index is stored on disk. the smaller, second-level index is stored in main memory. each element in the second-level index stores the ﬁrst key value in the corresponding disk block of the index ﬁle. in this example, the ﬁrst disk block of the linear index stores keys in the range 1 to 2001, and the second disk block stores keys in the range 2003 to 5688. thus, the ﬁrst entry of the second-level index is key value 1 (the ﬁrst key in the ﬁrst block of the linear index), while the second entry of the second-level index is key value 2003.
figure 10.3 a two-dimensional linear index. each row lists the primary keys associated with a particular secondary key value. in this example, the secondary key is a name. the primary key is a unique four-character code.
1024-entry table to ﬁnd the greatest value less than or equal to the search key. this directs the search to the proper block in the index ﬁle, which is then read into memory. at this point, a binary search within this block will produce a pointer to the actual record in the database. because the second-level index is stored in main memory, accessing a record by this method requires two disk reads: one from the index ﬁle and one from the database ﬁle for the actual record.
every time a record is inserted to or deleted from the database, all associated secondary indices must be updated. updates to a linear index are expensive, because the entire contents of the array might be shifted by one position. another problem is that multiple records with the same secondary key each duplicate that key value within the index. when the secondary key ﬁeld has many duplicates, such as when it has a limited range (e.g., a ﬁeld to indicate job category from among a small number of possible job categories), this duplication might waste considerable space.
one improvement on the simple sorted array is a two-dimensional array where each row corresponds to a secondary key value. a row contains the primary keys
4. b-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. this improves space efﬁciency while reducing the typical number of disk fetches necessary during a search or update operation.
a b-tree of order m is deﬁned to have the following shape properties: • the root is either a leaf or has at least two children. • each internal node, except for the root, has between dm/2e and m children. • all leaves are at the same level in the tree, so the tree is always height bal-
the b-tree is a generalization of the 2-3 tree. put another way, a 2-3 tree is a b-tree of order three. normally, the size of a node in the b-tree is chosen to ﬁll a disk block. a b-tree node implementation typically allows 100 or more children. thus, a b-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk ﬁle). in a typical application, b-tree block i/o will be managed using a buffer pool and a block-replacement scheme such as lru (see section 8.3).
1. perform a binary search on the records in the current node. if a record with the search key is found, then return that record. if the current node is a leaf node and the key is not found, then report an unsuccessful search.
for example, consider a search for the record with key value 47 in the tree of figure 10.16. the root node is examined and the second (right) branch taken. after
where a, b, c, and k are constants. in general, this recurrence describes a problem of size n divided into a subproblems of size n/b, while cnk is the amount of work necessary to combine the partial solutions. mergesort is an example of a divide and conquer algorithm, and its recurrence ﬁts this form. so does binary search. we use the method of expanding recurrences to derive the general solution for any divide and conquer recurrence, assuming that n = bm.
2. r = 1. because r = bk/a, we know that a = bk. from the deﬁnition of logarithms it follows immediately that k = logb a. we also note from equation 14.1 that m = logb n. thus,
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
the typical customer opens and closes accounts far less often than he or she accesses the account. customers are willing to wait many minutes while accounts are created or deleted but are typically not willing to wait more than a brief time for individual account transactions such as a deposit or withdrawal. these observations can be considered as informal speciﬁcations for the time constraints on the problem.
it is common practice for banks to provide two tiers of service. human tellers or automated teller machines (atms) support customer access to account balances and updates such as deposits and withdrawals. special service representatives are typically provided (during restricted hours) to handle opening and closing accounts. teller and atm transactions are expected to take little time. opening or closing an account can take much longer (perhaps up to an hour from the customer’s perspective).
from a database perspective, we see that atm transactions do not modify the database signiﬁcantly. for simplicity, assume that if money is added or removed, this transaction simply changes the value stored in an account record. adding a new account to the database is allowed to take several minutes. deleting an account need have no time constraint, because from the customer’s point of view all that matters is that all the money be returned (equivalent to a withdrawal). from the bank’s point of view, the account record might be removed from the database system after business hours, or at the end of the monthly account cycle.
when considering the choice of data structure to use in the database system that manages customer accounts, we see that a data structure that has little concern for the cost of deletion, but is highly efﬁcient for search and moderately efﬁcient for insertion, should meet the resource constraints imposed by this problem. records are accessible by unique account number (sometimes called an exact-match query). one data structure that meets these requirements is the hash table described in chapter 9.4. hash tables allow for extremely fast exact-match search. a record can be modiﬁed quickly when the modiﬁcation does not affect its space requirements. hash tables also support efﬁcient insertion of new records. while deletions can also be supported efﬁciently, too many deletions lead to some degradation in performance for the remaining operations. however, the hash table can be reorganized periodically to restore the system to peak efﬁciency. such reorganization can occur ofﬂine so as not to affect atm transactions.
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
element in l, that is, we check elements l[j], l[2j], and so on. so long as k is greater than the values we are checking, we continue on. but when we reach a value in l greater than k, we do a linear search on the piece of length j − 1 that we know brackets k if it is in the list. if mj ≤ n < (m+1)j, then the total cost of this algorithm is at most m+ j−1 3-way comparisons. therefore, the cost to run the algorithm on n items with a jump of size j is
take the derivative and solve for f0(j) = 0 to ﬁnd the minimum, which is √ this example teaches us some lessons about algorithm design. we want to balance the work done while selecting a sublist with the work done while searching a sublist. in general, it is a good strategy to make subproblems of equal effort. this is an example of a divide and conquer algorithm. what if we extend this idea to three levels? we would ﬁrst make jumps of some size j to ﬁnd a sublist of size j − 1 whose end vlaues bracket value k. ww would then work through this sublist by making jumps of some smaller size, say j1. finally, once we ﬁnd a bracketed sublist of size j1 − 1, we would do sequential search to complete the process.
this probably sounds convoluted to do two levels of jumping to be followed by a sequential search. while it might make sense to do a two-level algorithm (that is, jump search jumps to ﬁnd a sublist and then does sequential search on the sublist), it almost never seems to make sense to do a three-level algorithm. instead, when we go beyond two levels, we nearly always generalize by using recursion. this leads us to the most commonly used search algorithm for sorted arrays, the binary search described in section 3.5.
if we know nothing about the distribution of key values, then binary search is the best algorithm available for searching a sorted array (see exercise 9.22). however, sometimes we do know something about the expected key distribution. consider the typical behavior of a person looking up a word in a large dictionary. most people certainly do not use sequential search! typically, people use a modiﬁed form of binary search, at least until they get close to the word that they are
element in l, that is, we check elements l[j], l[2j], and so on. so long as k is greater than the values we are checking, we continue on. but when we reach a value in l greater than k, we do a linear search on the piece of length j − 1 that we know brackets k if it is in the list. if mj ≤ n < (m+1)j, then the total cost of this algorithm is at most m+ j−1 3-way comparisons. therefore, the cost to run the algorithm on n items with a jump of size j is
take the derivative and solve for f0(j) = 0 to ﬁnd the minimum, which is √ this example teaches us some lessons about algorithm design. we want to balance the work done while selecting a sublist with the work done while searching a sublist. in general, it is a good strategy to make subproblems of equal effort. this is an example of a divide and conquer algorithm. what if we extend this idea to three levels? we would ﬁrst make jumps of some size j to ﬁnd a sublist of size j − 1 whose end vlaues bracket value k. ww would then work through this sublist by making jumps of some smaller size, say j1. finally, once we ﬁnd a bracketed sublist of size j1 − 1, we would do sequential search to complete the process.
this probably sounds convoluted to do two levels of jumping to be followed by a sequential search. while it might make sense to do a two-level algorithm (that is, jump search jumps to ﬁnd a sublist and then does sequential search on the sublist), it almost never seems to make sense to do a three-level algorithm. instead, when we go beyond two levels, we nearly always generalize by using recursion. this leads us to the most commonly used search algorithm for sorted arrays, the binary search described in section 3.5.
if we know nothing about the distribution of key values, then binary search is the best algorithm available for searching a sorted array (see exercise 9.22). however, sometimes we do know something about the expected key distribution. consider the typical behavior of a person looking up a word in a large dictionary. most people certainly do not use sequential search! typically, people use a modiﬁed form of binary search, at least until they get close to the word that they are
it is not always practical to reduce an algorithm’s growth rate. there is a practicality window for every problem, in that we have a practical limit to how big an input we wish to solve for. if our problem size never grows too big, it might not matter if we can reduce the cost by an extra log factor, because the constant factors in the two algorithms might overwhelm the savings.
for our two algorithms, let us look further and check the actual number of comparisons used. for binary search, we need about log n − 1 total comparisons. quadratic binary search requires about 2.4 lg lg n comparisons. if we incorporate this observation into our table, we get a different picture about the relative differences.
but we still are not done. this is only a count of comparisons! binary search is inherently much simpler than qbs, because binary search only needs to calculate the midpoint position of the array before each comparison, while quadratic binary search must calculate an interpolation point which is more expensive. so the constant factors for qbs are even higher.
not only are the constant factors worse on average, but qbs is far more dependent than binary search on good data distribution to perform well. for example, imagine that you are searching a telephone directory for the name “young.” normally you would look near the back of the book. if you found a name beginning with ‘z,’ you might look just a little ways toward the front. if the next name you ﬁnd also begins with ’z,‘ you would look a little further toward the front. if this particular telephone directory were unusual in that half of the entries begin with ‘z,’ then you would need to move toward the front many times, each time eliminating relatively few records from the search. in the extreme, the performance of interpolation search might not be much better than sequential search if the distribution of key values is badly calculated.
while it turns out that qbs is not a practical algorithm, this is not a typical situation. fortunately, algorithm growth rates are usually well behaved, so that asymptotic algorithm analysis nearly always gives us a practical indication for which of two algorithms is better.
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
let pi be the probability that k is in position i of l. when k is not in l, sequential search will require n comparisons. let p0 be the probability that k is not in l. then the average cost t(n) will be
for large collections of records that are searched repeatedly, sequential search is unacceptably slow. one way to reduce search time is to preprocess the records by sorting them. given a sorted array, an obvious improvement over simple linear search is to test if the current element in l is greater than k. if it is, then we know that k cannot appear later in the array, and we can quit the search early. but this still does not improve the worst-case cost of the algorithm.
we can also observe that if we look ﬁrst at position 1 in sorted array l and ﬁnd that k is bigger, then we rule out positions 0 as well as position 1. because more is often better, what if we look at position 2 in l and ﬁnd that k is bigger yet? this rules out positions 0, 1, and 2 with one comparison. what if we carry this to the extreme and look ﬁrst at the last position in l and ﬁnd that k is bigger? then we know in one comparison that k is not in l. this is very useful to know, but what is wrong with this approach? while we learn a lot sometimes (in one comparison we might learn that k is not in the list), usually we learn only a little bit (that the last element is not k).
the question then becomes: what is the right amount to jump? this leads us to an algorithm known as jump search. for some value j, we check every j’th
element in l, that is, we check elements l[j], l[2j], and so on. so long as k is greater than the values we are checking, we continue on. but when we reach a value in l greater than k, we do a linear search on the piece of length j − 1 that we know brackets k if it is in the list. if mj ≤ n < (m+1)j, then the total cost of this algorithm is at most m+ j−1 3-way comparisons. therefore, the cost to run the algorithm on n items with a jump of size j is
take the derivative and solve for f0(j) = 0 to ﬁnd the minimum, which is √ this example teaches us some lessons about algorithm design. we want to balance the work done while selecting a sublist with the work done while searching a sublist. in general, it is a good strategy to make subproblems of equal effort. this is an example of a divide and conquer algorithm. what if we extend this idea to three levels? we would ﬁrst make jumps of some size j to ﬁnd a sublist of size j − 1 whose end vlaues bracket value k. ww would then work through this sublist by making jumps of some smaller size, say j1. finally, once we ﬁnd a bracketed sublist of size j1 − 1, we would do sequential search to complete the process.
this probably sounds convoluted to do two levels of jumping to be followed by a sequential search. while it might make sense to do a two-level algorithm (that is, jump search jumps to ﬁnd a sublist and then does sequential search on the sublist), it almost never seems to make sense to do a three-level algorithm. instead, when we go beyond two levels, we nearly always generalize by using recursion. this leads us to the most commonly used search algorithm for sorted arrays, the binary search described in section 3.5.
if we know nothing about the distribution of key values, then binary search is the best algorithm available for searching a sorted array (see exercise 9.22). however, sometimes we do know something about the expected key distribution. consider the typical behavior of a person looking up a word in a large dictionary. most people certainly do not use sequential search! typically, people use a modiﬁed form of binary search, at least until they get close to the word that they are
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
fortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. if these limitations prove unacceptable, then some other representation for the adt “integer” must be devised, and a new implementation must be used for the associated operations.
• insert a new integer at a particular position in the list. • return true if the list is empty. • reinitialize the list. • return the number of integers currently in the list. • delete the integer at a particular position in the list. from this description, the input and output of each operation should be
one application that makes use of some adt might use particular member functions of that adt more than a second application, or the two applications might have different time requirements for the various operations. these differences in the requirements of applications are the reason why a given adt might be supported by more than one implementation.
example 1.5 two popular implementations for large disk-based database applications are hashing (section 9.4) and the b+-tree (section 10.5). both support efﬁcient insertion and deletion of records, and both support exactmatch queries. however, hashing is more efﬁcient than the b+-tree for exact-match queries. on the other hand, the b+-tree can perform range queries efﬁciently, while hashing is hopelessly inefﬁcient for range queries. thus, if the database application limits searches to exact-match queries, hashing is preferred. on the other hand, if the application requires support for range queries, the b+-tree is preferred. despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records.
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
many large-scale computing applications are centered around datasets that are too large to ﬁt into main memory. the classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value k.” unfortunately, many applications require more general search capabilities. one example is a range query search for all records whose key lies within some range. other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. hash tables are not organized to support any of these queries efﬁciently.
this chapter introduces ﬁle structures used to organize a large collection of records stored on disk. such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches.
before discussing such ﬁle structures, we must become familiar with some basic ﬁle-processing terminology. an entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. the natural solution is to sort the records by order of the search key. however, a typical database, such as a collection of employee or customer records maintained by a business, might contain multiple search keys. to answer a question about a particular customer might require a search on the name of the customer. businesses often wish to sort and output the records by zip code order for a bulk mailing. government paperwork might require the ability to search by social security number. thus, there might not be a single “correct” order in which to store the records.
indexing is the process of associating a key with the location of a corresponding data record. section 8.5 discussed the concept of a key sort, in which an index
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
seen so far. it is reasonable to assume that it takes a ﬁxed amount of time to do one such comparison, regardless of the value of the two integers or their positions in the array.
because the most important factor affecting running time is normally size of the input, for a given input size n we often express the time t to run the algorithm as a function of n, written as t(n). we will always assume t(n) is a non-negative value.
let us call c the amount of time required to compare two integers in function largest. we do not care right now what the precise value of c might be. nor are we concerned with the time required to increment variable i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. we just want a reasonable approximation for the time taken to execute the algorithm. the total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. we say that function largest (and the largest-value sequential search algorithm in general) has a running time expressed by the equation
example 3.2 the running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. we can assume this assignment takes a constant amount of time regardless of the value. let us call c1 the amount of time necessary to copy an integer. no matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. thus, the equation for this algorithm is simply
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
n values. if we implement sequential search as a program and run it many times on many different arrays of size n, or search for many different values of k within the same array, we expect the algorithm on average to go halfway through the array before ﬁnding the value we seek. on average, the algorithm examines about n/2 values. we call this the average case for this algorithm.
when analyzing an algorithm, should we study the best, worst, or average case? normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. in other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. however, there are rare instances where a best-case analysis is useful — in particular, when the best case has high probability of occurring. in chapter 7 you will see some examples where taking advantage of the best-case running time for one sorting algorithm makes a second more efﬁcient.
how about the worst case? the advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. this is especially important for real-time applications, such as for the computers that monitor an air trafﬁc control system. here, it would not be acceptable to use an algorithm that can handle n airplanes quickly enough most of the time, but which fails to perform quickly enough when all n airplanes are coming from the same direction.
for other applications — particularly when we wish to aggregate the cost of running the program many times on many different inputs — worst-case analysis might not be a representative measure of the algorithm’s performance. often we prefer to know the average-case running time. this means that we would like to know the typical behavior of the algorithm on inputs of size n. unfortunately, average-case analysis is not always possible. average-case analysis ﬁrst requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. for example, it was stated previously that the sequential search algorithm on average examines half of the array values. this is only true if the element with value k is equally likely to appear in any position in the array. if this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. see section 9.2 for further discussion regarding the effects of data distribution on the sequential search algorithm.
the characteristics of a data distribution have a signiﬁcant effect on many search algorithms, such as those based on hashing (section 9.4) and search trees (e.g., see section 5.4). incorrect assumptions about data distribution can have dis-
the following is a precise deﬁnition for an upper bound. t(n) represents the true running time of the algorithm. f(n) is some expression for the upper bound.
for t(n) a non-negatively valued function, t(n) is in set o(f(n)) if there exist two positive constants c and n0 such that t(n) ≤ cf(n) for all n > n0.
constant n0 is the smallest value of n for which the claim of an upper bound holds true. usually n0 is small, such as 1, but does not need to be. you must also be able to pick some constant c, but it is irrelevant what the value for c actually is. in other words, the deﬁnition says that for all inputs of the type in question (such as the worst case for all inputs of size n) that are large enough (i.e., n > n0), the algorithm always executes in less than cf(n) steps for some constant c.
example 3.4 consider the sequential search algorithm for ﬁnding a speciﬁed value in an array of integers. if visiting and examining one value in the array requires cs steps where cs is a positive number, and if the value we search for has equal probability of appearing in any position in the array, then in the average case t(n) = csn/2. for all values of n > 1, csn/2 ≤ csn. therefore, by the deﬁnition, t(n) is in o(n) for n0 = 1 and c = cs.
example 3.5 for a particular algorithm, t(n) = c1n2 + c2n in the average case where c1 and c2 are positive numbers. then, c1n2 + c2n ≤ c1n2 + c2n2 ≤ (c1 + c2)n2 for all n > 1. so, t(n) ≤ cn2 for c = c1 + c2, and n0 = 1. therefore, t(n) is in o(n2) by the deﬁnition.
example 3.6 assigning the value from the ﬁrst position of an array to a variable takes constant time regardless of the size of the array. thus, t(n) = c (for the best, worst, and average cases). we could say in this case that t(n) is in o(c). however, it is traditional to say that an algorithm whose running time has a constant upper bound is in o(1).
just knowing that something is in o(f(n)) says only how bad things can get. perhaps things are not nearly so bad. because we know sequential search is in o(n) in the worst case, it is also true to say that sequential search is in o(n2). but
3.15 does every algorithm have a Θ running-time equation? in other words, are the upper and lower bounds for the running time (on any speciﬁed class of inputs) always the same?
3.16 does every problem for which there exists some algorithm have a Θ runningtime equation? in other words, for every problem, and for any speciﬁed class of inputs, is there some algorithm whose upper bound is equal to the problem’s lower bound?
3.17 given an array storing integers ordered by value, modify the binary search routine to return the position of the ﬁrst integer with value k in the situation where k can appear multiple times in the array. be sure that your algorithm is Θ(log n), that is, do not resort to sequential search once an occurrence of k is found.
3.18 given an array storing integers ordered by value, modify the binary search routine to return the position of the integer with the greatest value less than k when k itself does not appear in the array. return error if the least value in the array is greater than k.
3.19 modify the binary search routine to support search in an array of inﬁnite size. in particular, you are given as input a sorted array and a key value k to search for. call n the position of the smallest value in the array that is equal to or larger than x. provide an algorithm that can determine n in o(log n) comparisons in the worst case. explain why your algorithm meets the required time bound.
3.20 it is possible to change the way that we pick the dividing point in a binary search, and still get a working search routine. however, where we pick the dividing point could affect the performance of the algorithm. (a) if we change the dividing point computation in function binary from i = (l + r)/2 to i = (l + ((r − l)/3)), what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
use a variety of random search values in the range 0 to n − 1 on each size n. graph the resulting times. when is sequential search faster than binary search for a sorted array?
3.3 implement a program that runs and gives timings for the two fibonacci sequence functions provided in exercise 2.11. graph the resulting running times for as many values of n as your computer can handle.
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
let pi be the probability that k is in position i of l. when k is not in l, sequential search will require n comparisons. let p0 be the probability that k is not in l. then the average cost t(n) will be
for large collections of records that are searched repeatedly, sequential search is unacceptably slow. one way to reduce search time is to preprocess the records by sorting them. given a sorted array, an obvious improvement over simple linear search is to test if the current element in l is greater than k. if it is, then we know that k cannot appear later in the array, and we can quit the search early. but this still does not improve the worst-case cost of the algorithm.
we can also observe that if we look ﬁrst at position 1 in sorted array l and ﬁnd that k is bigger, then we rule out positions 0 as well as position 1. because more is often better, what if we look at position 2 in l and ﬁnd that k is bigger yet? this rules out positions 0, 1, and 2 with one comparison. what if we carry this to the extreme and look ﬁrst at the last position in l and ﬁnd that k is bigger? then we know in one comparison that k is not in l. this is very useful to know, but what is wrong with this approach? while we learn a lot sometimes (in one comparison we might learn that k is not in the list), usually we learn only a little bit (that the last element is not k).
the question then becomes: what is the right amount to jump? this leads us to an algorithm known as jump search. for some value j, we check every j’th
while self-organizing lists do not generally perform as well as search trees or a sorted list, both of which require o(log n) search time, there are many situations in which self-organizing lists prove a valuable tool. obviously they have an advantage over sorted lists in that they need not be sorted. this means that the cost to insert a new record is low, which could more than make up for the higher search cost when insertions are frequent. self-organizing lists are simpler to implement than search trees and are likely to be more efﬁcient for small lists. nor do they require additional space. finally, in the case of an application where sequential search is “almost” fast enough, changing an unsorted list to a self-organizing list might speed the application enough at a minor cost in additional code.
as an example of applying self-organizing lists, consider an algorithm for compressing and transmitting messages. the list is self-organized by the move-to-front rule. transmission is in the form of words and numbers, by the following rules:
both the sender and the receiver keep track of the position of words in the list in the same way (using the move-to-front rule), so they agree on the meaning of the numbers that encode repeated occurrences of words. for example, consider the following example message to be transmitted (for simplicity, ignore case in letters).
the ﬁrst three words have not been seen before, so they must be sent as full words. the fourth word is the second appearance of “the,” which at this point is the third word in the list. thus, we only need to transmit the position value “3.” the next two words have not yet been seen, so must be sent as full words. the seventh word is the third appearance of “the,” which coincidentally is again in the third position. the eighth word is the second appearance of “car,” which is now in the ﬁfth position of the list. “i” is a new word, and the last word “left” is now in the ﬁfth position. thus the entire transmission would be
this section presents the concept of amortized analysis, which is the analysis for a series of operations taken as a whole. in particular, amortized analysis allows us to deal with the situation where the worst-case cost for n operations is less than n times the worst-case cost of any one operation. rather than focusing on the individual cost of each operation independently and summing them, amortized analysis looks at the cost of the entire series and “charges” each individual operation with a share of the total cost.
we can apply the technique of amortized analysis in the case of a series of sequential searches in an unsorted array. for n random searches, the average-case cost for each search is n/2, and so the expected total cost for the series is n2/2. unfortunately, in the worst case all of the searches would be to the last item in the array. in this case, each search costs n for a total worst-case cost of n2. compare this to the cost for a series of n searches such that each item in the array is searched for precisely once. in this situation, some of the searches must be expensive, but also some searches must be cheap. the total number of searches, in the best, avi=i i ≈ n2/2. this is a factor
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
organizing and retrieving information is at the heart of most computer applications, and searching is surely the most frequently performed of all computing tasks. search can be viewed abstractly as a process to determine if an element with a particular value is a member of a particular set. the more common view of searching is an attempt to ﬁnd the record within a collection of records that has a particular key value, or those records in a collection whose key values meet some criterion such as falling within a range of values.
where ij is information associated with key kj for 1 ≤ j ≤ n. given a particular key value k, the search problem is to locate the record (kj, ij) in l such that kj = k (if one exists). searching is a systematic method for locating the record (or records) with key value kj = k.
a successful search is one in which a record with key kj = k is found. an unsuccessful search is one in which no record with kj = k is found (and no such record exists).
an exact-match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.
we can categorize search algorithms into three general approaches: 1. sequential and list methods. 2. direct access by key value (hashing). 3. tree indexing methods.
how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equivalent to the number required to successfully search for it. to insert a record, an empty slot along the record’s probe sequence must be found. this is equivalent to an unsuccessful search for the record (recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table).
when the hash table is empty, the ﬁrst record inserted will always ﬁnd its home position free. thus, it will require only one record access to ﬁnd a free slot. if all records are stored in their home positions, then successful searches will also require only one record access. as the table begins to ﬁll up, the probability that a record can be inserted in its home position decreases. if a record hashes to an occupied slot, then the collision resolution policy must locate another slot in which to store it. finding records not stored in their home position also requires additional record accesses as the record is searched for along its probe sequence. as the table ﬁlls up, more and more records are likely to be located ever further from their home positions.
from this discussion, we see that the expected cost of hashing is a function of how full the table is. deﬁne the load factor for the table as α = n/m, where n is the number of records currently in the table.
an estimate of the expected cost for an insertion (or an unsuccessful search) can be derived analytically as a function of α in the case where we assume that the probe sequence follows a random permutation of the slots in the hash table. assuming that every slot in the table has equal probability of being the home slot for the next record, the probability of ﬁnding the home position occupied is α. the probability of ﬁnding both the home position occupied and the next slot on the probe sequence occupied is n (n−1)
n values. if we implement sequential search as a program and run it many times on many different arrays of size n, or search for many different values of k within the same array, we expect the algorithm on average to go halfway through the array before ﬁnding the value we seek. on average, the algorithm examines about n/2 values. we call this the average case for this algorithm.
when analyzing an algorithm, should we study the best, worst, or average case? normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. in other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. however, there are rare instances where a best-case analysis is useful — in particular, when the best case has high probability of occurring. in chapter 7 you will see some examples where taking advantage of the best-case running time for one sorting algorithm makes a second more efﬁcient.
how about the worst case? the advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. this is especially important for real-time applications, such as for the computers that monitor an air trafﬁc control system. here, it would not be acceptable to use an algorithm that can handle n airplanes quickly enough most of the time, but which fails to perform quickly enough when all n airplanes are coming from the same direction.
for other applications — particularly when we wish to aggregate the cost of running the program many times on many different inputs — worst-case analysis might not be a representative measure of the algorithm’s performance. often we prefer to know the average-case running time. this means that we would like to know the typical behavior of the algorithm on inputs of size n. unfortunately, average-case analysis is not always possible. average-case analysis ﬁrst requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. for example, it was stated previously that the sequential search algorithm on average examines half of the array values. this is only true if the element with value k is equally likely to appear in any position in the array. if this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. see section 9.2 for further discussion regarding the effects of data distribution on the sequential search algorithm.
the characteristics of a data distribution have a signiﬁcant effect on many search algorithms, such as those based on hashing (section 9.4) and search trees (e.g., see section 5.4). incorrect assumptions about data distribution can have dis-
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
in the index. inverted lists reduce this problem, but they are only suitable for secondary key indices with many fewer secondary key values than records. the linear index would perform well as a primary key index if it could somehow be broken into pieces such that individual updates affect only a part of the index. this concept will be pursued throughout the rest of this chapter, eventually culminating in the b+-tree, the most widely used indexing method today. but ﬁrst, we begin by studying isam, an early attempt to solve the problem of large databases requiring frequent update. its weaknesses help to illustrate why the b+-tree works so well.
before the invention of effective tree indexing schemes, a variety of disk-based indexing methods were in use. all were rather cumbersome, largely because no adequate method for handling updates was known. typically, updates would cause the index to degrade in performance. isam is one example of such an index and was widely used by ibm prior to adoption of the b-tree.
isam is based on a modiﬁed form of the linear index, as illustrated by figure 10.6. records are stored in sorted order by primary key. the disk ﬁle is divided among a number of cylinders on disk.1 each cylindar holds a section of the list in sorted order. initially, each cylinder is not ﬁlled to capacity, and the extra space is set aside in the cylinder overﬂow. in memory is a table listing the lowest key value stored in each cylinder of the ﬁle. each cylinder contains a table listing the lowest
unfortunately, the bst can become unbalanced. even under relatively good conditions, the depth of leaf nodes can easily vary by a factor of two. this might not be a signiﬁcant concern when the tree is stored in main memory because the time required is still Θ(log n) for search and update. when the tree is stored on disk, however, the depth of nodes in the tree becomes crucial. every time a bst node b is visited, it is necessary to visit all nodes along the path from the root to b. each node on this path must be retrieved from disk. each disk access returns a block of information. if a node is on the same block as its parent, then the cost to ﬁnd that node is trivial once its parent is in main memory. thus, it is desirable to keep subtrees together on the same block. unfortunately, many times a node is not on the same block as its parent. thus, each access to a bst node could potentially require that another block to be read from disk. using a buffer pool to store multiple blocks in memory can mitigate disk access problems if bst accesses display good locality of reference. but a buffer pool cannot eliminate disk i/o entirely. the problem becomes greater if the bst is unbalanced, because nodes deep in the tree have the potential of causing many disk blocks to be read. thus, there are two signiﬁcant issues that must be addressed to have efﬁcient search from a disk-based bst. the ﬁrst is how to keep the tree balanced. the second is how to arrange the nodes on blocks so as to keep the number of blocks encountered on any path from the root to the leaves at a minimum.
we could select a scheme for balancing the bst and allocating bst nodes to blocks in a way that minimizes disk i/o, as illustrated by figure 10.7. however, maintaining such a scheme in the face of insertions and deletions is difﬁcult. in particular, the tree should remain balanced when an update takes place, but doing so might require much reorganization. each update should affect only a disk few blocks, or its cost will be too high. as you can see from figure 10.8, adopting a rule such as requiring the bst to be complete can cause a great deal of rearranging of data within the tree.
we can solve these problems by selecting another tree structure that automatically remains balanced after updates, and which is amenable to storing in blocks. there are a number of widely used balanced tree data structures, and there are also techniques for keeping bsts balanced. examples are the avl and splay trees discussed in section 13.2. as an alternative, section 10.4 presents the 2-3 tree, which has the property that its leaves are always at the same level. the main reason for discussing the 2-3 tree here in preference to the other balanced search trees is that
example 13.1 when searching for the value 7 (0000111 in binary) in the pat trie of figure 13.3, the root node indicates that bit position 0 (the leftmost bit) is checked ﬁrst. because the 0th bit for value 7 is 0, take the left branch. at level 1, branch depending on the value of bit 1, which again is 0. at level 2, branch depending on the value of bit 2, which again is 0. at level 3, the index stored in the node is 4. this means that bit 4 of the key is checked next. (the value of bit 3 is irrelevant, because all values stored in that subtree have the same value at bit position 3.) thus, the single branch that extends from the equivalent node in figure 13.1 is just skipped. for key value 7, bit 4 has value 1, so the rightmost branch is taken. because this leads to a leaf node, the search key is compared against the key stored in that node. if they match, then the desired record has been found.
note that during the search process, only a single bit of the search key is compared at each internal node. this is signiﬁcant, because the search key could be quite large. search in the pat trie requires only a single full-key comparison, which takes place once a leaf node has been reached.
example 13.2 consider the situation where we need to store a library of dna sequences. a dna sequence is a series of letters, usually many thousands of characters long, with the string coming from an alphabet of only four letters that stand for the four amino acids making up a dna strand. similar dna seqences might have long sections of ther string that are identical. the pat trie woudl avoid making multiple full key comparisons when searching for a speciﬁc sequence.
we have noted several times that the bst has a high risk of becoming unbalanced, resulting in excessively expensive search and update operations. one solution to this problem is to adopt another search tree structure such as the 2-3 tree. an alternative is to modify the bst access functions in some way to guarantee that the tree performs well. this is an appealing concept, and it works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. unfortunately, requiring that the bst always be in the shape of a complete binary tree requires excessive modiﬁcation to the tree during update, as discussed in section 10.3.
o(m log n) time for a tree of n nodes whenever m ≥ n. thus, a single insert or search operation could take o(n) time. however, m such operations are guaranteed to require a total of o(m log n) time, for an average cost of o(log n) per access operation. this is a desirable performance guarantee for any search-tree structure. unlike the avl tree, the splay tree is not guaranteed to be height balanced. what is guaranteed is that the total cost of the entire series of accesses will be cheap. ultimately, it is the cost of the series of operations that matters, not whether the tree is balanced. maintaining balance is really done only for the sake of reaching this time efﬁciency goal.
the splay tree access functions operate in a manner reminiscent of the moveto-front rule for self-organizing lists from section 9.2, and of the path compression technique for managing parent-pointer trees from section 6.2. these access functions tend to make the tree more balanced, but an individual access will not necessarily result in a more balanced tree.
whenever a node s is accessed (e.g., when s is inserted, deleted, or is the goal of a search), the splay tree performs a process called splaying. splaying moves s to the root of the bst. when s is being deleted, splaying moves the parent of s to the root. as in the avl tree, a splay of node s consists of a series of rotations. a rotation moves s higher in the tree by adjusting its position with respect to its parent and grandparent. a side effect of the rotations is a tendency to balance the tree. there are three types of rotation.
a single rotation is performed only if s is a child of the root node. the single rotation is illustrated by figure 13.7. it basically switches s with its parent in a way that retains the bst property. while figure 13.7 is slightly different from figure 13.5, in fact the splay tree single rotation is identical to the avl tree single rotation.
unlike the avl tree, the splay tree requires two types of double rotation. double rotations involve s, its parent (call it p), and s’s grandparent (call it g). the effect of a double rotation is to move s up two levels in the tree.
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
earlier chapters presented basic data structures and algorithms that operate on data stored in main memory. some applications require that large amounts of information be stored and processed — so much information that it cannot all ﬁt into main memory. in that case, the information must reside on disk and be brought into main memory selectively for processing.
you probably already realize that main memory access is much faster than access to data stored on disk or other storage devices. the relative difference in access times is so great that efﬁcient disk-based programs require a different approach to algorithm design than most programmers are used to. as a result, many programmers do a poor job when it comes to ﬁle processing applications.
this chapter presents the fundamental issues relating to the design of algorithms and data structures for disk-based applications.1 we begin with a description of the signiﬁcant differences between primary memory and secondary storage. section 8.2 discusses the physical aspects of disk drives. section 8.3 presents basic methods for managing buffer pools. section 8.4 discusses the java model for random access to data stored on disk. section 8.5 discusses the basic principles for sorting collections of records too large to ﬁt in main memory.
1computer technology changes rapidly. i provide examples of disk drive speciﬁcations and other hardware performance numbers that are reasonably up to date as of the time when the book was written. when you read it, the numbers might seem out of date. however, the basic principles do not change. the approximate ratios for time, space, and cost between memory and disk have remained surprisingly steady for over 20 years.
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
a great discussion on external sorting methods can be found in salzberg’s book. the presentation in this chapter is similar in spirit to salzberg’s.
for details on disk drive modeling and measurement, see the article by ruemmler and wilkes, “an introduction to disk drive modeling” [rw94]. see andrew s. tanenbaum’s structured computer organization [tan06] for an introduction to computer hardware and organization. an excellent, detailed description of memory and hard disk drives can be found online at “the pc guide,” by charles m. kozierok [koz05] (www.pcguide.com). the pc guide also gives detailed descriptions of the microsoft windows and unix (linux) ﬁle systems.
see “outperforming lru with an adaptive replacement cache algorithm” by megiddo and modha for an example of a more sophisticated algorithm than lru for managing buffer pools.
8.1 computer memory and storage prices change rapidly. find out what the current prices are for the media listed in figure 8.1. does your information change any of the basic conclusions regarding disk processing?
8.2 assume a disk drive from the late 1990s is conﬁgured as follows. the total storage is approximately 675mb divided among 15 surfaces. each surface has 612 tracks; there are 144 sectors/track, 512 bytes/sector, and 8 sectors/cluster. the disk turns at 3600 rpm. the track-to-track seek time is 20 ms, and the average seek time is 80 ms. now assume that there is a 360kb ﬁle on the disk. on average, how long does it take to read all of the data in the ﬁle? assume that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on adjacent tracks, and that the ﬁle completely ﬁlls each track on which it is found. a seek must be performed each time the i/o head moves to a new track. show your calculations.
8.3 using the speciﬁcations for the disk drive given in exercise 8.2, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
8.11 at the end of 2004, the fastest disk drive i could ﬁnd speciﬁcations for was the maxtor atlas. this drive had a nominal capacity of 73.4gb using 4 platters (8 surfaces) or 9.175gb/surface. assume there are 16,384 tracks with an average of 1170 sectors/track and 512 bytes/sector.3 the disk turns at 15,000 rpm. the track-to-track seek time is 0.4 ms and the average seek time is 3.6 ms. how long will it take on average to read a 6mb ﬁle, assuming that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on contiguous tracks, and that the ﬁle completely ﬁlls each track on which it is found. show your calculations.
8.12 using the speciﬁcations for the disk drive given in exercise 8.11, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
8.15 assume that a ﬁle contains one million records sorted by key value. a query to the ﬁle returns a single record containing the requested key value. files are stored on disk in sectors each containing 100 records. assume that the average time to read a sector selected at random is 10.0 ms. in contrast, it takes only 2.0 ms to read the sector adjacent to the current position of the i/o head. the “batch” algorithm for processing queries is to ﬁrst sort the queries by order of appearance in the ﬁle, and then read the entire ﬁle sequentially, processing all queries in sequential order as the ﬁle is read. this algorithm implies that the queries must all be available before processing begins. the “interactive” algorithm is to process each query in order of its arrival, searching for the requested sector each time (unless by chance two queries in a row are to the same sector). carefully deﬁne under what conditions the batch method is more efﬁcient than the interactive method.
of the data on disk. likewise, when writing to a particular logical byte position with respect to the beginning of the ﬁle, this position must be converted by the ﬁle manager into the corresponding physical location on the disk. to gain some appreciation for the the approximate time costs for these operations, you need to understand the physical structure and basic workings of a disk drive.
disk drives are often referred to as direct access storage devices. this means that it takes roughly equal time to access any record in the ﬁle. this is in contrast to sequential access storage devices such as tape drives, which require the tape reader to process data from the beginning of the tape until the desired position has been reached. as you will see, the disk drive is only approximately direct access: at any given time, some records are more quickly accessible than others.
8.2.1 disk drive architecture a hard disk drive is composed of one or more round platters, stacked one on top of another and attached to a central spindle. platters spin continuously at a constant rate. each usable surface of each platter is assigned a read/write head or i/o head through which data are read or written, somewhat like the arrangement of a phonograph player’s arm “reading” sound from a phonograph record. unlike a phonograph needle, the disk read/write head does not actually touch the surface of a hard disk. instead, it remains slightly above the surface, and any contact during normal operation would damage the disk. this distance is very small, much smaller than the height of a dust particle. it can be likened to a 5000-kilometer airplane trip across the united states, with the plane ﬂying at a height of one meter!
a hard disk drive typically has several platters and several read/write heads, as shown in figure 8.2(a). each head is attached to an arm, which connects to the boom. the boom moves all of the heads in or out together. when the heads are in some position over the platters, there are data on each platter directly accessible to each head. the data on a single platter that are accessible to any one position of the head for that platter are collectively called a track, that is, all data on a platter that are a ﬁxed distance from the spindle, as shown in figure 8.2(b). the collection of all tracks that are a ﬁxed distance from the spindle is called a cylinder. thus, a cylinder is all of the data that can be read when the arms are in a particular position. each track is subdivided into sectors. between each sector there are intersector gaps in which no data are stored. these gaps allow the read head to recognize the end of a sector. note that each sector contains the same amount of data. because the outer tracks have greater length, they contain fewer bits per inch than do the inner tracks. thus, about half of the potential storage space is wasted, because only the innermost tracks are stored at the highest possible data density. this ar-
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
how much time is required to read the track? on average, it will require half a rotation to bring the ﬁrst sector of the track under the i/o head, and then one complete rotation to read the track.
how long will it take to read a ﬁle of 1mb divided into 2048 sectorsized (512 byte) records? this ﬁle will be stored in 256 clusters, because each cluster holds 8 sectors. the answer to the question depends in large measure on how the ﬁle is stored on the disk, that is, whether it is all together or broken into multiple extents. we will calculate both cases to see how much difference this makes.
if the ﬁle is stored so as to ﬁll all of the sectors of eight adjacent tracks, then the cost to read the ﬁrst sector will be the time to seek to the ﬁrst track (assuming this requires a random seek), then a wait for the initial rotational delay, and then the time to read. this requires
if the ﬁle’s clusters are spread randomly across the disk, then we must perform a seek for each cluster, followed by the time for rotational delay. once the ﬁrst sector of the cluster comes under the i/o head, very little time is needed to read the cluster because only 8/256 of the track needs to rotate under the head, for a total time of about 5.9 ms for latency and read time. thus, the total time required is about
this example illustrates why it is important to keep disk ﬁles from becoming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. file fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed.
block contains the same number of ﬁxed-size data records. depending on the application, a record might be only a few bytes — composed of little or nothing more than the key — or might be hundreds of bytes with a relatively small key ﬁeld. records are assumed not to cross block boundaries. these assumptions can be relaxed for special-purpose sorting applications, but ignoring such complications makes the principles clearer.
as explained in section 8.2, a sector is the basic unit of i/o. in other words, all disk reads and writes are for one or more complete sectors. sector sizes are typically a power of two, in the range 512 to 16k bytes, depending on the operating system and the size and speed of the disk drive. the block size used for external sorting algorithms should be equal to or a multiple of the sector size.
under this model, a sorting algorithm reads a block of data into a buffer in main memory, performs some processing on it, and at some future time writes it back to disk. from section 8.1 we see that reading or writing a block from disk takes on the order of one million times longer than a memory access. based on this fact, we can reasonably expect that the records contained in a single block can be sorted by an internal sorting algorithm such as quicksort in less time than is required to read or write the block.
under good conditions, reading from a ﬁle in sequential order is more efﬁcient than reading blocks in random order. given the signiﬁcant impact of seek time on disk access, it might seem obvious that sequential processing is faster. however, it is important to understand precisely under what circumstances sequential ﬁle processing is actually faster than random access, because it affects our approach to designing an external sorting algorithm.
efﬁcient sequential access relies on seek time being kept to a minimum. the ﬁrst requirement is that the blocks making up a ﬁle are in fact stored on disk in sequential order and close together, preferably ﬁlling a small number of contiguous tracks. at the very least, the number of extents making up the ﬁle should be small. users typically do not have much control over the layout of their ﬁle on disk, but writing a ﬁle all at once in sequential order to a disk drive with a high percentage of free space increases the likelihood of such an arrangement.
the second requirement is that the disk drive’s i/o head remain positioned over the ﬁle throughout sequential processing. this will not happen if there is competition of any kind for the i/o head. for example, on a multi-user timeshared computer the sorting process might compete for the i/o head with the processes of other users. even when the sorting process has sole control of the i/o head, it is still likely that sequential processing will not be efﬁcient. imagine the situation where all processing is done on a single disk drive, with the typical arrangement
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
grows, there might not be free space physically adjacent. thus, a ﬁle might consist of several extents widely spaced on the disk. the fuller the disk, and the more that ﬁles on the disk change, the worse this ﬁle fragmentation (and the resulting seek time) becomes. file fragmentation leads to a noticeable degradation in performance as additional seeks are required to access data.
another type of problem arises when the ﬁle’s logical record size does not match the sector size. if the sector size is not a multiple of the record size (or vice versa), records will not ﬁt evenly within a sector. for example, a sector might be 2048 bytes long, and a logical record 100 bytes. this leaves room to store 20 records with 48 bytes left over. either the extra space is wasted, or else records are allowed to cross sector boundaries. if a record crosses a sector boundary, two disk accesses might be required to read it. if the space is left empty instead, such wasted space is called internal fragmentation.
a second example of internal fragmentation occurs at cluster boundaries. files whose size is not an even multiple of the cluster size must waste some space at the end of the last cluster. the worst case will occur when ﬁle size modulo cluster size is one (for example, a ﬁle of 4097 bytes and a cluster of 4096 bytes). thus, cluster size is a tradeoff between large ﬁles processed sequentially (where a large cluster size is desirable to minimize seeks) and small ﬁles (where small clusters are desirable to minimize wasted storage).
every disk drive organization requires that some disk space be used to organize the sectors, clusters, and so forth. the layout of sectors within a track is illustrated by figure 8.4. typical information that must be stored on the disk itself includes the file allocation table, sector headers that contain address marks and information about the condition (whether usable or not) for each sector, and gaps between sectors. the sector header also contains error detection codes to help verify that the data have not been corrupted. this is why most disk drives have a “nominal” size that is greater than the actual amount of user data that can be stored on the drive. the difference is the amount of space required to organize the information on the disk. additional space will be lost due to fragmentation.
the primary cost when accessing information on disk is normally the seek time. this assumes of course that a seek is necessary. when reading a ﬁle in sequential order (if the sectors comprising the ﬁle are contiguous on disk), little seeking is necessary. however, when accessing a random disk sector, seek time becomes the dominant cost for the data access. while the actual seek time is highly variable, depending on the distance between the track where the i/o head currently is and
determining bubble sort’s number of comparisons is easy. regardless of the arrangement of the values in the array, the number of comparisons made by the inner for loop is always i, leading to a total cost of
the number of swaps required depends on how often a value is less than the one immediately preceding it in the array. we can expect this to occur for about half the comparisons in the average case, leading to Θ(n2) for the expected number of swaps. the actual number of swaps performed by bubble sort will be identical to that performed by insertion sort.
7.2.3 selection sort consider again the problem of sorting a pile of phone bills for the past year. another intuitive approach might be to look through the pile until you ﬁnd the bill for january, and pull that out. then look through the remaining pile until you ﬁnd the bill for february, and add that behind january. proceed through the ever-shrinking pile of bills to select the next one in order until you are done. this is the inspiration for our last Θ(n2) sort, called selection sort. the ith pass of selection sort “selects” the ith smallest key in the array, placing that record into position i. in other words, selection sort ﬁrst ﬁnds the smallest key in an unsorted list, then the second smallest, and so on. its unique feature is that there are few record swaps. to ﬁnd the next smallest key value requires searching through the entire unsorted portion of the array, but only one swap is required to put the record in place. thus, the total number of swaps required will be n − 1 (we get the last record in place “for free”).
selection sort is essentially a bubble sort, except that rather than repeatedly swapping adjacent values to get the next smallest record into place, we instead
figure 7.5 summarizes the cost of insertion, bubble, and selection sort in terms of their required number of comparisons and swaps1 in the best, average, and worst cases. the running time for each of these sorts is Θ(n2) in the average and worst cases.
the remaining sorting algorithms presented in this chapter are signiﬁcantly better than these three under typical conditions. but before continuing on, it is instructive to investigate what makes these three sorts so slow. the crucial bottleneck is that only adjacent records are compared. thus, comparisons and moves (in all but selection sort) are by single steps. swapping adjacent records is called an exchange. thus, these sorts are sometimes referred to as exchange sorts. the cost of any exchange sort can be at best the total number of steps that the records in the array must move to reach their “correct” location (i.e., the number of inversions for each record). what is the average number of inversions? consider a list l containing n values. deﬁne lr to be l in reverse. l has n(n−1)/2 distinct pairs of values, each of which could potentially be an inversion. each such pair must either be an inversion in l or in lr. thus, the total number of inversions in l and lr together is exactly n(n− 1)/2 for an average of n(n− 1)/4 per list. we therefore know with certainty that any sorting algorithm which limits comparisons to adjacent items will cost at least n(n − 1)/4 = Ω(n2) in the average case.
1there is a slight anomaly with selection sort. the supposed advantage for selection sort is its low number of swaps required, yet selection sort’s best-case number of swaps is worse than that for insertion sort or bubble sort. this is because the implementation given for selection sort does not avoid a swap in the case where record i is already in position i. the reason is that it usually takes more time to repeatedly check for this situation than would be saved by avoiding such swaps.
a simple improvement might then be to replace quicksort with a faster sort for small numbers, say insertion sort or selection sort. however, there is an even better — and still simpler — optimization. when quicksort partitions are below a certain size, do nothing! the values within that partition will be out of order. however, we do know that all values in the array to the left of the partition are smaller than all values in the partition. all values in the array to the right of the partition are greater than all values in the partition. thus, even if quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. this is an ideal situation in which to take advantage of the best-case performance of insertion sort. the ﬁnal step is a single call to insertion sort to process the entire array, putting the elements into ﬁnal sorted order. empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer elements.
the last speedup to be considered reduces the cost of making recursive calls. quicksort is inherently recursive, because each quicksort operation must sort two sublists. thus, there is no simple way to turn quicksort into an iterative algorithm. however, quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. we need not store copies of a subarray, only the subarray bounds. furthermore, the stack depth can be kept small if care is taken on the order in which quicksort’s recursive calls are executed. we can also place the code for findpivot and partition inline to eliminate the remaining function calls. note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. thus, eliminating the remaining function calls will yield only a modest speedup.
our discussion of quicksort began by considering the practicality of using a binary search tree for sorting. the bst requires more space than the other sorting methods and will be slower than quicksort or mergesort due to the relative expense of inserting values into the tree. there is also the possibility that the bst might be unbalanced, leading to a Θ(n2) worst-case running time. subtree balance in the bst is closely related to quicksort’s partition step. quicksort’s pivot serves roughly the same purpose as the bst root value in that the left partition (subtree) stores values less than the pivot (root) value, while the right partition (subtree) stores values greater than or equal to the pivot (root).
a good sorting algorithm can be devised based on a tree structure more suited to the purpose. in particular, we would like the tree to be balanced, space efﬁcient,
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
we all have an intuitive understanding of what we mean by a “list,” so our ﬁrst step is to deﬁne precisely what is meant so that this intuitive understanding can eventually be converted into a concrete data structure and its operations. perhaps the most important concept related to lists is that of position. in other words, we perceive that there is a ﬁrst element in the list, a second element, and so on. thus, we should view a list as embodying the mathematical concepts of a sequence, as deﬁned in section 2.1.
we deﬁne a list to be a ﬁnite, ordered sequence of data items known as elements. “ordered” in this deﬁnition means that each element has a position in the list. (we will not use “ordered” in this context to mean that the list is sorted.) each list element has a data type. in the simple list implementations discussed in this chapter, all elements of the list have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it (see section 12.1). the operations deﬁned as part of the list adt do not depend on the elemental data type. for example, the list adt can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists.
a list is said to be empty when it contains no elements. the number of elements currently stored is called the length of the list. the beginning of the list is called the head, the end of the list is called the tail. there might or might not be some relationship between the value of an element and its position in the list. for example, sorted lists have their elements positioned in ascending order of value, while unsorted lists have no particular relationship between element values and positions. this section will consider only unsorted lists. chapters 7 and 9 treat the problems of how to create and search sorted lists efﬁciently.
when presenting the contents of a list, we use the same notation as was introduced for sequences in section 2.1. to be consistent with java array indexing, the ﬁrst position on the list is denoted as 0. thus, if there are n elements in the list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. the subscript indicates an element’s position within the list. using this notation, the empty list would appear as hi.
before selecting a list implementation, a program designer should ﬁrst consider what basic operations the implementation must support. our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. we should be able to insert and remove elements from anywhere in the list. we should be able to gain access to any element’s value, either to read it or to change it. we must be able to create and clear (or reinitialize) lists. it is also convenient to access the next or previous element from the “current” one.
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
slow, but perhaps there are better ones waiting to be discovered. of course, while having a problem with high running time is bad, it is even worse to have a problem that cannot be solved at all! problems of the later type do exist, and some are presented in section 17.3.
this chapter presents a brief introduction to the theory of expensive and impossible problems. section 17.1 presents the concept of a reduction, which is the central tool used for analyzing the difﬁculty of a problem (as opposed to analyzing the cost of an algorithm). reductions allow us to relate the difﬁculty of various problems, which is often much easier than doing the analysis for a problem from ﬁrst principles. section 17.2 discusses “hard” problems, by which we mean problems that require, or at least appear to require, time exponential on the input size. finally, section 17.3 considers various problems that, while often simple to deﬁne and comprehend, are in fact impossible to solve using a computer program. the classic example of such a problem is deciding whether an arbitrary computer program will go into an inﬁnite loop when processing a speciﬁed input. this is known as the halting problem.
we begin with an important concept for understanding the relationships between problems, called reduction. reduction allows us to solve one problem in terms of another. equally importantly, when we wish to understand the difﬁculty of a problem, reduction allows us to make relative statements about upper and lower bounds on the cost of a problem (as opposed to an algorithm or program).
because the concept of a problem is discussed extensively in this chapter, we want notation to simplify problem descriptions. throughout this chapter, a problem will be deﬁned in terms of a mapping between inputs and outputs, and the name of the problem will be given in all capital letters. thus, a complete deﬁnition of the sorting problem could appear as follows:
once you have bought or written a program to solve one problem, such as sorting, you might be able to use it as a tool to solve a different problem. this is
figure 17.1 an illustration of pairing. the two lists of numbers are paired up so that the least values from each list make a pair, the next smallest values from each list make a pair, and so on.
output: a pairing of the elements in the two sequences such that the least value in x is paired with the least value in y, the next least value in x is paired with the next least value in y, and so on.
figure 17.1 illustrates pairing. one way to solve pairing is to use an existing sorting program by sorting each of the two sequences, and then pairing-off items based on their position in sorted order. technically, we say that pairing is reduced to sorting, because sorting is used to solve pairing.
notice that reduction is a three-step process. the ﬁrst step is to convert an instance of pairing into two instances of sorting. the conversion step is not very interesting; it simply takes each sequence and assigns it to an array to be passed to sorting. the second step is to sort the two arrays (i.e., apply sorting to each array). the third step is to convert the output of sorting to the output for pairing. this is done by pairing the ﬁrst elements in the sorted arrays, the second elements, and so on.
6.9 devise a series of equivalence statements for a collection of sixteen items that yields a tree of height 5 when both the weighted union rule and path compression are used. what is the total number of parent pointers followed to perform this series?
6.10 one alternative to path compression that gives similar performance gains is called path halving. in path halving, when the path is traversed from the node to the root, we make the grandparent of every other node i on the path the new parent of i. write a version of find that implements path halving. your find operation should work as you move up the tree, rather than require the two passes needed by path compression.
6.11 analyze the fraction of overhead required by the “list of children” implementation, the “left-child/right-sibling” implementation, and the two linked implementations of section 6.3.3. how do these implementations compare in space efﬁciency?
6.12 using the general tree adt of figure 6.2, write a function that takes as input the root of a general tree and returns a binary tree generated by the conversion process illustrated by figure 6.14.
6.13 use mathematical induction to prove that the number of leaves in a nonempty full k-ary tree is (k − 1)n + 1, where n is the number of internal nodes.
6.14 derive the formulae for computing the relatives of a non-empty complete k-ary tree node stored in the complete tree representation of section 5.3.3. 6.15 find the overhead fraction for a full k-ary tree implementation with space
(c) all nodes store data and a parent pointer, and internal nodes store k child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
(d) only leaf nodes store data; only internal nodes store k child pointers. the data ﬁeld requires four bytes and each pointer requires two bytes. (a) write out the sequential representation for figure 6.18 using the coding
(a) write a function to decode the sequential representation for binary trees illustrated by example 6.5. the input should be the sequential representation and the output should be a pointer to the root of the resulting binary tree.
(b) write a function to decode the sequential representation for full binary trees illustrated by example 6.6. the input should be the sequential representation and the output should be a pointer to the root of the resulting binary tree.
(c) write a function to decode the sequential representation for general trees illustrated by example 6.8. the input should be the sequential representation and the output should be a pointer to the root of the resulting general tree.
next we consider a fundamentally different approach to implementing trees. the goal is to store a series of node values with the minimum information needed to reconstruct the tree structure. this approach, known as a sequential tree implementation, has the advantage of saving space because no pointers are stored. it has the disadvantage that accessing any node in the tree requires sequentially processing all nodes that appear before it in the node list. in other words, node access must start at the beginning of the node list, processing nodes sequentially in whatever order they are stored until the desired node is reached. thus, one primary virtue of the other implementations discussed in this section is lost: efﬁcient access (typically Θ(log n) time) to arbitrary nodes in the tree. sequential tree implementations are ideal for archiving trees on disk for later use because they save space, and the tree structure can be reconstructed as needed for later processing.
seqential tree implementations can also be used to serialize a tree structure. serialization is the process of storing an object as a series of bytes, typically so that the data structure can be transmitted between computers. this capability is important when using data structures in a distributed processing environment.
a sequential tree implementation stores the node values as they would be enumerated by a preorder traversal, along with sufﬁcient information to describe the tree’s shape. if the tree has restricted form, for example if it is a full binary tree, then less information about structure typically needs to be stored. a general tree, because it has the most ﬂexible shape, tends to require the most additional shape information. there are many possible sequential tree implementation schemes. we will begin by describing methods appropriate to binary trees, then generalize to an implementation appropriate to a general tree structure.
because every node of a binary tree is either a leaf or has two (possibly empty) children, we can take advantage of this fact to implicitly represent the tree’s structure. the most straightforward sequential tree implementation lists every node value as it would be enumerated by a preorder traversal. unfortunately, the node values alone do not provide enough information to recover the shape of the tree. in particular, as we read the series of node values, we do not know when a leaf node has been reached. however, we can treat all non-empty nodes as internal nodes with two (possibly empty) children. only null values will be interpreted as leaf nodes, and these can be listed explicitly. such an augmented node list provides enough information to recover the tree structure.
this chapter presents mathematical notation, background, and techniques used throughout the book. this material is provided primarily for review and reference. you might wish to return to the relevant sections when you encounter unfamiliar notation or mathematical techniques in later chapters.
section 2.7 on estimating might be unfamiliar to many readers. estimating is not a mathematical technique, but rather a general engineering skill. it is enormously useful to computer scientists doing design work, because any proposed solution whose estimated resource requirements fall well outside the problem’s resource constraints can be discarded immediately.
the concept of a set in the mathematical sense has wide application in computer science. the notations and techniques of set theory are commonly used when describing and implementing algorithms because the abstractions associated with sets often help to clarify and simplify algorithm design.
a set is a collection of distinguishable members or elements. the members are typically drawn from some larger population known as the base type. each member of a set is either a primitive element of the base type or is a set itself. there is no concept of duplication in a set. each value from the base type is either in the set or not in the set. for example, a set named p might be the three integers 7, 11, and 42. in this case, p’s members are 7, 11, and 42, and the base type is integer. figure 2.1 shows the symbols commonly used to express sets and their relationships. here are some examples of this notation in use. first deﬁne two sets, p and q.
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
|p| = 3 (because p has three members) and |q| = 2 (because q has two members). the union of p and q, written p ∪ q, is the set of elements in either p or q, which is {2, 3, 5, 10}. the intersection of p and q, written p ∩ q, is the set of elements that appear in both p and q, which is {5}. the set difference of p and q, written p − q, is the set of elements that occur in p but not in q, which is {2, 3}. note that p ∪ q = q ∪ p and that p ∩ q = q ∩ p, but in general p − q 6= q − p. in this example, q − p = {10}. note that the set {4, 3, 5} is indistinguishable from set p, because sets have no concept of order. likewise, set {4, 3, 4, 5} is also indistinguishable from p, because sets have no concept of duplicate elements. s = {a, b, c}. the powerset of s is
sometimes we wish to deﬁne a collection of elements with no order (like a set), but with duplicate-valued elements. such a collection is called a bag.1 to distinguish bags from sets, i use square brackets [] around a bag’s elements. for
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
the set difference a − b can be implemented in java using the expression a&˜b (˜ is the symbol for bitwise negation). for larger sets that do not ﬁt into a single computer word, the equivalent operations can be performed in turn on the series of words making up the entire bit vector.
this method of computing sets from bit vectors is sometimes applied to document retrieval. consider the problem of picking from a collection of documents those few which contain selected keywords. for each keyword, the document retrieval system stores a bit vector with one bit for each document. if the user wants to know which documents contain a certain three keywords, the corresponding three bit vectors are and’ed together. those bit positions resulting in a value of 1 correspond to the desired documents. alternatively, a bit vector can be stored for each document to indicate those keywords appearing in the document. such an organization is called a signature ﬁle. the signatures can be manipulated to ﬁnd documents with desired combinations of keywords.
this section presents a completely different approach to searching tables: by direct access based on key value. the process of ﬁnding a record using some computation to map its key value to a position in the table is called hashing. most hashing schemes place records in the table in whatever order satisﬁes the needs of the address calculation, thus the records are not ordered by value or frequency. the function that maps key values to positions is called a hash function and is usually denoted by h. the array that holds the records is called the hash table and will be denoted by ht. a position in the hash table is also known as a slot. the number of slots in hash table ht will be denoted by the variable m, with slots numbered from 0 to m − 1. the goal for a hashing system is to arrange things such that, for any key value k and some hash function h, i = h(k) is a slot in the table such that 0 ≤ h(k) < m, and we have the key of the record stored at ht[i] equal to k.
hashing only works to store sets. that is, hashing cannnot be used for applications where multiple records with the same key value are permitted. hashing is not a good method for answering range searches. in other words, we cannot easily ﬁnd all records (if any) whose key values fall within a certain range. nor can we easily ﬁnd the record with the minimum or maximum key value, or visit the records in key order. hashing is most appropriate for answering the question, “what record, if any, has key value k?” for applications where access involves only exact-match queries, hashing is usually the search method of choice because it is extremely efﬁcient when implemented correctly. as you will see in this section, however, there
this chapter presents mathematical notation, background, and techniques used throughout the book. this material is provided primarily for review and reference. you might wish to return to the relevant sections when you encounter unfamiliar notation or mathematical techniques in later chapters.
section 2.7 on estimating might be unfamiliar to many readers. estimating is not a mathematical technique, but rather a general engineering skill. it is enormously useful to computer scientists doing design work, because any proposed solution whose estimated resource requirements fall well outside the problem’s resource constraints can be discarded immediately.
the concept of a set in the mathematical sense has wide application in computer science. the notations and techniques of set theory are commonly used when describing and implementing algorithms because the abstractions associated with sets often help to clarify and simplify algorithm design.
a set is a collection of distinguishable members or elements. the members are typically drawn from some larger population known as the base type. each member of a set is either a primitive element of the base type or is a set itself. there is no concept of duplication in a set. each value from the base type is either in the set or not in the set. for example, a set named p might be the three integers 7, 11, and 42. in this case, p’s members are 7, 11, and 42, and the base type is integer. figure 2.1 shows the symbols commonly used to express sets and their relationships. here are some examples of this notation in use. first deﬁne two sets, p and q.
|p| = 3 (because p has three members) and |q| = 2 (because q has two members). the union of p and q, written p ∪ q, is the set of elements in either p or q, which is {2, 3, 5, 10}. the intersection of p and q, written p ∩ q, is the set of elements that appear in both p and q, which is {5}. the set difference of p and q, written p − q, is the set of elements that occur in p but not in q, which is {2, 3}. note that p ∪ q = q ∪ p and that p ∩ q = q ∩ p, but in general p − q 6= q − p. in this example, q − p = {10}. note that the set {4, 3, 5} is indistinguishable from set p, because sets have no concept of order. likewise, set {4, 3, 4, 5} is also indistinguishable from p, because sets have no concept of duplicate elements. s = {a, b, c}. the powerset of s is
sometimes we wish to deﬁne a collection of elements with no order (like a set), but with duplicate-valued elements. such a collection is called a bag.1 to distinguish bags from sets, i use square brackets [] around a bag’s elements. for
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
must make its way to the top of the array. this would occur if the keys are initially arranged from highest to lowest, in the reverse of sorted order. in this case, the number of comparisons will be one the ﬁrst time through the for loop, two the second time, and so on. thus, the total number of comparisons will be
in contrast, consider the best-case cost. this occurs when the keys begin in sorted order from lowest to highest. in this case, every pass through the inner for loop will fail immediately, and no values will be moved. the total number of comparisons will be n − 1, which is the number of times the outer for loop executes. thus, the cost for insertion sort in the best case is Θ(n).
while the best case is signiﬁcantly faster than the worst case, the worst case is usually a more reliable indication of the “typical” running time. however, there are situations where we can expect the input to be in sorted or nearly sorted order. one example is when an already sorted list is slightly disordered; restoring sorted order using insertion sort might be a good idea if we know that the disordering is slight. examples of algorithms that take advantage of insertion sort’s best-case running time are the shellsort algorithm of section 7.3 and the quicksort algorithm of section 7.5.
what is the average-case cost of insertion sort? when record i is processed, the number of times through the inner for loop depends on how far “out of order” the record is. in particular, the inner for loop is executed once for each key greater than the key of record i that appears in array positions 0 through i−1. for example, in the leftmost column of figure 7.1 the value 15 is preceded by ﬁve values greater than 15. each such occurrence is called an inversion. the number of inversions (i.e., the number of values greater than a given value that occur prior to it in the array) will determine the number of comparisons and swaps that must take place. we need to determine what the average number of inversions will be for the record in position i. we expect on average that half of the keys in the ﬁrst i − 1 array positions will have a value greater than that of the key at position i. thus, the average case should be about half the cost of the worst case, which is still Θ(n2). so, the average case is no better than the worst case in asymptotic complexity.
counting comparisons or swaps yields similar results because each time through the inner for loop yields both a comparison and a swap, except the last (i.e., the comparison that fails the inner for loop’s test), which has no swap. thus, the number of swaps for the entire sort operation is n − 1 less than the number of comparisons. this is 0 in the best case, and Θ(n2) in the average and worst cases.
the next sort we consider is called shellsort, named after its inventor, d.l. shell. it is also sometimes called the diminishing increment sort. unlike insertion and selection sort, there is no real life intuitive equivalent to shellsort. unlike the exchange sorts, shellsort makes comparisons and swaps between non-adjacent elements. shellsort also exploits the best-case performance of insertion sort. shellsort’s strategy is to make the list “mostly sorted” so that a ﬁnal insertion sort can ﬁnish the job. when properly implemented, shellsort will give substantially better performance than Θ(n2) in the worst case.
shellsort uses a process that forms the basis for many of the sorts presented in the following sections: break the list into sublists, sort them, then recombine the sublists. shellsort breaks the array of elements into “virtual” sublists. each sublist is sorted using an insertion sort. another group of sublists is then chosen and sorted, and so on.
during each iteration, shellsort breaks the list into disjoint sublists so that each element in a sublist is a ﬁxed number of positions apart. for example, let us assume for convenience that n, the number of values to be sorted, is a power of two. one possible implementation of shellsort will begin by breaking the list into n/2 sublists of 2 elements each, where the array index of the 2 elements in each sublist differs by n/2. if there are 16 elements in the array indexed from 0 to 15, there would initially be 8 sublists of 2 elements each. the ﬁrst sublist would be the elements in positions 0 and 8, the second in positions 1 and 9, and so on. each list of two elements is sorted using insertion sort.
the second pass of shellsort looks at fewer, bigger lists. for our example the second pass would have n/4 lists of size 4, with the elements in the list being n/4 positions apart. thus, the second pass would have as its ﬁrst sublist the 4 elements in positions 0, 4, 8, and 12; the second sublist would have elements in positions 1, 5, 9, and 13; and so on. each sublist of four elements would also be sorted using an insertion sort.
the culminating pass in this example would be a “normal” insertion sort of all elements. figure 7.6 illustrates the process for an array of 16 values where the sizes of the increments (the distances between elements on the successive passes) are 8, 4, 2, and 1. below is a java implementation for shellsort.
some choices for increments will make shellsort run more efﬁciently than others. in particular, the choice of increments described above (2k, 2k−1, ..., 2, 1) turns out to be relatively inefﬁcient. a better choice is the following series based on division by three: (..., 121, 40, 13, 4, 1).
the analysis of shellsort is difﬁcult, so we must accept without proof that the average-case performance of shellsort (for “divisions by three” increments) is o(n1.5). other choices for the increment series can reduce this upper bound somewhat. thus, shellsort is substantially better than insertion sort, or any of the Θ(n2) sorts presented in section 7.2. in fact, shellsort is competitive with the asymptotically better sorts to be presented whenever n is of medium size. shellsort illustrates how we can sometimes exploit the special properties of an algorithm (in this case insertion sort) even if in general that algorithm is unacceptably slow.
a natural approach to problem solving is divide and conquer. in terms of sorting, we might consider breaking the list to be sorted into pieces, process the pieces, and then put them back together somehow. a simple way to do this would be to split the list in half, sort the halves, and then merge the sorted halves together. this is the idea behind mergesort.
mergesort is one of the simplest sorting algorithms conceptually, and has good performance both in the asymptotic sense and in empirical running time. suprisingly, even though it is based on a simple concept, it is relatively difﬁcult to implement in practice. figure 7.7 illustrates mergesort. a pseudocode sketch of mergesort is as follows: list mergesort(list inlist) {
if (inlist.length() <= 1) return inlist;; list l1 = half of the items from inlist; list l2 = other half of the items from inlist; return merge(mergesort(l1), mergesort(l2));
before discussing how to implement mergesort, we will ﬁrst examine the merge function. merging two sorted sublists is quite simple. function merge examines the ﬁrst element of each sublist and picks the smaller value as the smallest element overall. this smaller value is removed from its sublist and placed into the output list. merging continues in this way, comparing the front elements of the sublists and continually appending the smaller to the output list until no more input elements remain.
figure 7.13 empirical comparison of sorting algorithms run on a 3.4-ghz intel pentium 4 cpu running linux. shellsort, quicksort, mergesort, and heapsort each are shown with regular and optimized versions. radix sort is shown for 4and 8-bit-per-pass versions. all times shown are milliseconds.
sublists below length nine. the ﬁrst heapsort version uses the class deﬁnitions from section 5.5. the second version removes all the class deﬁnitions and operates directly on the array using inlined code for all access functions.
in all cases, the values sorted are random 32-bit numbers. the input to each algorithm is a random array of integers. this affects the timing for some of the sorting algorithms. for example, selection sort is not being used to best advantage because the record size is small, so it does not get the best possible showing. the radix sort implementation certainly takes advantage of this key range in that it does not look at more digits than necessary. on the other hand, it was not optimized to use bit shifting instead of division, even though the bases used would permit this. the various sorting algorithms are shown for lists of sizes 10, 100, 1000, 10,000, 100,000, and 1,000,000. the ﬁnal two columns of each ﬁgure show the performance for the algorithms when run on inputs of size 10,000 where the numbers are in ascending (sorted) and descending (reverse sorted) order, respectively. these columns demonstrate best-case performance for some algorithms and worstcase performance for others. these columns also show that for some algorithms, the order of input has little effect.
these ﬁgures show a number of interesting results. as expected, the o(n2) sorts are quite poor performers for large arrays. insertion sort is by far the best of this group, unless the array is already reverse sorted. shellsort is clearly superior to any of these o(n2) sorts for lists of even 100 elements. optimized quicksort is clearly the best overall algorithm for all but lists of 10 elements. even for small
7.13 graph f1(n) = n log n, f2(n) = n1.5, and f3(n) = n2 in the range 1 ≤ n ≤ 1000 to visually compare their growth rates. typically, the constant factor in the running-time expression for an implementation of insertion sort will be less than the constant factors for shellsort or quicksort. how many times greater can the constant factor be for shellsort to be faster than insertion sort when n = 1000? how many times greater can the constant factor be for quicksort to be faster than insertion sort when n = 1000?
7.14 imagine that there exists an algorithm splitk that can split a list l of n elements into k sublists, each containing one or more elements, such that sublist i contains only elements whose values are less than all elements in sublist j for i < j <= k. if n < k, then k− n sublists are empty, and the rest are of length 1. assume that splitk has time complexity o(length of l). furthermore, assume that the k lists can be concatenated again in constant time. consider the following algorithm: list sortk(list l) {
7.15 here is a variation on sorting. the problem is to sort a collection of n nuts and n bolts by size. it is assumed that for each bolt in the collection, there is a corresponding nut of the same size, but initially we do not know which nut goes with which bolt. the differences in size between two nuts or two bolts can be too small to see by eye, so you cannot rely on comparing the sizes of two nuts or two bolts directly. instead, you can only compare the sizes of a nut and a bolt by attempting to screw one into the other (assume this comparison to be a constant time operation). this operation tells you that either the nut is bigger than the bolt, the bolt is bigger than the nut, or they are the same size. what is the minimum number of comparisons needed to sort the nuts and bolts in the worst case?
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
on a road map, a road connecting two towns is typically labeled with its distance. we can model a road network as a directed graph whose edges are labeled with real numbers. these numbers represent the distance (or other cost metric, such as travel time) between two vertices. these labels may be called weights, costs, or distances, depending on the application. given such a graph, a typical problem is to ﬁnd the total length of the shortest path between two speciﬁed vertices. this is not a trivial problem, because the shortest path may not be along the edge (if any) connecting two vertices, but rather may be along a path involving one or more intermediate vertices. for example, in figure 11.15, the cost of the path from a to b to d is 15. the cost of the edge directly from a to d is 20. the cost of the path from a to c to b to d is 10. thus, the shortest path from a to d is 10 (not along the edge connecting a to d). we use the notation d(a, d) = 10 to indicate that the shortest distance from a to d is 10. in figure 11.15, there is no path from e to b, so we set d(e, b) = ∞. we deﬁne w(a, d) = 20 to be the weight of edge (a, d), that is, the weight of the direct connection from a to d. because there is no edge from e to b, w(e, b) = ∞. note that w(d, a) = ∞ because the graph of figure 11.15 is directed. we assume that all weights are positive.
11.4.1 single-source shortest paths this section presents an algorithm to solve the single-source shortest-paths problem. given vertex s in graph g, ﬁnd a shortest path from s to every other vertex in g. we might want only the shortest path between two vertices, s and t. however in the worst case, while ﬁnding the shortest path from s to t, we might ﬁnd the shortest paths from s to every other vertex as well. so there is no better algorithm (in the worst case) for ﬁnding the shortest path to a single vertex than to ﬁnd shortest paths to all vertices. the algorithm described here will only compute the
figure 11.18 illustrates dijkstra’s algorithm. the start vertex is a. all vertices except a have an initial value of ∞. after processing vertex a, its neighbors have their d estimates updated to be the direct distance from a. after processing c (the closest vertex to a), vertices b and e are updated to reﬂect the shortest path through c. the remaining vertices are processed in order b, d, and e.
this section presents two algorithms for determining the minimum-cost spanning tree (mst) for a graph. the mst problem takes as input a connected, undirected graph g, where each edge has a distance or weight measure attached. the mst is the graph containing the vertices of g along with the subset of g’s edges that (1) has minimum total cost as measured by summing the values for all of the edges in the subset, and (2) keeps the vertices connected. applications where a solution to this problem is useful include soldering the shortest set of wires needed to connect a set of terminals on a circuit board, and connecting a set of cities by telephone lines in such a way as to require the least amount of cable.
the mst contains no cycles. if a proposed set of edges did have a cycle, a cheaper mst could be had by removing any one of the edges in the cycle. thus, the mst is a free tree with |v|−1 edges. the name “minimum-cost spanning tree” comes from the fact that the required set of edges forms a tree, it spans the vertices (i.e., it connects them together), and it has minimum cost. figure 11.19 shows the mst for an example graph.
11.10 show the shortest paths generated by running dijkstra’s shortest-paths algorithm on the graph of figure 11.25, beginning at vertex 4. show the d values as each vertex is processed, as in figure 11.18.
11.12 the root of a dag is a vertex r such that every vertex of the dag can be reached by a directed path from r. write an algorithm that takes a directed graph as input and determines the root (if there is one) for the graph. the running time of your algorithm should be Θ(|v| + |e|).
11.13 write an algorithm to ﬁnd the longest path in a dag, where the length of the path is measured by the number of edges that it contains. what is the asymptotic complexity of your algorithm? 11.14 write an algorithm to determine whether a directed graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v| + |e|) time. 11.15 write an algorithm to determine whether an undirected graph of |v| vertices contains a cycle. your algorithm should run in Θ(|v|) time.
11.16 the single-destination shortest-paths problem for a directed graph is to ﬁnd the shortest path from every vertex to a speciﬁed vertex v. write an algorithm to solve the single-destination shortest-paths problem.
11.17 list the order in which the edges of the graph in figure 11.25 are visited when running prim’s mst algorithm starting at vertex 3. show the ﬁnal mst.
11.18 list the order in which the edges of the graph in figure 11.25 are visited when running kruskal’s mst algorithm. each time an edge is added to the mst, show the result on the equivalence array, (e.g., show the array as in figure 6.7).
bilities of modern compilers to make extremely good optimizations of expressions. “optimization of expressions” here means a rearrangement of arithmetic or logical expressions to run more efﬁciently. be careful not to damage the compiler’s ability to do such optimizations for you in an effort to optimize the expression yourself. always check that your “optimizations” really do improve the program by running the program before and after the change on a suitable benchmark set of input. many times i have been wrong about the positive effects of code tuning in my own programs. most often i am wrong when i try to optimize an expression. it is hard to do better than the compiler.
this chapter has focused on asymptotic analysis. this is an analytic tool, whereby we model the key aspects of an algorithm to determine the growth rate of the algorithm as the input size grows. as pointed out previously, there are many limitations to this approach. these include the effects at small problem size, determining the ﬁner distinctions between algorithms with the same growth rate, and the inherent difﬁculty of doing mathematical modeling for more complex problems.
an alternative to analytical approaches are empirical approaches. the most obvious empirical approach is simply to run two competitors and see which performs better. in this way we might overcome the deﬁciencies of analytical approaches.
be warned that comparative timing of programs is a difﬁcult business, often subject to experimental errors arising from uncontrolled factors (system load, the language or compiler used, etc.). the most important point is not to be biased in favor of one of the programs. if you are biased, this is certain to be reﬂected in the timings. one look at competing software or hardware vendors’ advertisements should convince you of this. the most common pitfall when writing two programs to compare their performance is that one receives more code-tuning effort than the other. as mentioned in section 3.10, code tuning can often reduce running time by a factor of ten. if the running times for two programs differ by a constant factor regardless of input size (i.e., their growth rates are the same), then differences in code tuning might account for any difference in running time. be suspicious of empirical comparisons in this situation.
another approach to analysis is simulation. the idea of simulation is to model the problem with a computer program and then run it to get a result. in the context of algorithm analysis, simulation is distinct from empirical comparison of two competitors because the purpose of the simulation is to perform analysis that might otherwise be too difﬁcult. a good example of this appears in figure 9.8. this ﬁgure shows the cost for inserting or deleting a record from a hash table under two different assumptions for the policy used to ﬁnd a free slot in the table. the y axes is the cost in number of hash table slots evaluated, and the x axes is the percentage of slots in the table that are full. the mathematical equations for these curves can be determined, but this is not so easy. a reasonable alternative is to write simple variations on hashing. by timing the cost of the program for various loading conditions, it is not difﬁcult to construct a plot similar to figure 9.8. the purpose of this analysis is not to determine which approach to hashing is most efﬁcient, so we are not doing empirical comparison of hashing alternatives. instead, the purpose is to analyze the proper loading factor that would be used in an efﬁcient hashing system to balance time cost versus hash table size (space cost).
pioneering works on algorithm analysis include the art of computer programming by donald e. knuth [knu97, knu98], and the design and analysis of computer algorithms by aho, hopcroft, and ullman [ahu74]. the alternate deﬁnition for Ω comes from [ahu83]. the use of the notation “t(n) is in o(f(n))” rather than the more commonly used “t(n) = o(f(n))” i derive from brassard and bratley [bb96], though certainly this use predates them. a good book to read for further information on algorithm analysis techniques is compared to what? by gregory j.e. rawlins [raw92].
bentley [ben88] describes one problem in numerical analysis for which, between 1945 and 1988, the complexity of the best known algorithm had decreased from o(n7) to o(n3). for a problem of size n = 64, this is roughly equivalent to the speedup achieved from all advances in computer hardware during the same time period.
while the most important aspect of program efﬁciency is the algorithm, much improvement can be gained from efﬁcient coding of a program. as cited by frederick p. brooks in the mythical man-month [bro95], an efﬁcient programmer can often produce programs that run ﬁve times faster than an inefﬁcient programmer, even when neither takes special efforts to speed up their code. for excellent and enjoyable essays on improving your coding efﬁciency, and ways to speed up your code when it really matters, see the books by jon bentley [ben82, ben00, ben88]. the
this chapter talks about some fundamental patterns of algorithms. we discuss greed algorithms, dynamic programming, randomized algorithms, numerical algorithms, and the concept of a transform.
section 16.3.1 presents the skip list, a probabilistic data structure that can be used to implement the dictionary adt. the skip list is comparable in complexity to the bst, yet often outperforms the bst, because the skip list’s efﬁciency is not tied to the values of the dataset being stored.
observe that the following are all greedy algorithms (that work!): kruskal’s mst, prim’s mst, dijkstra’s shortest paths, huffman’s coding algorithm. various greedy knapsack algorithms, such as continuous-knapsack problem (see johnsonbaugh and schaefer, sec 7.6).
see the treatment by kleinberg & tardos. consider that a heap has a “greedy” deﬁnition: the value of any node a is bigger than its children. the bst’s deﬁnition is that the value of any node a is greater than all nodes in the left subtree, and less than all nodes in the right subtree. if we try a greedy deﬁnition (a is greater than its left child and less than its right child), we can get a tree that meets this deﬁnition but is not a bst. see the example in section 5.2.
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
set to be update[i]->forward[i], and the nodes stored in update[i] for indices 0 through 2 have their forward[i] pointers changed to point to the new node. this “splices” the new node into the skip list at all levels.
the remove function is left as an exercise. it is similar to inserting in that the update array is built as part of searching for the record to be deleted; then those nodes speciﬁed by the update array have their forward pointers adjusted to point around the node being deleted.
a newly inserted node could have a high level generated by randomlevel, or a low level. it is possible that many nodes in the skip list could have many pointers, leading to unnecessary insert cost and yielding poor (i.e., Θ(n)) performance during search, because not many nodes will be skipped. conversely, too many nodes could have a low level. in the worst case, all nodes could be at level 0, equivalent to a regular linked list. if so, search will again require Θ(n) time. however, the probability that performance will be poor is quite low. there is only once chance in 1024 that ten nodes in a row will be at level 0. the motto of probabilistic data structures such as the skip list is “don’t worry, be happy.” we simply accept the results of randomlevel and expect that probability will eventually work in our favor. the advantage of this approach is that the algorithms are simple, while requiring only Θ(log n) time for all operations in the average case.
in practice, the skip list will probably have better performance than a bst. the bst can have bad performance caused by the order in which data are inserted. for example, if n nodes are inserted into a bst in ascending order of their key value, then the bst will look like a linked list with the deepest node at depth n − 1. the skip list’s performance does not depend on the order in which values are inserted into the list. as the number of nodes in the skip list increases, the probability of encountering the worst case decreases geometrically. thus, the skip list illustrates a tension between the theoretical worst case (in this case, Θ(n) for a skip list operation), and a rapidly increasing probability of average-case performance of Θ(log n), that characterizes probabilistic data structures.
• raise a number to a power. • find common factors for two numbers. • tell whether a number is prime. • generate a random integer. • multiply two integers.
this just does the transform on one of the two polynomials. the full process is: 1. transform each polynomial. 2. multiply the resulting values (o(n) multiplies). 3. do the inverse transformation on the result.
16.4 the implementation for floyd’s algorithm given in section 16.2.2 is inefﬁcient for adjacency lists because the edges are visited in a bad order when initializing array d. what is the cost of of this initialization step for the adjacency list? how can this initialization step be revised so that it costs Θ(|v|2) in the worst case?
16.6 show the skip list that results from inserting the following values. draw the skip list after each insert. with each value, assume the depth of its corresponding node is as given in the list.
16.7 if we had a linked list that would never be modiﬁed, we can use a simpler approach than the skip list to speed access. the concept would remain the same in that we add additional pointers to list nodes for efﬁcient access to the ith element. how can we add a second pointer to each element of a singly linked list to allow access to an arbitrary element in o(log n) time?
16.8 what is the expected (average) number of pointers for a skip list node? 16.9 write a function to remove a node with given value from a skip list. 16.10 write a function to ﬁnd the ith node on a skip list.
16.2 implement both a standard Θ(n3) matrix multiplication algorithm and strassen’s matrix multiplication algorithm (see exercise 14.16.4.3). using empirical testing, try to estimate the constant factors for the runtime equations of the two algorithms. how big must n be before strassen’s algorithm becomes more efﬁcient than the standard algorithm?
in this book, nearly all logarithms used have a base of two. this is because data structures and algorithms most often divide things in half, or store codes with binary bits. whenever you see the notation log n in this book, either log2 n is meant or else the term is being used asymptotically and the actual base does not matter. if any base for the logarithm other than two is intended, then the base will be shown explicitly.
1. log(nm) = log n + log m. 2. log(n/m) = log n − log m. 3. log(nr) = r log n. 4. loga n = logb n/ logb a. the ﬁrst two properties state that the logarithm of two numbers multiplied (or divided) can be found by adding (or subtracting) the logarithms of the two numbers.4 property (3) is simply an extension of property (1). property (4) tells us that, for variable n and any two integer constants a and b, loga n and logb n differ by the constant factor logb a, regardless of the value of n. most runtime analyses in this book are of a type that ignores constant factors in costs. property (4) says that such analyses need not be concerned with the base of the logarithm, because this can change the total cost only by a constant factor. note that 2log n = n.
when discussing logarithms, exponents often lead to confusion. property (3) tells us that log n2 = 2 log n. how do we indicate the square of the logarithm (as opposed to the logarithm of n2)? this could be written as (log n)2, but it is traditional to use log2 n. on the other hand, we might want to take the logarithm of the logarithm of n. this is written log log n.
a special notation is used in the rare case where we would like to know how many times we must take the log of a number before we reach a value ≤ 1. this quantity is written log∗ n. for example, log∗ 1024 = 4 because log 1024 = 10, log 10 ≈ 3.33, log 3.33 ≈ 1.74, and log 1.74 < 1, which is a total of 4 log operations.
4these properties are the idea behind the slide rule. adding two numbers can be viewed as joining two lengths together and measuring their combined length. multiplication is not so easily done. however, if the numbers are ﬁrst converted to the lengths of their logarithms, then those lengths can be added and the inverse logarithm of the resulting length gives the answer for the multiplication (this is simply logarithm property (1)). a slide rule measures the length of the logarithm for the numbers, lets you slide bars representing these lengths to add up the total length, and ﬁnally converts this total length to the correct numeric answer by taking the inverse of the logarithm for the result.
thus, if taking logarithms and anti-logarithms were cheap, then we could reduce multiplication to addition by taking the log of the two operands, adding, and then taking the anti-log of the sum.
under normal circumstances, taking logarithms and anti-logarithms is expensive, and so this reduction would not be considered practical. however, this reduction is precisely the basis for the slide rule. the slide rule uses a logarithmic scale to measure the lengths of two numbers, in effect doing the conversion to logarithms automatically. these two lengths are then added together, and the inverse logarithm of the sum is read off another logarithmic scale. the part normally considered expensive (taking logarithms and anti-logarithms) is cheap because it is a physical part of the slide rule. thus, the entire multiplication process can be done cheaply via a reduction to addition.
what if taking logs and antilogs were easy? the slide rule does exactly this! it is essentially two rulers in log scale. slide the scales to add the lengths of the two numbers (in log form). the third scale shows the value for the total length. uniquely represent a polynomial of degree n − 1
alternatively, a polynomial can be uniquely represented by a list of its values at n distinct points. finding the value for a polynomial at a given point is called evaluation. finding the coefﬁcients for the polynomial given the values at n points is called interpolation. to multiply two n − 1-degree polynomials a and b normally takes Θ(n2) coefﬁcient multiplications. however, if we evaluate both polynomials (at the same points), we can simply multiply the corresponding pairs of values to get the corresponding values for polynomial ab. process:
algorithms: an algorithm is a method or a process followed to solve a problem. if the problem is viewed as a function, then an algorithm is an implementation for the function that transforms an input to the corresponding output. a problem can be solved by many different algorithms. a given algorithm solves only one problem (i.e., computes a particular function). this book covers many problems, and for several of these problems i present more than one algorithm. for the important problem of sorting i present nearly a dozen algorithms!
the advantage of knowing several solutions to a problem is that solution a might be more efﬁcient than solution b for a speciﬁc variation of the problem, or for a speciﬁc class of inputs to the problem, while solution b might be more efﬁcient than a for another variation or class of inputs. for example, one sorting algorithm might be the best for sorting a small collection of integers, another might be the best for sorting a large collection of integers, and a third might be the best for sorting a collection of variable-length strings.
by deﬁnition, an algorithm possesses several properties. something can only be called an algorithm to solve a particular problem if it has all of the following properties.
1. it must be correct. in other words, it must compute the desired function, converting each input to the correct output. note that every algorithm implements some function because every algorithm maps every input to some output (even if that output is a system crash). at issue here is whether a given algorithm implements the intended function.
2. it is composed of a series of concrete steps. concrete means that the action described by that step is completely understood — and doable — by the person or machine that must perform the algorithm. each step must also be doable in a ﬁnite amount of time. thus, the algorithm gives us a “recipe” for solving the problem by performing a series of steps, where each such step is within our capacity to perform. the ability to perform a step can depend on who or what is intended to execute the recipe. for example, the steps of a cookie recipe in a cookbook might be considered sufﬁciently concrete for instructing a human cook, but not for programming an automated cookiemaking factory.
3. there can be no ambiguity as to which step will be performed next. often it is the next step of the algorithm description. selection (e.g., the if statements in java) is normally a part of any language for describing algorithms. selection allows a choice for which step will be performed next, but the selection process is unambiguous at the time when the choice is made.
1.4 deﬁne an adt for a list of integers. first, decide what functionality your adt should provide. example 1.4 should give you some ideas. then, specify your adt in java in the form of an abstract class declaration, showing the functions, their parameters, and their return types.
1.5 brieﬂy describe how integer variables are typically represented on a computer. (look up one’s complement and two’s complement arithmetic in an introductory computer science textbook if you are not familiar with these.) why does this representation for integers qualify as a data structure as deﬁned in section 1.2?
1.6 deﬁne an adt for a two-dimensional array of integers. specify precisely the basic operations that can be performed on such arrays. next, imagine an application that stores an array with 1000 rows and 1000 columns, where less than 10,000 of the array values are non-zero. describe two different implementations for such arrays that would be more space efﬁcient than a standard two-dimensional array implementation requiring one million positions.
1.7 you have been assigned to implement a sorting program. the goal is to make this program general purpose, in that you don’t want to deﬁne in advance what record or key types are used. describe ways to generalize a simple sorting algorithm (such as insertion sort, or any other sort you are familiar with) to support this generalization.
1.8 you have been assigned to implement a simple seqential search on an array. the problem is that you want the search to be as general as possible. this means that you need to support arbitrary record and key types. describe ways to generalize the search function to support this goal. consider the possibility that the function will be used multiple times in the same program, on differing record types. consider the possibility that the function will need to be used on different keys (possibly with the same or different types) of the same record. for example, a student data record might be searched by zip code, by name, by salary, or by gpa.
1.9 does every problem have an algorithm? 1.10 does every algorithm have a java program? 1.11 consider the design for a spelling checker program meant to run on a home computer. the spelling checker should be able to handle quickly a document of less than twenty pages. assume that the spelling checker comes with a dictionary of about 20,000 words. what primitive operations must be implemented on the dictionary, and what is a reasonable time constraint for each operation?
1.18 imagine that you are a programmer who must write a function to sort an array of about 1000 integers from lowest value to highest value. write down at least ﬁve approaches to sorting the array. do not write algorithms in java or pseudocode. just write a sentence or two for each approach to describe how it would work.
1.19 think of an algorithm to ﬁnd the maximum value in an (unsorted) array. now, think of an algorithm to ﬁnd the second largest value in the array. which is harder to implement? which takes more time to run (as measured by the number of comparisons performed)? now, think of an algorithm to ﬁnd the third largest value. finally, think of an algorithm to ﬁnd the middle value. which is the most difﬁcult of these problems to solve?
1.20 an unsorted list of integers allows for constant-time insert simply by adding a new integer at the end of the list. unfortunately, searching for the integer with key value x requires a sequential search through the unsorted list until you ﬁnd x, which on average requires looking at half the list. on the other hand, a sorted array-based list of n integers can be searched in log n time by using a binary search. unfortunately, inserting a new integer requires a lot of time because many integers might be shifted in the array if we want to keep it sorted. how might data be organized to support both insertion and search in log n time?
compiled with the same compiler and run on the same computer under the same conditions. as much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efﬁcient.” in this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally.
if you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. ideally we would measure the running time of the algorithm under standard benchmark conditions. however, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. the only alternative is to use some other measure as a surrogate for running time.
of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. the terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. size is often the number of inputs processed. for example, when comparing sorting algorithms, the size of the problem is typically measured by the number of records to be sorted. a basic operation must have the property that its time to complete does not depend on the particular values of its operands. adding or comparing two integer variables are examples of basic operations in most programming languages. summing the contents of an array containing n integers is not, because the cost depends on the value of n (i.e., the size of the input).
example 3.1 consider a simple algorithm to solve the problem of ﬁnding the largest value in an array of n integers. the algorithm looks at each integer in turn, saving the position of the largest value seen so far. this algorithm is called the largest-value sequential search and is illustrated by the following java function:
int currlarge = 0; // holds largest element position for (int i=1; i<a.length; i++) // for each element // if a[i] is larger // remember its position // return largest position
here, the size of the problem is n, the number of integers stored in a. the basic operation is to compare an integer’s value to that of the largest value
n values. if we implement sequential search as a program and run it many times on many different arrays of size n, or search for many different values of k within the same array, we expect the algorithm on average to go halfway through the array before ﬁnding the value we seek. on average, the algorithm examines about n/2 values. we call this the average case for this algorithm.
when analyzing an algorithm, should we study the best, worst, or average case? normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. in other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. however, there are rare instances where a best-case analysis is useful — in particular, when the best case has high probability of occurring. in chapter 7 you will see some examples where taking advantage of the best-case running time for one sorting algorithm makes a second more efﬁcient.
how about the worst case? the advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. this is especially important for real-time applications, such as for the computers that monitor an air trafﬁc control system. here, it would not be acceptable to use an algorithm that can handle n airplanes quickly enough most of the time, but which fails to perform quickly enough when all n airplanes are coming from the same direction.
for other applications — particularly when we wish to aggregate the cost of running the program many times on many different inputs — worst-case analysis might not be a representative measure of the algorithm’s performance. often we prefer to know the average-case running time. this means that we would like to know the typical behavior of the algorithm on inputs of size n. unfortunately, average-case analysis is not always possible. average-case analysis ﬁrst requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. for example, it was stated previously that the sequential search algorithm on average examines half of the array values. this is only true if the element with value k is equally likely to appear in any position in the array. if this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. see section 9.2 for further discussion regarding the effects of data distribution on the sequential search algorithm.
the characteristics of a data distribution have a signiﬁcant effect on many search algorithms, such as those based on hashing (section 9.4) and search trees (e.g., see section 5.4). incorrect assumptions about data distribution can have dis-
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
properly used to describe our understanding of a problem or an algorithm, it is best to consider an example where you do not already know a lot about the problem.
let us look ahead to analyzing the problem of sorting to see how this process works. what is the least possible cost for any sorting algorithm in the worst case? the algorithm must at least look at every element in the input, just to determine that the input is truly sorted. it is also possible that each of the n values must be moved to another location in the sorted output. thus, any sorting algorithm must take at least cn time. for many problems, this observation that each of the n inputs must be looked at leads to an easy Ω(n) lower bound.
in your previous study of computer science, you have probably seen an example of a sorting algorithm whose running time is in o(n2) in the worst case. the simple bubble sort and insertion sort algorithms typically given as examples in a ﬁrst year programming course have worst case running times in o(n2). thus, the problem of sorting can be said to have an upper bound in o(n2). how do we close the gap between Ω(n) and o(n2)? can there be a better sorting algorithm? if you can think of no algorithm whose worst-case growth rate is better than o(n2), and if you have discovered no analysis technique to show that the least cost for the problem of sorting in the worst case is greater than Ω(n), then you cannot know for sure whether or not there is a better algorithm.
chapter 7 presents sorting algorithms whose running time is in o(n log n) for the worst case. this greatly narrows the gap. witht his new knowledge, we now have a lower bound in Ω(n) and an upper bound in o(n log n). should we search for a faster algorithm? many have tried, without success. fortunately (or perhaps unfortunately?), chapter 7 also includes a proof that any sorting algorithm must have running time in Ω(n log n) in the worst case.2 this proof is one of the most important results in the ﬁeld of algorithm analysis, and it means that no sorting algorithm can possibly run faster than cn log n for the worst-case input of size n. thus, we can conclude that the problem of sorting is Θ(n log n) in the worst case, because the upper and lower bounds have met.
knowing the lower bound for a problem does not give you a good algorithm. but it does help you to know when to stop looking. if the lower bound for the problem matches the upper bound for the algorithm (within a constant factor), then we know that we can ﬁnd an algorithm that is better only by a constant factor.
position i along the x axis). this is why we must always say that function f(n) is in o(g(n)) in the best, average, or worst case! if we leave off which class of inputs we are discussing, we cannot know which cost measure we are referring to for most algorithms.
sometimes the proper analysis for an algorithm requires multiple parameters to describe the cost. to illustrate the concept, consider an algorithm to compute the rank ordering for counts of all pixel values in a picture. pictures are often represented by a two-dimensional array, and a pixel is one cell in the array. the value of a pixel is either the code value for the color, or a value for the intensity of the picture at that pixel. assume that each pixel can take any integer value in the range 0 to c − 1. the problem is to ﬁnd the number of pixels of each color value and then sort the color values with respect to the number of times each value appears in the picture. assume that the picture is a rectangle with p pixels. a pseudocode algorithm to solve the problem follows.
in this example, count is an array of size c that stores the number of pixels for each color value. function value(i) returns the color value for pixel i.
the time for the ﬁrst for loop (which initializes count) is based on the number of colors, c. the time for the second loop (which determines the number of pixels with each color) is Θ(p ). the time for the ﬁnal line, the call to sort, depends on the cost of the sorting algorithm used. from the discussion of section 3.6, we can assume that the sorting algorithm has cost Θ(p log p ) if p items are sorted, thus yielding Θ(p log p ) as the total algorithm cost.
is this a good representation for the cost of this algorithm? what is actually being sorted? it is not the pixels, but rather the colors. what if c is much smaller than p ? then the estimate of Θ(p log p ) is pessimistic, because much fewer than p items are being sorted. instead, we should use p as our analysis variable for steps that look at each pixel, and c as our analysis variable for steps that look at colors. then we get Θ(c) for the initialization loop, Θ(p ) for the pixel count loop, and Θ(c log c) for the sorting operation. this yields a total cost of Θ(p + c log c).
we sort many things in our everyday lives: a handful of cards when playing bridge; bills and other piles of paper; jars of spices; and so on. and we have many intuitive strategies that we can use to do the sorting, depending on how many objects we have to sort and how hard they are to move around. sorting is also one of the most frequently performed computing tasks. we might sort the records in a database so that we can search the collection efﬁciently. we might sort the records by zip code so that we can print and mail them more cheaply. we might use sorting as an intrinsic part of an algorithm to solve some other problem, such as when computing the minimum-cost spanning tree (see section 11.5).
because sorting is so important, naturally it has been studied intensively and many algorithms have been devised. some of these algorithms are straightforward adaptations of schemes we use in everyday life. others are totally alien to how humans do things, having been invented to sort thousands or even millions of records stored on the computer. after years of study, there are still unsolved problems related to sorting. new algorithms are still being developed and reﬁned for specialpurpose applications.
while introducing this central problem in computer science, this chapter has a secondary purpose of illustrating many important issues in algorithm design and analysis. the collection of sorting algorithms presented will illustate that divideand-conquer is a powerful approach to solving a problem, and that there are multiple ways to do the dividing. mergesort divides a list in half. quicksort divides a list into big values and small values. and radix sort divides the problem by working on one digit of the key at a time.
sorting algorithms will be used to illustrate a wide variety of analysis techniques in this chapter. we’ll ﬁnd that it is possible for an algorithm to have an average case whose growth rate is signiﬁcantly smaller than its worse case (quicksort). we’ll see how it is possible to speed up sorting algorithms (both shellsort
7.7 it has been proposed that heapsort can be optimized by altering the heap’s siftdown function. call the value being sifted down x. siftdown does two comparisons per level: first the children of x are compared, then the winner is compared to x. if x is too small, it is swapped with its larger child and the process repeated. the proposed optimization dispenses with the test against x. instead, the larger child automatically replaces x, until x reaches the bottom level of the heap. at this point, x might be too large to remain in that position. this is corrected by repeatedly comparing x with its parent and swapping as necessary to “bubble” it up to its proper level. the claim is that this process will save a number of comparisons because most nodes when sifted down end up near the bottom of the tree anyway. implement both versions of siftdown, and do an empirical study to compare their running times.
let pi be the probability that k is in position i of l. when k is not in l, sequential search will require n comparisons. let p0 be the probability that k is not in l. then the average cost t(n) will be
for large collections of records that are searched repeatedly, sequential search is unacceptably slow. one way to reduce search time is to preprocess the records by sorting them. given a sorted array, an obvious improvement over simple linear search is to test if the current element in l is greater than k. if it is, then we know that k cannot appear later in the array, and we can quit the search early. but this still does not improve the worst-case cost of the algorithm.
we can also observe that if we look ﬁrst at position 1 in sorted array l and ﬁnd that k is bigger, then we rule out positions 0 as well as position 1. because more is often better, what if we look at position 2 in l and ﬁnd that k is bigger yet? this rules out positions 0, 1, and 2 with one comparison. what if we carry this to the extreme and look ﬁrst at the last position in l and ﬁnd that k is bigger? then we know in one comparison that k is not in l. this is very useful to know, but what is wrong with this approach? while we learn a lot sometimes (in one comparison we might learn that k is not in the list), usually we learn only a little bit (that the last element is not k).
the question then becomes: what is the right amount to jump? this leads us to an algorithm known as jump search. for some value j, we check every j’th
while self-organizing lists do not generally perform as well as search trees or a sorted list, both of which require o(log n) search time, there are many situations in which self-organizing lists prove a valuable tool. obviously they have an advantage over sorted lists in that they need not be sorted. this means that the cost to insert a new record is low, which could more than make up for the higher search cost when insertions are frequent. self-organizing lists are simpler to implement than search trees and are likely to be more efﬁcient for small lists. nor do they require additional space. finally, in the case of an application where sequential search is “almost” fast enough, changing an unsorted list to a self-organizing list might speed the application enough at a minor cost in additional code.
as an example of applying self-organizing lists, consider an algorithm for compressing and transmitting messages. the list is self-organized by the move-to-front rule. transmission is in the form of words and numbers, by the following rules:
both the sender and the receiver keep track of the position of words in the list in the same way (using the move-to-front rule), so they agree on the meaning of the numbers that encode repeated occurrences of words. for example, consider the following example message to be transmitted (for simplicity, ignore case in letters).
the ﬁrst three words have not been seen before, so they must be sent as full words. the fourth word is the second appearance of “the,” which at this point is the third word in the list. thus, we only need to transmit the position value “3.” the next two words have not yet been seen, so must be sent as full words. the seventh word is the third appearance of “the,” which coincidentally is again in the third position. the eighth word is the second appearance of “car,” which is now in the ﬁfth position of the list. “i” is a new word, and the last word “left” is now in the ﬁfth position. thus the entire transmission would be
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
algorithm is said to be stable if it does not change the relative ordering of records with identical key values. many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes.
when comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. an example of such timings is presented in figure 7.13. however, such a comparison can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. in particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms.
when analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. this measure is usually closely related to the running time for the algorithm and has the advantage of being machine and datatype independent. however, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. if so, it might be appropriate to measure the number of swap operations performed by the algorithm. in most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. some special situations “change the rules” for comparing sorting algorithms. for example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. some applications require that a small number of records be sorted, but that the sort be performed frequently. an example would be an application that repeatedly sorts groups of ﬁve numbers. in such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. finally, some situations require that a sorting algorithm use as little memory as possible. we will note which sorting algorithms require signiﬁcant extra memory beyond the input array.
this section presents three simple sorting algorithms. while easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort. nonetheless, there are situations where one of these simple algorithms is the best tool for the job.
replaces several (potentially) sequential passes with a single random access pass. if the processing would not be sequential anyway (such as when all processing is on a single disk drive), no time is lost by doing so.
multiway merging can greatly reduce the number of passes required. if there is room in memory to store one block for each run, then all runs can be merged in a single pass. thus, replacement selection can build initial runs in one pass, and multiway merging can merge all runs in one pass, yielding a total cost of two passes. however, for truly large ﬁles, there might be too many runs for each to get a block in memory. if there is room to allocate b blocks for a b-way merge, and the number of runs r is greater than b, then it will be necessary to do multiple merge passes. in other words, the ﬁrst b runs are merged, then the next b, and so on. these super-runs are then merged by subsequent passes, b super-runs at a time.
how big a ﬁle can be merged in one pass? assuming b blocks were allocated to the heap for replacement selection (resulting in runs of average length 2b blocks), followed by a b-way merge, we can process on average a ﬁle of size 2b2 blocks in a single multiway merge. 2bk+1 blocks on average can be processed in k bway merges. to gain some appreciation for how quickly this grows, assume that we have available 0.5mb of working memory, and that a block is 4kb, yielding 128 blocks in working memory. the average run size is 1mb (twice the working memory size). in one pass, 128 runs can be merged. thus, a ﬁle of size 128mb can, on average, be processed in two passes (one to build the runs, one to do the merge) with only 0.5mb of working memory. a larger block size would reduce the size of the ﬁle that can be processed in one merge pass for a ﬁxed-size working memory; a smaller block size or larger working memory would increase the ﬁle size that can be processed in one merge pass. with 0.5mb of working memory and 4kb blocks, a ﬁle of size 16 gigabytes could be processed in two merge passes, which is big enough for most applications. thus, this is a very effective algorithm for single disk drive external sorting.
figure 8.11 shows a comparison of the running time to sort various-sized ﬁles for the following implementations: (1) standard mergesort with two input runs and two output runs, (2) two-way mergesort with large initial runs (limited by the size of available memory), and (3) r-way mergesort performed after generating large initial runs. in each case, the ﬁle was composed of a series of four-byte records (a two-byte key and a two-byte data value), or 256k records per megabyte of ﬁle size. we can see from this table that using even a modest memory size (two blocks) to create initial runs results in a tremendous savings in time. doing 4-way merges of the runs provides another considerable speedup, however largescale multi-way
figure 8.11 a comparison of three external sorts on a collection of small records for ﬁles of various sizes. each entry in the table shows time in seconds and total number of blocks read and written by the program. file sizes are in megabytes. for the third sorting algorithm, on ﬁle size of 4mb, the time and blocks shown in the last column are for a 32-way merge. 32 is used instead of 16 because 32 is a root of the number of blocks in the ﬁle (while 16 is not), thus allowing the same number of runs to be merged at every pass.
merges for r beyond about 4 or 8 runs does not help much because a lot of time is spent determining which is the next smallest element among the r runs.
we see from this experiment that building large initial runs reduces the running time to slightly more than one third that of standard mergesort, depending on ﬁle and memory sizes. using a multiway merge further cuts the time nearly in half. in summary, a good external sorting algorithm will seek to do the following: • make the initial runs as long as possible. • at all stages, overlap input, processing, and output as much as possible. • use as much working memory as possible. applying more memory usually speeds processing. in fact, more memory will have a greater effect than a faster disk. a faster cpu is unlikely to yield much improvement in running time for external sorting, because disk i/o speed is the limiting factor.
a good general text on ﬁle processing is folk and zoellig’s file structures: a conceptual toolkit [fz98]. a somewhat more advanced discussion on key issues in ﬁle processing is betty salzberg’s file structures: an analytical approach [sal88].
figure 7.5 summarizes the cost of insertion, bubble, and selection sort in terms of their required number of comparisons and swaps1 in the best, average, and worst cases. the running time for each of these sorts is Θ(n2) in the average and worst cases.
the remaining sorting algorithms presented in this chapter are signiﬁcantly better than these three under typical conditions. but before continuing on, it is instructive to investigate what makes these three sorts so slow. the crucial bottleneck is that only adjacent records are compared. thus, comparisons and moves (in all but selection sort) are by single steps. swapping adjacent records is called an exchange. thus, these sorts are sometimes referred to as exchange sorts. the cost of any exchange sort can be at best the total number of steps that the records in the array must move to reach their “correct” location (i.e., the number of inversions for each record). what is the average number of inversions? consider a list l containing n values. deﬁne lr to be l in reverse. l has n(n−1)/2 distinct pairs of values, each of which could potentially be an inversion. each such pair must either be an inversion in l or in lr. thus, the total number of inversions in l and lr together is exactly n(n− 1)/2 for an average of n(n− 1)/4 per list. we therefore know with certainty that any sorting algorithm which limits comparisons to adjacent items will cost at least n(n − 1)/4 = Ω(n2) in the average case.
1there is a slight anomaly with selection sort. the supposed advantage for selection sort is its low number of swaps required, yet selection sort’s best-case number of swaps is worse than that for insertion sort or bubble sort. this is because the implementation given for selection sort does not avoid a swap in the case where record i is already in position i. the reason is that it usually takes more time to repeatedly check for this situation than would be saved by avoiding such swaps.
this section presents a simple, compact implementation for complete binary trees. recall that complete binary trees have all levels except the bottom ﬁlled out completely, and the bottom level has all of its nodes ﬁlled in from left to right. thus, a complete binary tree of n nodes has only one possible shape. you might think that a complete binary tree is such an unusual occurrence that there is no reason to develop a special implementation for it. however, the complete binary tree has practical uses, the most important being the heap data structure discussed in section 5.5. heaps are often used to implement priority queues (section 5.5) and for external sorting algorithms (section 8.5.2).
we begin by assigning numbers to the node positions in the complete binary tree, level by level, from left to right as shown in figure 5.12(a). an array can store the tree’s data values efﬁciently, placing each data value in the array position corresponding to that node’s position within the tree. figure 5.12(b) lists the array indices for the children, parent, and siblings of each node in figure 5.12(a). from figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives within the array. simple formulae can be derived for calculating the array index for each relative of a node r from r’s index. no explicit pointers are necessary to reach a node’s left or right child. this means there is no overhead to the array implementation if the array is selected to be of size n for a tree of n nodes.
the formulae for calculating the array indices of the various relatives of a node are as follows. the total number of nodes in the tree is n. the index of the node in question is r, which must fall in the range 0 to n − 1.
• parent(r) = b(r − 1)/2c if r 6= 0. • left child(r) = 2r + 1 if 2r + 1 < n. • right child(r) = 2r + 2 if 2r + 2 < n. • left sibling(r) = r − 1 if r is even. • right sibling(r) = r + 1 if r is odd and r + 1 < n.
section 4.4 presented the dictionary adt, along with dictionary implementations based on sorted and unsorted lists. when implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. however, searching an unsorted list for a particular record requires Θ(n) time in the average case. for a large database, this is probably much too slow. alternatively, the records can be stored in a sorted list. if the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. on the other hand, if we use a sorted
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
and fast. the algorithm should take advantage of the fact that sorting is a specialpurpose application in that all of the values to be stored are available at the start. this means that we do not necessarily need to insert one value at a time into the tree structure.
heapsort is based on the heap data structure presented in section 5.5. heapsort has all of the advantages just listed. the complete binary tree is balanced, its array representation is space efﬁcient, and we can load all values into the tree at once, taking advantage of the efﬁcient buildheap function. the asymptotic performance of heapsort is Θ(n log n) in the best, average, and worst cases. it is not as fast as quicksort in the average case (by a constant factor), but heapsort has special properties that will make it particularly useful when sorting data sets too large to ﬁt in main memory, as discussed in chapter 8.
a sorting algorithm based on max-heaps is quite straightforward. first we use the heap building algorithm of section 5.5 to convert the array into max-heap order. then we repeatedly remove the maximum value from the heap, restoring the heap property each time that we do so, until the heap is empty. note that each time we remove the maximum element from the heap, it is placed at the end of the array. assume the n elements are stored in array positions 0 through n−1. after removing the maximum value from the heap and readjusting, the maximum value will now be placed in position n − 1 of the array. the heap is now considered to be of size n − 1. removing the new maximum (root) value places the second largest value in position n − 2 of the array. at the end of the process, the array will be properly sorted from least to greatest. this is why heapsort uses a max-heap rather than a min-heap as might have been expected. figure 7.10 illustrates heapsort. the complete java implementation is as follows:
because building the heap takes Θ(n) time (see section 5.5), and because n deletions of the maximum element each take Θ(log n) time, we see that the entire heapsort operation takes Θ(n log n) time in the worst, average, and best cases. while typically slower than quicksort by a constant factor, heapsort has one special advantage over the other sorts studied so far. building the heap is relatively cheap, requiring Θ(n) time. removing the maximum element from the heap requires Θ(log n) time. thus, if we wish to ﬁnd the k largest elements in an array, we can do so in time Θ(n+ k log n). if k is small, this is a substantial improvement
earlier chapters presented basic data structures and algorithms that operate on data stored in main memory. some applications require that large amounts of information be stored and processed — so much information that it cannot all ﬁt into main memory. in that case, the information must reside on disk and be brought into main memory selectively for processing.
you probably already realize that main memory access is much faster than access to data stored on disk or other storage devices. the relative difference in access times is so great that efﬁcient disk-based programs require a different approach to algorithm design than most programmers are used to. as a result, many programmers do a poor job when it comes to ﬁle processing applications.
this chapter presents the fundamental issues relating to the design of algorithms and data structures for disk-based applications.1 we begin with a description of the signiﬁcant differences between primary memory and secondary storage. section 8.2 discusses the physical aspects of disk drives. section 8.3 presents basic methods for managing buffer pools. section 8.4 discusses the java model for random access to data stored on disk. section 8.5 discusses the basic principles for sorting collections of records too large to ﬁt in main memory.
1computer technology changes rapidly. i provide examples of disk drive speciﬁcations and other hardware performance numbers that are reasonably up to date as of the time when the book was written. when you read it, the numbers might seem out of date. however, the basic principles do not change. the approximate ratios for time, space, and cost between memory and disk have remained surprisingly steady for over 20 years.
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
8.3 implement a disk-based buffer pool class based on the lru buffer pool replacement strategy. disk blocks are numbered consecutively from the beginning of the ﬁle with the ﬁrst block numbered as 0. assume that blocks are 4096 bytes in size, with the ﬁrst 4 bytes used to store the block id corresponding to that buffer. use the ﬁrst bufferpool abstract class given in section 8.3 as the basis for your implementation.
8.4 implement an external sort based on replacement selection and multiway merging as described in this chapter. test your program both on ﬁles with small records and on ﬁles with large records. for what size record do you ﬁnd that key sorting would be worthwhile?
8.5 implement a quicksort for large ﬁles on disk by replacing all array access in the normal quicksort application with access to a virtual array implemented using a buffer pool. that is, whenever a record in the array would be read or written by quicksort, use a call to a buffer pool function instead. compare the running time of this implementation with implementations for external sorting based on mergesort as described in this chapter.
8.6 section 8.5.1 suggests that an easy modiﬁcation to the basic 2-way mergesort is to read in a large chunk of data into main memory, sort it with quicksort, and write it out for initial runs. then, a standard 2-way merge is used in a series of passes to merge the runs together. however, this makes use of only two blocks of working memory at a time. each block read is essentially random access, because the various ﬁles are read in an unknown order, even though each of the input and output ﬁles is processed sequentially on each pass. a possible improvement would be, on the merge passes, to divide working memory into four equal sections. one section is allocated to each of the two input ﬁles and two output ﬁles. all reads during merge passes would be in full sections, rather than single blocks. while the total number of blocks read and written would be the same as a regular 2-way mergsort, it is possible that this would speed processing because a series of blocks that are logically adjacent in the various input and output ﬁles would be read/written each time. implement this variation, and compare its running time against a standard series of 2-way merge passes that read/write only a single block at a time. before beginning implementation, write down your hypothesis on how the running time will be affected by this change. after implementing, did you ﬁnd that this change has any meaningful effect on performance?
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
arrays, optimized quicksort performs well because it does one partition step before calling insertion sort. compared to the other o(n log n) sorts, unoptimized heapsort is quite slow due to the overhead of the class structure. when all of this is stripped away and the algorithm is implemented to manipulate an array directly, it is still somewhat slower than mergesort. in general, optimizating the various algorithms makes a noticible improvement for larger array sizes.
overall, radix sort is a surprisingly poor performer. if the code had been tuned to use bit shifting of the key value, it would likely improve substantially; but this would seriously limit the range of element types that the sort could support.
this book contains many analyses for algorithms. these analyses generally deﬁne the upper and lower bounds for algorithms in their worst and average cases. for most of the algorithms presented so far, analysis is easy. this section considers a more difﬁcult task — an analysis for the cost of a problem as opposed to an algorithm. the upper bound for a problem can be deﬁned as the asymptotic cost of the fastest known algorithm. the lower bound deﬁnes the best possible efﬁciency for any algorithm that solves the problem, including algorithms not yet invented. once the upper and lower bounds for the problem meet, we know that no future algorithm can possibly be (asymptotically) more efﬁcient.
a simple estimate for a problem’s lower bound can be obtained by measuring the size of the input that must be read and the output that must be written. certainly no algorithm can be more efﬁcient than the problem’s i/o time. from this we see that the sorting problem cannot be solved by any algorithm in less than Ω(n) time because it takes at least n steps to read and write the n values to be sorted. based on our current knowledge of sorting algorithms and the size of the input, we know that the problem of sorting is bounded by Ω(n) and o(n log n).
computer scientists have spent much time devising efﬁcient general-purpose sorting algorithms, but no one has ever found one that is faster than o(n log n) in the worst or average cases. should we keep searching for a faster sorting algorithm? or can we prove that there is no faster sorting algorithm by ﬁnding a tighter lower bound?
this section presents one of the most important and most useful proofs in computer science: no sorting algorithm based on key comparisons can possibly be faster than Ω(n log n) in the worst case. this proof is important for three reasons. first, knowing that widely used sorting algorithms are asymptotically optimal is reassuring. in particular, it means that you need not bang your head against the wall searching for an o(n) sorting algorithm (or at least not one in any way based on key
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
algorithm is said to be stable if it does not change the relative ordering of records with identical key values. many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes.
when comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. an example of such timings is presented in figure 7.13. however, such a comparison can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. in particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms.
when analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. this measure is usually closely related to the running time for the algorithm and has the advantage of being machine and datatype independent. however, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. if so, it might be appropriate to measure the number of swap operations performed by the algorithm. in most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. some special situations “change the rules” for comparing sorting algorithms. for example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. some applications require that a small number of records be sorted, but that the sort be performed frequently. an example would be an application that repeatedly sorts groups of ﬁve numbers. in such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. finally, some situations require that a sorting algorithm use as little memory as possible. we will note which sorting algorithms require signiﬁcant extra memory beyond the input array.
this section presents three simple sorting algorithms. while easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort. nonetheless, there are situations where one of these simple algorithms is the best tool for the job.
the initial cn term is the cost of doing the findpivot and partition steps, for some constant c. the closed-form solution to this recurrence relation is Θ(n log n). thus, quicksort has average-case cost Θ(n log n).
this is an unusual situation that the average case cost and the worst case cost have asymptotically different growth rates. consider what “average case” actually means. we compute an average cost for inputs of size n by summing up for every possible input of size n the product of the running time cost of that input times the probability that that input will occur. to simplify things, we assumed that every permutation is equally likely to occur. thus, ﬁnding the average means summing up the cost for every permutation and dividing by the number of inputs (n!). we know that some of these n! inputs cost o(n2). but the sum of all the permutation costs has to be (n!)(o(n log n)). given the extremely high cost of the worst inputs, there must be very few of them. in fact, there cannot be a constant fraction of the inputs with cost o(n2). even, say, 1% of the inputs with cost o(n2) would lead to an average cost of o(n2). thus, as n grows, the fraction of inputs with high cost must be going toward a limit of zero. we can conclude that quicksort will always have good behavior if we can avoid those very few bad input permutations.
the running time for quicksort can be improved (by a constant factor), and much study has gone into optimizing this algorithm. the most obvious place for improvement is the findpivot function. quicksort’s worst case arises when the pivot does a poor job of splitting the array into equal size subarrays. if we are willing to do more work searching for a better pivot, the effects of a bad pivot can be decreased or even eliminated. one good choice is to use the “median of three” algorithm, which uses as a pivot the middle of three randomly selected values. using a random number generator to choose the positions is relatively expensive, so a common compromise is to look at the ﬁrst, middle, and last positions of the current subarray. however, our simple findpivot function that takes the middle value as its pivot has the virtue of making it highly unlikely to get a bad input by chance, and it is quite cheap to implement. this is in sharp contrast to selecting the ﬁrst or last element as the pivot, which would yield bad performance for many permutations that are nearly sorted or nearly reverse sorted.
a signiﬁcant improvement can be gained by recognizing that quicksort is relatively slow when n is small. this might not seem to be relevant if most of the time we sort large arrays, nor should it matter how long quicksort takes in the rare instance when a small array is sorted because it will be fast anyway. but you should notice that quicksort itself sorts many, many small arrays! this happens as a natural by-product of the divide and conquer approach.
• a binary tree of height n can store at most 2n − 1 nodes. • equivalently, a tree with n nodes requires at least dlog(n + 1)e levels. what is the minimum number of nodes that must be in the decision tree for any comparison-based sorting algorithm for n values? because sorting algorithms are in the business of determining which unique permutation of the input corresponds to the sorted list, all sorting algorithms must contain at least one leaf node for each possible permutation. there are n! permutations for a set of n numbers (see section 2.2).
because there are at least n! nodes in the tree, we know that the tree must have Ω(log n!) levels. from stirling’s approximation (section 2.2), we know log n! is in Ω(n log n). the decision tree for any comparison-based sorting algorithm must have nodes Ω(n log n) levels deep. thus, in the worst case, any such sorting algorithm must require Ω(n log n) comparisons.
any sorting algorithm requiring Ω(n log n) comparisons in the worst case requires Ω(n log n) running time in the worst case. because any sorting algorithm requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n) time. we already know of sorting algorithms with o(n log n) running time, so we can conclude that the problem of sorting requires Θ(n log n) time. as a corollary, we know that no comparison-based sorting algorithm can improve on existing Θ(n log n) time sorting algorithms by more than a constant factor.
the deﬁnitive reference on sorting is donald e. knuth’s sorting and searching [knu98]. a wealth of details is covered there, including optimal sorts for small size n and special purpose sorting networks. it is a thorough (although somewhat dated) treatment on sorting. for an analysis of quicksort and a thorough survey on its optimizations, see robert sedgewick’s quicksort [sed80]. sedgewick’s algorithms [sed03] discusses most of the sorting algorithms described here and pays special attention to efﬁcient implementation. the optimized mergesort version of section 7.4 comes from sedgewick.
while Ω(n log n) is the theoretical lower bound in the worst case for sorting, many times the input is sufﬁciently well ordered that certain algorithms can take advantage of this fact to speed the sorting process. a simple example is insertion sort’s best-case running time. sorting algorithms whose running time is based on the amount of disorder in the input are called adaptive. for more information on adaptive sorting algorithms, see “a survey of adaptive sorting algorithms” by estivill-castro and wood [ecw92].
(a) devise an algorithm to sort three numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
(b) devise an algorithm to sort ﬁve numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
(c) devise an algorithm to sort eight numbers. it should make as few comparisons as possible. how many comparisons and swaps are required in the best, worst, and average cases?
7.17 devise an efﬁcient algorithm to sort a set of numbers with values in the range 0 to 30,000. there are no duplicates. keep memory requirements to a minimum.
7.18 which of the following operations are best implemented by ﬁrst sorting the list of numbers? for each operation, brieﬂy describe an algorithm to implement it, and state the algorithm’s asymptotic complexity. (a) find the minimum value. (b) find the maximum value. (c) compute the arithmetic mean. (d) find the median (i.e., the middle value). (e) find the mode (i.e., the value that appears the most times).
7.19 consider a recursive mergesort implementation that calls insertion sort on sublists smaller than some threshold. if there are n calls to mergesort, how many calls will there be to insertion sort? why?
7.20 implement mergesort for the case where the input is a linked list. 7.21 counting sort (assuming the input key values are integers in the range 0 to m − 1) works by counting the number of records with each key value in the ﬁrst pass, and then uses this information to place the records in order in a second pass. write an implementation of counting sort (see the implementation of radix sort for some ideas). what can we say about the relative values of m and n for this to be effective? if m < n, what is the running time of this algorithm?
7.22 use an argument similar to that given in section 7.9 to prove that log n is a worst-case lower bound for the problem of searching for a given value in a sorted array containing n elements.
7.1 one possible improvement for bubble sort would be to add a ﬂag variable and a test that determines if an exchange was made during the current iteration. if no exchange was made, then the list is sorted and so the algorithm
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
7.7 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. we can make any algorithm stable if we alter the input keys so that (potentially) duplicate key values are made unique in a way that the ﬁrst occurance of the original duplicate value is less than the second occurance, which in turn is less than the third, and so on. in the worst case, it is possible that all n input records have the same key value. give an algorithm to modify the key values such that every modiﬁed key value is unique, the resulting key values give the same sort order as the original keys, the result is stable (in that the duplicate original key values remain in their original order), and the process of altering the keys is done in linear time using only a constant amount of additional space.
recursion to reduce the number of function calls made. (a) how deep can the stack get in the worst case? (b) quicksort makes two recursive calls. the algorithm could be changed to make these two calls in a speciﬁc order. in what order should the two calls be made, and how does this affect how deep the stack can become?
7.10 assume l is an array, length(l) returns the number of records in the array, and qsort(l, i, j) sorts the records of l from i to j (leaving the records sorted in l) using the quicksort algorithm. what is the averagecase time complexity for each of the following code fragments? (a) for (i=0; i<l.length; i++)
7.11 modify quicksort to ﬁnd the smallest k values in an array of records. your output should be the array modiﬁed so that the k smallest values are sorted in the ﬁrst k positions of the array. your algorithm should do the minimum amount of work necessary.
7.12 modify quicksort to sort a sequence of variable-length strings stored one after the other in a character array, with a second array (storing pointers to strings) used to index the strings. your function should modify the index array so that the ﬁrst pointer points to the beginning of the lowest valued string, and so on.
and quicksort) by taking advantage of the best case behavior of another algorithm (insertion sort). we’ll see several examples of how we can tune an algorithm for better performance. we’ll see that special case behavior by some algorithms makes them the best solution for special niche applications (heapsort). sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. sorting will also be used to motivate the introduction to ﬁle processing presented in chapter 8.
the present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. it begins with a discussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. the ﬁnal sorting method presented requires only Θ(n) worst-case time under special conditions. the chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case.
except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. records are compared to one another by means of a comparator class, as introduced in section 4.4. to simplify the discussion we will assume that each record has a key ﬁeld whose value is extracted from the record by the comparator. the key method of the comparator class is prior, which returns true when its ﬁrst argument should appear prior to its second argument in the sorted list. we also assume that for every record type there is a swap function that can interchange the contents of two records in the array (see the appendix).
given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the sorting problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn. in other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order.
as deﬁned, the sorting problem allows input with two or more records that have the same key value. certain applications require that input not contain duplicate key values. the sorting algorithms presented in this chapter and in chapter 8 can handle duplicate key values unless noted otherwise.
when duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. it might be desirable to maintain this initial ordering among duplicates. a sorting
algorithm is said to be stable if it does not change the relative ordering of records with identical key values. many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes.
when comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. an example of such timings is presented in figure 7.13. however, such a comparison can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. in particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms.
when analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. this measure is usually closely related to the running time for the algorithm and has the advantage of being machine and datatype independent. however, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. if so, it might be appropriate to measure the number of swap operations performed by the algorithm. in most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. some special situations “change the rules” for comparing sorting algorithms. for example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. some applications require that a small number of records be sorted, but that the sort be performed frequently. an example would be an application that repeatedly sorts groups of ﬁve numbers. in such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. finally, some situations require that a sorting algorithm use as little memory as possible. we will note which sorting algorithms require signiﬁcant extra memory beyond the input array.
this section presents three simple sorting algorithms. while easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort. nonetheless, there are situations where one of these simple algorithms is the best tool for the job.
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
whose result is shown in figure 13.10(b). the second is a zigzag rotation, whose result is shown in figure 13.10(c). the ﬁnal step is a single rotation resulting in the tree of figure 13.10(d). notice that the splaying process has made the tree shallower.
all of the search trees discussed so far — bsts, avl trees, splay trees, 2-3 trees, b-trees, and tries — are designed for searching on a one-dimensional key. a typical example is an integer key, whose one-dimensional range can be visualized as a number line. these various tree structures can be viewed as dividing this onedimensional numberline into pieces.
some databases require support for multiple keys, that is, records can be searched based on any one of several keys. typically, each such key has its own onedimensional index, and any given search query searches one of these independent indices as appropriate.
imagine that we have a database of city records, where each city has a name and an xycoordinate. a bst or splay tree provides good performance for searches on city name, which is a one-dimensional key. separate bsts could be used to index the xand y-coordinates. this would allow us to insert and delete cities, and locate them by name or by one coordinate. however, search on one of the two coordinates is not a natural way to view search in a two-dimensional space. another option is to combine the xy-coordinates into a single key, say by concatenating the two coordinates, and index cities by the resulting key in a bst. that would allow search by coordinate, but would not allow for efﬁcient two-dimensional range queries such as searching for all cities within a given distance of a speciﬁed point. the problem is that the bst only works well for one-dimensional keys, while a coordinate is a two-dimensional key where neither dimension is more important than the other.
multidimensional range queries are the deﬁning feature of a spatial application. because a coordinate gives a position in space, it is called a spatial attribute. to implement spatial applications efﬁciently requires the use of spatial data structures. spatial data structures store data objects organized by position and are an important class of data structures used in geographic information systems, computer graphics, robotics, and many other ﬁelds.
this section presents two spatial data structures for storing point data in two or more dimensions. they are the k-d tree and the pr quadtree. the k-d tree is a
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
unfortunately, the bst can become unbalanced. even under relatively good conditions, the depth of leaf nodes can easily vary by a factor of two. this might not be a signiﬁcant concern when the tree is stored in main memory because the time required is still Θ(log n) for search and update. when the tree is stored on disk, however, the depth of nodes in the tree becomes crucial. every time a bst node b is visited, it is necessary to visit all nodes along the path from the root to b. each node on this path must be retrieved from disk. each disk access returns a block of information. if a node is on the same block as its parent, then the cost to ﬁnd that node is trivial once its parent is in main memory. thus, it is desirable to keep subtrees together on the same block. unfortunately, many times a node is not on the same block as its parent. thus, each access to a bst node could potentially require that another block to be read from disk. using a buffer pool to store multiple blocks in memory can mitigate disk access problems if bst accesses display good locality of reference. but a buffer pool cannot eliminate disk i/o entirely. the problem becomes greater if the bst is unbalanced, because nodes deep in the tree have the potential of causing many disk blocks to be read. thus, there are two signiﬁcant issues that must be addressed to have efﬁcient search from a disk-based bst. the ﬁrst is how to keep the tree balanced. the second is how to arrange the nodes on blocks so as to keep the number of blocks encountered on any path from the root to the leaves at a minimum.
we could select a scheme for balancing the bst and allocating bst nodes to blocks in a way that minimizes disk i/o, as illustrated by figure 10.7. however, maintaining such a scheme in the face of insertions and deletions is difﬁcult. in particular, the tree should remain balanced when an update takes place, but doing so might require much reorganization. each update should affect only a disk few blocks, or its cost will be too high. as you can see from figure 10.8, adopting a rule such as requiring the bst to be complete can cause a great deal of rearranging of data within the tree.
we can solve these problems by selecting another tree structure that automatically remains balanced after updates, and which is amenable to storing in blocks. there are a number of widely used balanced tree data structures, and there are also techniques for keeping bsts balanced. examples are the avl and splay trees discussed in section 13.2. as an alternative, section 10.4 presents the 2-3 tree, which has the property that its leaves are always at the same level. the main reason for discussing the 2-3 tree here in preference to the other balanced search trees is that
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
if we are willing to weaken the balance requirements, we can come up with alternative update routines that perform well both in terms of cost for the update and in balence for the resulting tree structure. the avl tree works in this way, using insertion and deletion routines altered from those of the bst to ensure that, for every node, the depths of the left and right subtrees differ by at most one. the avl tree is described in section 13.2.1.
a different approach to improving the performance of the bst is to not require that the tree always be balanced, but rather to expend some effort toward making the bst more balanced every time it is accessed. this is a little like the idea of path compression used by the union/find algorithm presented in section 6.2. one example of such a compromise is called the splay tree. the splay tree is described in section 13.2.2.
the avl tree (named for its inventors adelson-velskii and landis) should be viewed as a bst with the following additional property: for every node, the heights of its left and right subtrees differ by at most 1. as long as the tree maintains this property, if the tree contains n nodes, then it has a depth of at most o(log n). as a result, search for any node will cost o(log n), and if the updates can be done in time proportional to the depth of the node inserted or deleted, then updates will also cost o(log n), even in the worst case.
the key to making the avl tree work is to make the proper alterations to the insert and delete routines so as to maintain the balance property. of course, to be practical, we must be able to implement the revised update routines in Θ(log n) time.
consider what happens when we insert a node with key value 5, as shown in figure 13.4. the tree on the left meets the avl tree balance requirements. after the insertion, two nodes no longer meet the requirements. because the original tree met the balance requirement, nodes in the new tree can only be unbalanced by a difference of at most 2 in the subtrees. for the bottommost unbalanced node, call it s, there are 4 cases:
1. the extra node is in the left child of the left child of s. 2. the extra node is in the right child of the left child of s. 3. the extra node is in the left child of the right child of s. 4. the extra node is in the right child of the right child of s.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
13.4 revise the bst class of section 5.4 to use the splay tree rotations. your new implementation should not modify the original bst class adt. compare your splay tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
13.5 implement a city database using the k-d tree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.6 implement a city database using the pr quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.7 implement a city database using the bintree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.8 implement a city database using the point quadtree. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. you should also support region queries, that is, a request to print all records within a given distance of a speciﬁed point.
13.9 use the pr quadtree to implement an efﬁcient solution to problem 6.5. that is, store the set of points in a pr quadtree. for each point, the pr quadtree is used to ﬁnd those points within distance d that should be equivalenced. what is the asymptotic complexity of this solution?
13.10 select any two of the point representations described in this chapter (i.e., the k-d tree, the pr quadtree, the bintree, and the point quadtree). implement your two choices and compare them over a wide range of data sets. describe which is easier to implement, which appears to be more space efﬁcient, and which appears to be more time efﬁcient.
this book, including the cost of a series of union/find operations (section 6.2), the cost of a series of splay tree operations (section 13.2), and the cost of a series of operations on self-organizing lists (section 9.2). section 14.3 discusses the topic in more detail.
in section 2.6.3 it was proved by induction that this summation has the well-known closed form n(n + 1)/2. but while induction is a good technique for proving that a proposed closed-form expression is correct, how do we ﬁnd a candidate closedform expression to test in the ﬁrst place? let us try to approach this summation from ﬁrst principles, as though we had never seen it before.
a good place to begin analyzing a summation it is to give an estimate of its value for a given n. observe that the biggest term for this summation is n, and there are n terms being summed up. so the total must be less than n2. actually, most terms are much less than n, and the sizes of the terms grows linearly. if we were to draw a picture with bars for the size of the terms, their heights would form a line, and we could enclose them in a box n units wide and n units high. it is easy to see from this that a closer estimate for the summation is about (n2)/2. having this estimate in hand helps us when trying to determine an exact closed-form solution, because we will hopefully recognize if our proposed solution is badly wrong.
let us now consider some ways that we might hit upon an exact value for the closed form solution to this summation. one particularly clever approach we can take is to observe that we can “pair up” the ﬁrst and last terms, the second and (n − 1)th terms, and so on. each pair sums to n + 1. the number of pairs is n/2. thus, the solution is n(n + 1)/2. this is pretty, and there’s no doubt about it being correct. the problem is that it is not a useful technique for solving many other summations.
now let us try to do something a bit more general. we already recognized that, because the largest term is n and there are n terms, the summation is less than n2. if we are lucky, the closed form solution is a polynomial. using that as a working assumption, we can invoke a technique called guess-and-test. we will guess that the closed-form solution for this summation is a polynomial of the form
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
if your program needs to store a few things — numbers, payroll records, or job descriptions for example — the simplest and most effective approach might be to put them in a list. only when you have to organize or search through a large number of things do more sophisticated data structures usually become necessary. (we will study how to organize and search through medium amounts of data in chapters 5, 7, and 9, and discuss how to deal with large amounts of data in chapters 8–10.) many applications don’t require any form of search, and they do not require that any ordering be placed on the objects being stored. some applications require processing in a strict chronological order, perhaps processing objects in the order that they arrived, or perhaps processing objects in the reverse of the order that they arrived. for all these situations, a simple list structure is appropriate.
this chapter describes representations for lists in general, as well as two important list-like structures called the stack and the queue. along with presenting these fundamental data structures, the other goals of the chapter are to: (1) give examples of separating a logical representation in the form of an adt from a physical implementation for a data structure. (2) illustrate the use of asymptotic analysis in the context of some simple operations that you might already be familiar with. in this way you can begin to see how asymptotic analysis works, without the complications that arise when analyzing more sophisticated algorithms and data structures. (3) introduce the concept and use of dictionaries and comparator classes.
we begin by deﬁning an adt for lists in section 4.1. two implementations for the list adt — the array-based list and the linked list — are covered in detail and their relative merits discussed. sections 4.2 and 4.3 cover stacks and queues, respectively. java implementations for each of these data structures are presented. section 4.4 presents an adt for storing and retrieving data that will set a context for implementing search structures such as the binary search tree of section 5.4.
figure 4.3 inserting an element at the head of an array-based list requires shifting all existing elements in the array by one position toward the tail. (a) a list containing ﬁve elements before inserting an element with value 23. (b) the list after shifting all existing elements one position to the right. (c) the list after 23 has been inserted in array position 0. shading indicates the unused part of the array.
good practice to make a separate list node class. an additional beneﬁt to creating a list node class is that it can be reused by the linked implementations for the stack and queue data structures presented later in this chapter. figure 4.4 shows the implementation for list nodes, called the link class. objects in the link class contain an element ﬁeld to store the element value, and a next ﬁeld to store a pointer to the next node on the list. the list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list.
the link class is quite simple. there are two forms for its constructor, one with an initial element value and one without. because the link class is also used by the stack and queue implementations presented later, its data members are made public. while technically this is breaking encapsulation, in practice the link class should be implemented as a private class of the linked list (or stack or queue) implementation, and thus not visible to the rest of the program.
figure 4.5(a) shows a graphical depiction for a linked list storing four integers. the value stored in a pointer variable is indicated by an arrow “pointing” to something. java uses the special symbol null for a pointer value that points nowhere, such as for the last list node’s next ﬁeld. a null pointer is indicated graphically by a diagonal slash through a pointer variable’s box. the vertical line between the nodes labeled 23 and 12 in figure 4.5(a) indicates the current position.
the ﬁrst link node of the list is accessed from a pointer named head. to speed access to the end of the list, in particular to allow the append method to
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
we ﬁrst deﬁne an enumerated type called tohop, with two values move and toh, to indicate calls to the move function and recursive calls to toh, respectively. note that an array-based stack is used, because we know that the stack will need to store exactly 2n + 1 elements. the new version of toh begins by placing on the stack a description of the initial problem of n rings. the rest of the function is simply a while loop that pops the stack and executes the appropriate operation. in the case of a toh operation (for n > 0), we store on the stack representations for the three operations executed by the recursive version. however, these operations must be placed on the stack in reverse order, so that they will be popped off in the correct order.
some “naturally recursive” applications lend themselves to efﬁcient implementation with a stack, because the amount of information needed to describe a subproblem is small. for example, section 7.5 discusses a stack-based implementation for quicksort.
like the stack, the queue is a list-like structure that provides restricted access to its elements. queue elements may only be inserted at the back (called an enqueue
your computer. (a) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type int.
(b) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type double.
4.14 modify the array-based queue deﬁnition of figure 4.25 to use a separate boolean member to keep track of whether the queue is empty, rather than require that one array position remain empty.
4.15 a palindrome is a string that reads the same forwards as backwards. using only a ﬁxed number of stacks and queues, the stack and queue adt functions, and a ﬁxed number of int and char variables, write an algorithm to determine if a string is a palindrome. assume that the string is read from standard input one character at a time. the algorithm should output true or false as appropriate.
4.18 let q be a non-empty queue, and let s be an empty stack. using only the stack and queue adt functions and a single element variable x, write an algorithm to reverse the order of the elements in q.
leaf nodes in the tree. prove by induction that if tree t is a full binary tree with n internal nodes, i is t’s internal path length, and e is t’s external path length, then e = i + 2n for n ≥ 0.
5.4 explain why function preorder2 from section 5.2 makes half as many recursive calls as function preorder. explain why it makes twice as many accesses to left and right children. (a) modify the preorder traversal of section 5.2 to perform an inorder
5.6 write a recursive function named search that takes as input the pointer to the root of a binary tree (not a bst!) and a value k, and returns true if value k appears in the tree and false otherwise.
5.7 write an algorithm that takes as input the pointer to the root of a binary tree and prints the node values of the tree in level order. level order ﬁrst prints the root, then all nodes of level 1, then all nodes of level 2, and so on. hint: preorder traversals make use of a stack through recursive calls. consider making use of another data structure to help implement the levelorder traversal.
5.8 write a recursive function that returns the height of a binary tree. 5.9 write a recursive function that returns a count of the number of leaf nodes in
5.11 assume that a given bst stores integer values in its nodes. write a recursive function that traverses a binary tree, and prints the value of every node who’s grandparent has a value that is a multiple of ﬁve.
(c) all nodes store data and a parent pointer, and internal nodes store two child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
for each of the following scenarios, which of these choices would be best? explain your answer. (a) the records are guaranteed to arrive already sorted from lowest to highest (i.e., whenever a record is inserted, its key value will always be greater than that of the last record inserted). a total of 1000 inserts will be interspersed with 1000 searches.
(b) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1,000,000 insertions are performed, followed by 10 searches.
(c) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are interspersed with 1000 searches.
(d) the records arrive with values having a uniform random distribution (so the bst is likely to be well balanced). 1000 insertions are performed, followed by 1,000,000 searches.
5.2 one way to deal with the “problem” of null pointers in binary trees is to use that space for some other purpose. one example is the threaded binary tree. extending the node implementation of figure 5.7, the threaded binary tree stores with each node two additional bit ﬁelds that indicate if the child pointers lc and rc are regular pointers to child nodes or threads. if lc is not a pointer to a non-empty child (i.e., if it would be null in a regular binary tree), then it instead stores a pointer to the inorder predecessor of that node. the inorder predecessor is the node that would be printed immediately before the current node in an inorder traversal. if rc is not a pointer to a child, then it instead stores a pointer to the node’s inorder successor. the inorder successor is the node that would be printed immediately after the current node in an inorder traversal. the main advantage of threaded binary trees is that operations such as inorder traversal can be implemented without using recursion or a stack. reimplement the bst as a threaded binary tree, and include a non-recursive version of the preorder traversal
5.3 implement a city database using a bst to store the database records. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x- and y-coordinates.
7.1 using induction, prove that insertion sort will always produce a sorted array. 7.2 write an insertion sort algorithm for integer key values. however, here’s the catch: the input is a stack (not an array), and the only variables that your algorithm may use are a ﬁxed number of integers and a ﬁxed number of stacks. the algorithm should return a stack containing the records in sorted order (with the least value being at the top of the stack). your algorithm should be Θ(n2) in the worst case.
would the new implementation work correctly? would the change affect the asymptotic complexity of the algorithm? how would the change affect the running time of the algorithm? 7.4 when implementing insertion sort, a binary search could be used to locate the position within the ﬁrst i − 1 elements of the array into which element i should be inserted. how would this affect the number of comparisons required? how would using such a binary search affect the asymptotic running time for insertion sort?
7.5 figure 7.5 shows the best-case number of swaps for selection sort as Θ(n). this is because the algorithm does not check to see if the ith record is already in the ith position; that is, it might perform unnecessary swaps. (a) modify the algorithm so that it does not make unnecessary swaps. (b) what is your prediction regarding whether this modiﬁcation actually
7.6 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. of the sorting algorithms insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort, binsort, and radix sort, which of these are stable, and which are not? for each one, describe either why it is or is not stable. if a minor change to the implementation would make it stable, describe the change.
7.7 recall that a sorting algorithm is said to be stable if the original ordering for duplicate keys is preserved. we can make any algorithm stable if we alter the input keys so that (potentially) duplicate key values are made unique in a way that the ﬁrst occurance of the original duplicate value is less than the second occurance, which in turn is less than the third, and so on. in the worst case, it is possible that all n input records have the same key value. give an algorithm to modify the key values such that every modiﬁed key value is unique, the resulting key values give the same sort order as the original keys, the result is stable (in that the duplicate original key values remain in their original order), and the process of altering the keys is done in linear time using only a constant amount of additional space.
recursion to reduce the number of function calls made. (a) how deep can the stack get in the worst case? (b) quicksort makes two recursive calls. the algorithm could be changed to make these two calls in a speciﬁc order. in what order should the two calls be made, and how does this affect how deep the stack can become?
7.10 assume l is an array, length(l) returns the number of records in the array, and qsort(l, i, j) sorts the records of l from i to j (leaving the records sorted in l) using the quicksort algorithm. what is the averagecase time complexity for each of the following code fragments? (a) for (i=0; i<l.length; i++)
7.11 modify quicksort to ﬁnd the smallest k values in an array of records. your output should be the array modiﬁed so that the k smallest values are sorted in the ﬁrst k positions of the array. your algorithm should do the minimum amount of work necessary.
7.12 modify quicksort to sort a sequence of variable-length strings stored one after the other in a character array, with a second array (storing pointers to strings) used to index the strings. your function should modify the index array so that the ﬁrst pointer points to the beginning of the lowest valued string, and so on.
can stop early. this makes the best case performance become o(n) (because if the list is already sorted, then no iterations will take place on the ﬁrst pass, and the sort will stop right there). modify the bubble sort implementation to add this ﬂag and test. compare the modiﬁed implementation on a range of inputs to determine if it does or does not improve performance in practice.
7.2 starting with the java code for quicksort given in this chapter, write a series of quicksort implementations to test the following optimizations on a wide range of input data sizes. try these optimizations in various combinations to try and develop the fastest possible quicksort implementation that you can. (a) look at more values when selecting a pivot. (b) do not make a recursive call to qsort when the list size falls below a given threshold, and use insertion sort to complete the sorting process. test various values for the threshold size.
7.3 write your own collection of sorting programs to implement the algorithms described in this chapter, and compare their running times. be sure to implement optimized versions, trying to make each program as fast as possible. do you get the same relative timings as shown in figure 7.13? if not, why do you think this happened? how do your results compare with those of your classmates? what does this say about the difﬁculty of doing empirical timing studies?
7.4 perform a study of shellsort, using different increments. compare the version shown in section 7.3, where each increment is half the previous one, with others. in particular, try implementing “division by 3” where the increments on a list of length n will be n/3, n/9, etc. do other increment schemes work as well?
7.5 the implementation for mergesort given in section 7.4 takes an array as input and sorts that array. at the beginning of section 7.4 there is a simple pseudocode implementation for sorting a linked list using mergesort. implement both a linked list-based version of mergesort and the array-based version of mergesort, and compare their running times.
7.6 radix sort is typically implemented to support only a radix that is a power of two. this allows for a direct conversion from the radix to some number of bits in an integer key value. for example, if the radix is 16, then a 32-bit key will be processed in 8 steps of 4 bits each. this can lead to a more efﬁcient implementation because bit shifting can replace the division operations shown in the implementation of section 7.7. reimplement the radix sort
bit for all vertices is cleared. the mark bit for a vertex is set when the vertex is ﬁrst visited during the traversal. if a marked vertex is encountered during traversal, it is not visited a second time. this keeps the program from going into an inﬁnite loop when it encounters a cycle.
once the traversal algorithm completes, we can check to see if all vertices have been processed by checking the mark bit array. if not all vertices are marked, we can continue the traversal from another unmarked vertex. note that this process works regardless of whether the graph is directed or undirected. to ensure visiting all vertices, graphtraverse could be called as follows on a graph g:
11.3.1 depth-first search the ﬁrst method of organized graph traversal is called depth-ﬁrst search (dfs). whenever a vertex v is visited during the search, dfs will recursively visit all of v’s unvisited neighbors. equivalently, dfs will add all edges leading out of v to a stack. the next vertex to be visited is determined by popping the stack and following that edge. the effect is to follow one branch through the graph to its conclusion, then it will back up and follow another branch, and so on. the dfs process can be used to deﬁne a depth-ﬁrst search tree. this tree is composed of the edges that were followed to any new (unvisited) vertex during the traversal and leaves out the edges that lead to already visited vertices. dfs can be applied to directed or undirected graphs. here is an implementation for the dfs algorithm:
figure 11.9 a detailed illustration of the dfs process for the graph of figure 11.8(a) starting at vertex a. the steps leading to each change in the recursion stack are described.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
in terms of list functions, all insert and remove at position 0 in the array. operations would then be on the element in position 0. this implementation is inefﬁcient, because now every push or pop operation will require that all elements currently in the stack be shifted one position in the array, for a cost of Θ(n) if there are n elements. the other choice is have the top element be at position n − 1 when there are n elements in the stack. in other words, as elements are pushed onto the stack, they are appended to the tail of the list. method pop removes the tail element. in this case, the cost for each push or pop operation is only Θ(1).
for the implementation of figure 4.18, top is deﬁned to be the array index of the ﬁrst free position in the stack. thus, an empty stack has top set to 0, the ﬁrst available free position in the array. (alternatively, top could have been deﬁned to be the index for the top element in the stack, rather than the ﬁrst free position. if this had been done, the empty list would initialize top as −1.) methods push and pop simply place an element into, or remove an element from, the array position indicated by top. because top is assumed to be at the ﬁrst free position, push ﬁrst inserts its value into the top position and then increments top, while pop ﬁrst decrements top and then removes the top element.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
in terms of list functions, all insert and remove at position 0 in the array. operations would then be on the element in position 0. this implementation is inefﬁcient, because now every push or pop operation will require that all elements currently in the stack be shifted one position in the array, for a cost of Θ(n) if there are n elements. the other choice is have the top element be at position n − 1 when there are n elements in the stack. in other words, as elements are pushed onto the stack, they are appended to the tail of the list. method pop removes the tail element. in this case, the cost for each push or pop operation is only Θ(1).
for the implementation of figure 4.18, top is deﬁned to be the array index of the ﬁrst free position in the stack. thus, an empty stack has top set to 0, the ﬁrst available free position in the array. (alternatively, top could have been deﬁned to be the index for the top element in the stack, rather than the ﬁrst free position. if this had been done, the empty list would initialize top as −1.) methods push and pop simply place an element into, or remove an element from, the array position indicated by top. because top is assumed to be at the ﬁrst free position, push ﬁrst inserts its value into the top position and then increments top, while pop ﬁrst decrements top and then removes the top element.
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
in terms of list functions, all insert and remove at position 0 in the array. operations would then be on the element in position 0. this implementation is inefﬁcient, because now every push or pop operation will require that all elements currently in the stack be shifted one position in the array, for a cost of Θ(n) if there are n elements. the other choice is have the top element be at position n − 1 when there are n elements in the stack. in other words, as elements are pushed onto the stack, they are appended to the tail of the list. method pop removes the tail element. in this case, the cost for each push or pop operation is only Θ(1).
for the implementation of figure 4.18, top is deﬁned to be the array index of the ﬁrst free position in the stack. thus, an empty stack has top set to 0, the ﬁrst available free position in the array. (alternatively, top could have been deﬁned to be the index for the top element in the stack, rather than the ﬁrst free position. if this had been done, the empty list would initialize top as −1.) methods push and pop simply place an element into, or remove an element from, the array position indicated by top. because top is assumed to be at the ﬁrst free position, push ﬁrst inserts its value into the top position and then increments top, while pop ﬁrst decrements top and then removes the top element.
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
a similar effect can be had by using the exclusive-or operator. this fact is widely used in computer graphics. a region of the computer screen can be highlighted by xoring the outline of a box around it. xoring the box outline a second time restores the original contents of the screen.
the stack is a list-like structure in which elements may be inserted or removed from only one end. while this restriction makes stacks less ﬂexible than lists, it also makes stacks both efﬁcient (for those operations they can do) and easy to implement. many applications require only the limited form of insert and remove operations that stacks provide. in such cases, it is more efﬁcient to use the simpler stack data structure rather than the generic list. for example, the freelist of section 4.1.2 is really a stack.
despite their restrictions, stacks have many uses. thus, a special vocabulary for stacks has developed. accountants used stacks long before the invention of the computer. they called the stack a “lifo” list, which stands for “last-in, firstout.” note that one implication of the lifo policy is that stacks remove elements in reverse order of their arrival.
it is traditional to call the accessible element of the stack the top element. elements are not said to be inserted; instead they are pushed onto the stack. when removed, an element is said to be popped from the stack. figure 4.17 shows a sample stack adt.
as with lists, there are many variations on stack implementation. the two approaches presented here are array-based and linked stacks, which are analogous to array-based and linked lists, respectively.
figure 4.18 shows a complete implementation for the array-based stack class. as with the array-based list implementation, listarray must be declared of ﬁxed size when the stack is created. in the stack constructor, size serves to indicate this size. method top acts somewhat like a current position value (because the “current” position is always at the top of the stack), as well as indicating the number of elements currently in the stack.
the array-based stack implementation is essentially a simpliﬁed version of the array-based list. the only important design decision to be made is which end of the array should represent the top of the stack. one choice is to make the top be
in terms of list functions, all insert and remove at position 0 in the array. operations would then be on the element in position 0. this implementation is inefﬁcient, because now every push or pop operation will require that all elements currently in the stack be shifted one position in the array, for a cost of Θ(n) if there are n elements. the other choice is have the top element be at position n − 1 when there are n elements in the stack. in other words, as elements are pushed onto the stack, they are appended to the tail of the list. method pop removes the tail element. in this case, the cost for each push or pop operation is only Θ(1).
for the implementation of figure 4.18, top is deﬁned to be the array index of the ﬁrst free position in the stack. thus, an empty stack has top set to 0, the ﬁrst available free position in the array. (alternatively, top could have been deﬁned to be the index for the top element in the stack, rather than the ﬁrst free position. if this had been done, the empty list would initialize top as −1.) methods push and pop simply place an element into, or remove an element from, the array position indicated by top. because top is assumed to be at the ﬁrst free position, push ﬁrst inserts its value into the top position and then increments top, while pop ﬁrst decrements top and then removes the top element.
the linked stack implementation is a simpliﬁed version of the linked list implementation. the freelist of section 4.1.2 is an example of a linked stack. elements are inserted and removed only from the head of the list. the header node is not used because no special-case code is required for lists of zero or one elements. figure 4.19 shows the complete class implementation for the linked stack. the only data member is top, a pointer to the ﬁrst (top) link node of the stack. method push ﬁrst modiﬁes the next ﬁeld of the newly created link node to point to the top of the stack and then sets top to point to the new link node. method pop is also quite simple. the variable temp stores the value of the top node, while ltemp keeps a link to the top node as it is removed from the stack. the stack is updated by setting top to point to the next element in the stack. the old top node
all operations for the array-based and linked stack implementations take constant time, so from a time efﬁciency perspective, neither has a signiﬁcant advantage. another basis for comparison is the total space required. the analysis is similar to that done for list implementations. the array-based stack must declare a ﬁxed-size array initially, and some of that space is wasted whenever the stack is not full. the linked stack can shrink and grow but requires the overhead of a link ﬁeld for every element.
when multiple stacks are to be implemented, it is possible to take advantage of the one-way growth of the array-based stack. this can be done by using a single array to store two stacks. one stack grows inward from each end as illustrated by figure 4.20, hopefully leading to less wasted space. however, this only works well when the space requirements of the two stacks are inversely correlated. in other words, ideally when one stack grows, the other will shrink. this is particularly effective when elements are taken from one stack and given to the other. if instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly.
perhaps the most common computer application that uses stacks is not even visible to its users. this is the implementation of subroutine calls in most programming language runtime environments. a subroutine call is normally implemented by placing necessary information about the subroutine (including the return address, parameters, and local variables) onto a stack. this information is called an activation record. further subroutine calls add to the stack. each return from a subroutine pops the top activation record off the stack. figure 4.21 illustrates the implementation of the recursive factorial function of section 2.5 from the runtime environment’s point of view.
your computer. (a) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type int.
(b) calculate the break-even point, as a function of n, beyond which the array-based list is more space efﬁcient than the linked list for lists whose elements are of type double.
4.14 modify the array-based queue deﬁnition of figure 4.25 to use a separate boolean member to keep track of whether the queue is empty, rather than require that one array position remain empty.
4.15 a palindrome is a string that reads the same forwards as backwards. using only a ﬁxed number of stacks and queues, the stack and queue adt functions, and a ﬁxed number of int and char variables, write an algorithm to determine if a string is a palindrome. assume that the string is read from standard input one character at a time. the algorithm should output true or false as appropriate.
4.18 let q be a non-empty queue, and let s be an empty stack. using only the stack and queue adt functions and a single element variable x, write an algorithm to reverse the order of the elements in q.
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
the largest common factor of two numbers is the largest integer that divides both evenly. observation: if k divides n and m, then k divides n − m. so, f(n, m) = f(n − m, n) = f(m, n − m) = f(m, n). observation: there exists k and l such that
16.4.3 matrix multiplication the standard algorithm for multiplying two n × n matrices requires Θ(n3) time. it is possible to do better than this by rearranging and grouping the multiplications in various ways. one example of this is known as strassen’s matrix multiplication algorithm. assume that n is a power of two. in the following, a and b are n× n arrays, while aij and bij refer to arrays of size n/2× n/2. strassen’s algorithm is to
16.7 if we had a linked list that would never be modiﬁed, we can use a simpler approach than the skip list to speed access. the concept would remain the same in that we add additional pointers to list nodes for efﬁcient access to the ith element. how can we add a second pointer to each element of a singly linked list to allow access to an arbitrary element in o(log n) time?
16.8 what is the expected (average) number of pointers for a skip list node? 16.9 write a function to remove a node with given value from a skip list. 16.10 write a function to ﬁnd the ith node on a skip list.
16.2 implement both a standard Θ(n3) matrix multiplication algorithm and strassen’s matrix multiplication algorithm (see exercise 14.16.4.3). using empirical testing, try to estimate the constant factors for the runtime equations of the two algorithms. how big must n be before strassen’s algorithm becomes more efﬁcient than the standard algorithm?
(a) show the result of building a bintree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (45, 25), h (45, 30), i (50, 30).
(b) show the result of deleting point c from the tree you built in part (a). (c) show the result of deleting point f from the resulting tree in part (b).
13.16 compare the trees constructed for exercises 12 and 15 in terms of the number of internal nodes, full leaf nodes, empty leaf nodes, and total depths of the two trees.
13.17 show the result of building a point quadtree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (31, 35), g (45, 26), h (44, 30), i (50, 30).
13.1 use the trie data structure to devise a program to sort variable-length strings. the program’s running time should be proportional to the total number of letters in all of the strings. note that some strings might be very long while most are short.
13.2 deﬁne the set of sufﬁx strings for a string s to be s, s without its ﬁrst character, s without its ﬁrst two characters, and so on. for example, the complete set of sufﬁx strings for “hello” would be
a sufﬁx tree is a pat trie that contains all of the sufﬁx strings for a given string, and associates each sufﬁx with the complete string. the advantage of a sufﬁx tree is that it allows a search for strings using “wildcards.” for example, the search key “th*” means to ﬁnd all strings with “th” as the ﬁrst two characters. this can easily be done with a regular trie. searching for “*th” is not efﬁcient in a regular trie, but it is efﬁcient in a sufﬁx tree. implement the sufﬁx tree for a dictionary of words or phrases, with support for wildcard search.
13.3 revise the bst class of section 5.4 to use the avl tree rotations. your new implementation should not modify the original bst class adt. compare your avl tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
most programs contain loop constructs. when analyzing running time costs for programs with loops, we need to add up the costs for each time the loop is executed. this is an example of a summation. summations are simply the sum of costs for some function applied to a range of parameter values. summations are typically written with the following “sigma” notation:
this notation indicates that we are summing the value of f(i) over some range of (integer) values. the parameter to the expression and its initial value are indicated
below thep symbol. here, the notation i = 1 indicates that the parameter is i and that it begins with the value 1. at the top of thep symbol is the expression n. this
indicates the maximum value for the parameter i. thus, this notation means to sum the values of f(i) as i ranges from 1 through n. this can also be written
given a summation, you often wish to replace it with a direct equation with the same value as the summation. this is known as a closed-form solution, and the process of replacing the summation with its closed-form solution is known as solvi=1 1 is simply the expression “1” summed n times (remember that i ranges from 1 to n). because the sum of n 1s is n, the closed-form solution is n. the following is a list of useful summations, along with their closed-form solutions.
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
the two conditions that make up the induction proof combine to demonstrate that thrm holds for n = 2 as an extension of the fact that thrm holds for n = 1. this fact, combined again with condition (2) or (2a), indicates that thrm also holds for n = 3, and so on. thus, thrm holds for all values of n (larger than the base cases) once the two conditions have been proved.
what makes mathematical induction so powerful (and so mystifying to most people at ﬁrst) is that we can take advantage of the assumption that thrm holds for all values less than n to help us prove that thrm holds for n. this is known as the induction hypothesis. having this assumption to work with makes the induction step easier to prove than tackling the original theorem itself. being able to rely on the induction hypothesis provides extra information that we can bring to bear on the problem.
there are important similarities between recursion and induction. both are anchored on one or more base cases. a recursive function relies on the ability to call itself to get the answer for smaller instances of the problem. likewise, induction proofs rely on the truth of the induction hypothesis to prove the theorem. the induction hypothesis does not come out of thin air. it is true if and only if the theorem itself is true, and therefore is reliable within the proof context. using the induction hypothesis it do work is exactly the same as using a recursive call to do work.
1. check the base case. for n = 1, verify that s(1) = 1(1 + 1)/2. s(1) is simply the sum of the ﬁrst positive number, which is 1. because 1(1 + 1)/2 = 1, the formula is correct for the base case.
example 2.13 this example shows how we can use induction to prove that a proposed closed-form solution for a recurrence relation is correct. theorem 2.4 the recurrence relation t(n) = t(n−1)+1; t(1) = 0 has closed-form solution t(n) = n − 1. proof: to prove the base case, we observe that t(1) = 1 − 1 = 0. the induction hypothesis is that t(n − 1) = n − 2. combining the deﬁnition of the recurrence with the induction hypothesis, we see immediately that
theorem 2.5 2c/ and 5c/ stamps can be used to form any value (for values ≥ 4). proof: the theorem deﬁnes the problem for values ≥ 4 because it does not hold for the values 1 and 3. using 4 as the base case, a value of 4c/ can be made from two 2c/ stamps. the induction hypothesis is that a value of n− 1 can be made from some combination of 2c/ and 5c/ stamps. we now use the induction hypothesis to show how to get the value n from 2c/ and 5c/ stamps. either the makeup for value n− 1 includes a 5c/ stamp, or it does not. if so,
(a) use induction to show that n2 − n is always even. (b) give a direct proof in one or two sentences that n2 − n is always even. (c) show that n3 − n is always divisible by three. (d) is n5 − n aways divisible by 5? explain your answer.
i=0 2.21 prove equation 2.2 using mathematical induction. 2.22 prove equation 2.6 using mathematical induction. 2.23 prove equation 2.7 using mathematical induction. 2.24 find a closed-form solution and prove (using induction) that your solution is
theorem 2.10 when n + 1 pigeons roost in n holes, there must be some hole containing at least two pigeons. (a) prove the pigeonhole principle using proof by contradiction. (b) prove the pigeonhole principle using mathematical induction.
(b) what is the average number of “1” bits for an n-bit random number? (c) what is the expected value for the position of the leftmost “1” bit? in other words, how many positions on average must we examine when moving from left to right before encountering a “1” bit? show the appropriate summation.
2.35 what is the total volume of your body in liters (or, if you prefer, gallons)? 2.36 an art historian has a database of 20,000 full-screen color images.
(a) about how much space will this require? how many cd-roms would be required to store the database? (a cd-rom holds about 600mb of data). be sure to explain all assumptions you made to derive your answer.
(b) now, assume that you have access to a good image compression technique that can store the images in only 1/10 of the space required for an uncompressed image. will the entire database ﬁt onto a single cdrom if the images are compressed?
2.37 how many cubic miles of water ﬂow out of the mouth of the mississippi river each day? do not look up the answer or any supplemental facts. be sure to describe all assumptions made in arriving at your answer.
but each time the cost of the inner loop is different because it costs c3i with i changing each time. you should see that for the ﬁrst execution of the outer loop, i is 1. for the second execution of the outer loop, i is 2. each time through the outer loop, i becomes one greater, until the last time through the loop when i = n. thus, the total cost of the loop is c3 times the sum of the integers 1 through n. from equation 2.1, we know that
in the ﬁrst double loop, the inner for loop always executes n times. because the outer loop executes n times, it should be obvious that the statement sum1++ is executed precisely n2 times. the second loop is similar j=1 j. this is ap2 n2. thus, both double loops cost Θ(n2), though the second
(b) if we change the dividing point computation in function binary from i = (l + r)/2 to i = r − 2, what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
3.21 design an algorithm to assemble a jigsaw puzzle. assume that each piece has four sides, and that each piece’s ﬁnal orientation is known (top, bottom, etc.). assume that you have available a function bool compare(piece a, piece b, side ad) that can tell, in constant time, whether piece a connects to piece b on a’s side ad and b’s opposite side bd. the input to your algorithm should consist of an n × m array of random pieces, along with dimensions n and m. the algorithm should put the pieces in their correct positions in the array. your algorithm should be as efﬁcient as possible in the asymptotic sense. write a summation for the running time of your algorithm on n pieces, and then derive a closed-form solution for the summation.
3.1 imagine that you are trying to store 32 boolean values, and must access them frequently. compare the time required to access boolean values stored alternatively as a single bit ﬁeld, a character, a short integer, or a long integer. there are two things to be careful of when writing your program. first, be sure that your program does enough variable accesses to make meaningful measurements. a single access is much smaller than the measurement rate for all four methods. second, be sure that your program spends as much time as possible doing variable accesses rather than other things such as calling timing functions or incrementing for loop counters.
3.2 implement sequential search and binary search algorithms on your computer. run timings for each algorithm on arrays of size n = 10i for i ranging from 1 to as large a value as your computer’s memory and compiler will allow. for both algorithms, store the values 0 through n − 1 in order in the array, and
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
figure 5.21 final stage in the heap-building algorithm. both subtrees of node r are heaps. all that remains is to push r down to its proper level in the heap.
figure 5.22 the siftdown operation. the subtrees of the root are assumed to be heaps. (b) values 1 and 7 are swapped. (c) values 1 and 6 are swapped to form the ﬁnal heap.
array, with the ﬁrst internal node. the exchanges shown in figure 5.20(b) result from this process. method buildheap implements the building algorithm.
what is the cost of buildheap? clearly it is the sum of the costs for the calls to siftdown. each siftdown operation can cost at most the number of levels it takes for the node being sifted to reach the bottom of the tree. in any complete tree, approximately half of the nodes are leaves and so cannot be moved downward at all. one quarter of the nodes are one level above the leaves, and so their elements can move down at most one level. at each step up the tree we get half the number of nodes as were at the previous level, and an additional height of one. the maximum sum of total distances that elements can go is therefore i − 1 2i−1 .
from equation 2.9 we know that this summation has a closed-form solution of approximately 2, so this algorithm takes Θ(n) time in the worst case. this is far better than building the heap one element at a time, which would cost Θ(n log n) in the worst case. it is also faster than the Θ(n log n) average-case time and Θ(n2) worst-case time required to build the bst.
in the worst case, quicksort is Θ(n2). this is terrible, no better than bubble sort.2 when will this worst case occur? only when each pivot yields a bad partitioning of the array. if the pivot values are selected at random, then this is extremely unlikely to happen. when selecting the middle position of the current subarray, it is still unlikely to happen. it does not take many good partitionings for quicksort to work fairly well.
quicksort’s best case occurs when findpivot always breaks the array into two equal halves. quicksort repeatedly splits the array into smaller partitions, as shown in figure 7.9. in the best case, the result will be log n levels of partitions, with the top level having one array of size n, the second level two arrays of size n/2, the next with four arrays of size n/4, and so on. thus, at each level, all partition steps for that level do a total of n work, for an overall cost of n log n work when quicksort ﬁnds perfect pivots.
quicksort’s average-case behavior falls somewhere between the extremes of worst and best case. average-case analysis considers the cost for all possible arrangements of input, summing the costs and dividing by the number of cases. we make one reasonable simplifying assumption: at each partition step, the pivot is equally likely to end in any position in the (sorted) array. in other words, the pivot is equally likely to break an array into partitions of sizes 0 and n−1, or 1 and n−2, and so on.
this equation is in the form of a recurrence relation. recurrence relations are discussed in chapters 2 and 14, and this one is solved in section 14.2.4. this equation says that there is one chance in n that the pivot breaks the array into subarrays of size 0 and n − 1, one chance in n that the pivot breaks the array into subarrays of size 1 and n− 2, and so on. the expression “t(k) + t(n− 1− k)” is the cost for the two recursive calls to quicksort on two arrays of size k and n−1−k.
while lists are most commonly ordered by key value, this is not the only viable option. another approach to organizing lists to speed search is to order the records by expected frequency of access. while the beneﬁts might not be as great as when oganized by key value, the cost to organize by frequency of access is much cheaper, and thus is of use in some situations.
assume that we know, for each key ki, the probability pi that the record with key ki will be requested. assume also that the list is ordered so that the most frequently requested record is ﬁrst, then the next most frequently requested record, and so on. search in the list will be done sequentially, beginning with the ﬁrst position. over the course of many searches, the expected number of comparisons required for one search is
in other words, the cost to access the ﬁrst record is one (because one key value is looked at), and the probability of this occurring is p1. the cost to access the second record is two (because we must look at the ﬁrst and the second records’ key values), with probability p2, and so on. for n records, assuming that all searches are for records that actually exist, the probabilities p1 through pn must sum to one.
example 9.1 calculate the expected cost to search a list when each record has equal chance of being accessed (the classic sequential search through an unsorted list). setting pi = 1/n yields
this result matches our expectation that half the records will be accessed on average by normal sequential search. if the records truly have equal access probabilities, then ordering records by frequency yields no beneﬁt. we saw in section 9.1 the more general case where we must conside the probability (labeled p0) that the search key does not match that for any record in the array. in that case, in accordance with our general formula, we get
some applications must represent a large, two-dimensional matrix where many of the elements have a value of zero. one example is the lower triangular matrix that results from solving systems of simultaneous equations. a lower triangular matrix stores zero values at positions [r, c] such that r < c, as shown in figure 12.6(a). thus, the upper-right triangle of the matrix is always zero. another example is the representation of undirected graphs in an adjacency matrix (see project 11.2). because all edges between vertices i and j go in both directions, there is no need to store both. instead we can just store one edge going from the higher-indexed vertex to the lower-indexed vertex. in this case, only the lower triangle of the matrix can have non-zero values. we can take advantage of this fact to save space. instead of storing n(n + 1)/2 pieces of information in an n × n array, it would save space to use a list of length n(n + 1)/2. this is only practical if some means can be found to locate within the list the element that would correspond to position [r, c] in the original matrix.
to derive an equation to do this computation, note that row 0 of the matrix has one non-zero value, row 1 has two non-zero values, and so on. thus, row r k=1 k = (r2 + r)/2 non-zero elements. adding c to reach the cth position in the rth row yields the following equation to convert position [r, c] in the original matrix to the correct position in the list.
a similar equation can be used to store an upper triangular matrix, that is, a matrix with zero values at positions [r, c] such that r > c, as shown in figure 12.6(b). for an n × n upper triangular matrix, the equation would be
a more difﬁcult situation arises when the vast majority of values stored in an n × n matrix are zero, but there is no restriction on which positions are zero and which are non-zero. this is known as a sparse matrix.
this book contains many examples of asymptotic analysis of the time requirements for algorithms and the space requirements for data structures. often it is easy to invent an equation to model the behavior of the algorithm or data structure in question, and also easy to derive a closed-form solution for the equation should it contain a recurrence or summation.
sometimes an analysis proves more difﬁcult. it may take a clever insight to derive the right model, such as the snowplow argument for analyzing the average run length resulting from replacement selection (section 8.5.2). in this case, once the snowplow argument is understood, the resulting equations are simple. sometimes, developing the model is straightforward but analyzing the resulting equations is not. an example is the average-case analysis for quicksort. the equation given in section 7.5 simply enumerates all possible cases for the pivot position, summing corresponding costs for the recursive calls to quicksort. however, deriving a closed-form solution for the resulting recurrence relation is not as easy.
many iterative algorithms require that we compute a summation to determine the cost of a loop. techniques for ﬁnding closed-form solutions to summations are presented in section 14.1. time requirements for many algorithms based on recursion are best modeled by recurrence relations. a discussion of techniques for solving recurrences is provided in section 14.2. these sections extend the introduction to summations and recurrences provided in section 2.4, so the reader should already be familiar with that material.
section 14.3 provides an introduction to the topic of amortized analysis. amortized analysis deals with the cost of a series of operations. perhaps a single operation in the series has high cost, but as a result the cost of the remaining operations is limited in such a way that the entire series can be done efﬁciently. amortized analysis has been used successfully to analyze several of the algorithms presented in
the ith term of 2f (n) is i · 2i+1, while the (i + 1)th term of f (n) is (i + 1) · 2i+1. subtracting one expression from the other yields the summation of 2i and a few non-canceled terms:
14.2.2 expanding recurrences estimating bounds is effective if you only need an approximation to the answer. more precise techniques are required to ﬁnd an exact solution. one such technique is called expanding the recurrence. in this method, the smaller terms on the right side of the equation are in turn replaced by their deﬁnition. this is the expanding step. these terms are again expanded, and so on, until a full series with no recurrence results. this yields a summation, and techniques for solving summations can then be used. a couple of simple expansions were shown in section 2.4. a more complex example is given below.
this is the exact solution to the recurrence for n a power of two. at this point, we should use a simple induction proof to verify that our solution is indeed correct.
most programs contain loop constructs. when analyzing running time costs for programs with loops, we need to add up the costs for each time the loop is executed. this is an example of a summation. summations are simply the sum of costs for some function applied to a range of parameter values. summations are typically written with the following “sigma” notation:
this notation indicates that we are summing the value of f(i) over some range of (integer) values. the parameter to the expression and its initial value are indicated
below thep symbol. here, the notation i = 1 indicates that the parameter is i and that it begins with the value 1. at the top of thep symbol is the expression n. this
indicates the maximum value for the parameter i. thus, this notation means to sum the values of f(i) as i ranges from 1 through n. this can also be written
given a summation, you often wish to replace it with a direct equation with the same value as the summation. this is known as a closed-form solution, and the process of replacing the summation with its closed-form solution is known as solvi=1 1 is simply the expression “1” summed n times (remember that i ranges from 1 to n). because the sum of n 1s is n, the closed-form solution is n. the following is a list of useful summations, along with their closed-form solutions.
the sum of reciprocals from 1 to n, called the harmonic series and written hn, has a value between loge n and loge n + 1. to be more precise, as n grows, the summation grows closer to
most of these equalities can be proved easily by mathematical induction (see section 2.6.3). unfortunately, induction does not help us derive a closed-form solution. it only conﬁrms when a proposed closed-form solution is correct. techniques for deriving closed-form solutions are discussed in section 14.1.
the running time for a recursive algorithm is most easily expressed by a recursive expression because the total time for the recursive algorithm includes the time to run the recursive call(s). a recurrence relation deﬁnes a function by means of an expression that includes one or more (smaller) instances of itself. a classic example is the recursive deﬁnition for the factorial function:
most programs contain loop constructs. when analyzing running time costs for programs with loops, we need to add up the costs for each time the loop is executed. this is an example of a summation. summations are simply the sum of costs for some function applied to a range of parameter values. summations are typically written with the following “sigma” notation:
this notation indicates that we are summing the value of f(i) over some range of (integer) values. the parameter to the expression and its initial value are indicated
below thep symbol. here, the notation i = 1 indicates that the parameter is i and that it begins with the value 1. at the top of thep symbol is the expression n. this
indicates the maximum value for the parameter i. thus, this notation means to sum the values of f(i) as i ranges from 1 through n. this can also be written
given a summation, you often wish to replace it with a direct equation with the same value as the summation. this is known as a closed-form solution, and the process of replacing the summation with its closed-form solution is known as solvi=1 1 is simply the expression “1” summed n times (remember that i ranges from 1 to n). because the sum of n 1s is n, the closed-form solution is n. the following is a list of useful summations, along with their closed-form solutions.
c1n2 + c2n + c3 for some constants c1, c2, and c3. if this is the case, we can plug in the answers to small cases of the summation to solve for the coefﬁcients. for this example, substituting 0, 1, and 2 for n leads to three simultaneous equations. because the summation when n = 0 is just 0, c3 must be 0. for n = 1 and n = 2 we get the two equations
at this point, we still must do the “test” part of the guess-and-test approach. we can use an induction proof to verify whether our candidate closed-form solution is correct. in this case it is indeed correct, as shown by example 2.11. the induction proof is necessary because our initial assumption that the solution is a simple polynomial could be wrong. for example, it might have been possible that the true solution includes a logarithmic term, such as c1n2 + c2n log n. the process shown here is essentially ﬁtting a curve to a ﬁxed number of points. because there is always an n-degree polynomial that ﬁts n + 1 points, we had not done enough work to be sure that we to know the true equation without the induction proof.
guess-and-test is useful whenever the solution is a polynomial expression. in i=1 i2, or more generally i=1 ic for c any positive integer. why is this not a universal approach to solving summations? because many summations do not have a polynomial as their closed form solution.
a more general approach is based on the subtract-and-guess or divide-andguess strategies. one form of subtract-and-guess is known as the shifting method. the shifting method subtracts the summation from a variation on the summation. the variation selected for the subtraction should be one that makes most of the terms cancel out. to solve sum f, we pick a known function g and ﬁnd a pattern in terms of f(n) − g(n) or f(n)/g(n).
the ith term of 2f (n) is i · 2i+1, while the (i + 1)th term of f (n) is (i + 1) · 2i+1. subtracting one expression from the other yields the summation of 2i and a few non-canceled terms:
the cn term is an upper bound on the findpivot and partition steps. this equation comes from assuming that the partitioning element is equally likely to occur in any position k. it can be simpliﬁed by observing that the two recurrence terms t(k) and t(n − 1 − k) are equivalent, because one simply counts up from t (0) to t (n − 1) while the other counts down from t (n − 1) to t (0). this yields
this form is known as a recurrence with full history. the key to solving such a recurrence is to cancel out the summation terms. the shifting method for summations provides a way to do this. multiply both sides by n and subtract the result from the formula for nt(n + 1):
at this point, we have eliminated the summation and can now use our normal methods for solving recurrences to get a closed-form solution. note that c(2n+1) n+1 < 2c, so we can simplify the result. expanding the recurrence, we get
permutations: a permutation of a sequence s is simply the members of s arranged in some order. for example, a permutation of the integers 1 through n would be those values arranged in some order. if the sequence contains n distinct members, then there are n! different permutations for the sequence. this is because there are n choices for the ﬁrst member in the permutation; for each choice of ﬁrst member there are n − 1 choices for the second member, and so on. sometimes one would like to obtain a random permutation for a sequence, that is, one of the n! possible permutations is selected in such a way that each permutation has equal probability of being selected. a simple java function for generating a random permutation is as follows. here, the n values of the sequence are stored in positions 0 through n − 1 of array a, function swap(a, i, j) exchanges elements i and j in array a, and random(n) returns an integer value in the range 0 to n − 1 (see the appendix for more information on swap and random).
boolean variables: a boolean variable is a variable (of type boolean in java) that takes on one of the two values true and false. these two values are often associated with the values 1 and 0, respectively, although there is no reason why this needs to be the case. it is poor programming practice to rely on the correspondence between 0 and false, because these are logically distinct objects of different types. floor and ceiling: the ﬂoor of x (written bxc) takes real value x and returns the greatest integer ≤ x. for example, b3.4c = 3, as does b3.0c, while b−3.4c = −4 and b−3.0c = −3. the ceiling of x (written dxe) takes real value x and returns the least integer ≥ x. for example, d3.4e = 4, as does d4.0e, while d−3.4e = d−3.0e = −3. modulus operator: the modulus (or mod) function returns the remainder of an integer division. sometimes written n mod m in mathematical expressions, the syntax for the java modulus operator is n % m. from the deﬁnition of remainder, n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|. therefore, the result of n mod m must be between 0 and m − 1 when n and m are
this and the following chapter treat these three approaches in turn. any of these approaches are potentially suitable for implementing the dictionary adt introduced in section 4.4. however, each has different performance characteristics that make it the method of choice in particular circumstances.
the current chapter considers methods for searching data stored in lists and tables. a table is simply another term for an array. list in this context means any list implementation including a linked list or an array. most of these methods are appropriate for sequences (i.e., duplicate key values are allowed), although special techniques applicable to sets are discussed in section 9.3. the techniques from the ﬁrst three sections of this chapter are most appropriate for searching a collection of records stored in ram. section 9.4 discusses hashing, a technique for organizing data in a table such that the location of each record within the table is a function of its key value. hashing is appropriate when records are stored either in ram or on disk.
chapter 10 discusses tree-based methods for organizing information on disk, including a commonly used ﬁle structure called the b-tree. nearly all programs that must organize large collections of records stored on disk use some variant of either hashing or the b-tree. hashing is practical for only certain access functions (exactmatch queries) and is generally appropriate only when duplicate key values are not allowed. b-trees are the method of choice for disk-based applications anytime hashing is not appropriate.
the simplest form of search has already been presented in example 3.1: the sequential search algorithm. sequential search on an unsorted list requires Θ(n) time in the worst case.
how many comparisons does linear search do on average? a major consideration is whether k is in list l at all. we can simplify our analysis by ignoring everything about the input except the position of k if it is found in l. thus, we have n + 1 distinct possible events: that k is in one of positions 0 to n − 1 in l (each with its own probability), or that it is not in l at all. we can express the probability that k is not in l as
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
of the data on disk. likewise, when writing to a particular logical byte position with respect to the beginning of the ﬁle, this position must be converted by the ﬁle manager into the corresponding physical location on the disk. to gain some appreciation for the the approximate time costs for these operations, you need to understand the physical structure and basic workings of a disk drive.
disk drives are often referred to as direct access storage devices. this means that it takes roughly equal time to access any record in the ﬁle. this is in contrast to sequential access storage devices such as tape drives, which require the tape reader to process data from the beginning of the tape until the desired position has been reached. as you will see, the disk drive is only approximately direct access: at any given time, some records are more quickly accessible than others.
8.2.1 disk drive architecture a hard disk drive is composed of one or more round platters, stacked one on top of another and attached to a central spindle. platters spin continuously at a constant rate. each usable surface of each platter is assigned a read/write head or i/o head through which data are read or written, somewhat like the arrangement of a phonograph player’s arm “reading” sound from a phonograph record. unlike a phonograph needle, the disk read/write head does not actually touch the surface of a hard disk. instead, it remains slightly above the surface, and any contact during normal operation would damage the disk. this distance is very small, much smaller than the height of a dust particle. it can be likened to a 5000-kilometer airplane trip across the united states, with the plane ﬂying at a height of one meter!
a hard disk drive typically has several platters and several read/write heads, as shown in figure 8.2(a). each head is attached to an arm, which connects to the boom. the boom moves all of the heads in or out together. when the heads are in some position over the platters, there are data on each platter directly accessible to each head. the data on a single platter that are accessible to any one position of the head for that platter are collectively called a track, that is, all data on a platter that are a ﬁxed distance from the spindle, as shown in figure 8.2(b). the collection of all tracks that are a ﬁxed distance from the spindle is called a cylinder. thus, a cylinder is all of the data that can be read when the arms are in a particular position. each track is subdivided into sectors. between each sector there are intersector gaps in which no data are stored. these gaps allow the read head to recognize the end of a sector. note that each sector contains the same amount of data. because the outer tracks have greater length, they contain fewer bits per inch than do the inner tracks. thus, about half of the potential storage space is wasted, because only the innermost tracks are stored at the highest possible data density. this ar-
• write(byte[] b): write some bytes at the current position in the ﬁle (overwriting the bytes already at that position). the current position moves forward as the bytes are written.
we now consider the problem of sorting collections of records too large to ﬁt in main memory. because the records must reside in peripheral or external memory, such sorting methods are called external sorts. this is in contrast to the internal sorts discussed in chapter 7 which assume that the records to be sorted are stored in main memory. sorting large collections of records is central to many applications, such as processing payrolls and other large business databases. as a consequence, many external sorting algorithms have been devised. years ago, sorting algorithm designers sought to optimize the use of speciﬁc hardware conﬁgurations, such as multiple tape or disk drives. most computing today is done on personal computers and low-end workstations with relatively powerful cpus, but only one or at most two disk drives. the techniques presented here are geared toward optimized processing on a single disk drive. this approach allows us to cover the most important issues in external sorting while skipping many less important machine-dependent details. readers who have a need to implement efﬁcient external sorting algorithms that take advantage of more sophisticated hardware conﬁgurations should consult the references in section 8.6.
when a collection of records is too large to ﬁt in main memory, the only practical way to sort it is to read some records from disk, do some rearranging, then write them back to disk. this process is repeated until the ﬁle is sorted, with each record read perhaps many times. given the high cost of disk i/o, it should come as no surprise that the primary goal of an external sorting algorithm is to minimize the amount of information that must be read from or written to disk. a certain amount of additional cpu processing can proﬁtably be traded for reduced disk access.
before discussing external sorting techniques, consider again the basic model for accessing information from disk. the ﬁle to be sorted is viewed by the programmer as a sequential series of ﬁxed-size blocks. assume (for simplicity) that each
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
removing the maximum (root) value from a heap containing n elements requires that we maintain the complete binary tree shape, and that the remaining n − 1 node values conform to the heap property. we can maintain the proper shape by moving the element in the last position in the heap (the current last element in the array) to the root position. we now consider the heap to be one element smaller. unfortunately, the new root value is probably not the maximum value in the new heap. this problem is easily solved by using siftdown to reorder the heap. because the heap is log n levels deep, the cost of deleting the maximum element is Θ(log n) in the average and worst cases.
the heap is a natural implementation for the priority queues discussed at the beginning of this section. jobs can be added to the heap (using their priority value as the ordering key) when needed. method removemax can be called whenever a new job is to be executed.
some applications of priority queues require the ability to change the priority of an object already stored in the queue. this might require that the object’s position in the heap representation be updated. unfortunately, a max-heap is not efﬁcient when searching for an arbitrary value; it is only good for ﬁnding the maximum value. however, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. the remove method takes as input the position of the node to be removed from the heap. a typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efﬁcient search for objects (such as a bst). records in the auxiliary data structure will store the object’s heap index, so that the object can be deleted from the heap and reinserted with its new priority (see project 5.5). sections 11.4.1 and 11.5.1 present applications for a priority queue with priority updating.
the space/time tradeoff principle from section 3.9 suggests that one can often gain an improvement in space requirements in exchange for a penalty in running time. there are many situations where this is a desirable tradeoff. a typical example is storing ﬁles on disk. if the ﬁles are not actively used, the owner might wish to compress them to save space. later, they can be uncompressed for use, which costs some time, but only once.
we often represent a set of items in a computer program by assigning a unique code to each item. for example, the standard ascii coding scheme assigns a unique eight-bit value to each character. it takes a certain minimum number of bits to provide unique codes for each character. for example, it takes dlog 128e or
compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. most commercial compression programs use two or three coding schemes to adjust to different types of ﬁles.
in the preceding example, “deed” was coded in 8 bits, a saving of 33% over the twelve bits required from a ﬁxed-length coding. however, “muck” requires 18 bits, more space than required by the corresponding ﬁxed-length coding. the problem is that “muck” is composed of letters that are not expected to occur often. if the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either.
see shaffer and brown [sb93] for an example of a tree implementation where an internal node pointer ﬁeld stores the value of its child instead of a pointer to its child when the child is a leaf node.
many techniques exist for maintaining reasonably balanced bsts in the face of an unfriendly series of insert and delete operations. one example is the avl tree of adelson-velskii and landis, which is discussed by knuth [knu98]. the avl tree (see section 13.2) is actually a bst whose insert and delete routines reorganize the tree structure so as to guarantee that the subtrees rooted by the children of any node will differ in height by at most one. another example is the splay tree [st85], also discussed in section 13.2.
the proof of section 5.6.1 that the huffman coding tree has minimum external path weight is from knuth [knu97]. for more information on data compression techniques, see managing gigabytes by witten, moffat, and bell [wmb99], and codes and cryptography by dominic welsh [wel88]. tables 5.23 and 5.24 are derived from welsh [wel88].
5.2 deﬁne the degree of a node as the number of its non-empty children. prove by induction that the number of degree 2 nodes in any binary tree is one less than the number of leaves.
5.3 deﬁne the internal path length for a tree as the sum of the depths of all internal nodes, while the external path length is the sum of the depths of all
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
occupied the slot but does so no longer. if a tombstone is encountered when searching through a probe sequence, the search procedure is to continue with the search. when a tombstone is encountered during insertion, that slot can be used to store the new record. however, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. however, the new record would actually be inserted into the slot of the ﬁrst tombstone encountered. the use of tombstones allows searches to work correctly and allows reuse of deleted slots. however, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. this will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. a typical database application will ﬁrst load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. after the table is loaded with the initial collection of records, the ﬁrst few deletions will lengthen the average probe sequence distance for records (it will add tombstones). over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by ﬁlling in tombstone slots. for example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). after a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. this seems like a small increase, but it is three times longer on average beyond the home position than before deletions.
two possible solutions to this problem are 1. do a local reorganization upon deletion to try to shorten the average path length. for example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove a key from its probe sequence). this will not work for all collision resolution policies. 2. periodically rehash the table by reinserting all records into a new hash table. not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions.
for a comparison of the efﬁciencies for various self-organizing techniques, see bentley and mcgeoch, “amortized analysis of self-organizing sequential search heuristics” [bm85]. the text compression example of section 9.2 comes from
be sure to indicate how you are using h1 and h2 to do the hashing. function rev(k) reverses the decimal digits of k, for example, rev(37) = 73; rev(7) = 7. h1(k) = k mod 13. h2(k) = (rev(k + 1) mod 11). keys: 2, 8, 31, 20, 19, 18, 53, 27.
9.19 write an algorithm for a deletion function for hash tables that replaces the record with a special value indicating a tombstone. modify the functions hashinsert and hashsearch to work correctly with tombstones.
analyze what will happen if this permutation is used by an implementation of pseudo-random probing on a hash table of size seven. will this permutation solve the problem of primary clustering? what does this say about selecting a permutation for use when implementing pseudo-random probing?
9.1 implement a binary search and the quadratic binary search of section 9.1. run your implementations over a large range of problem sizes, timing the results for each algorithm. graph and compare these timing results.
9.2 implement the three self-organizing list heuristics count, move-to-front, and transpose. compare the cost for running the three heuristics on various input data. the cost metric should be the total number of comparisons required when searching the list. it is important to compare the heuristics using input data for which self-organizing lists are reasonable, that is, on frequency distributions that are uneven. one good approach is to read text ﬁles. the list should store individual words in the text ﬁle. begin with an empty list, as was done for the text compression example of section 9.2. each time a word is encountered in the text ﬁle, search for it in the self-organizing list. if the word is found, reorder the list as appropriate. if the word is not in the list, add it to the end of the list and then reorder as appropriate.
9.3 implement the text compression system described in section 9.2. 9.4 implement a system for managing document retrieval. your system should have the ability to insert (abstract references to) documents into the system, associate keywords with a given document, and to search for documents with speciﬁed keywords.
it is also true that the equation of example 3.7 is in Ω(n). however, as with big-oh notation, we wish to get the “tightest” (for Ω notation, the largest) bound possible. thus, we prefer to say that this running time is in Ω(n2).
recall the sequential search algorithm to ﬁnd a value k within an array of integers. in the average and worst cases this algorithm is in Ω(n), because in both the average and worst cases we must examine at least cn values (where c is 1/2 in the average case and 1 in the worst case).
3.4.3 Θ notation the deﬁnitions for big-oh and Ω give us ways to describe the upper bound for an algorithm (if we can ﬁnd an equation for the maximum cost of a particular class of inputs of size n) and the lower bound for an algorithm (if we can ﬁnd an equation for the minimum cost for a particular class of inputs of size n). when the upper and lower bounds are the same within a constant factor, we indicate this by using Θ (big-theta) notation. an algorithm is said to be Θ(h(n)) if it is in o(h(n)) and it is in Ω(h(n)). note that we drop the word “in” for Θ notation, because there is a strict equality for two equations with the same Θ. in other words, if f(n) is Θ(g(n)), then g(n) is Θ(f(n)).
given an algebraic equation describing the time requirement for an algorithm, the upper and lower bounds always meet. that is because in some sense we have a perfect analysis for the algorithm, embodied by the running-time equation. for
for this equation for t(n), it is true that all inputs of size n take at least cn time. but an inﬁnite number of inputs of size n take cn2 time, so we would like to say that the algorithm is in Ω(n2). unfortunately, using our ﬁrst deﬁnition will yield a lower bound of Ω(n) because it is not possible to pick constants c and n0 such that t(n) ≥ cn2 for all n > n0. the alternative deﬁnition does result in a lower bound of Ω(n2) for this algorithm, which seems to ﬁt common sense more closely. fortunately, few real algorithms or computer programs display the pathological behavior of this example. our ﬁrst deﬁnition for Ω generally yields the expected result.
rule (3) says that given two parts of a program run in sequence (whether two statements or two sections of code), you need consider only the more expensive part. this rule applies to Ω and Θ notations as well: for both, you need consider only the more expensive part.
rule (4) is used to analyze simple loops in programs. if some action is repeated some number of times, and each repetition has the same cost, then the total cost is the cost of the action multiplied by the number of times that the action takes place. this rule applies to Ω and Θ notations as well.
taking the ﬁrst three rules collectively, you can ignore all constants and all lower-order terms to determine the asymptotic growth rate for any cost function. the advantages and dangers of ignoring constants were discussed near the beginning of this section. ignoring lower-order terms is reasonable when performing an asymptotic analysis. the higher-order terms soon swamp the lower-order terms in their contribution to the total cost as n becomes larger. thus, if t(n) = 3n4 + 5n2, then t(n) is in o(n4). the n2 term contributes relatively little to the total cost.
3.4.5 classifying functions given functions f(n) and g(n) whose growth rates are expressed as algebraic equations, we might like to determine if one grows faster than the other. the best way to do this is to take the limit of the two functions as n grows towards inﬁnity,
if the limit goes to ∞, then f(n) is in Ω(g(n)) because f(n) grows faster. if the limit goes to zero, then f(n) is in o(g(n)) because g(n) grows faster. if the limit goes to some constant other than zero, then f(n) = Θ(g(n)) because both grow at the same rate.
3.15 does every algorithm have a Θ running-time equation? in other words, are the upper and lower bounds for the running time (on any speciﬁed class of inputs) always the same?
3.16 does every problem for which there exists some algorithm have a Θ runningtime equation? in other words, for every problem, and for any speciﬁed class of inputs, is there some algorithm whose upper bound is equal to the problem’s lower bound?
3.17 given an array storing integers ordered by value, modify the binary search routine to return the position of the ﬁrst integer with value k in the situation where k can appear multiple times in the array. be sure that your algorithm is Θ(log n), that is, do not resort to sequential search once an occurrence of k is found.
3.18 given an array storing integers ordered by value, modify the binary search routine to return the position of the integer with the greatest value less than k when k itself does not appear in the array. return error if the least value in the array is greater than k.
3.19 modify the binary search routine to support search in an array of inﬁnite size. in particular, you are given as input a sorted array and a key value k to search for. call n the position of the smallest value in the array that is equal to or larger than x. provide an algorithm that can determine n in o(log n) comparisons in the worst case. explain why your algorithm meets the required time bound.
3.20 it is possible to change the way that we pick the dividing point in a binary search, and still get a working search routine. however, where we pick the dividing point could affect the performance of the algorithm. (a) if we change the dividing point computation in function binary from i = (l + r)/2 to i = (l + ((r − l)/3)), what will the worst-case running time be in asymptotic terms? if the difference is only a constant time factor, how much slower or faster will the modiﬁed program be compared to the original version of binary?
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
11.3.3 topological sort assume that we need to schedule a series of tasks, such as classes or construction jobs, where we cannot start one task until after its prerequisites are completed. we wish to organize the tasks into a linear order that allows us to complete them one at a time without violating any prerequisites. we can model the problem using a dag. the graph is directed because one task is a prerequisite of another — the vertices have a directed relationship. it is acyclic because a cycle would indicate a conﬂicting series of prerequisites that could not be completed without violating at least one prerequisite. the process of laying out the vertices of a dag in a linear order to meet the prerequisite rules is called a topological sort. figure 11.13 illustrates the problem. an acceptable topological sort for this example is j1, j2, j3, j4, j5, j6, j7.
a topological sort may be found by performing a dfs on the graph. when a vertex is visited, no action is taken (i.e., function previsit does nothing). when the recursion pops back to that vertex, function postvisit prints the vertex. this yields a topological sort in reverse order. it does not matter where the sort starts, as long as all vertices are visited in the end. here is an implementation for the dfs-based algorithm.
using this algorithm starting at j1 and visiting adjacent neighbors in alphabetic order, vertices of the graph in figure 11.13 are printed out in the order j7, j5, j4, j6, j2, j3, j1. when reversed, this yields the legal topological sort j1, j3, j2, j6, j4, j5, j7.
we can also implement topological sort using a queue instead of recursion. to do so, we ﬁrst visit all edges, counting the number of edges that lead to each vertex (i.e., count the number of prerequisites for each vertex). all vertices with no
queue<integer> q = new aqueue<integer>(g.n()); int[] count = new int[g.n()]; int v; for (v=0; v<g.n(); v++) count[v] = 0; // initialize for (v=0; v<g.n(); v++)
prerequisites are placed on the queue. we then begin processing the queue. when vertex v is taken off of the queue, it is printed, and all neighbors of v (that is, all vertices that have v as a prerequisite) have their counts decremented by one. any neighbor whose count is now zero is placed on the queue. if the queue becomes empty without printing all of the vertices, then the graph contains a cycle (i.e., there is no possible ordering for the tasks that does not violate some prerequisite). the printed order for the vertices of the graph in figure 11.13 using the queue version of topological sort is j1, j2, j3, j6, j4, j5, j7. figure 11.14 shows an implementation for the queue-based topological sort algorithm.
parptrtree a = new parptrtree(g.n()); // equivalence array kruskalelem[] e = new kruskalelem[g.e()]; // minheap array int edgecnt = 0; // count of edges
11.8 the bfs topological sort algorithm can report the existence of a cycle if one is encountered. modify this algorithm to print the vertices possibly appearing in cycles (if there are any cycles).
11.9 explain why, in the worst case, dijkstra’s algorithm is (asymptotically) as efﬁcient as any algorithm for ﬁnding the shortest path from some vertex i to another vertex j.
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
2.5 deﬁne an adt for a set of integers (remember that a set has no concept of duplicate elements, and has no concept of order). your adt should consist of the functions that can be performed on a set to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.6 deﬁne an adt for a bag of integers (remember that a bag may contain duplicates, and has no concept of order). your adt should consist of the functions that can be performed on a bag to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.7 deﬁne an adt for a sequence of integers (remember that a sequence may contain duplicates, and supports the concept of position for its elements). your adt should consist of the functions that can be performed on a sequence to control its membership, check the size, check if a given element is in the set, and so on. each function should be deﬁned in terms of its input and output.
2.8 an investor places $30,000 into a stock fund. 10 years later the account has a value of $69,000. using logarithms and anti-logarithms, present a formula for calculating the average annual rate of increase. then use your formula to determine the average annual growth rate for this fund.
2.9 rewrite the factorial function of section 2.5 without using recursion. 2.10 rewrite the for loop for the random permutation generator of section 2.2
// fibr(91) is the largest value that fits in a long assert (n > 0) && (n <= 91) : "n out of range"; if ((n == 1) || (n == 2)) return 1; return fibr(n-1) + fibr(n-2);
case. however, there is always the possibility that the bst will become unbalanced, leading to bad performance. instead, we would like to ﬁnd a data structure that is guaranteed to have good performance for this special application.
this section presents the heap4 data structure. a heap is deﬁned by two properties. first, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees presented in section 5.3.3. second, the values stored in a heap are partially ordered. this means that there is a relationship between the value stored at any node and the values of its children. there are two variants of the heap, depending on the deﬁnition of this relationship. a max-heap has the property that every node stores a value that is greater than or equal to the value of either of its children. because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree.
a min-heap has the property that every node stores a value that is less than or equal to that of its children. because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree.
note that there is no necessary relationship between the value of a node and that of its sibling in either the min-heap or the max-heap. for example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. we can contrast bsts and heaps by the strength of their ordering relationships. a bst deﬁnes a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right.” in contrast, a heap implements a partial order. given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendent of the other.
min-heaps and max-heaps both have their uses. for example, the heapsort of section 7.6 uses the max-heap, while the replacement selection algorithm of section 8.5.2 uses a min-heap. the examples in the rest of this section will use a max-heap.
be careful not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. the two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array.
the ﬁrst two lines of the function constitute the base cases. if n ≤ 1, then one of the base cases computes a solution for the problem. if n > 1, then fact calls a function that knows how to ﬁnd the factorial of n − 1. of course, the function that knows how to compute the factorial of n − 1 happens to be fact itself. but we should not think too hard about this while writing the algorithm. the design for recursive algorithms can always be approached in this way. first write the base cases. then think about solving the problem by combining the results of one or more smaller — but similar — subproblems. if the algorithm you write is correct, then certainly you can rely on it (recursively) to solve the smaller subproblems. the secret to success is: do not worry about how the recursive call solves the subproblem. simply accept that it will solve it correctly, and use this result to in turn correctly solve the original problem. what could be simpler?
recursion has no counterpart in everyday problem solving. the concept can be difﬁcult to grasp because it requires you to think about problems in a new way. to use recursion effectively, it is necessary to train yourself to stop analyzing the recursive process beyond the recursive call. the subproblems will take care of themselves. you just worry about the base cases and how to recombine the subproblems.
the recursive version of the factorial function might seem unnecessarily complicated to you because the same effect can be achieved by using a while loop. here is another example of recursion, based on a famous puzzle called “towers of hanoi.” the natural algorithm to solve this problem has multiple recursive calls. it cannot be rewritten easily using while loops.
the towers of hanoi puzzle begins with three poles and n rings, where all rings start on the leftmost pole (labeled pole 1). the rings each have a different size, and are stacked in order of decreasing size with the largest ring at the bottom, as shown in figure 2.2.a. the problem is to move the rings from the leftmost pole to the rightmost pole (labeled pole 3) in a series of steps. at each step the top ring on some pole is moved to another pole. there is one limitation on where rings may be moved: a ring can never be moved on top of a smaller ring.
if (n == 0) return; toh(n-1, start, temp, goal); // recursive call: n-1 rings move(start, goal); toh(n-1, temp, goal, start); // recursive call: n-1 rings
those who are unfamiliar with recursion might ﬁnd it hard to accept that it is used primarily as a tool for simplifying the design and description of algorithms. a recursive algorithm usually does not yield the most efﬁcient computer program for solving the problem because recursion involves function calls, which are typically more expensive than other alternatives such as a while loop. however, the recursive approach usually provides an algorithm that is reasonably efﬁcient in the sense discussed in chapter 3. (but not always! see exercise 2.11.) if necessary, the clear, recursive solution can later be modiﬁed to yield a faster implementation, as described in section 4.2.4.
many data structures are naturally recursive, in that they can be deﬁned as being made up of self-similar parts. tree structures are an example of this. thus, the algorithms to manipulate such data structures are often presented recursively. many searching and sorting algorithms are based on a strategy of divide and conquer. that is, a solution is found by breaking the problem into smaller (similar) subproblems, solving the subproblems, then combining the subproblem solutions to form the solution to the original problem. this process is often implemented using recursion. thus, recursion plays an important role throughout this book, and many more examples of recursive functions will be given.
solving any problem has two distinct parts: the investigation and the argument. students are too used to seeing only the argument in their textbooks and lectures. but to be successful in school (and in life after school), one needs to be good at both, and to understand the differences between these two phases of the process. to solve the problem, you must investigate successfully. that means engaging the problem, and working through until you ﬁnd a solution. then, to give the answer to your client (whether that “client” be your instructor when writing answers on a homework assignment or exam, or a written report to your boss), you need to be able to make the argument in a way that gets the solution across clearly and succinctly. the argument phase involves good technical writing skills — the ability to make a clear, logical argument.
being conversant with standard proof techniques can help you in this process. knowing how to write a good proof helps in many ways. first, it clariﬁes your
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
this book describes many data structures that can be used in a wide variety of problems. there are also many examples of efﬁcient algorithms. in general, our search algorithms strive to be at worst in o(log n) to ﬁnd a record, while our sorting algorithms strive to be in o(n log n). a few algorithms have higher asymptotic complexity, such as floyd’s all-pairs shortest-paths algorithm, whose running time is Θ(n3).
we can solve many problems efﬁciently because we have available (and choose to use) efﬁcient algorithms. given any problem for which you know some algorithm, it is always possible to write an inefﬁcient algorithm to “solve” the problem. for example, consider a sorting algorithm that tests every possible permutation of its input until it ﬁnds the correct permutation that provides a sorted list. the running time for this algorithm would be unacceptably high, because it is proportional to the number of permutations which is n! for n inputs. when solving the minimumcost spanning tree problem, if we were to test every possible subset of edges to see which forms the shortest minimum spanning tree, the amount of work would be proportional to 2|e| for a graph with |e| edges. fortunately, for both of these problems we have more clever algorithms that allow us to ﬁnd answers (relatively) quickly without explicitly testing every possible solution.
unfortunately, there are many computing problems for which the best possible algorithm takes a long time to run. a simple example is the towers of hanoi problem, which requires 2n moves to “solve” a tower with n disks. it is not possible for any computer program that solves the towers of hanoi problem to run in less than Ω(2n) time, because that many moves must be printed out.
besides those problems whose solutions must take a long time to run, there are also many problems for which we simply do not know if there are efﬁcient algorithms or not. the best algorithms that we know for such problems are very
symmetric. we can convert matrix b to a symmetric matrix in a similar manner. if symmetric matrices could be multiplied “quickly” (in particular, if they could be multiplied together in Θ(n2) time), then we could ﬁnd the result of multiplying two arbitrary n × n matrices in Θ(n2) time by taking advantage of the following observation:
there are several ways that a problem could be considered hard. for example, we might have trouble understanding the deﬁnition of the problem itself. at the beginning of a large data collection and analysis project, developers and their clients might have only a hazy notion of what their goals actually are, and need to work that out over time. for other types of problems, we might have trouble ﬁnding or understanding an algorithm to solve the problem. understanding spoken engish and translating it to written text is an example of a problem whose goals are easy to deﬁne, but whose solution is not easy to discover. but even though a natural language processing algorithm might be difﬁcult to write, the program’s running time might be fairly fast. there are many practical systems today that solve aspects of this problem in reasonable time.
none of these is what is commonly meant when a computer theoretician uses the word “hard.” throughout this section, “hard” means that the best-known algorithm for the problem is expensive in its running time. one example of a hard problem is towers of hanoi. it is easy to understand this problem and its solution. it is also easy to write a program to solve this problem. but, it takes an extremely long time to run for any “reasonably” large value of n. try running a program to solve towers of hanoi for only 30 disks!
the towers of hanoi problem takes exponential time, that is, its running time is Θ(2n). this is radically different from an algorithm that takes Θ(n log n) time or Θ(n2) time. it is even radically different from a problem that takes Θ(n4) time. these are all examples of polynomial running time, because the exponents for all terms of these equations are constants. recall from chapter 3 that if we buy a new computer that runs twice as fast, the size of problem with complexity Θ(n4) that we can solve in a certain amount of time is increased by the fourth root of two. in other words, there is a multiplicative factor increase, even if it is a rather small one. this is true for any algorithm whose running time can be represented by a polynomial.
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
figure 9.8 growth of expected record accesses with α. the horizontal axis is the value for α, the vertical axis is the expected number of accesses to the hash table. solid lines show the cost for “random” probing (a theoretical lower bound on the cost), while dashed lines show the cost for linear probing (a relatively poor collision resolution strategy). the two leftmost lines show the cost for insertion (equivalently, unsuccessful search); the two rightmost lines show the cost for deletion (equivalently, successful search).
full. beyond that point performance will degrade rapidly. this requires that the implementor have some idea of how many records are likely to be in the table at maximum loading, and select the table size accordingly.
you might notice that a recommendation to never let a hash table become more than half full contradicts the disk-based space/time tradeoff principle, which strives to minimize disk space to increase information density. hashing represents an unusual situation in that there is no beneﬁt to be expected from locality of reference. in a sense, the hashing system implementor does everything possible to eliminate the effects of locality of reference! given the disk block containing the last record accessed, the chance of the next record access coming to the same disk block is no better than random chance in a well-designed hash system. this is because a good hashing implementation breaks up relationships between search keys. instead of improving performance by taking advantage of locality of reference, hashing trades increased hash table space for an improved chance that the record will be in its home position. thus, the more space available for the hash table, the more efﬁcient hashing should be.
one important aspect of algorithm design is referred to as the space/time tradeoff principle. the space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacriﬁce space or vice versa. many programs can be modiﬁed to reduce storage requirements by “packing” or encoding information. “unpacking” or decoding the information requires additional time. thus, the resulting program uses less space but runs slower. conversely, many programs can be modiﬁed to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. typically, such changes in time and space are both by a constant factor.
a classic example of a space/time tradeoff is the lookup table. a lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. for example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. if you are writing a program that often computes factorials, it is likely to be much more time efﬁcient to simply pre-compute the 12 storable values in a table. whenever the program needs the value of n! for n ≤ 12, it can simply check the lookup table. (if n > 12, the value is too large to store as an int variable anyway.) compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table.
lookup tables can also store approximations for an expensive function such as sine or cosine. if you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. note that initially building the lookup table requires a certain amount of time. your application must use the lookup table often enough to make this initialization worthwhile.
another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. here is a simple code fragment for sorting an array of integers. we assume that this is a special case where there are n integers whose values are a permutation of the integers from 0 to n − 1. this is an example of a binsort, which is discussed in section 7.7. binsort assigns each value to an array position corresponding to its value.
this is efﬁcient and requires Θ(n) time. however, it also requires two arrays of size n. next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort).
function swap(a, i, j) exchanges elements i and j in array a (see the appendix). it may not be obvious that the second code fragment actually sorts the array. to see that this does work, notice that each pass through the for loop will at least move the integer with value i to its correct position in the array, and that during this iteration, the value of a[i] must be greater than or equal to i. a total of at most n swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. thus, this code fragment has cost Θ(n). however, it requires more time to run than the ﬁrst code fragment. on my computer the second version takes nearly twice as long to run as the ﬁrst, but it only requires half the space.
a second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk, as discussed in chapter 8 and thereafter. strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory.
the disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk.
in practice, there is not such a big difference in running time between an algorithm whose growth rate is Θ(n) and another whose growth rate is Θ(n log n). there is, however, an enormous difference in running time between algorithms with growth rates of Θ(n log n) and Θ(n2). as you shall see during the course of your study of common data structures and algorithms, it is not unusual that a problem whose obvious solution requires Θ(n2) time also has a solution that requires Θ(n log n)
the next step is to deﬁne the adt for a list object in terms of a set of operations on that object. we will use the java notation of an interface to formally deﬁne the list adt. interface list deﬁnes the member functions that any list implementation inheriting from it must support, along with their parameters and return types. we increase the ﬂexibility of the list adt by making it a template.
true to the notion of an adt, an abstract class does not specify how operations are implemented. two complete implementations are presented later in this section, both of which use the same list adt to deﬁne their operations, but they are considerably different in approaches and in their space/time tradeoffs.
figure 4.1 presents our list adt. class list is a generic of one parameter, named e. e serves as a placeholder for whatever element type the user would like to store in a list. the comments given in figure 4.1 describe precisely what each member function is intended to do. however, some explanation of the basic design is in order. given that we wish to support the concept of a sequence, with access to any position in the list, the need for many of the member functions such as insert and movetopos is clear. the key design decision embodied in this adt is support for the concept of a current position. for example, member movetostart sets the current position to be the ﬁrst element on the list, while methods next and prev move the current position to the next and previous elements, respectively. the intention is that any implementation for this adt support the concept of a current position.
given that our adt deﬁnes lists to have a current position, it is helpful to modify our list display notation to indicate this position. i will use a vertical bar, such as h20, 23 | 12, 15i to indicate the list of four elements, with the current position immediately to the right of the bar. given this conﬁguration, calling insert with value 10 will change the list to be h20, 23 | 10, 12, 15i.
if you examine figure 4.1, you should ﬁnd that the list member functions provided allow you to build a list with elements in any desired order, and to access any desired position in the list. you might have noticed that the clear method is not necessary, in that it could be implemented by means of the other member functions in the same asymptotic time. it is included merely for convenience.
method getvalue returns a reference to the current element. it is considered a violation of getvalue’s preconditions to ask for the value of a non-existent element (i.e., there must be an element at the current position). in our concrete list implementations, the java’s assert mechanism will be used to enforce such preconditions. in a commercial implementation, such violations would be best implemented by the java’s exception mechanism.
removing the maximum (root) value from a heap containing n elements requires that we maintain the complete binary tree shape, and that the remaining n − 1 node values conform to the heap property. we can maintain the proper shape by moving the element in the last position in the heap (the current last element in the array) to the root position. we now consider the heap to be one element smaller. unfortunately, the new root value is probably not the maximum value in the new heap. this problem is easily solved by using siftdown to reorder the heap. because the heap is log n levels deep, the cost of deleting the maximum element is Θ(log n) in the average and worst cases.
the heap is a natural implementation for the priority queues discussed at the beginning of this section. jobs can be added to the heap (using their priority value as the ordering key) when needed. method removemax can be called whenever a new job is to be executed.
some applications of priority queues require the ability to change the priority of an object already stored in the queue. this might require that the object’s position in the heap representation be updated. unfortunately, a max-heap is not efﬁcient when searching for an arbitrary value; it is only good for ﬁnding the maximum value. however, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. the remove method takes as input the position of the node to be removed from the heap. a typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efﬁcient search for objects (such as a bst). records in the auxiliary data structure will store the object’s heap index, so that the object can be deleted from the heap and reinserted with its new priority (see project 5.5). sections 11.4.1 and 11.5.1 present applications for a priority queue with priority updating.
the space/time tradeoff principle from section 3.9 suggests that one can often gain an improvement in space requirements in exchange for a penalty in running time. there are many situations where this is a desirable tradeoff. a typical example is storing ﬁles on disk. if the ﬁles are not actively used, the owner might wish to compress them to save space. later, they can be uncompressed for use, which costs some time, but only once.
we often represent a set of items in a computer program by assigning a unique code to each item. for example, the standard ascii coding scheme assigns a unique eight-bit value to each character. it takes a certain minimum number of bits to provide unique codes for each character. for example, it takes dlog 128e or
there are generally two approaches to minimizing disk accesses. the ﬁrst is to arrange information so that if you do access data from secondary memory, you will get what you need in as few accesses as possible, and preferably on the ﬁrst access. file structure is the term used for a data structure that organizes data stored in secondary memory. file structures should be organized so as to minimize the required number of disk accesses. the other way to minimize disk accesses is to arrange information so that each disk access retrieves additional data that can be used to minimize the need for future accesses, that is, to guess accurately what information will be needed later and retrieve it from disk now, if this can be done cheaply. as you shall see, there is little or no difference in the time required to read several hundred contiguous bytes from disk as compared to reading one byte, so this strategy is indeed practical.
one way to minimize disk accesses is to compress the information stored on disk. section 3.9 discusses the space/time tradeoff in which space requirements can be reduced if you are willing to sacriﬁce time. however, the disk-based space/time tradeoff principle stated that the smaller you can make your disk storage requirements, the faster your program will run. this is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation to unpack the data is going to be less than the disk read time saved by reducing the storage requirements. this is precisely what happens when ﬁles are compressed. cpu time is required to uncompress information, but this time is likely to be much less than the time saved by reducing the number of bytes read from disk. current ﬁle compression programs are not designed to allow random access to parts of a compressed ﬁle, so the disk-based space/time tradeoff principle cannot easily be taken advantage of in normal processing using commercial disk compression utilities. however, in the future disk drive controllers might automatically compress and decompress ﬁles stored on disk, thus taking advantage of the disk-based space/time tradeoff principle to save both space and time. many cartridge tape drives (which must process data sequentially) automatically compress and decompress information during i/o.
a java programmer views a random access ﬁle stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. this is called the logical ﬁle. the physical ﬁle actually stored on disk is usually not a contiguous series of bytes. it could well be in pieces spread all over the disk. the ﬁle manager, a part of the operating system, is responsible for taking requests for data from a logical ﬁle and mapping those requests to the physical location
figure 9.8 growth of expected record accesses with α. the horizontal axis is the value for α, the vertical axis is the expected number of accesses to the hash table. solid lines show the cost for “random” probing (a theoretical lower bound on the cost), while dashed lines show the cost for linear probing (a relatively poor collision resolution strategy). the two leftmost lines show the cost for insertion (equivalently, unsuccessful search); the two rightmost lines show the cost for deletion (equivalently, successful search).
full. beyond that point performance will degrade rapidly. this requires that the implementor have some idea of how many records are likely to be in the table at maximum loading, and select the table size accordingly.
you might notice that a recommendation to never let a hash table become more than half full contradicts the disk-based space/time tradeoff principle, which strives to minimize disk space to increase information density. hashing represents an unusual situation in that there is no beneﬁt to be expected from locality of reference. in a sense, the hashing system implementor does everything possible to eliminate the effects of locality of reference! given the disk block containing the last record accessed, the chance of the next record access coming to the same disk block is no better than random chance in a well-designed hash system. this is because a good hashing implementation breaks up relationships between search keys. instead of improving performance by taking advantage of locality of reference, hashing trades increased hash table space for an improved chance that the record will be in its home position. thus, the more space available for the hash table, the more efﬁcient hashing should be.
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
on a road map, a road connecting two towns is typically labeled with its distance. we can model a road network as a directed graph whose edges are labeled with real numbers. these numbers represent the distance (or other cost metric, such as travel time) between two vertices. these labels may be called weights, costs, or distances, depending on the application. given such a graph, a typical problem is to ﬁnd the total length of the shortest path between two speciﬁed vertices. this is not a trivial problem, because the shortest path may not be along the edge (if any) connecting two vertices, but rather may be along a path involving one or more intermediate vertices. for example, in figure 11.15, the cost of the path from a to b to d is 15. the cost of the edge directly from a to d is 20. the cost of the path from a to c to b to d is 10. thus, the shortest path from a to d is 10 (not along the edge connecting a to d). we use the notation d(a, d) = 10 to indicate that the shortest distance from a to d is 10. in figure 11.15, there is no path from e to b, so we set d(e, b) = ∞. we deﬁne w(a, d) = 20 to be the weight of edge (a, d), that is, the weight of the direct connection from a to d. because there is no edge from e to b, w(e, b) = ∞. note that w(d, a) = ∞ because the graph of figure 11.15 is directed. we assume that all weights are positive.
11.4.1 single-source shortest paths this section presents an algorithm to solve the single-source shortest-paths problem. given vertex s in graph g, ﬁnd a shortest path from s to every other vertex in g. we might want only the shortest path between two vertices, s and t. however in the worst case, while ﬁnding the shortest path from s to t, we might ﬁnd the shortest paths from s to every other vertex as well. so there is no better algorithm (in the worst case) for ﬁnding the shortest path to a single vertex than to ﬁnd shortest paths to all vertices. the algorithm described here will only compute the
n searches are performed. in other words, if we had known the series of (at least n) searches in advance and had stored the records in order of frequency so as to minimize the total cost for these accesses, this cost would be at least half the cost required by the move-to-front heuristic. (this will be proved using amortized analysis in section 14.3.) finally, move-to-front responds well to local changes in frequency of access, in that if a record is frequently accessed for a brief period of time it will be near the front of the list during that period of access. move-to-front does poorly when the records are processed in sequential order, especially if that sequential order is then repeated multiple times.
3. swap any record found with the record immediately preceding it in the list. this heuristic is called transpose. transpose is good for list implementations based on either linked lists or arrays. frequently used records will, over time, move to the front of the list. records that were once frequently accessed but are no longer used will slowly drift toward the back. thus, it appears to have good properties with respect to changing frequency of access. unfortunately, there are some pathological sequences of access that can make transpose perform poorly. consider the case where the last record of the list (call it x) is accessed. this record is then swapped with the next-to-last record (call it y), making y the last record. if y is now accessed, it swaps with x. a repeated series of accesses alternating between x and y will continually search to the end of the list, because neither record will ever make progress toward the front. however, such pathological cases are unusual in practice.
example 9.4 assume that we have eight records, with key values a to h, and that they are initially placed in alphabetical order. now, consider the result of applying the following access pattern:
and the total cost for the twelve accesses will be 45 comparisons. (assume that when a record’s frequency count goes up, it moves forward in the list to become the last record with that value for its frequency count. after the ﬁrst two accesses, f will be the ﬁrst record and d will be the second.)
while self-organizing lists do not generally perform as well as search trees or a sorted list, both of which require o(log n) search time, there are many situations in which self-organizing lists prove a valuable tool. obviously they have an advantage over sorted lists in that they need not be sorted. this means that the cost to insert a new record is low, which could more than make up for the higher search cost when insertions are frequent. self-organizing lists are simpler to implement than search trees and are likely to be more efﬁcient for small lists. nor do they require additional space. finally, in the case of an application where sequential search is “almost” fast enough, changing an unsorted list to a self-organizing list might speed the application enough at a minor cost in additional code.
as an example of applying self-organizing lists, consider an algorithm for compressing and transmitting messages. the list is self-organized by the move-to-front rule. transmission is in the form of words and numbers, by the following rules:
both the sender and the receiver keep track of the position of words in the list in the same way (using the move-to-front rule), so they agree on the meaning of the numbers that encode repeated occurrences of words. for example, consider the following example message to be transmitted (for simplicity, ignore case in letters).
the ﬁrst three words have not been seen before, so they must be sent as full words. the fourth word is the second appearance of “the,” which at this point is the third word in the list. thus, we only need to transmit the position value “3.” the next two words have not yet been seen, so must be sent as full words. the seventh word is the third appearance of “the,” which coincidentally is again in the third position. the eighth word is the second appearance of “car,” which is now in the ﬁfth position of the list. “i” is a new word, and the last word “left” is now in the ﬁfth position. thus the entire transmission would be
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
in polynomial time), and thus the problem can be solved in polynomial time by our hypothetical magical computer. another view of this concept is that if you cannot get the answer to a problem in polynomial time by guessing the right answer and then checking it, then you cannot do it in polynomial time in any other way.
the idea of “guessing” the right answer to a problem — or checking all possible solutions in parallel to determine which is correct — is called non-determinism. an algorithm that works in this manner is called a non-deterministic algorithm, and any problem with an algorithm that runs on a non-deterministic machine in polynomial time is given a special name: it is said to be a problem in np. thus, problems in np are those problems that can be solved in polynomial time on a non-deterministic machine.
not all problems requiring exponential time on a regular computer are in np. for example, towers of hanoi is not in np, because it must print out o(2n) moves for n disks. a non-deterministic machine cannot “guess” and print the correct answer in less time.
figure 17.2 illustrates this problem. five vertices are shown, with edges and associated costs between each pair of edges. (for simplicity, we assume that the cost is the same in both directions, though this need not be the case.) if the salesman visits the cities in the order abcdea, he will travel a total distance of 13. a better route would be abdcea, with cost 11. the best route for this particular graph would be abedca, with cost 9.
we cannot solve this problem in polynomial time with a guess-and-test nondeterministic computer. the problem is that, given a candidate cycle, while we can quickly check that the answer is indeed a cycle of the appropriate form, and while we can quickly calculate the length of the cycle, we have no easy way of knowing if it is in fact the shortest such cycle. however, we can solve a variant of this problem cast in the form of a decision problem. a decision problem is simply one whose answer is either yes or no. the decision problem form of traveling salesman is as follows:
tions, every other problem that is in np can also be solved in polynomial time on a regular computer! deﬁne a problem to be np-hard if any problem in np can be reduced to x in polynomial time. thus, x is as hard as any problem in np. a problem x is deﬁned to be np-complete if
1. x is in np, and 2. x is np-hard. the requirement that a problem be np-hard might seem to be impossible, but in fact there are hundreds of such problems, including traveling salesman. another such problem is called clique.
nobody knows whether there is a polynomial time solution for clique, but if such an algorithm is found for clique or for traveling salesman, then that solution can be modiﬁed to solve the other, or any other problem in np, in polynomial time. the primary theoretical advantage of knowing that a problem p1 is np-complete is that it can be used to show that another problem p2 is np-complete. this is done by ﬁnding a polynomial time reduction of p1 to p2. because we already know that all problems in np can be reduced to p1 in polynomial time (by the deﬁnition of np-complete), we now know that all problems can be reduced to p2 as well by the simple algorithm of reducing to p1 and then from there reducing to p2. there is a practical advantage to knowing that a problem is np-complete. it relates to knowing that if a polynomial time solution can be found for any problem that is np-complete, then a polynomial solution can be found for all such problems. the implication is that,
1. because no one has yet found such a solution, it must be difﬁcult or impos2. effort to ﬁnd a polynomial time solution for one np-complete problem can
how is np-completeness of practical signiﬁcance for typical programmers? well, if your boss demands that you provide a fast algorithm to solve a problem, she will not be happy if you come back saying that the best you could do was
figure 17.4 the graph generated from boolean expression b = (x1+x2)·(x1+ x2 + x3)· (x1 + x3). literals from the ﬁrst clause are labeled c1, and literals from the second clause are labeled c2. there is an edge between every two pairs of vertices except when both vertices represent instances of literals from the same clause, or a negation of the same variable. thus, the vertex labeled c1 : y1 does not connect to the vertex labeled c1 : y2 (because they are literals in the same clause) or the vertex labeled c2 : y1 (because they are opposite values for the same variable).
there are several techniques to try. one approach is to run only small instances of the problem. for some problems, this is not acceptable. for example, traveling salesman grows so quickly that it cannot be run on modern computers for problem sizes much over 20 cities, which is not an unreasonable problem size for real-life situations. however, some other problems in np, while requiring exponential time, still grow slowly enough that they allow solutions for problems of a useful size.
consider the knapsack problem from section 16.2.1. we have a dynamic programming algorithm whose cost is Θ(nk) for n objects being ﬁt into a knapsack of size k. but it turns out that knapsack is np-complete. isn’t this a contradiction? not when we consider the relationship between n and k. how big is k? input size is typically o(n lg k) because the item sizes are smaller than k. thus, Θ(nk) is exponential on input size.
this dynamic programming algorithm is tractable if the numbers are “reasonable.” that is, we can successfully ﬁnd solutions to the problem when nk is in the thousands. such an algorithm is called a pseudo-polynomial time algorithm. this is different from traveling salesman which cannot possibly be solved when n = 100 given current algorithms. a second approach to handling np-complete problems is to solve a special instance of the problem that is not so hard. for example, many problems on graphs
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
17.21 consider a program named comp that takes two strings as input. it returns true if the strings are the same. it returns false if the strings are different. why doesn’t the argument that we used to prove that a program to solve the halting problem does not exist work to prove that comp does not exist?
17.1 implement vertex cover; that is, given graph g and integer k, answer the question of whether or not there is a vertex cover of size k or less. begin by using a brute-force algorithm that checks all possible sets of vertices of size k to ﬁnd an acceptable vertex cover, and measure the running time on a number of input graphs. then try to reduce the running time through the use of any heuristics you can think of. next, try to ﬁnd approximate solutions to the problem in the sense of ﬁnding the smallest set of vertices that forms a vertex cover.
17.3 implement an approximation of traveling salesman; that is, given a graph g with costs for all edges, ﬁnd the cheapest cycle that visits all vertices in g. try various heuristics to ﬁnd the best approximations for a wide variety of input graphs.
17.4 write a program that, given a positive integer n as input, prints out the collatz sequence for that number. what can you say about the types of integers that have long collatz sequences? what can you say about the length of the collatz sequence for various types of integers?
at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes you might want to eliminate the overhead imposed by the recursive function calls. in some cases, such as the factorial function of section 2.5, recursion can easily be replaced by iteration.
example 4.2 as a simple example of replacing recursion with a stack, consider the following non-recursive version of the factorial function. static long fact(int n) { // compute n!
assert (n >= 0) && (n <= 20) : "n out of range"; // make a stack just big enough stack<integer> s = new astack<integer>(n); while (n > 1) s.push(n--); long result = 1; while (s.length() > 0)
here, we simply push successively smaller values of n onto the stack until the base case is reached, then repeatedly pop off the stored values and multiply them into the result.
in practice, an iterative form of the factorial function would be both simpler and faster than the version shown in example 4.2. unfortunately, it is not always possible to replace recursion with iteration. recursion, or some imitation of it, is necessary when implementing algorithms that require multiple branching such as in the towers of hanoi algorithm, or when traversing a binary tree. the mergesort and quicksort algorithms of chapter 7 are also examples in which recursion is required. fortunately, it is always possible to imitate recursion with a stack. let us now turn to a non-recursive version of the towers of hanoi function, which cannot be done iteratively.
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
aspects of binary trees that are relevent to all applications. for example, we must be able to initialize a binary tree, and we might wish to determine if the tree is empty. some activities might be unique to the application. for example, we might wish to combine two binary trees by making their roots be the children of a new root node. other activities are centered around the nodes. for example, we might need access to the left or right child of a node, or to the node’s data value.
clearly there are activities that relate to nodes (e.g., reach a node’s child or get a node’s value), and activities that relate to trees (e.g., tree initialization). this indicates that nodes and trees should be implemented as separate classes. for now, we concentrate on the class to implement binary tree nodes. this class will be used by some of the binary tree structures presented later.
figure 5.5 shows an interface for binary tree nodes, called binnode. class binnode is a generic with parameter e, which is the type for the data record stored in a node. member functions are provided that set or return the element value, return a reference to the left child, return a reference to the right child, or indicate whether the node is a leaf.
often we wish to process a binary tree by “visiting” each of its nodes, each time performing a speciﬁc action such as printing the contents of the node. any process for visiting all of the nodes in some order is called a traversal. any traversal that lists every node in the tree exactly once is called an enumeration of the tree’s nodes. some applications do not require that the nodes be visited in any particular order as long as each node is visited precisely once. for other applications, nodes
a more difﬁcult situation is illustrated by the following problem. given an arbitrary binary tree we wish to determine if, for every node a, are all nodes in a’s left subtree less than the value of a, and are all nodes in a’s right subtree greater than the value of a? (this happens to be the deﬁnition for a binary search tree, described in section 5.4.) unfortunately, to make this decision we need to know some context that is not available just by looking at the node’s parent or children. as shown by figure 5.6, it is not enough to verify that a’s left child has a value less than that of a, and that a’s right child has a greater value. nor is it enough to verify that a has a value consistent with that of its parent. in fact, we need to know information about what range of values is legal for a given node. that information might come from any ancestor for the node. thus, relevent range information must be passed down the tree. we can implement this function as follows.
in this section we will examine ways to implement binary tree nodes. we begin with some options for pointer-based binary tree node implementations. then comes a
figure 5.12 a complete binary tree and its array implementation. (a) the complete binary tree with twelve nodes. each node has been labeled with its position in the tree. (b) the positions for the relatives of each node. a dash indicates that the relative does not exist.
array-based list to implement the dictionary, then binary search can be used to ﬁnd a record in only Θ(log n) time. however, insertion will now require Θ(n) time on average because, once the proper location for the new record in the sorted list has been found, many records might be shifted to make room for the new record.
is there some way to organize a collection of records so that inserting records and searching for records can both be done quickly? this section presents the binary search tree (bst), which allows an improved solution to this problem.
a bst is a binary tree that conforms to the following condition, known as the binary search tree property: all nodes stored in the left subtree of a node whose key value is k have key values less than k. all nodes stored in the right subtree of a node whose key value is k have key values greater than or equal to k. figure 5.13 shows two bsts for a collection of values. one consequence of the binary search tree property is that if the bst nodes are printed using an inorder traversal (see section 5.2), the resulting enumeration will be in sorted order from lowest to highest.
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
leaf nodes in the tree. prove by induction that if tree t is a full binary tree with n internal nodes, i is t’s internal path length, and e is t’s external path length, then e = i + 2n for n ≥ 0.
5.4 explain why function preorder2 from section 5.2 makes half as many recursive calls as function preorder. explain why it makes twice as many accesses to left and right children. (a) modify the preorder traversal of section 5.2 to perform an inorder
5.6 write a recursive function named search that takes as input the pointer to the root of a binary tree (not a bst!) and a value k, and returns true if value k appears in the tree and false otherwise.
5.7 write an algorithm that takes as input the pointer to the root of a binary tree and prints the node values of the tree in level order. level order ﬁrst prints the root, then all nodes of level 1, then all nodes of level 2, and so on. hint: preorder traversals make use of a stack through recursive calls. consider making use of another data structure to help implement the levelorder traversal.
5.8 write a recursive function that returns the height of a binary tree. 5.9 write a recursive function that returns a count of the number of leaf nodes in
5.11 assume that a given bst stores integer values in its nodes. write a recursive function that traverses a binary tree, and prints the value of every node who’s grandparent has a value that is a multiple of ﬁve.
(c) all nodes store data and a parent pointer, and internal nodes store two child pointers. the data ﬁeld requires eight bytes and each pointer requires four bytes.
function next locates the edge following edge (i, j) (if any) by continuing down the row of vertex i starting at position j + 1, looking for an edge. if no such edge exists, next returns n. functions setedge and deledge adjust the appropriate value in the array. function weight returns the value stored in the appropriate position in the array.
figure 11.7 presents an implementation of the adjacency list representation for graphs. its main data structure is an array of linked lists, one linked list for each vertex. these linked lists store objects of type edge, which merely stores the index for the vertex pointed to by the edge, along with the weight of the edge.
implementation for graphl member functions is straightforward in principle, with the key functions being setedge, deledge, and weight. the simplest implementation would start at the beginning of the adjacency list and move along it until the desired vertex has been found. however, many graph algorithms work by taking advantage of the first and next functions to process all edges extending from a given vertex in turn. thus, there is a signiﬁcant time savings if setedge, deledge, and weight ﬁrst check to see if the desired edge is the current one on the relevant linked list. the implementation of figure 11.7 does exactly this.
often it is useful to visit the vertices of a graph in some speciﬁc order based on the graph’s topology. this is known as a graph traversal and is similar in concept to a tree traversal. recall that tree traversals visit every node exactly once, in some speciﬁed order such as preorder, inorder, or postorder. multiple tree traversals exist because various applications require the nodes to be visited in a particular order. for example, to print a bst’s nodes in ascending order requires an inorder traversal as opposed to some other traversal. standard graph traversal orders also exist. each is appropriate for solving certain problems. for example, many problems in artiﬁcial intelligence programming are modeled using graphs. the problem domain may consist of a large collection of states, with connections between various pairs
aspects of binary trees that are relevent to all applications. for example, we must be able to initialize a binary tree, and we might wish to determine if the tree is empty. some activities might be unique to the application. for example, we might wish to combine two binary trees by making their roots be the children of a new root node. other activities are centered around the nodes. for example, we might need access to the left or right child of a node, or to the node’s data value.
clearly there are activities that relate to nodes (e.g., reach a node’s child or get a node’s value), and activities that relate to trees (e.g., tree initialization). this indicates that nodes and trees should be implemented as separate classes. for now, we concentrate on the class to implement binary tree nodes. this class will be used by some of the binary tree structures presented later.
figure 5.5 shows an interface for binary tree nodes, called binnode. class binnode is a generic with parameter e, which is the type for the data record stored in a node. member functions are provided that set or return the element value, return a reference to the left child, return a reference to the right child, or indicate whether the node is a leaf.
often we wish to process a binary tree by “visiting” each of its nodes, each time performing a speciﬁc action such as printing the contents of the node. any process for visiting all of the nodes in some order is called a traversal. any traversal that lists every node in the tree exactly once is called an enumeration of the tree’s nodes. some applications do not require that the nodes be visited in any particular order as long as each node is visited precisely once. for other applications, nodes
figure 5.12 a complete binary tree and its array implementation. (a) the complete binary tree with twelve nodes. each node has been labeled with its position in the tree. (b) the positions for the relatives of each node. a dash indicates that the relative does not exist.
array-based list to implement the dictionary, then binary search can be used to ﬁnd a record in only Θ(log n) time. however, insertion will now require Θ(n) time on average because, once the proper location for the new record in the sorted list has been found, many records might be shifted to make room for the new record.
is there some way to organize a collection of records so that inserting records and searching for records can both be done quickly? this section presents the binary search tree (bst), which allows an improved solution to this problem.
a bst is a binary tree that conforms to the following condition, known as the binary search tree property: all nodes stored in the left subtree of a node whose key value is k have key values less than k. all nodes stored in the right subtree of a node whose key value is k have key values greater than or equal to k. figure 5.13 shows two bsts for a collection of values. one consequence of the binary search tree property is that if the bst nodes are printed using an inorder traversal (see section 5.2), the resulting enumeration will be in sorted order from lowest to highest.
next we consider a fundamentally different approach to implementing trees. the goal is to store a series of node values with the minimum information needed to reconstruct the tree structure. this approach, known as a sequential tree implementation, has the advantage of saving space because no pointers are stored. it has the disadvantage that accessing any node in the tree requires sequentially processing all nodes that appear before it in the node list. in other words, node access must start at the beginning of the node list, processing nodes sequentially in whatever order they are stored until the desired node is reached. thus, one primary virtue of the other implementations discussed in this section is lost: efﬁcient access (typically Θ(log n) time) to arbitrary nodes in the tree. sequential tree implementations are ideal for archiving trees on disk for later use because they save space, and the tree structure can be reconstructed as needed for later processing.
seqential tree implementations can also be used to serialize a tree structure. serialization is the process of storing an object as a series of bytes, typically so that the data structure can be transmitted between computers. this capability is important when using data structures in a distributed processing environment.
a sequential tree implementation stores the node values as they would be enumerated by a preorder traversal, along with sufﬁcient information to describe the tree’s shape. if the tree has restricted form, for example if it is a full binary tree, then less information about structure typically needs to be stored. a general tree, because it has the most ﬂexible shape, tends to require the most additional shape information. there are many possible sequential tree implementation schemes. we will begin by describing methods appropriate to binary trees, then generalize to an implementation appropriate to a general tree structure.
because every node of a binary tree is either a leaf or has two (possibly empty) children, we can take advantage of this fact to implicitly represent the tree’s structure. the most straightforward sequential tree implementation lists every node value as it would be enumerated by a preorder traversal. unfortunately, the node values alone do not provide enough information to recover the shape of the tree. in particular, as we read the series of node values, we do not know when a leaf node has been reached. however, we can treat all non-empty nodes as internal nodes with two (possibly empty) children. only null values will be interpreted as leaf nodes, and these can be listed explicitly. such an augmented node list provides enough information to recover the tree structure.
/** general tree adt */ interface gentree<e> { public void clear(); public gtnode<e> root(); // make the tree have a new root, give first child and sib public void newroot(e value, gtnode<e> first,
access to the left and right child pointers. unfortunately, because we do not know in advance how many children a given node will have in the general tree, we cannot give explicit functions to access each child. an alternative must be found that works for any number of children.
one choice would be to provide a function that takes as its parameter the index for the desired child. that combined with a function that returns the number of children for a given node would support the ability to access any node or process all children of a node. unfortunately, this view of access tends to bias the choice for node implementations in favor of an array-based approach, because these functions favor random access to a list of children. in practice, an implementation based on a linked list is often preferred.
an alternative is to provide access to the ﬁrst (or leftmost) child of a node, and to provide access to the next (or right) sibling of a node. figure 6.2 shows class declarations for general trees and their nodes. based on these two access functions, the children of a node can be traversed like a list. trying to ﬁnd the next sibling of the rightmost sibling would return null.
in section 5.2, three tree traversals were presented for binary trees: preorder, postorder, and inorder. for general trees, preorder and postorder traversals are deﬁned with meanings similar to their binary tree counterparts. preorder traversal of a general tree ﬁrst visits the root of the tree, then performs a preorder traversal of each subtree from left to right. a postorder traversal of a general tree performs a postorder traversal of the root’s subtrees from left to right, then visits the root. inorder traversal does not have a natural deﬁnition for the general tree, because there is no particular number of children for an internal node. an arbitrary deﬁnition — such as visit the leftmost subtree in inorder, then the root, then visit the remaining subtrees in inorder — can be invented. however, inorder traversals are generally not useful with general trees.
to perform a preorder traversal, it is necessary to visit each of the children for a given node (say r) from left to right. this is accomplished by starting at r’s leftmost child (call it t). from t, we can move to t’s right sibling, and then to that node’s right sibling, and so on.
using the adt of figure 6.2, here is an implementation to print the nodes of a general tree in preorder note the for loop at the end, which processes the list of children by beginning with the leftmost child, then repeatedly moving to the next child until calling next returns null.
perhaps the simplest general tree implementation is to store for each node only a pointer to that node’s parent. we will call this the parent pointer implementation.
6.2 write an algorithm to determine if two binary trees are identical when the ordering of the subtrees for a node is ignored. for example, if a tree has root node with value r, left child with value a and right child with value b, this would be considered identical to another tree with root node value r, left child value b, and right child value a. make the algorithm as efﬁcient as you can. analyze your algorithm’s running time. how much harder would it be to make this algorthm work on a general tree?
6.4 write a function that takes as input a general tree and returns the number of nodes in that tree. write your function to use the gentree and gtnode adts of figure 6.2.
6.5 describe how to implement the weighted union rule efﬁciently. in particular, describe what information must be stored with each node and how this information is updated when two trees are merged. modify the implementation of figure 6.4 to support the weighted union rule.
6.6 a potential alternatative to the weighted union rule for combining two trees is the height union rule. the height union rule requires that the root of the tree with greater height become the root of the union. explain why the height union rule can lead to worse average time behavior than the weighted union rule.
6.7 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (0, 2) (1, 2) (3, 4) (3, 1) (3, 5) (9, 11) (12, 14) (3, 9) (4, 14) (6, 7) (8, 10) (8, 7) (7, 0) (10, 15) (10, 13)
6.8 using the weighted union rule and path compression, show the array for the parent pointer implementation that results from the following series of equivalences on a set of objects indexed by the values 0 through 15. initially, each element in the set should be in a separate equivalence class. when two trees to be merged are the same size, make the root with greater index value be the child of the root with lesser index value. (2, 3) (4, 5) (6, 5) (3, 5) (1, 0) (7, 8) (1, 8) (3, 8) (9, 10) (11, 14) (11, 10) (12, 13) (11, 13) (14, 1)
graphs provide the ultimate in data structure ﬂexibility. graphs can model both real-world systems and abstract problems, so they are used in hundreds of applications. here is a small sampling of the range of problems that graphs are applied to.
we begin in section 11.1 with some basic graph terminology and then deﬁne two fundamental representations for graphs, the adjacency matrix and adjacency list. section 11.2 presents a graph adt and simple implementations based on the adjacency matrix and adjacency list. section 11.3 presents the two most commonly used graph traversal algorithms, called depth-ﬁrst and breadth-ﬁrst search, with application to topological sorting. section 11.4 presents algorithms for solving some problems related to ﬁnding shortest routes in a graph. finally, section 11.5 presents algorithms for ﬁnding the minimum-cost spanning tree, useful for determining lowest-cost connectivity in a network. besides being useful and interesting in their own right, these algorithms illustrate the use of some data structures presented in earlier chapters.
function next locates the edge following edge (i, j) (if any) by continuing down the row of vertex i starting at position j + 1, looking for an edge. if no such edge exists, next returns n. functions setedge and deledge adjust the appropriate value in the array. function weight returns the value stored in the appropriate position in the array.
figure 11.7 presents an implementation of the adjacency list representation for graphs. its main data structure is an array of linked lists, one linked list for each vertex. these linked lists store objects of type edge, which merely stores the index for the vertex pointed to by the edge, along with the weight of the edge.
implementation for graphl member functions is straightforward in principle, with the key functions being setedge, deledge, and weight. the simplest implementation would start at the beginning of the adjacency list and move along it until the desired vertex has been found. however, many graph algorithms work by taking advantage of the first and next functions to process all edges extending from a given vertex in turn. thus, there is a signiﬁcant time savings if setedge, deledge, and weight ﬁrst check to see if the desired edge is the current one on the relevant linked list. the implementation of figure 11.7 does exactly this.
often it is useful to visit the vertices of a graph in some speciﬁc order based on the graph’s topology. this is known as a graph traversal and is similar in concept to a tree traversal. recall that tree traversals visit every node exactly once, in some speciﬁed order such as preorder, inorder, or postorder. multiple tree traversals exist because various applications require the nodes to be visited in a particular order. for example, to print a bst’s nodes in ascending order requires an inorder traversal as opposed to some other traversal. standard graph traversal orders also exist. each is appropriate for solving certain problems. for example, many problems in artiﬁcial intelligence programming are modeled using graphs. the problem domain may consist of a large collection of states, with connections between various pairs
queue<integer> q = new aqueue<integer>(g.n()); int[] count = new int[g.n()]; int v; for (v=0; v<g.n(); v++) count[v] = 0; // initialize for (v=0; v<g.n(); v++)
prerequisites are placed on the queue. we then begin processing the queue. when vertex v is taken off of the queue, it is printed, and all neighbors of v (that is, all vertices that have v as a prerequisite) have their counts decremented by one. any neighbor whose count is now zero is placed on the queue. if the queue becomes empty without printing all of the vertices, then the graph contains a cycle (i.e., there is no possible ordering for the tasks that does not violate some prerequisite). the printed order for the vertices of the graph in figure 11.13 using the queue version of topological sort is j1, j2, j3, j6, j4, j5, j7. figure 11.14 shows an implementation for the queue-based topological sort algorithm.
figure 10.13 example of inserting a record that causes the 2-3 tree root to split. (a) the value 19 is added to the 2-3 tree of figure 10.9. this causes the node containing 20 and 21 to split, promoting 20. (b) this in turn causes the internal node containing 23 and 30 to split, promoting 23. (c) finally, the root node splits, promoting 23 to become the left record in the new root. the result is that the tree becomes one level higher.
the 2-3 tree insert and delete routines do not add new nodes at the bottom of the tree. instead they cause leaf nodes to split or merge, possibly causing a ripple effect moving up the tree to the root. if necessary the root will split, causing a new root node to be created and making the tree one level deeper. on deletion, if the last two children of the root merge, then the root node is removed and the tree will lose a level. in either case, all leaf nodes are always at the same level. when all leaf nodes are at the same level, we say that a tree is height balanced. because the 2-3 tree is height balanced, and every internal node has at least two children, we know that the maximum depth of the tree is log n. thus, all 2-3 tree insert, ﬁnd, and delete operations require Θ(log n) time.
this section presents the b-tree. b-trees are usually attributed to r. bayer and e. mccreight who described the b-tree in a 1972 paper. by 1979, b-trees had replaced virtually all large-ﬁle access methods other than hashing. b-trees, or some variant of b-trees, are the standard ﬁle organization for applications requiring insertion, deletion, and key range searches. b-trees address effectively all of the major problems encountered when implementing disk-based search trees:
1. b-trees are always height balanced, with all leaf nodes at the same level. 2. update and search operations affect only a few disk blocks. the fewer the
3. b-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk i/o on searches due to locality of reference.
4. b-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. this improves space efﬁciency while reducing the typical number of disk fetches necessary during a search or update operation.
a b-tree of order m is deﬁned to have the following shape properties: • the root is either a leaf or has at least two children. • each internal node, except for the root, has between dm/2e and m children. • all leaves are at the same level in the tree, so the tree is always height bal-
the b-tree is a generalization of the 2-3 tree. put another way, a 2-3 tree is a b-tree of order three. normally, the size of a node in the b-tree is chosen to ﬁll a disk block. a b-tree node implementation typically allows 100 or more children. thus, a b-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk ﬁle). in a typical application, b-tree block i/o will be managed using a buffer pool and a block-replacement scheme such as lru (see section 8.3).
1. perform a binary search on the records in the current node. if a record with the search key is found, then return that record. if the current node is a leaf node and the key is not found, then report an unsuccessful search.
for example, consider a search for the record with key value 47 in the tree of figure 10.16. the root node is examined and the second (right) branch taken. after
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
the list representations of chapter 4 have a fundamental limitation: either search or insert can be made efﬁcient, but not both at the same time. tree structures permit both efﬁcient access and update to large collections of data. binary trees in particular are widely used and relatively easy to implement. but binary trees are useful for many things besides searching. just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms.
this chapter begins by presenting deﬁnitions and some key properties of binary trees. section 5.2 discusses how to process all nodes of the binary tree in an organized manner. section 5.3 presents various methods for implementing binary trees and their nodes. sections 5.4 through 5.6 present three examples of binary trees used in speciﬁc applications: the binary search tree (bst) for implementing dictionaries, heaps for implementing priority queues, and huffman coding trees for text compression. the bst, heap, and huffman coding tree each have distinctive features that affect their implementation and use.
a binary tree is made up of a ﬁnite set of elements called nodes. this set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (disjoint means that they have no nodes in common.) the roots of these subtrees are children of the root. there is an edge from a node to each of its children, and a node is said to be the parent of its children. if n1, n2, ..., nk is a sequence of nodes in the tree such that ni is the parent of ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. the length
discussion on techniques for determining the space requirements for a given binary tree node implementation. the section concludes with an introduction to the arraybased implementation for complete binary trees.
by deﬁnition, all binary tree nodes have two children, though one or both children can be empty. binary tree nodes normally contain a value ﬁeld, with the type of the ﬁeld depending on the application. the most common node implementation includes a value ﬁeld and pointers to the two children.
figure 5.7 shows a simple implementation for the binnode abstract class, which we will name bstnode. class bstnode includes a data member of type element, (which is the second generic parameter) for the element type. to support search structures such as the binary search tree, an additional ﬁeld is included, with corresponding access methods, store a key value (whose purpose is explained in section 4.4). its type is determined by the ﬁrst generic parameter, named k. every bstnode object also has two pointers, one to its left child and another to its right child. figure 5.8 shows an illustration of the bstnode implementation.
some programmers ﬁnd it convenient to add a pointer to the node’s parent, allowing easy upward movement in the tree. using a parent pointer is somewhat analogous to adding a link to the previous node in a doubly linked list. in practice, the parent pointer is almost always unnecessary and adds to the space overhead for the tree implementation. it is not just a problem that parent pointers take space. more importantly, many uses of the parent pointer are driven by improper understanding of recursion and so indicate poor programming. if you are inclined toward using a parent pointer, consider if there is a more efﬁcient implementation possible. an important decision in the design of a pointer-based node implementation is whether the same class deﬁnition will be used for leaves and internal nodes. using the same class for both will simplify the implementation, but might be an inefﬁcient use of space. some applications require data values only for the leaves. other applications require one type of value for the leaves and another for the internal nodes. examples include the binary trie of section 13.1, the pr quadtree of section 13.3, the huffman coding tree of section 5.6, and the expression tree illustrated by figure 5.9. by deﬁnition, only internal nodes have non-empty children. if we use the same node implementation for both internal and leaf nodes, then both must store the child pointers. but it seems wasteful to store child pointers in the leaf nodes. thus, there are many reasons why it can save space to have separate implementations for internal and leaf nodes.
are real numbers or arbitrary length strings, then some care will be necessary in implementation. in particular, radix sort will need to be careful about deciding when the “last digit” has been found to distinguish among real numbers, or the last character in variable length strings. implementing the concept of radix sort with the trie data structure (section 13.1) is most appropriate for these situations.
at this point, the perceptive reader might begin to question our earlier assumption that key comparison takes constant time. if the keys are “normal integer” values stored in, say, an integer variable, what is the size of this variable compared to n? in fact, it is almost certain that 32 (the number of bits in a standard int variable) is greater than log n for any practical computation. in this sense, comparison of two long integers requires Ω(log n) work.
computers normally do arithmetic in units of a particular size, such as a 32-bit word. regardless of the size of the variables, comparisons use this native word size and require a constant amount of time. in practice, comparisons of two 32-bit values take constant time, even though 32 is much greater than log n. to some extent the truth of the proposition that there are constant time operations (such as integer comparison) is in the eye of the beholder. at the gate level of computer architecture, individual bits are compared. however, constant time comparison for integers is true in practice on most computers, and we rely on such assumptions as the basis for our analyses. in contrast, radix sort must do several arithmetic calculations on key values (each requiring constant time), where the number of such calculations is proportional to the key length. thus, radix sort truly does Ω(n log n) work to process n distinct key values.
which sorting algorithm is fastest? asymptotic complexity analysis lets us distinguish between Θ(n2) and Θ(n log n) algorithms, but it does not help distinguish between algorithms with the same asymptotic complexity. nor does asymptotic analysis say anything about which algorithm is best for sorting small lists. for answers to these questions, we can turn to empirical testing.
figure 7.13 shows timing results for actual implementations of the sorting algorithms presented in this chapter. the algorithms compared include insertion sort, bubble sort, selection sort, shellsort, quicksort, mergesort, heapsort and radix sort. shellsort shows both the basic version from section 7.3 and another with increments based on division by three. mergesort shows both the basic implementation from section 7.4 and the optimized version with calls to insertion sort for lists of length below nine. for quicksort, two versions are compared: the basic implementation from section 7.5 and an optimized version that does not partition
this chapter introduces several tree structures designed for use in specialized applications. the trie of section 13.1 is commonly used to store strings and is suitable for storing and searching collections of strings. it also serves to illustrate the concept of a key space decomposition. the avl tree and splay tree of section 13.2 are variants on the bst. they are examples of self-balancing search trees and have guaranteed good performance regardless of the insertion order for records. an introduction to several spatial data structures used to organize point data by xycoordinates is presented in section 13.3.
descriptions of the fundamental operations are given for each data structure. because an important goal for this chapter is to provide material for class programming projects, detailed implementations are left for the reader.
recall that the shape of a bst is determined by the order in which its data records are inserted. one permutation of the records might yield a balanced tree while another might yield an unbalanced tree in the shape of a linked list. the reason is that the value of the key stored in the root node splits the key range into two parts: those key values less than the root’s key value, and those key values greater than the root’s key value. depending on the relationship between the root node’s key value and the distribution of the key values for the other records in the the tree, the resulting bst might be balanced or unbalanced. thus, the bst is an example of a data structure whose organization is based on an object space decomposition, so called because the decomposition of the key range is driven by the objects (i.e., the key values of the data records) stored in the tree.
the alternative to object space decomposition is to predeﬁne the splitting position within the key range for each node in the tree. in other words, the root could be
example 13.1 when searching for the value 7 (0000111 in binary) in the pat trie of figure 13.3, the root node indicates that bit position 0 (the leftmost bit) is checked ﬁrst. because the 0th bit for value 7 is 0, take the left branch. at level 1, branch depending on the value of bit 1, which again is 0. at level 2, branch depending on the value of bit 2, which again is 0. at level 3, the index stored in the node is 4. this means that bit 4 of the key is checked next. (the value of bit 3 is irrelevant, because all values stored in that subtree have the same value at bit position 3.) thus, the single branch that extends from the equivalent node in figure 13.1 is just skipped. for key value 7, bit 4 has value 1, so the rightmost branch is taken. because this leads to a leaf node, the search key is compared against the key stored in that node. if they match, then the desired record has been found.
note that during the search process, only a single bit of the search key is compared at each internal node. this is signiﬁcant, because the search key could be quite large. search in the pat trie requires only a single full-key comparison, which takes place once a leaf node has been reached.
example 13.2 consider the situation where we need to store a library of dna sequences. a dna sequence is a series of letters, usually many thousands of characters long, with the string coming from an alphabet of only four letters that stand for the four amino acids making up a dna strand. similar dna seqences might have long sections of ther string that are identical. the pat trie woudl avoid making multiple full key comparisons when searching for a speciﬁc sequence.
we have noted several times that the bst has a high risk of becoming unbalanced, resulting in excessively expensive search and update operations. one solution to this problem is to adopt another search tree structure such as the 2-3 tree. an alternative is to modify the bst access functions in some way to guarantee that the tree performs well. this is an appealing concept, and it works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. unfortunately, requiring that the bst always be in the shape of a complete binary tree requires excessive modiﬁcation to the tree during update, as discussed in section 10.3.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
(a) show the result of building a bintree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (30, 35), g (45, 25), h (45, 30), i (50, 30).
(b) show the result of deleting point c from the tree you built in part (a). (c) show the result of deleting point f from the resulting tree in part (b).
13.16 compare the trees constructed for exercises 12 and 15 in terms of the number of internal nodes, full leaf nodes, empty leaf nodes, and total depths of the two trees.
13.17 show the result of building a point quadtree from the following points (inserted in the order given). assume the tree is representing a space of 64 by 64 units. a (20, 20), b (10, 30), c (25, 50), d (35, 25), e (30, 45), f (31, 35), g (45, 26), h (44, 30), i (50, 30).
13.1 use the trie data structure to devise a program to sort variable-length strings. the program’s running time should be proportional to the total number of letters in all of the strings. note that some strings might be very long while most are short.
13.2 deﬁne the set of sufﬁx strings for a string s to be s, s without its ﬁrst character, s without its ﬁrst two characters, and so on. for example, the complete set of sufﬁx strings for “hello” would be
a sufﬁx tree is a pat trie that contains all of the sufﬁx strings for a given string, and associates each sufﬁx with the complete string. the advantage of a sufﬁx tree is that it allows a search for strings using “wildcards.” for example, the search key “th*” means to ﬁnd all strings with “th” as the ﬁrst two characters. this can easily be done with a regular trie. searching for “*th” is not efﬁcient in a regular trie, but it is efﬁcient in a sufﬁx tree. implement the sufﬁx tree for a dictionary of words or phrases, with support for wildcard search.
13.3 revise the bst class of section 5.4 to use the avl tree rotations. your new implementation should not modify the original bst class adt. compare your avl tree against an implementation of the standard bst over a wide variety of input data. under what conditions does the splay tree actually save time?
figure 13.1 the binary trie for the collection of values 2, 7, 24, 31, 37, 40, 42, 120. all data values are stored in the leaf nodes. edges are labeled with the value of the bit used to determine the branching direction of each node. the binary form of the key value determines the path to the record, assuming that each key is represented as a 7-bit value representing a number in the range 0 to 127.
one application for tries is storing a dictionary of words. such a trie will be referred to as an alphabet trie. for simplicity, our examples will ignore case in letters. we add a special character ($) to the 26 standard english letters. the $ character is used to represent the end of a string. thus, the branching factor for each node is (up to) 27. once constructed, the alphabet trie is used to determine if a given word is in the dictionary. consider searching for a word in the alphabet trie of figure 13.2. the ﬁrst letter of the search word determines which branch to take from the root, the second letter determines which branch to take at the next level, and so on. only the letters that lead to a word are shown as branches. in figure 13.2(b) the leaf nodes of the trie store a copy of the actual words, while in figure 13.2(a) the word is built up from the letters associated with each branch. one way to implement a node of the alphabet trie is as an array of 27 pointers indexed by letter. because most nodes have branches to only a small fraction of the possible letters in the alphabet, an alternate implementation is to use a linked list of pointers to the child nodes, as in figure 6.9.
the depth of a leaf node in the alphabet trie of figure 13.2(b) has little to do with the number of nodes in the trie. rather, a node’s depth depends on the number of characters required to distinguish this node’s word from any other. for example, if the words “anteater” and “antelope” are both stored in the trie, it is not until the ﬁfth letter that the two words can be distinguished. thus, these words must be stored at least as deep as level ﬁve. in general, the limiting factor on the depth of nodes in the alphabet trie is the length of the words stored.
predeﬁned to split the key range into two equal halves, regardless of the particular values or order of insertion for the data records. those records with keys in the lower half of the key range will be stored in the left subtree, while those records with keys in the upper half of the key range will be stored in the right subtree. while such a decomposition rule will not necessarily result in a balanced tree (the tree will be unbalanced if the records are not well distributed within the key range), at least the shape of the tree will not depend on the order of key insertion. furthermore, the depth of the tree will be limited by the resolution of the key range; that is, the depth of the tree can never be greater than the number of bits required to store a key value. for example, if the keys are integers in the range 0 to 1023, then the resolution for the key is ten bits. thus, two keys might be identical only until the tenth bit. in the worst case, two keys will follow the same path in the tree only until the tenth branch. as a result, the tree will never be more than ten levels deep. in contrast, a bst containing n records could be as much as n levels deep.
decomposition based on a predetermined subdivision of the key range is called key space decomposition. in computer graphics, a related technique is known as image space decomposition, and this term is sometimes applied to data structures based on key space decomposition as well. any data structure based on key space decomposition is called a trie. folklore has it that “trie” comes from “retrieval.” unfortunately, that would imply that the word is pronounced “tree,” which would lead to confusion with regular use of the word “tree.” “trie” is actually pronounced as “try.”
like the b+-tree, a trie stores data records only in leaf nodes. internal nodes serve as placeholders to direct the search process. figure 13.1 illustrates the trie concept. upper and lower bounds must be imposed on the key values so that we can compute the middle of the key range. because the largest value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is the smallest power of two greater than 120. the binary value of the key determines whether to select the left or right branch at any given point during the search. the most signiﬁcant bit determines the branch direction at the root. figure 13.1 shows a binary trie, so called because in this example the trie structure is based on the value of the key interpreted as a binary number, which results in a binary tree.
the huffman coding tree of section 5.6 is another example of a binary trie. all data values in the huffman tree are at the leaves, and each branch splits the range of possible letter codes in half. the huffman codes are actually derived from the letter positions within the trie.
these are examples of binary tries, but tries can be built with any branching factor. normally the branching factor is determined by the alphabet used. for
figure 13.3 the pat trie for the collection of values 2, 7, 24, 32, 37, 40, 42, 120. contrast this with the binary trie of figure 13.1. in the pat trie, all data values are stored in the leaf nodes, while internal nodes store the bit position used to determine the branching decision, assuming that each key is represented as a 7bit value representing a number in the range 0 to 127. some of the branches in this pat trie have been labeled to indicate the binary representation for all values in that subtree. for example, all values in the left subtree of the node labeled 0 must have value 0xxxxxx (where x means that bit can be either a 0 or a 1). all nodes in the right subtree of the node labeled 3 must have value 0101xxx. however, we can skip branching on bit 2 for this subtree because all values currently stored have a value of 0 for that bit.
poor balance and clumping can result when certain preﬁxes are heavily used. for example, an alphabet trie storing the common words in the english language would have many words in the “th” branch of the tree, but none in the “zq” branch. any multiway branching trie can be replaced with a binary trie by replacing the original trie’s alphabet with an equivalent binary code. alternatively, we can use the techniques of section 6.3.4 for converting a general tree to a binary tree without modifying the alphabet.
the trie implementations illustrated by figures 13.1 and 13.2 are potentially quite inefﬁcient as certain key sets might lead to a large number of nodes with only a single child. a variant on trie implementation is known as patricia, which stands for “practical algorithm to retrieve information coded in alphanumeric.” in the case of a binary alphabet, a patricia trie (referred to hereafter as a pat trie) is a full binary tree that stores data records in the leaf nodes. internal nodes store only the position within the key’s bit pattern that is used to decide on the next branching point. in this way, internal nodes with single children (equivalently, bit positions within the key that do not distinguish any of the keys within the current subtree) are eliminated. a pat trie corresponding to the values of figure 13.1 is shown in figure 13.3.
example 13.1 when searching for the value 7 (0000111 in binary) in the pat trie of figure 13.3, the root node indicates that bit position 0 (the leftmost bit) is checked ﬁrst. because the 0th bit for value 7 is 0, take the left branch. at level 1, branch depending on the value of bit 1, which again is 0. at level 2, branch depending on the value of bit 2, which again is 0. at level 3, the index stored in the node is 4. this means that bit 4 of the key is checked next. (the value of bit 3 is irrelevant, because all values stored in that subtree have the same value at bit position 3.) thus, the single branch that extends from the equivalent node in figure 13.1 is just skipped. for key value 7, bit 4 has value 1, so the rightmost branch is taken. because this leads to a leaf node, the search key is compared against the key stored in that node. if they match, then the desired record has been found.
note that during the search process, only a single bit of the search key is compared at each internal node. this is signiﬁcant, because the search key could be quite large. search in the pat trie requires only a single full-key comparison, which takes place once a leaf node has been reached.
example 13.2 consider the situation where we need to store a library of dna sequences. a dna sequence is a series of letters, usually many thousands of characters long, with the string coming from an alphabet of only four letters that stand for the four amino acids making up a dna strand. similar dna seqences might have long sections of ther string that are identical. the pat trie woudl avoid making multiple full key comparisons when searching for a speciﬁc sequence.
we have noted several times that the bst has a high risk of becoming unbalanced, resulting in excessively expensive search and update operations. one solution to this problem is to adopt another search tree structure such as the 2-3 tree. an alternative is to modify the bst access functions in some way to guarantee that the tree performs well. this is an appealing concept, and it works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. unfortunately, requiring that the bst always be in the shape of a complete binary tree requires excessive modiﬁcation to the tree during update, as discussed in section 10.3.
spatial data structures can also be used to store line object, rectangle object, or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in three dimensions). a simple, yet effective, data structure for storing rectangles or arbitrary polygonal shapes can be derived from the pr quadtree. pick a threshold value c, and subdivide any region into four quadrants if it contains more than c objects. a special case must be dealt with when more than c object intersect.
some of the most interesting developments in spatial data structures have to do with adapting them for disk-based applications. however, all such disk-based implementations boil down to storing the spatial data structure within some variant on either b-trees or hashing.
patricia tries and other trie implementations are discussed in information retrieval: data structures & algorithms, frakes and baeza-yates, eds. [fby92].
the world of spatial data structures is rich and rapidly evolving. for a good introduction, see foundations of multidimensional and metric data structures by hanan samet [sam06]. this is also the best reference for more information on the pr quadtree. the k-d tree was invented by john louis bentley. for further information on the k-d tree, in addition to [sam06], see [ben75]. for information on using a quadtree to store arbitrary polygonal objects, see [sh92].
for a discussion on the relative space requirements for two-way versus multiway branching, see “a generalized comparison of quadtree and bintree storage requirements” by shaffer, juvvadi, and heath [sjh93].
closely related to spatial data structures are data structures for storing multidimensional data (which might not necessarily be spatial in nature). a popular data structure for storing such data is the r-tree, originally proposed by guttman [gut84].
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
that is, if a decision problem f yields yes on input i, then there is a language l containing string i0 where i0 is some suitable transformation of input i. conversely, if f would give answer no for input i, then i’s transformed version i0 is not in the language l.
turing machines are a simple model of computation for writing programs that are language acceptors. there is a “universal” turing machine that can take as input a description for a turing machine, and an input string, and return the execution of that machine on that string. this turing machine in turn can be cast as a boolean expression such that the expression is satisﬁable if and only if the turing machine yields accept for that string. cook used turing machines in his proof because they are simple enough that he could develop this transformation of turing machines to boolean expressions, but rich enough to be able to compute any function that a regular computer can compute. the signiﬁcance of this transformation is that any decision problem that is performable by the turing machine is transformable to sat. thus, sat is np-hard. as explained above, to show that a decision problem x is np-complete, we prove that x is in np (normally easy, and normally done by giving a suitable polynomial-time, nondeterministic algorithm) and then prove that x is np-hard. to prove that x is np-hard, we choose a known np-complete problem, say a. we describe a polynomial-time transformation that takes an arbitrary instance i of a to an instance i0 of x. we then describe a polynomial-time transformation from s0 to s such that s is the solution for i. the following example provides a model for how an np-completeness proof is done.
example 17.1 3 sat is a special case of sat. is 3 sat easier than sat? not if we can prove it to be np-complete. theorem 17.1 3 sat is np-complete. proof: prove that 3 sat is in np: guess (nondeterministically) truth values for the variables. the correctness of the guess can be veriﬁed in polynomial time. prove that 3 sat is np-hard: we need a polynomial-time reduction from sat to 3 sat. let e = c1 · c2 · ... · ck be any instance of sat. our strategy is to replace any clause ci that does not have exactly three literals
then replace a 5c/ stamp with three 2c/ stamps. if not, then the makeup must have included at least two 2c/ stamps (because it is at least of size 4 and contains only 2c/ stamps). in this case, replace two of the 2c/ stamps with a single 5c/ stamp. in either case, we now have a value of n made up of 2c/ and 5c/ stamps. thus, by mathematical induction, the theorem is correct. 2
example 2.15 here is an example using strong induction. theorem 2.6 for n > 1, n is divisible by some prime number. proof: for the base case, choose n = 2. 2 is divisible by the prime number 2. the induction hypothesis is that any value a, 2 ≤ a < n, is divisible by some prime number. there are now two cases to consider when proving the theorem for n. if n is a prime number, then n is divisible by itself. if n is not a prime number, then n = a × b for a and b, both integers less than n but greater than 1. the induction hypothesis tells us that a is divisible by some prime number. that same prime number must also divide n. thus, by mathematical induction, the theorem is correct. 2
our next example of mathematical induction proves a theorem from geometry. it also illustrates a standard technique of induction proof where we take n objects and remove some object to use the induction hypothesis.
example 2.16 deﬁne a two-coloring for a set of regions as a way of assigning one of two colors to each region such that no two regions sharing a side have the same color. for example, a chessboard is two-colored. figure 2.3 shows a two-coloring for the plane with three lines. we will assume that the two colors to be used are black and white. theorem 2.7 the set of regions formed by n inﬁnite lines in the plane can be two-colored. proof: consider the base case of a single inﬁnite line in the plane. this line splits the plane into two regions. one region can be colored black and the other white to get a valid two-coloring. the induction hypothesis is that the set of regions formed by n − 1 inﬁnite lines can be two-colored. to prove the theorem for n, consider the set of regions formed by the n − 1 lines remaining when any one of the n lines is removed. by the induction hypothesis, this set of regions can be two-colored. now, put the nth line back. this splits the plane into two half-planes, each of which (independently) has a valid two-coloring inherited from the two-coloring of the plane with
construct a bst of n nodes by inserting the nodes one at a time. if we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average Θ(log n), for a total cost of Θ(n log n). however, if the nodes are inserted in order of increasing value, then the resulting tree will be a chain of height n. the cost of
traversing a bst costs Θ(n) regardless of the shape of the tree. each node is visited exactly once, and each child pointer is followed exactly once. below is an example traversal, named printhelp. it performs an inorder traversal on the bst to print the node values in ascending order.
while the bst is simple to implement and efﬁcient when the tree is balanced, the possibility of its being unbalanced is a serious liability. there are techniques for organizing a bst to guarantee good performance. two examples are the avl tree and the splay tree of section 13.2. other search trees are guaranteed to remain balanced, such as the 2-3 tree of section 10.4.
there are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. for example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived ﬁrst. when scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. the next job selected is the one with the highest priority. priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list).
when a collection of objects is organized by importance or priority, we call this a priority queue. a normal queue data structure will not implement a priority queue efﬁciently because search for the element with highest priority will take Θ(n) time. a list, whether sorted or not, will also require Θ(n) time for either insertion or removal. a bst that organizes records by priority could be used, with the total of n inserts and n remove operations requiring Θ(n log n) time in the average
ﬁle is created whose records consist of key/pointer pairs. here, each key is associated with a pointer to a complete record in the main database ﬁle. the index ﬁle could be sorted or organized using a tree structure, thereby imposing a logical order on the records without physically rearranging them. one database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld.
each record of a database normally has a unique identiﬁer, called the primary key. for example, the primary key for a set of personnel records might be the social security number or id number for the individual. unfortunately, the id number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. instead, the searcher might know the desired employee’s name. alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. if these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. however, key values in the name and salary indices are not likely to be unique.
a key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. most searches are performed using a secondary key. the secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. at this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. in the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index.
indexing is an important technique for organizing large databases, and many indexing methods have been developed. direct access through hashing is discussed in section 9.4. a simple list sorted by key value can also serve as an index to the record ﬁle. indexing disk ﬁles by sorted lists are discussed in the following section. unfortunately, a sorted list does not perform well for insert and delete operations. a third approach to indexing is the tree index. trees are typically used to organize large databases that must support record insertion, deletion, and key range searches. section 10.2 brieﬂy describes isam, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. its shortcomings help to illustrate the value of tree indexing techniques. section 10.3 introduces the basic issues related to tree indexing. section 10.4 introduces the 2-3 tree, a balanced tree structure that is a simple form of the b-tree covered in section 10.5. b-trees are the most widely used indexing method for large disk-based databases, and many variations have been invented. section 10.5
figure 10.7 breaking the bst into blocks. the bst is divided among disk blocks, each with space for three nodes. the path from the root to any leaf is contained on two blocks.
figure 10.8 an attempt to rebalance a bst after insertion can be expensive. (a) a bst with six nodes in the shape of a complete binary tree. (b) a node with value 1 is inserted into the bst of (a). to maintain both the complete binary tree shape and the bst property, a major reorganization of the tree is required.
in addition to these shape properties, the 2-3 tree has a search tree property analogous to that of a bst. for every node, the values of all descendants in the left subtree are less than the value of the ﬁrst key, while values in the center subtree
figure 10.13 example of inserting a record that causes the 2-3 tree root to split. (a) the value 19 is added to the 2-3 tree of figure 10.9. this causes the node containing 20 and 21 to split, promoting 20. (b) this in turn causes the internal node containing 23 and 30 to split, promoting 23. (c) finally, the root node splits, promoting 23 to become the left record in the new root. the result is that the tree becomes one level higher.
the 2-3 tree insert and delete routines do not add new nodes at the bottom of the tree. instead they cause leaf nodes to split or merge, possibly causing a ripple effect moving up the tree to the root. if necessary the root will split, causing a new root node to be created and making the tree one level deeper. on deletion, if the last two children of the root merge, then the root node is removed and the tree will lose a level. in either case, all leaf nodes are always at the same level. when all leaf nodes are at the same level, we say that a tree is height balanced. because the 2-3 tree is height balanced, and every internal node has at least two children, we know that the maximum depth of the tree is log n. thus, all 2-3 tree insert, ﬁnd, and delete operations require Θ(log n) time.
4. b-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. this improves space efﬁciency while reducing the typical number of disk fetches necessary during a search or update operation.
a b-tree of order m is deﬁned to have the following shape properties: • the root is either a leaf or has at least two children. • each internal node, except for the root, has between dm/2e and m children. • all leaves are at the same level in the tree, so the tree is always height bal-
the b-tree is a generalization of the 2-3 tree. put another way, a 2-3 tree is a b-tree of order three. normally, the size of a node in the b-tree is chosen to ﬁll a disk block. a b-tree node implementation typically allows 100 or more children. thus, a b-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk ﬁle). in a typical application, b-tree block i/o will be managed using a buffer pool and a block-replacement scheme such as lru (see section 8.3).
1. perform a binary search on the records in the current node. if a record with the search key is found, then return that record. if the current node is a leaf node and the key is not found, then report an unsuccessful search.
for example, consider a search for the record with key value 47 in the tree of figure 10.16. the root node is examined and the second (right) branch taken. after
figure 10.20 simple deletion from a b+-tree. the record with key value 18 is removed from the tree of figure 10.17. note that even though 18 is also a placeholder used to direct search in the parent node, that value need not be removed from internal nodes even if no record in the tree has key value 18. thus, the leftmost node at level one in this example retains the key with value 18 after the record with key value 18 has been removed from the second leaf node.
other b+-tree nodes are affected. if l is already full, split it in two (dividing the records evenly among the two nodes) and promote a copy of the least-valued key in the newly formed right node. as with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the b+-tree to gain a new level. b+-tree insertion keeps all leaf nodes at equal depth. figure 10.18 illustrates the insertion process through several examples. figure 10.19 shows a java-like pseudocode sketch of the b+-tree insert algorithm.
to delete record r from the b+-tree, ﬁrst locate the leaf l that contains r. if l is more than half full, then we need only remove r, leaving l still at least half full. this is demonstrated by figure 10.20.
if deleting a record reduces the number of records in the node below the minimum threshold (called an underﬂow), then we must do something to keep the node sufﬁciently full. the ﬁrst choice is to look at the node’s adjacent siblings to
10.10 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the tree that results from inserting these records when the 2-3 tree is modiﬁed to be a 2-3+ tree, that is, the internal nodes act only as placeholders. assume that the leaf nodes are capable of holding up to two records.
10.11 show the result of inserting the value 55 into the b-tree of figure 10.16. 10.12 show the result of inserting the values 1, 2, 3, 4, 5, and 6 (in that order) into
10.14 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the b+-tree of order four that results from inserting these records. assume that the leaf nodes are capable of storing up to three records.
10.15 assume that you have a b+-tree whose internal nodes can store up to 100 children and whose leaf nodes can store up to 15 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.16 assume that you have a b+-tree whose internal nodes can store up to 50 children and whose leaf nodes can store up to 50 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.1 implement a two-level linear index for variable-length records as illustrated by figures 10.1 and 10.2. assume that disk blocks are 1024 bytes in length. records in the database ﬁle should typically range between 20 and 200 bytes, including a 4-byte key value. each record of the index ﬁle should store a key value and the byte offset in the database ﬁle for the ﬁrst byte of the corresponding record. the top-level index (stored in memory) should be a simple array storing the lowest key value on the corresponding block in the index ﬁle.
10.2 implement the 2-3+ tree, that is, a 2-3 tree where the internal nodes act only as placeholders. your 2-3+ tree should implement the dictionary interface of section 4.4.
example 13.1 when searching for the value 7 (0000111 in binary) in the pat trie of figure 13.3, the root node indicates that bit position 0 (the leftmost bit) is checked ﬁrst. because the 0th bit for value 7 is 0, take the left branch. at level 1, branch depending on the value of bit 1, which again is 0. at level 2, branch depending on the value of bit 2, which again is 0. at level 3, the index stored in the node is 4. this means that bit 4 of the key is checked next. (the value of bit 3 is irrelevant, because all values stored in that subtree have the same value at bit position 3.) thus, the single branch that extends from the equivalent node in figure 13.1 is just skipped. for key value 7, bit 4 has value 1, so the rightmost branch is taken. because this leads to a leaf node, the search key is compared against the key stored in that node. if they match, then the desired record has been found.
note that during the search process, only a single bit of the search key is compared at each internal node. this is signiﬁcant, because the search key could be quite large. search in the pat trie requires only a single full-key comparison, which takes place once a leaf node has been reached.
example 13.2 consider the situation where we need to store a library of dna sequences. a dna sequence is a series of letters, usually many thousands of characters long, with the string coming from an alphabet of only four letters that stand for the four amino acids making up a dna strand. similar dna seqences might have long sections of ther string that are identical. the pat trie woudl avoid making multiple full key comparisons when searching for a speciﬁc sequence.
we have noted several times that the bst has a high risk of becoming unbalanced, resulting in excessively expensive search and update operations. one solution to this problem is to adopt another search tree structure such as the 2-3 tree. an alternative is to modify the bst access functions in some way to guarantee that the tree performs well. this is an appealing concept, and it works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. unfortunately, requiring that the bst always be in the shape of a complete binary tree requires excessive modiﬁcation to the tree during update, as discussed in section 10.3.
14.18 use theorem 14.1 to prove that binary search requires Θ(log n) time. 14.19 recall that when a hash table gets to be more than about one half full, its performance quickly degrades. one solution to this problem is to reinsert all elements of the hash table into a new hash table that is twice as large. assuming that the (expected) average case cost to insert into a hash table is Θ(1), prove that the average cost to insert is still Θ(1) when this reinsertion policy is used.
14.21 one approach to implementing an array-based list where the list size is unknown is to let the array grow and shrink. this is known as a dynamic array. when necessary, we can grow or shrink the array by copying the array’s contents to a new array. if we are careful about the size of the new array, this copy operation can be done rarely enough so as not to affect the amortized cost of the operations. (a) what is the amortized cost of inserting elements into the list if the array is initially of size 1 and we double the array size whenever the number of elements that we wish to store exceeds the size of the array? assume that the insert itself cost o(1) time per operation and so we are just concerned with minimizing the copy time to the new array.
(b) consider an underﬂow strategy that cuts the array size in half whenever the array falls below half full. give an example where this strategy leads to a bad amortized cost. again, we are only interested in measuring the time of the array copy operations.
(c) give a better underﬂow strategy than that suggested in part (b). your goal is to ﬁnd a strategy whose amortized analysis shows that array copy requires o(n) time for a series of n operations.
14.22 recall that two vertices in an undirected graph are in the same connected component if there is a path connecting them. a good algorithm to ﬁnd the connected components of an undirected graph begins by calling a dfs on the ﬁrst vertex. all vertices reached by the dfs are in the same connected component and are so marked. we then look through the vertex mark array until an unmarked vertex i is found. again calling the dfs on i, all vertices reachable from i are in a second connected component. we continue working through the mark array until all vertices have been assigned to some connected component. a sketch of the algorithm is as follows:
skip lists are designed to overcome a basic limitation of array-based and linked lists: either search or update operations require linear time. the skip list is an example of a probabilistic data structure, because it makes some of its decisions at random.
skip lists provide an alternative to the bst and related tree structures. the primary problem with the bst is that it may easily become unbalanced. the 2-3 tree of chapter 10 is guaranteed to remain balanced regardless of the order in which data values are inserted, but it is rather complicated to implement. chapter 13 presents the avl tree and the splay tree, which are also guaranteed to provide good performance, but at the cost of added complexity as compared to the bst. the skip list is easier to implement than known balanced tree structures. the skip list is not guaranteed to provide good performance (where good performance is deﬁned as Θ(log n) search, insertion, and deletion time), but it will provide good performance with extremely high probability (unlike the bst which has a good chance of performing poorly). as such it represents a good compromise between difﬁculty of implementation and performance.
figure 16.2 illustrates the concept behind the skip list. figure 16.2(a) shows a simple linked list whose nodes are ordered by key value. to search a sorted linked list requires that we move down the list one node at a time, visiting Θ(n) nodes in the average case. imagine that we add a pointer to every other node that lets us skip alternating nodes, as shown in figure 16.2(b). deﬁne nodes with only a single pointer as level 0 skip list nodes, while nodes with two pointers are level 1 skip list nodes.
to search, follow the level 1 pointers until a value greater than the search key has been found, then revert to a level 0 pointer to travel one more node if necessary. this effectively cuts the work in half. we can continue adding pointers to selected nodes in this way — give a third pointer to every fourth node, give a fourth pointer to every eighth node, and so on — until we reach the ultimate of log n pointers in the ﬁrst and middle nodes for a list of n nodes as illustrated in figure 16.2(c). to search, start with the bottom row of pointers, going as far as possible and skipping many nodes at a time. then, shift up to shorter and shorter steps as required. with this arrangement, the worst-case number of accesses is Θ(log n).
forward that stores the pointers as shown in figure 16.2(c). position forward[0] stores a level 0 pointer, forward[1] stores a level 1 pointer, and so on. the skip
example 1.2 a company is developing a database system containing information about cities and towns in the united states. there are many thousands of cities and towns, and the database program should allow users to ﬁnd information about a particular place by name (another example of an exact-match query). users should also be able to ﬁnd all places that match a particular value or range of values for attributes such as location or population size. this is known as a range query.
a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact-match query, a few seconds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire operation may be allowed to take longer, perhaps on the order of a minute. to meet this requirement, it will be necessary to support operations that process range queries efﬁciently by processing all cities in the range as a batch, rather than as a series of operations on individual cities.
the hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efﬁcient range queries. the b+-tree of section 10.5.1 supports large databases, insertion and deletion of data records, and range queries. however, a simple linear index as described in section 10.1 would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a cd-rom.
the previous section used the terms “data item” and “data structure” without properly deﬁning them. this section presents terminology and motivates the design process embodied in the three-step approach to selecting a data structure. this motivation stems from the need to manage the tremendous complexity of computer programs.
a type is a collection of values. for example, the boolean type consists of the values true and false. the integers also form a type. an integer is a simple type because its values contain no subparts. a bank account record will typically contain several pieces of information such as name, address, account number, and account balance. such a record is an example of an aggregate type or composite type. a data item is a piece of information or a record whose value is drawn from a type. a data item is said to be a member of a type.
how can this be? programmers look at programs regularly to determine if they will halt. surely this can be automated. as a warning to those who believe any program can be analyzed in this way, carefully examine the following code fragment before reading on.
this is a famous piece of code. the sequence of values that is assigned to n by this code is sometimes called the collatz sequence for input value n. does this code fragment halt for all values of n? nobody knows the answer. every input that has been tried halts. but does it always halt? note that for this code fragment, because we do not know if it halts, we also do not know an upper bound for its running time. as for the lower bound, we can easily show Ω(log n)(see exercise 3.14).
personally, i have faith that someday some smart person will completely analyze the collitz function and prove once and for all that the code fragment halts for all values of n. doing so may well give us techniques that advance our ability to do algorithm analysis in general. unfortunately, proofs from computability — the branch of computer science that studies what is impossible to do with a computer — compel us to believe that there will always be another bit of program code that we cannot analyze. this comes as a result of the fact that the halting problem is unsolvable.
before proving that the halting problem is unsolvable, we ﬁrst prove that not all functions can be implemented as a computer program. this is so because the number of programs is much smaller than the number of possible functions.
a set is said to be countable (or countably inﬁnite if it is a set with inﬁnite members) if every member of the set can be uniquely assigned to a positive integer. a set is said to be uncountable (or uncountably inﬁnite) if it is not possible to assign every member of the set to a positive integer.
to understand what is meant when we say “assigned to a positive integer,” imagine that there is an inﬁnite row of bins, labeled 1, 2, 3, and so on. take a set and start placing members of the set into bins, with at most one member per bin. if we can ﬁnd a way to assign all of the members to bins, then the set is countable. for example, consider the set of positive even integers 2, 4, and so on. we can
somebody presents a way of assigning functions to bins that they claim includes all of the functions. we can build a new function that has not been assigned to any bin, as follows. take the output value for input 1 from the function in the ﬁrst bin. call this value f1(1). add 1 to it, and assign the result as the output of a new function for input value 1. regardless of the remaining values assigned to our new function, it must be different from the ﬁrst function in the table, because the two give different outputs for input 1. now take the output value for 2 from the second function in the table (known as f2(2)). add 1 to this value and assign it as the output for 2 in our new function. thus, our new function must be different from the function of bin 2, because they will differ at least at the second value. continue in this manner, assigning fnew(i) = fi(i) + 1 for all values i. thus, the new function must be different from any function fi at least at position i. this procedure for constructing a new function not already in the table is called diagonalization. because the new function is different from every other function, it must not be in the table. this is true no matter how we try to assign functions to bins, and so the number of integer functions is uncountable. the signiﬁcance of this is that not all functions can possibly be assigned to programs, so there must be functions with no corresponding program. figure 17.6 illustrates this argument.
while there might be intellectual appeal to knowing that there exists some function that cannot be computed by a computer program, does this mean that there is any such useful function? after all, does it really matter if no program can compute a “nonsense” function such as shown in bin 4 of figure 17.5? now we will prove
example 2.4 for the integers, the relations < and ≤ both deﬁne partial orders. operation < is a total order because, for every pair of integers x and y such that x 6= y, either x < y or y < x. likewise, ≤ is a total order because, for every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.
example 2.5 for the powerset of the integers, the subset operator deﬁnes a partial order (because it is antisymmetric and transitive). for example, {1, 2} ⊆ {1, 2, 3}. however, sets {1, 2} and {1, 3} are not comparable by the subset operator, because neither is a subset of the other. therefore, the subset operator does not deﬁne a total order on the powerset of the integers.
units of measure: i use the following notation for units of measure. “b” will be used as an abbreviation for bytes, “b” for bits, “kb” for kilobytes (210 = 1024 bytes), “mb” for megabytes (220 bytes), “gb” for gigabytes (230 bytes), and “ms” for milliseconds (a millisecond is 1000 of a second). spaces are not placed between the number and the unit abbreviation when a power of two is intended. thus a disk drive of size 25 gigabytes (where a gigabyte is intended as 230 bytes) will be written as “25gb.” spaces are used when a decimal value is intended. an amount of 2000 bits would therefore be written “2 kb” while “2kb” represents 2048 bits. 2000 milliseconds is written as 2000 ms. note that in this book large amounts of storage are nearly always measured in powers of two and times in powers of ten.
factorial function: the factorial function, written n! for n an integer greater than 0, is the product of the integers between 1 and n, inclusive. thus, 5! = 1 · 2 · 3 · 4 · 5 = 120. as a special case, 0! = 1. the factorial function grows quickly as n becomes larger. because computing the factorial function directly good approximation. stirling’s approximation states that n! ≈ √ is a time-consuming process, it can be useful to have an equation that provides a e )n, where e ≈ 2.71828 (e is the base for the system of natural logarithms).3 thus we see that
typical car is driven about 12,000 miles per year. if gasoline costs $2/gallon, then the yearly gas bill is $1200 for the less efﬁcient car and $800 for the more efﬁcient car. if we ignore issues such as the payback that would be received if we invested $2000 in a bank, it would take 5 years to make up the difference in price. at this point, the buyer must decide if price is the only criterion and if a 5-year payback time is acceptable. naturally, a person who drives more will make up the difference more quickly, and changes in gasoline prices will also greatly affect the outcome.
example 2.20 when at the supermarket doing the week’s shopping, can you estimate about how much you will have to pay at the checkout? one simple way is to round the price of each item to the nearest dollar, and add this value to a mental running total as you put the item in your shopping cart. this will likely give an answer within a couple of dollars of the true total.
most of the topics covered in this chapter are considered part of discrete mathematics. an introduction to this ﬁeld is discrete mathematics with applications by susanna s. epp [epp04]. an advanced treatment of many mathematical topics useful to computer scientists is concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
see “technically speaking” from the february 1995 issue of ieee spectrum [sel95] for a discussion on the standard for indicating units of computer storage used in this book.
for more information on recursion, see thinking recursively by eric s. roberts [rob86]. to learn recursion properly, it is worth your while to learn the programming language lisp, even if you never intend to write a lisp program. in particular, friedman and felleisen’s the little lisper [ff89] is designed to teach you how to think recursively as well as teach you lisp. this book is entertaining reading as well.
a good book on writing mathematical proofs is daniel solow’s how to read and do proofs [sol90]. to improve your general mathematical problem-solving abilities, see the art and craft of problem solving by paul zeitz [zei07]. zeitz
sort routine such as the unix qsort function. interestingly, quicksort is hampered by exceedingly poor worst-case performance, thus making it inappropriate for certain applications.
before we get to quicksort, consider for a moment the practicality of using a binary search tree for sorting. you could insert all of the values to be sorted into the bst one by one, then traverse the completed tree using an inorder traversal. the output would form a sorted list. this approach has a number of drawbacks, including the extra space required by bst pointers and the amount of time required to insert nodes into the tree. however, this method introduces some interesting ideas. first, the root of the bst (i.e., the ﬁrst node inserted) splits the list into two sublits: the left subtree contains those values in the list less than the root value while the right subtree contains those values in the list greater than or equal to the root value. thus, the bst implicitly implements a “divide and conquer” approach to sorting the left and right subtrees. quicksort implements this concept in a much more efﬁcient way.
quicksort ﬁrst selects a value called the pivot. assume that the input array contains k values less than the pivot. the records are then rearranged in such a way that the k values less than the pivot are placed in the ﬁrst, or leftmost, k positions in the array, and the values greater than or equal to the pivot are placed in the last, or rightmost, n − k positions. this is called a partition of the array. the values placed in a given partition need not (and typically will not) be sorted with respect to each other. all that is required is that all values end up in the correct partition. the pivot value itself is placed in position k. quicksort then proceeds to sort the resulting subarrays now on either side of the pivot, one of size k and the other of size n − k − 1. how are these values sorted? because quicksort is such a good algorithm, using quicksort on the subarrays would be appropriate.
unlike some of the sorts that we have seen earlier in this chapter, quicksort might not seem very “natural” in that it is not an approach that a person is likely to use to sort real objects. but it should not be too suprising that a really efﬁcient sort for huge numbers of abstract objects on a computer would be rather different from our experiences with sorting a relatively few physical objects.
the java code for quicksort is as follows. parameters i and j deﬁne the left and right indices, respectively, for the subarray being sorted. the initial call to quicksort would be qsort(array, 0, n-1).
the information ﬂow at a constant rate along the spiral, the drive must speed up the rate of disk spin as the i/o head moves toward the center of the disk. this makes for a more complicated and slower mechanism.
three separate steps take place when reading a particular byte or series of bytes of data from a hard disk. first, the i/o head moves so that it is positioned over the track containing the data. this movement is called a seek. second, the sector containing the data rotates to come under the head. when in use the disk is always spinning. at the time of this writing, typical disk spin rates are 7200 rotations per minute (rpm). the time spent waiting for the desired sector to come under the i/o head is called rotational delay or rotational latency. the third step is the actual transfer (i.e., reading or writing) of data. it takes relatively little time to read information once the ﬁrst byte is positioned under the i/o head, simply the amount of time required for it all to move under the head. in fact, disk drives are designed not to read one byte of data, but rather to read an entire sector of data at each request. thus, a sector is the minimum amount of data that can be read or written at one time.
contiguous sectors are often grouped to form a cluster. a cluster is the smallest unit of allocation for a ﬁle, so all ﬁles are a multiple of the cluster size. the cluster size is determined by the operating system. the ﬁle manager keeps track of which clusters make up each ﬁle.
in microsoft windows systems, there is a designated portion of the disk called the file allocation table, which stores information about which sectors belong to which ﬁle. in contrast, unix does not use clusters. the smallest unit of ﬁle allocation and the smallest unit that can be read/written is a sector, which in unix terminology is called a block. unix maintains information about ﬁle organization in certain disk blocks called i-nodes.
a group of physically contiguous clusters from the same ﬁle is called an extent. ideally, all clusters making up a ﬁle will be contiguous on the disk (i.e., the ﬁle will consist of one extent), so as to minimize seek time required to access different portions of the ﬁle. if the disk is nearly full when a ﬁle is created, there might not be an extent available that is large enough to hold the new ﬁle. furthermore, if a ﬁle
a great discussion on external sorting methods can be found in salzberg’s book. the presentation in this chapter is similar in spirit to salzberg’s.
for details on disk drive modeling and measurement, see the article by ruemmler and wilkes, “an introduction to disk drive modeling” [rw94]. see andrew s. tanenbaum’s structured computer organization [tan06] for an introduction to computer hardware and organization. an excellent, detailed description of memory and hard disk drives can be found online at “the pc guide,” by charles m. kozierok [koz05] (www.pcguide.com). the pc guide also gives detailed descriptions of the microsoft windows and unix (linux) ﬁle systems.
see “outperforming lru with an adaptive replacement cache algorithm” by megiddo and modha for an example of a more sophisticated algorithm than lru for managing buffer pools.
8.1 computer memory and storage prices change rapidly. find out what the current prices are for the media listed in figure 8.1. does your information change any of the basic conclusions regarding disk processing?
8.2 assume a disk drive from the late 1990s is conﬁgured as follows. the total storage is approximately 675mb divided among 15 surfaces. each surface has 612 tracks; there are 144 sectors/track, 512 bytes/sector, and 8 sectors/cluster. the disk turns at 3600 rpm. the track-to-track seek time is 20 ms, and the average seek time is 80 ms. now assume that there is a 360kb ﬁle on the disk. on average, how long does it take to read all of the data in the ﬁle? assume that the ﬁrst track of the ﬁle is randomly placed on the disk, that the entire ﬁle lies on adjacent tracks, and that the ﬁle completely ﬁlls each track on which it is found. a seek must be performed each time the i/o head moves to a new track. show your calculations.
8.3 using the speciﬁcations for the disk drive given in exercise 8.2, calculate the expected time to read one entire track, one sector, and one byte. show your calculations.
examining the managed memory pool to determine which parts are still being used and which parts are garbage. in particular, a list is kept of all program variables, and any memory locations not reachable from one of these variables are considered to be garbage. when the garbage collector executes, all unused memory locations are placed in free store for future access. this approach has the advantage that it allows for easy collection of garbage. it has the disadvantage, from a user’s point of view, that every so often the system must halt while it performs garbage collection. for example, garbage collection is noticeable in the emacs text editor, which is normally implemented in lisp. occasionally the user must wait for a moment while the memory management system performs garbage collection.
the java programming language also makes use of garbage collection. as in lisp, it is common practice in java to allocate dynamic memory as needed, and to later drop all references to that memory. the garbage collector is responsible for reclaiming such unused space as necessary. this might require extra time when running the program, but it makes life considerably easier for the programmer. in contrast, many large applications written in c++ (even commonly used commercial software) contain memory leaks that will in time cause the program to fail.
several algorithms have been used for garbage collection. one is the reference count algorithm. here, every dynamically allocated memory block includes space for a count ﬁeld. whenever a pointer is directed to a memory block, the reference count is increased. whenever a pointer is directed away from a memory block, the reference count is decreased. if the count ever becomes zero, then the memory block is considered garbage and is immediately placed in free store. this approach has the advantage that it does not require an explicit garbage collection phase, because information is put in free store immediately when it becomes garbage.
the reference count algorithm is used by the unix ﬁle system. files can have multiple names, called links. the ﬁle system keeps a count of the number of links to each ﬁle. whenever a ﬁle is “deleted,” in actuality its link ﬁeld is simply reduced by one. if there is another link to the ﬁle, then no space is recovered by the ﬁle system. whenever the number of links goes to zero, the ﬁle’s space becomes available for reuse.
reference counts have several major disadvantages. first, a reference count must be maintained for each memory object. this works well when the objects are large, such as a ﬁle. however, it will not work well in a system such as lisp where the memory objects typically consist of two pointers or a value (an atom). another major problem occurs when garbage contains cycles. consider figure 12.17. here each memory object is pointed to once, but the collection of objects is still garbage because no pointer points to the collection. thus, reference counts only work when
how long will it take to process the company payroll once we complete our planned merger? should i buy a new payroll program from vendor x or vendor y? if a particular program is slow, is it badly implemented or is it solving a hard problem? questions like these ask us to consider the difﬁculty of a problem, or the relative efﬁciency of two or more approaches to solving a problem.
this chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. we focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. asymptotic analysis attempts to estimate the resource consumption of an algorithm. it allows us to compare the relative costs of two or more algorithms for solving the same problem. asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. after reading this chapter, you should understand
• the concept of a growth rate, the rate at which the cost of an algorithm grows • the concept of upper and lower bounds for a growth rate, and how to estimate • the difference between the cost of an algorithm (or program) and the cost of
the chapter concludes with a brief discussion of the practical difﬁculties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efﬁciency.
how do you compare two algorithms for solving some problem in terms of efﬁciency? one way is to implement both algorithms as computer programs and then
when we want an estimate of the running time or other resource requirements of an algorithm. this simpliﬁes the analysis and keeps us thinking about the most important aspect: the growth rate. this is called asymptotic algorithm analysis. to be precise, asymptotic analysis refers to the study of an algorithm as the input size “gets big” or reaches a limit (in the calculus sense). however, it has proved to be so useful to ignore all constant factors that asymptotic analysis is used for most algorithm comparisons.
it is not always reasonable to ignore the constants. when comparing algorithms meant to run on small values of n, the constant can have a large effect. for example, if the problem is to sort a collection of exactly ﬁve records, then an algorithm designed for sorting thousands of records is probably not appropriate, even if its asymptotic analysis indicates good performance. there are rare cases where the constants for two algorithms under comparison can differ by a factor of 1000 or more, making the one with lower growth rate impractical for most purposes due to its large constant. asymptotic analysis is a form of “back of the envelope” estimation for algorithm resource consumption. it provides a simpliﬁed model of the running time or other resource needs of an algorithm. this simpliﬁcation usually helps you understand the behavior of your algorithms. just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important.
several terms are used to describe the running-time equation for an algorithm. these terms — and their associated symbols — indicate precisely what aspect of the algorithm’s behavior is being described. one is the upper bound for the growth of the algorithm’s running time. it indicates the upper or highest growth rate that the algorithm can have.
to make any statement about the upper bound of an algorithm, we must be making it about some class of inputs of size n. we measure this upper bound nearly always on the best-case, average-case, or worst-case inputs. thus, we cannot say, “this algorithm has an upper bound to its growth rate of n2.” we must say something like, “this algorithm has an upper bound to its growth rate of n2 in the average case.”
because the phrase “has an upper bound to its growth rate of f(n)” is long and often used when discussing algorithms, we adopt a special notation, called big-oh notation. if the upper bound for an algorithm’s growth rate (for, say, the worst case) is f(n), then we would write that this algorithm is “in the set o(f(n))in the worst case” (or just “in o(f(n))in the worst case”). for example, if n2 grows as
many algorithms (or their instantiations as programs), it is easy to come up with the equation that deﬁnes their runtime behavior. most algorithms presented in this book are well understood and we can almost always give a Θ analysis for them. however, chapter 17 discusses a whole class of algorithms for which we have no Θ analysis, just some unsatisfying big-oh and Ω analyses. exercise 3.14 presents a short, simple program fragment for which nobody currently knows the true upper or lower bounds.
while some textbooks and programmers will casually say that an algorithm is “order of” or “big-oh” of some cost function, it is generally better to use Θ notation rather than big-oh notation whenever we have sufﬁcient knowledge about an algorithm to be sure that the upper and lower bounds indeed match. throughout this book, Θ notation will be used in preference to big-oh notation whenever our state of knowledge makes that possible. limitations on our ability to analyze certain algorithms may require use of big-oh or Ω notations. in rare occasions when the discussion is explicitly about the upper or lower bound of a problem or algorithm, the corresponding notation will be used in preference to Θ notation.
once you determine the running-time equation for an algorithm, it really is a simple matter to derive the big-oh, Ω, and Θ expressions from the equation. you do not need to resort to the formal deﬁnitions of asymptotic analysis. instead, you can use the following rules to determine the simplest form.
1. if f(n) is in o(g(n)) and g(n) is in o(h(n)), then f(n) is in o(h(n)). 2. if f(n) is in o(kg(n)) for any constant k > 0, then f(n) is in o(g(n)). 3. if f1(n) is in o(g1(n)) and f2(n) is in o(g2(n)), then f1(n) + f2(n) is in
the ﬁrst rule says that if some function g(n) is an upper bound for your cost function, then any upper bound for g(n) is also an upper bound for your cost function. a similar property holds true for Ω notation: if g(n) is a lower bound for your cost function, then any lower bound for g(n) is also a lower bound for your cost function. likewise for Θ notation.
the signiﬁcance of rule (2) is that you can ignore any multiplicative constants in your equations when using big-oh notation. this rule also holds true for Ω and Θ notations.
4.3 use singly linked lists to implement integers of unlimited size. each node of the list should store one digit of the integer. you should implement addition, subtraction, multiplication, and exponentiation operations. limit exponents to be positive integers. what is the asymptotic running time for each of your operations, expressed in terms of the number of digits for the two operands of each function?
4.5 implement a city database using unordered lists. each database record contains the name of the city (a string of arbitrary length) and the coordinates of the city expressed as integer x and y coordinates. your database should allow records to be inserted, deleted by name or coordinate, and searched by name or coordinate. another operation that should be supported is to print all records within a given distance of a speciﬁed point. implement the database using an array-based list implementation, and then a linked list implementation. collect running time statistics for each operation in both implementations. what are your conclusions about the relative advantages and disadvantages of the two implementations? would storing records on the list in alphabetical order by city name speed any of the operations? would keeping the list in alphabetical order slow any of the operations?
4.6 modify the code of figure 4.18 to support storing variable-length strings of at most 255 characters. the stack array should have type char. a string is represented by a series of characters (one character per stack element), with the length of the string stored in the stack element immediately above the string itself, as illustrated by figure 4.32. the push operation would store an element requiring i storage units in the i positions beginning with the current value of top and store the size in the position i storage units above top. the value of top would then be reset above the newly inserted element. the pop operation need only look at the size value stored in position top−1 and then pop off the appropriate number of units. you may store the string on the stack in reverse order if you prefer, provided that when it is popped from the stack, it is returned in its proper order.
4.7 implement a collection of freelists for variable-length strings, as described at the end of section 4.1.2. for each such freelist, you will need an access function to get it if it exists, and implement it if it does not. a major design consideration is how to organize the collection of freelists, which are distinguished by the length of the strings. essentially, what is needed is a dictionary of freelists, organized by string lengths.
figure 10.1 linear indexing for variable-length records. each record in the index ﬁle is of ﬁxed length and contains a pointer to the beginning of the corresponding record in the database ﬁle.
begins with a discussion of the variant normally referred to simply as a “b-tree.” section 10.5.1 presents the most widely implemented variant, the b+-tree.
a linear index is an index ﬁle organized as a sequence of key/pointer pairs where the keys are in sorted order and the pointers either (1) point to the position of the complete record on disk, (2) point to the position of the primary key in the primary index, or (3) are actually the value of the primary key. depending on its size, a linear index might be stored in main memory or on disk. a linear index provides a number of advantages. it provides convenient access to variable-length database records, because each entry in the index ﬁle contains a ﬁxed-length key ﬁeld and a ﬁxed-length pointer to the beginning of a (variable-length) record as shown in figure 10.1. a linear index also allows for efﬁcient search and random access to database records, becase it is amenable to binary search.
if the database contains enough records, the linear index might be too large to store in main memory. this makes binary search of the index more expensive because many disk accesses would typically be required by the search process. one solution to this problem is to store a second-level linear index in main memory that indicates which disk block in the index ﬁle stores a desired key. for example, the linear index on disk might reside in a series of 1024-byte blocks. if each key/pointer pair in the linear index requires 8 bytes, then 128 keys are stored per block. the second-level index, stored in main memory, consists of a simple table storing the value of the key in the ﬁrst position of each block in the linear index ﬁle. this arrangement is shown in figure 10.2. if the linear index requires 1024 disk blocks (1mb), the second-level index contains only 1024 entries, one per disk block. to ﬁnd which disk block contains a desired search key value, ﬁrst search through the
10.10 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the tree that results from inserting these records when the 2-3 tree is modiﬁed to be a 2-3+ tree, that is, the internal nodes act only as placeholders. assume that the leaf nodes are capable of holding up to two records.
10.11 show the result of inserting the value 55 into the b-tree of figure 10.16. 10.12 show the result of inserting the values 1, 2, 3, 4, 5, and 6 (in that order) into
10.14 you are given a series of records whose keys are letters. the records are inserted in the following order: c, s, d, t, a, m, p, i, b, w, n, g, u, r, k, e, h, o, l, j. show the b+-tree of order four that results from inserting these records. assume that the leaf nodes are capable of storing up to three records.
10.15 assume that you have a b+-tree whose internal nodes can store up to 100 children and whose leaf nodes can store up to 15 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.16 assume that you have a b+-tree whose internal nodes can store up to 50 children and whose leaf nodes can store up to 50 records. what are the minimum and maximum number of records that can be stored by the b+-tree for 1, 2, 3, 4, and 5 levels?
10.1 implement a two-level linear index for variable-length records as illustrated by figures 10.1 and 10.2. assume that disk blocks are 1024 bytes in length. records in the database ﬁle should typically range between 20 and 200 bytes, including a 4-byte key value. each record of the index ﬁle should store a key value and the byte offset in the database ﬁle for the ﬁrst byte of the corresponding record. the top-level index (stored in memory) should be a simple array storing the lowest key value on the corresponding block in the index ﬁle.
10.2 implement the 2-3+ tree, that is, a 2-3 tree where the internal nodes act only as placeholders. your 2-3+ tree should implement the dictionary interface of section 4.4.
for convenience, we will adopt a convention of allowing sublists and atoms to be labeled, such as “l1:”. whenever a label is repeated, the element corresponding to that label will be substituted when we write out the list. thus, the bracket notation for the list of figure 12.2 could be written
a cyclic list is a list structure whose graph corresponds to any directed graph, possibly containing cycles. figure 12.3 illustrates such a list. labels are required to write this in bracket notation. here is the notation for the list of figure 12.3.
multilists can be implemented in a number of ways. most of these should be familiar from implementations suggested earlier in the book for list, tree, and graph data structures.
one simple approach is to use an array representation. this works well for chains with ﬁxed-length elements, equivalent to the simple array-based list of chapter 4. we can view nested sublists as variable-length elements. to use this approach, we require some indication of the beginning and end of each sublist. in essence, we are using a sequential tree implementation as discussed in section 6.5. this should be no surprise, because the pure list is equivalent to a general tree structure. unfortunately, as with any sequential representation, access to the nth sublist must be done sequentially from the beginning of the list.
because pure lists are equivalent to trees, we can also use linked allocation methods to support direct access to the list of children. simple linear lists are
and solving for x, we ﬁnd that the sparse matrix using this implementation is more space efﬁcient when x < 1/7, that is, when less than about 14% of the elements are non-zero. different values for the relative sizes of data values, pointers, or matrix indices can lead to a different break-even point for the two implementations. the time required to process a sparse matrix depends on the number of nonzero elements stored. when searching for an element, the cost is the number of elements preceding the desired element on its row or column list. the cost for operations such as adding two matrices should be Θ(n + m) in the worst case when the one matrix stores n non-zero elements and the other stores m non-zero elements.
most of the data structure implementations described in this book store and access objects of uniform size, such as integers stored in a list or a tree. a few simple methods have been described for storing variable-size records in an array or a stack. this section discusses memory management techniques for the general problem of handling space requests of variable size.
the basic model for memory management is that we have a (large) block of contiguous memory locations, which we will call the memory pool. periodically, memory requests are issued for some amount of space in the pool. the memory manager must ﬁnd a contiguous block of locations of at least the requested size from somewhere within the memory pool. honoring such a request is called a memory allocation. the memory manager will typically return some piece of information that permits the user to recover the data that were just stored. this piece of information is called a handle. previously allocated memory might be returned to the memory manager at some future time. this is called a memory deallocation. we can deﬁne an adt for the memory manager as shown in figure 12.8.
the user of the memmanager adt provides a pointer (in parameter space) to space that holds some message to be stored or retrieved. this is similar to the basic ﬁle read/write methods presented in section 8.4. the fundamental idea is that the client gives messages to the memory manager for safe keeping. the memory manager returns a “receipt” for the message in the form of a memhandle object. the client holds the memhandle until it wishes to get the message back.
method insert lets the client tell the memory manager the length and contents of the message to be stored. this adt assumes that the memory manager will remember the length of the message associated with a given handle, thus method get does not include a length parameter but instead returns the length of the mes-
algorithm is said to be stable if it does not change the relative ordering of records with identical key values. many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes.
when comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. an example of such timings is presented in figure 7.13. however, such a comparison can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. in particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms.
when analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. this measure is usually closely related to the running time for the algorithm and has the advantage of being machine and datatype independent. however, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. if so, it might be appropriate to measure the number of swap operations performed by the algorithm. in most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. some special situations “change the rules” for comparing sorting algorithms. for example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. some applications require that a small number of records be sorted, but that the sort be performed frequently. an example would be an application that repeatedly sorts groups of ﬁve numbers. in such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. finally, some situations require that a sorting algorithm use as little memory as possible. we will note which sorting algorithms require signiﬁcant extra memory beyond the input array.
this section presents three simple sorting algorithms. while easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort. nonetheless, there are situations where one of these simple algorithms is the best tool for the job.
example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5], while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. however, bag [3, 4, 5, 4] is indistinguishable from bag [3, 4, 4, 5].
a sequence is a collection of elements with an order, and which may contain duplicate-valued elements. a sequence is also sometimes called a tuple or a vector. in a sequence, there is a 0th element, a 1st element, 2nd element, and so on. i indicate a sequence by using angle brackets hi to enclose its elements. for example, h3, 4, 5, 4i is a sequence. note that sequence h3, 5, 4, 4i is distinct from sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i. relation, if s is {a, b, c}, then
is a different relation. if tuple hx, yi is in relation r, we may use the inﬁx notation xry. we often use relations such as the less than operator (<) on the natural numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or h2, 2i. rather than writing the relationship in terms of ordered pairs, we typically use an inﬁx notation for such relations, writing 1 < 3.
• r is reﬂexive if ara for all a ∈ s. • r is symmetric if whenever arb, then bra, for all a, b ∈ s. • r is antisymmetric if whenever arb and bra, then a = b, for all a, b ∈ s. • r is transitive if whenever arb and brc, then arc, for all a, b, c ∈ s. as examples, for the natural numbers, < is antisymmetric and transitive; ≤ is reﬂexive, antisymmetric, and transitive, and = is reﬂexive, antisymmetric, and transitive. for people, the relation “is a sibling of” is symmetric and transitive. if we deﬁne a person to be a sibling of himself, then it is reﬂexive; if we deﬁne a person not to be a sibling of himself, then it is not reﬂexive.
r is an equivalence relation on set s if it is reﬂexive, symmetric, and transitive. an equivalence relation can be used to partition a set into equivalence classes. if two elements a and b are equivalent to each other, we write a ≡ b. a partition of a set s is a collection of subsets that are disjoint from each other and whose union is s. an equivalence relation on set s partitions the set into subsets whose elements are equivalent. see section 6.2 for a discussion on how to represent equivalence classes on a set. one application for disjoint sets appears in section 11.5.2.
the array is close to full. using the equation, we can solve for n to determine the break-even point beyond which the array-based implementation is more space efﬁcient in any particular situation. this occurs when
if p = e, then the break-even point is at d/2. this would happen if the element ﬁeld is either a four-byte int value or a pointer, and the next ﬁeld is a typical fourbyte pointer. that is, the array-based implementation would be more efﬁcient (if the link ﬁeld and the element ﬁeld are the same size) whenever the array is more than half full.
as a rule of thumb, linked lists are better when implementing lists whose number of elements varies widely or is unknown. array-based lists are generally more space efﬁcient when the user knows in advance approximately how large the list will become.
array-based lists are faster for random access by position. positions can easily be adjusted forwards or backwards by the next and prev methods. these operations always take Θ(1) time. in contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the speciﬁed position. both of these operations require Θ(n) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or movetopos.
given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only Θ(1) time. array-based lists must shift the remainder of the list up or down within the array. this requires Θ(n) time in the average and worst cases. for many applications, the time to insert and delete elements dominates all other operations. for this reason, linked lists are often preferred to array-based lists.
when implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. this data structure is known as a dynamic array. for example, the java vector class implements a dynamic array. dynamic arrays allow the programmer to get around the limitation on the standard array that its size cannot be changed once the array has been created. this also means that space need not be allocated to the dynamic array until it is to be used. the disadvantage of this approach is that it takes time to deal with space adjustments on the array. each time the array grows in size, its contents must be copied. a good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall
example 17.2 in this example, we make use of a simple conversion between two graph problems. theorem 17.2 vertex cover is np-complete. proof: prove that vertex cover is in np: simply guess a subset of the graph and determine in polynomial time whether that subset is in fact a vertex cover of size k or less. prove that vertex cover is np-hard: we will assume that clique is already known to be np-complete. (we will see this proof in the next example. for now, just accept that it is true.) given that clique is np-complete, we need to ﬁnd a polynomialtime transformation from the input to clique to the input to vertex cover, and another polynomial-time transformation from the output for vertex cover to the output for clique. this turns out to be a simple matter, given the following observation. consider a graph g and a vertex cover s on g. denote by s0 the set of vertices in g but not in s. there can be no edge connecting any two vertices in s0 because, if there were, then s would not be a vertex cover. denote by g0 the inverse graph for g, that is, the graph formed from the edges not in g. if s is of size k, then s0 forms a clique of size n − k in graph g0. thus, we can reduce clique to vertex cover simply by converting graph g to g0, and asking if g0 has a vertex cover of size n − k or smaller. if yes, then there is a clique in g of size k; if no then there is not.
example 17.3 so far, our np-completenss proofs have involved transformations between inputs of the same “type,” such as from a boolean expression to a boolean expression or from a graph to a graph. sometimes an np-completeness proof involves a transformation between types of inputs, as shown next. theorem 17.3 clique is np-complete.
are np-complete, but the same problem on certain restricted types of graphs is not as difﬁcult. for example, while the vertex cover and clique problems are np-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-satisfiability (where every clause in a boolean expression has at most two literals) has a polynomial time solution. several geometric problems requre only polynomial time in two dimensions, but are np-complete in three dimensions or more. knapsack is considered to run in polynomial time if the numbers (and k) are “small.” small here means that they are polynomial on n, the number of items.
in general, if we want to guarentee that we get the correct answer for an npcomplete problem, we potentially need to examine all of the (exponential number of) possible solutions. however, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. for example, dynamic programming (section 16.2) attempts to organize the processing of all the subproblems to a problem so that the work is done efﬁciently.
if we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. for example, satisfiability has 2n possible ways to assign truth values to the n variables contained in the boolean expression being satisﬁed. we can view this as a tree of solutions by considering that we have a choice of making the ﬁrst variable true or false. thus, we can put all solutions where the ﬁrst variable is true on one side of the tree, and the remaining solutions on the other. we then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisﬁable expression). at this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. if this fails, we know to back up further in the tree as necessary and follow alternate branches, until ﬁnally we either ﬁnd a solution that satisﬁes the expression or exhaust the tree. in some cases we avoid processing many potential solutions, or ﬁnd a solution quickly. in others, we end up visiting a large portion of the 2n possible solutions. banch-and-bounds is an extension of backtracking that applies to optimization problems such as traveling salesman where we are trying to ﬁnd the shortest tour through the cities. we traverse the solution tree as with backtracking. however, we remember the best value found so far. proceeding down a given branch is equivalent to deciding which order to visit cities. so any node in the solution tree represents some collection of cities visited so far. if the sum of these
distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. at this point we can immediately back up and take another branch. if we have a quick method for ﬁnding a good (but not necessarily) best solution, we can use this as an initial bound value to effectively prune portions of the tree.
a third approach is to ﬁnd an approximate solution to the problem. there are many approaches to ﬁnding approximate solutions. one way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. for example, the traveling salesman problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. this rarely gives the shortest path, but the solution might be good enough. there are many other heuristics for traveling salesman that do a better job.
some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. for example, consider this simple heuristic for the vertex cover problem: let m be a maximal (not necessarily maximum) matching in g. a matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. maximum means the matching that gives the most pairs possible for a given graph. if opt is the size of a minimum vertex cover, then |m| ≤ 2 · opt because at least one endpoint of every matched edge must be in any vertex cover.
bin packing (in its decision tree form) is known to be np-complete. one simple heuristic for solving this problem is to use a “ﬁrst ﬁt” approach. we put the ﬁrst number in the ﬁrst bin. we then put the second number in the ﬁrst bin if it ﬁts, otherwise we put it in the second bin. for each subsequent number, we simply go through the bins in the order we generated them and place the number in the ﬁrst bin that ﬁts. the number of bins used is no more than twice the sum of the numbers,
17.8 a hamiltonian cycle in graph g is a cycle that visits every vertex in the graph exactly once before returning to the start vertex. the problem hamiltonian cycle asks whether graph g does in fact contain a hamiltonian cycle. assuming that hamiltonian cycle is np-complete, prove that the decision-problem form of traveling salesman is np-complete.
17.9 assuming that vertex cover is np-complete, prove that clique is also np-complete by ﬁnding a polynomial time reduction from vertex cover to clique.
input: a graph g and an integer k. output: yes if there is a subset s of the vertices in g of size k or greater such that no edge connects any two vertices in s, and no otherwise. assuming that clique is np-complete, prove that independent set is np-complete.
input: a collection of integers. output: yes if the collection can be split into two such that the sum of the integers in each partition sums to the same amount. no otherwise.
17.21 consider a program named comp that takes two strings as input. it returns true if the strings are the same. it returns false if the strings are different. why doesn’t the argument that we used to prove that a program to solve the halting problem does not exist work to prove that comp does not exist?
17.1 implement vertex cover; that is, given graph g and integer k, answer the question of whether or not there is a vertex cover of size k or less. begin by using a brute-force algorithm that checks all possible sets of vertices of size k to ﬁnd an acceptable vertex cover, and measure the running time on a number of input graphs. then try to reduce the running time through the use of any heuristics you can think of. next, try to ﬁnd approximate solutions to the problem in the sense of ﬁnding the smallest set of vertices that forms a vertex cover.
17.3 implement an approximation of traveling salesman; that is, given a graph g with costs for all edges, ﬁnd the cheapest cycle that visits all vertices in g. try various heuristics to ﬁnd the best approximations for a wide variety of input graphs.
17.4 write a program that, given a positive integer n as input, prints out the collatz sequence for that number. what can you say about the types of integers that have long collatz sequences? what can you say about the length of the collatz sequence for various types of integers?
if p = d, the overhead drops to about one half of the total space. however, if only leaf nodes store useful information, the overhead fraction for this implementation is actually three quarters of the total space, because half of the “data” space is unused. if a full binary tree needs to store data only at the leaf nodes, a better implementation would have the internal nodes store two pointers and no data ﬁeld while the leaf nodes store only a data ﬁeld. this implementation requires 2p n + d(n + 1) units of space. if p = d, then the overhead is about 2p/(2p +d) = 2/3. it might seem counter-intuitive that the overhead ratio has gone up while the total amount of space has gone down. the reason is because we have changed our deﬁnition of “data” to refer only to what is stored in the leaf nodes, so while the overhead fraction is higher, it is from a total storage requirement that is lower.
there is one serious ﬂaw with this analysis. when using separate implementations for internal and leaf nodes, there must be a way to distinguish between the node types. when separate node types are implemented via java subclasses, the runtime environment stores information with each object allowing it to determine, for example, the correct subclass to use when the isleaf virtual function is called. thus, each node requires additional space. only one bit is truly necessary to distinguish the two possibilities. in rare applications where space is a critical resource, implementors can often ﬁnd a spare bit within the node’s value ﬁeld in which to store the node type indicator. an alternative is to use a spare bit within a node pointer to indicate node type. for example, this is often possible when the compiler requires that structures and objects start on word boundaries, leaving the last bit of a pointer value always zero. thus, this bit can be used to store the nodetype ﬂag and is reset to zero before the pointer is dereferenced. another alternative when the leaf value ﬁeld is smaller than a pointer is to replace the pointer to a leaf with that leaf’s value. when space is limited, such techniques can make the difference between success and failure. in any other situation, such “bit packing” tricks should be avoided because they are difﬁcult to debug and understand at best, and are often machine dependent at worst.2
the previous section points out that a large fraction of the space in a typical binary tree node implementation is devoted to structural overhead, not to storing data.
2in the early to mid 1980s, i worked on a geographic information system that stored spatial data in quadtrees (see section 13.3). at the time space was a critical resource, so we used a bit-packing approach where we stored the nodetype ﬂag as the last bit in the parent node’s pointer. this worked perfectly on various 32-bit workstations. unfortunately, in those days ibm pc-compatibles used 16-bit pointers. we never did ﬁgure out how to port our code to the 16-bit machine.
tion was ﬁrst accessed. typically it is more important to know how many times the information has been accessed, or how recently the information was last accessed. another approach is called “least frequently used” (lfu). lfu tracks the number of accesses to each buffer in the buffer pool. when a buffer must be reused, the buffer that has been accessed the fewest number of times is considered to contain the “least important” information, and so it is used next. lfu, while it seems intuitively reasonable, has many drawbacks. first, it is necessary to store and update access counts for each buffer. second, what was referenced many times in the past might now be irrelevant. thus, some time mechanism where counts “expire” is often desirable. this also avoids the problem of buffers that slowly build up big counts because they get used just often enough to avoid being replaced. an alternative is to maintain counts for all sectors ever read, not just the sectors currently in the buffer pool.
the third approach is called “least recently used” (lru). lru simply keeps the buffers in a list. whenever information in a buffer is accessed, this buffer is brought to the front of the list. when new information must be read, the buffer at the back of the list (the one least recently used) is taken and its “old” information is either discarded or written to disk, as appropriate. this is an easily implemented approximation to lfu and is often the method of choice for managing buffer pools unless special knowledge about information access patterns for an application suggests a special-purpose buffer management scheme.
the main purpose of a buffer pool is to minimize disk i/o. when the contents of a block are modiﬁed, we could write the updated information to disk immediately. but what if the block is changed again? if we write the block’s contents after every change, that might be a lot of disk write operations that can be avoided. it is more efﬁcient to wait until either the ﬁle is to be closed, or the buffer containing that block is ﬂushed from the buffer pool.
when a buffer’s contents are to be replaced in the buffer pool, we only want to write the contents to disk if it is necessary. that would be necessary only if the contents have changed since the block was read in originally from the ﬁle. the way to insure that the block is written when necessary, but only when necessary, is to maintain a boolean variable with the buffer (often referred to as the dirty bit) that is turned on when the buffer’s contents are modiﬁed by the client. at the time when the block is ﬂushed from the buffer pool, it is written to disk if and only if the dirty bit has been turned on.
modern operating systems support virtual memory. virtual memory is a technique that allows the programmer to pretend that there is more of the faster main memory (such as ram) than actually exists. this is done by means of a buffer pool
figure 8.5 an illustration of virtual memory. the complete collection of information resides in the slowe, secondary storage (on disk). those sectors recently accessed are held in the fast main memory (in ram). in this example, copies of sectors 1, 7, 5, 3, and 8 from secondary storage are currently stored in the main memory. if a memory access to sector 9 is received, one of the sectors currently in main memory must be replaced.
the buffer pool class itself. the ﬁrst approach is to pass “messages” between the two. this approach is illustrated by the following abstract class:
this simple class provides an interface with two member functions, insert and getbytes. the information is passed between the buffer pool user and the buffer pool through the space parameter. this is storage space, provided by the bufferpool client and at least sz bytes long, which the buffer pool can take information from (the insert function) or put information into (the getbytes function). parameter pos indicates where the information will be placed in the
if your operating system supports virtual memory, the simplest “external” sort is to read the entire ﬁle into virtual memory and run an internal sorting method such as quicksort. this approach allows the virtual memory manager to use its normal buffer pool mechanism to control disk accesses. unfortunately, this might not always be a viable option. one potential drawback is that the size of virtual memory is usually limited to something much smaller than the disk space available. thus, your input ﬁle might not ﬁt into virtual memory. limited virtual memory can be overcome by adapting an internal sorting method to make use of your own buffer pool.
a more general problem with adapting an internal sorting algorithm to external sorting is that it is not likely to be as efﬁcient as designing a new algorithm with the speciﬁc goal of minimizing disk fetches. consider the simple adaptation of quicksort to use a buffer pool. quicksort begins by processing the entire array of records, with the ﬁrst partition step moving indices inward from the two ends. this can be implemented efﬁciently using a buffer pool. however, the next step is to process each of the subarrays, followed by processing of sub-subarrays, and so on. as the subarrays get smaller, processing quickly approaches random access to the disk drive. even with maximum use of the buffer pool, quicksort still must read and write each record log n times on average. we can do much better. finally, even if the virtual memory manager can give good performance using a standard quicksort, wthi will come at the cost of using a lot of the system’s working memory, which will mean that the system cannot use this space for other work. better methods can save time while also using less memory.
our approach to external sorting is derived from the mergesort algorithm. the simplest form of external mergesort performs a series of sequential passes over the records, merging larger and larger sublists on each pass. the ﬁrst pass merges sublists of size 1 into sublists of size 2; the second pass merges the sublists of size 2 into sublists of size 4; and so on. a sorted sublist is called a run. thus, each pass is merging pairs of runs to form longer runs. each pass copies the contents of the ﬁle to another ﬁle. here is a sketch of the algorithm, as illustrated by figure 8.6.
1. split the original ﬁle into two equal-sized run ﬁles. 2. read one block from each run ﬁle into input buffers. 3. take the ﬁrst record from each input buffer, and write a run of length two to
8.16 assume that a virtual memory is managed using a buffer pool. the buffer pool contains ﬁve buffers and each buffer stores one block of data. memory accesses are by block id. assume the following series of memory accesses takes place:
for each of the following buffer pool replacement strategies, show the contents of the buffer pool at the end of the series, and indicate how many times a block was found in the buffer pool (instead of being read into memory). assume that the buffer pool is initially empty. (a) first-in, ﬁrst out. (b) least frequently used (with counts kept only for blocks currently in memory, counts for a page are lost when that page is removed, and the oldest item with the smallest count is removed when there is a tie).
(d) least recently used. (e) most recently used (replace the block that was most recently accessed). 8.17 suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are 32 records per block), and that working memory is 1mb (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by a single pass of multiway merge? explain how you got your answer.
8.18 assume that working memory size is 256kb broken into blocks of 8192 bytes (there is also additional space available for i/o buffers, program variables, etc.). what is the expected size for the largest ﬁle that can be merged using replacement selection followed by two passes of multiway merge? explain how you got your answer.
8.19 prove or disprove the following proposition: given space in memory for a heap of m records, replacement selection will completely sort a ﬁle if no record in the ﬁle is preceded by m or more keys of greater value.
8.20 imagine a database containing ten million records, with each record being 100 bytes long. provide an estimate of the time it would take (in seconds) to sort the database on a typical workstation.
8.21 assume that a company has a computer conﬁguration satisfactory for processing their monthly payroll. further assume that the bottleneck in payroll
using the parent pointer array representation. now, consider what happens when equivalence relationship (a, b) is processed. the root of the tree containing a is a, and the root of the tree containing b is b. to make them equivalent, one of these two nodes is set to be the parent of the other. in this case it is irrelevant which points to which, so we arbitrarily select the ﬁrst in alphabetical order to be the root. this is represented in the parent pointer array by setting the parent ﬁeld of b (the node in array position 1 of the array) to store a pointer to a. equivalence pairs (c, h), (g, f), and (d, e) are processed in similar fashion. when processing the equivalence pair (i, f), because i and f are both their own roots, i is set to point to f. note that this also makes g equivalent to i. the result of processing these ﬁve equivalences is shown in figure 6.7(b).
the parent pointer representation places no limit on the number of nodes that can share a parent. to make equivalence processing as efﬁcient as possible, the distance from each node to the root of its respective tree should be as small as possible. thus, we would like to keep the height of the trees small when merging two equivalence classes together. ideally, each tree would have all nodes pointing directly to the root. achieving this goal all the time would require too much additional processing to be worth the effort, so we must settle for getting as close as possible.
a low-cost approach to reducing the height is to be smart about how two trees are joined together. one simple technique, called the weighted union rule, joins the tree with fewer nodes to the tree with more nodes by making the smaller tree’s root point to the root of the bigger tree. this will limit the total depth of the tree to o(log n), because the depth of nodes only in the smaller tree will now increase by one, and the depth of the deepest node in the combined tree can only be at most one deeper than the deepest node before the trees were combined. the total number of nodes in the combined tree is therefore at least twice the number in the smaller subtree. thus, the depth of any node can be increased at most log n times when n equivalences are processed.
example 6.3 when processing equivalence pair (i, f) in figure 6.7(b), f is the root of a tree with two nodes while i is the root of a tree with only one node. thus, i is set to point to f rather than the other way around. figure 6.7(c) shows the result of processing two more equivalence pairs: (h, a) and (e, g). for the ﬁrst pair, the root for h is c while the root for a is itself. both trees contain two nodes, so it is an arbitrary decision
as to which node is set to be the root for the combined tree. in the case of equivalence pair (e, g), the root of e is d while the root of g is f. because f is the root of the larger tree, node d is set to point to f.
not all equivalences will combine two trees. if edge (f, g) is processed when the representation is in the state shown in figure 6.7(c), no change will be made because f is already the root for g.
the weighted union rule helps to minimize the depth of the tree, but we can do better than this. path compression is a method that tends to create extremely shallow trees. path compression takes place while ﬁnding the root for a given node x. call this root r. path compression resets the parent of every node on the path from x to r to point directly to r. this can be implemented by ﬁrst ﬁnding r. a second pass is then made along the path from x to r, assigning the parent ﬁeld of each node encountered to r. alternatively, a recursive algorithm can be implemented as follows. this version of find not only returns the root of the current node, but also makes all ancestors of the current node point to the root.
example 6.4 figure 6.7(d) shows the result of processing equivalence pair (h, e) on the the representation shown in figure 6.7(c) using the standard weighted union rule without path compression. figure 6.8 illustrates the path compression process for the same equivalence pair. after locating the root for node h, we can perform path compression to make h point directly to root object a. likewise, e is set to point directly to its root, f. finally, object a is set to point to root object f.
note that path compression takes place during the find operation, not during the merge operation. in figure 6.8, this means that nodes b, c, and h have node a remain as their parent, rather than changing their parent to be f. while we might prefer to have these nodes point to f, to accomplish this would require that additional information from the find operation be passed back to the union operation. this would not be practical.
path compression keeps the cost of each find operation very close to constant. to be more precise about what is meant by “very close to constant,” the cost of path compression for n find operations on n nodes (when combined with the weighted
(a) t (n) = 2t (n/2) + n2. (b) t (n) = 2t (n/2) + 5. (c) t (n) = 4t (n/2) + n. (d) t (n) = 2t (n/2) + n2. (e) t (n) = 4t (n/2) + n3. (f) t (n) = 4t (n/3) + n. (g) t (n) = 4t (n/3) + n2. (h) t (n) = 2t (n/2) + log n. (i) t (n) = 2t (n/2) + n log n.
14.1 implement the union/find algorithm of section 6.2 using both path compression and the weighted union rule. count the total number of node accesses required for various series of equivalences to determine if the actual performance of the algorithm matches the expected cost of Θ(n log∗ n).
2n n3 n2 n 24 216 212 28 28 2256 216 224 210 10 · 210 ≈ 213 220 230 21024 216 16 · 216 = 220 232 248 264k 220 20 · 220 ≈ 224 240 260 21m 230 30 · 230 ≈ 235 260 290 21g
we can get some further insight into relative growth rates for various algorithms from figure 3.2. most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm.
consider the problem of ﬁnding the factorial of n. for this problem, there is only one input of a given “size” (that is, there is only a single instance of size n for each value of n). now consider our largest-value sequential search algorithm of example 3.1, which always examines every array value. this algorithm works on many inputs of a given size n. that is, there are many possible arrays of any given size. however, no matter what array the algorithm looks at, its cost will always be the same in that it always looks at every element in the array one time.
for some algorithms, different inputs of a given size require different amounts of time. for example, consider the problem of searching an array containing n integers to ﬁnd the one with a particular value k (assume that k appears exactly once in the array). the sequential search algorithm begins at the ﬁrst position in the array and looks at each value in turn until k is found. once k is found, the algorithm stops. this is different from the largest-value sequential search algorithm of example 3.1, which always examines every array value.
there is a wide range of possible running times for the sequential search algorithm. the ﬁrst integer in the array could have value k, and so only one integer is examined. in this case the running time is short. this is the best case for this algorithm, because it is not possible for sequential search to look at less than one value. alternatively, if the last position in the array contains k, then the running time is relatively long, because the algorithm must examine n values. this is the worst case for this algorithm, because sequential search never looks at more than
in summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. if not, then we must resort to worst-case analysis.
imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to n2. unfortunately, the resulting program takes ten times too long to run. if you replace your current computer with a new one that is ten times faster, will the n2 algorithm become acceptable? if the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. but a funny thing happens to most people who get a faster computer. they don’t run the same problem faster. they run a bigger problem! say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. on your new computer you might hope to sort 100,000 records in the same time. you won’t be back from lunch any sooner, so you are better off solving a larger problem. and because the new machine is ten times faster, you would like to sort ten times as many records.
if your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size n is t(n) = cn for some constant c), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. if the algorithm’s growth rate is greater than cn, such as c1n2, then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster.
how much larger a problem can be solved in a given amount of time by a faster computer? assume that the new machine is ten times faster than the old. say that the old machine could solve a problem of size n in an hour. what is the largest problem that the new machine can solve in one hour? figure 3.3 shows how large a problem can be solved on the two machines for the ﬁve running-time functions from figure 3.1.
this table illustrates many important points. the ﬁrst two equations are both linear; only the value of the constant factor has changed. in both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. in other words, while the value of the constant does affect the absolute size of the problem that can be solved in a ﬁxed amount of time, it does not affect the improvement in
when we want an estimate of the running time or other resource requirements of an algorithm. this simpliﬁes the analysis and keeps us thinking about the most important aspect: the growth rate. this is called asymptotic algorithm analysis. to be precise, asymptotic analysis refers to the study of an algorithm as the input size “gets big” or reaches a limit (in the calculus sense). however, it has proved to be so useful to ignore all constant factors that asymptotic analysis is used for most algorithm comparisons.
it is not always reasonable to ignore the constants. when comparing algorithms meant to run on small values of n, the constant can have a large effect. for example, if the problem is to sort a collection of exactly ﬁve records, then an algorithm designed for sorting thousands of records is probably not appropriate, even if its asymptotic analysis indicates good performance. there are rare cases where the constants for two algorithms under comparison can differ by a factor of 1000 or more, making the one with lower growth rate impractical for most purposes due to its large constant. asymptotic analysis is a form of “back of the envelope” estimation for algorithm resource consumption. it provides a simpliﬁed model of the running time or other resource needs of an algorithm. this simpliﬁcation usually helps you understand the behavior of your algorithms. just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important.
several terms are used to describe the running-time equation for an algorithm. these terms — and their associated symbols — indicate precisely what aspect of the algorithm’s behavior is being described. one is the upper bound for the growth of the algorithm’s running time. it indicates the upper or highest growth rate that the algorithm can have.
to make any statement about the upper bound of an algorithm, we must be making it about some class of inputs of size n. we measure this upper bound nearly always on the best-case, average-case, or worst-case inputs. thus, we cannot say, “this algorithm has an upper bound to its growth rate of n2.” we must say something like, “this algorithm has an upper bound to its growth rate of n2 in the average case.”
because the phrase “has an upper bound to its growth rate of f(n)” is long and often used when discussing algorithms, we adopt a special notation, called big-oh notation. if the upper bound for an algorithm’s growth rate (for, say, the worst case) is f(n), then we would write that this algorithm is “in the set o(f(n))in the worst case” (or just “in o(f(n))in the worst case”). for example, if n2 grows as
for this example, the expected number of accesses is a constant. this is because the probability for accessing the ﬁrst record is high, the second is much lower but still much higher than for record three, and so on. this shows that for some probability distributions, ordering the list by frequency can yield an efﬁcient search technique.
in many search applications, real access patterns follow a rule of thumb called the 80/20 rule. the 80/20 rule says that 80% of the record accesses are to 20% of the records. the values of 80 and 20 are only estimates; every application has its own values. however, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by disk drive and cpu manufacturers for speeding access to data stored in slower memory; see the discussion on buffer pools in section 8.3). when the 80/20 rule applies, we can expect reasonable search performance from a list ordered by frequency of access.
example 9.3 the 80/20 rule is an example of a zipf distribution. naturally occurring distributions often follow a zipf distribution. examples include the observed frequency for the use of words in a natural language such as english, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). zipf distributions are related to the harmonic series deﬁned in equation 2.10. deﬁne the zipf frequency for item i in the distribution for n records as 1/(ihn) (see exercise 9.4). the expected cost for the series whose members follow this zipf distribution will be
1. natural distributions are geometric. for example, consider the populations of the 100 largest cities in the united states. if you plot these populations on a numberline, most of them will be clustered toward the low side, with a few outliers on the high side. this is an example of a zipf distribution (see section 9.2). viewed the other way, the home town for a given person is far more likely to be a particular large city than a particular small town.
1. we know nothing about the distribution of the incoming keys. in this case, we wish to select a hash function that evenly distributes the key range across the hash table, while avoiding obvious opportunities for clustering such as hash functions that are sensitive to the high- or low-order bits of the key value.
2. we know something about the distribution of the incoming keys. in this case, we should use a distribution-dependent hash function that avoids assigning clusters of related key values to the same hash table slot. for example, if hashing english words, we should not hash on the value of the ﬁrst character because this is likely to be unevenly distributed.
the value returned by this hash function depends solely on the least signiﬁcant four bits of the key. because these bits are likely to be poorly distributed (as an example, a high percentage of the keys might be even numbers, which means that the low order bit is zero), the result will also be poorly distributed. this example shows that the size of the table m can have a big effect on the performance of a hash system because this value is typically used as the modulus.
9.6 assume that the values a through h are stored in a self-organizing list, initially in ascending order. consider the three self-organizing list heuristics: count, move-to-front, and transpose. for count, assume that the record is moved ahead in the list passing over any other record that its count is now greater than. for each, show the resulting list and the total number of comparisons required resulting from the following series of accesses:
9.7 for each of the three self-organizing list heuristics (count, move-to-front, and transpose), describe a series of record accesses for which it would require the greatest number of comparisons of the three.
9.8 write an algorithm to implement the frequency count self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function freqcount that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list with a frequency count of one.
9.9 write an algorithm to implement the move-to-front self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function movetofront that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the beginning of the list.
9.10 write an algorithm to implement the transpose self-organizing list heuristic, assuming that the list is implemented using an array. in particular, write a function transpose that takes as input a value to be searched for and which adjusts the list appropriately. if the value is not already in the list, add it to the end of the list.
9.11 write functions for computing union, intersection, and set difference on arbitrarily long bit vectors used to represent set membership as described in section 9.3. assume that for each operation both vectors are of equal length. 9.12 compute the probabilities for the following situations. these probabilities can be computed analytically, or you may write a computer program to generate the probabilities by simulation. (a) out of a group of 23 students, what is the probability that 2 students
this approach to compression is similar in spirit to ziv-lempel coding, which is a class of coding algorithms commonly used in ﬁle compression utilities. zivlempel coding will replace repeated occurrences of strings with a pointer to the location in the ﬁle of the ﬁrst occurrence of the string. the codes are stored in a self-organizing list in order to speed up the time required to search for a string that has previously been seen.
determining whether a value is a member of a particular set is a special case of searching for keys in a sequence of records. thus, any of the search methods discussed in this book can be used to check for set membership. however, we can also take advantage of the restricted circumstances imposed by this problem to develop another representation.
in the case where the set elements fall within a limited key range, we can represent the set using a bit array with a bit position allocated for each potential member. those members actually in the set store a value of 1 in their corresponding bit; those members not in the set store a value of 0 in their corresponding bit. for example, consider the set of primes between 0 and 15. figure 9.1 shows the corresponding bit table. to determine if a particular value is prime, we simply check the corresponding bit. this representation scheme is called a bit vector or a bitmap. the mark array used in several of the graph algorithms of chapter 11 is an example of such a set representation.
if the set ﬁts within a single computer word, then set union, intersection, and difference can be performed by logical bitwise operations. the union of sets a and b is the bitwise or function (whose symbol is | in java). the intersection of sets a and b is the bitwise and function (whose symbol is & in java). for example, if we would like to compute the set of numbers between 0 and 15 that are both prime and odd numbers, we need only compute the expression
bentley et al., “a locally adaptive data compression scheme” [bstw86]. for more on ziv-lempel coding, see data compression: methods and theory by james a. storer [sto88]. knuth covers self-organizing lists and zipf distributions in volume 3 of the art of computer programming[knu98].
see the paper “practical minimal perfect hash functions for large databases” by fox et al. [fhcd92] for an introduction and a good algorithm for perfect hashing.
for further details on the analysis for various collision resolution policies, see knuth, volume 3 [knu98] and concrete mathematics: a foundation for computer science by graham, knuth, and patashnik [gkp94].
the model of hashing presented in this chapter has been of a ﬁxed-size hash table. a problem not addressed is what to do when the hash table gets half full and more records must be inserted. this is the domain of dynamic hashing methods. a good introduction to this topic is “dynamic hashing schemes” by r.j. enbody and h.c. du [ed88].
9.1 create a graph showing expected cost versus the probability of an unsuccessful search when performing sequential search (see section 9.1). what can you say qualitatively about the rate of increase in expected cost as the probability of unsuccessful search grows?
9.2 modify the binary search routine of section 3.5 to implement interpolation search. assume that keys are in the range 1 to 10,000, and that all key values within the range are equally likely to occur.
9.3 write an algorithm to ﬁnd the kth smallest value in an unsorted array of n numbers (k <= n). your algorithm should require Θ(n) time in the average case. hint: your algorithm shuld look similar to quicksort.
9.4 example 9.9.3 discusses a distribution where the relative frequencies of the records match the harmonic series. that is, for every occurance of the ﬁrst record, the second record will appear half as often, the third will appear one third as often, the fourth one quarter as often, and so on. the actual probability for the ith record was deﬁned to be 1/(ihn). explain why this is correct.
9.5 graph the equations t(n) = log2 n and t(n) = n/ loge n. which gives the better performance, binary search on a sorted list, or sequential search on a
